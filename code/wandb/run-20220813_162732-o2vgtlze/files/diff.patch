diff --git a/code/lstm.py b/code/lstm.py
index 723eb52..eb1a216 100644
--- a/code/lstm.py
+++ b/code/lstm.py
@@ -7,21 +7,20 @@ import pickle
 import random
 import wandb
 
-wandb.init(project="my-test-project")
 
 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
 
 
 class LSTM(nn.Module):
-    def __init__(self, in_size, hidden_size, n_layers=2):
+    def __init__(self, in_size, config):
         super().__init__()
         # Initialize the modules we need to build the network
-        self.n_layers = n_layers
-        self.hidden_size = hidden_size
+        self.n_layers = config.n_layers
+        self.hidden_size = config.hidden_size
         self.in_size = in_size
-        self.lstm = nn.LSTM(in_size, hidden_size, batch_first=True)
+        self.lstm = nn.LSTM(in_size, self.hidden_size, batch_first=True)
         self.layers = nn.Sequential(
-            nn.Linear(hidden_size, in_size)
+            nn.Linear(self.hidden_size, in_size)
         )
 
     def forward(self, x, hidden_state=None):
@@ -91,11 +90,14 @@ class MyDataset(data.Dataset):
         data_start = self.start_pos[idx]
         return data_point, data_target, data_start
 
+def train_log(loss, epoch):
+    wandb.log({"Epoch": epoch, "Train loss": loss}, step=epoch)
+    print(f"Loss after " + f" examples: {loss:.3f}")
 
-
-def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epochs=100, loss_type="L1"):
+def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epochs, config):
     # Set model to train mode
     model.train()
+    wandb.watch(model, loss_module, log="all", log_freq=10)
 
     # Training loop
     for epoch in range(num_epochs):
@@ -162,6 +164,8 @@ def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epo
             # optimizer.step()
 
         if epoch % 10 == 0:
+            train_log(loss_epoch/len(data_loader), epoch)
+
             true_loss, convert_loss = eval_model(model, test_loader, loss_module)
             model.train()
             print(epoch, round(loss_epoch.item()/len(data_loader), 10), "\t", round(true_loss, 10), '\t', round(convert_loss, 10))
@@ -197,71 +201,158 @@ def eval_model(model, data_loader, loss_module):
             total_loss += loss_module(preds, data_labels)
             total_convert_loss += loss_module(alt_preds, alt_labels)
 
+        wandb.log({"Converted test loss": total_convert_loss/len(data_loader)})
+
     return total_loss.item()/len(data_loader), total_convert_loss.item()/len(data_loader)
 
 
 
-n_frames = 20
-n_sims = 750
+# n_frames = 20
+# n_sims = 750
 
-data_type = "pos"
-n_data = 24 # xyz * 8
+# data_type = "pos"
+# n_data = 24 # xyz * 8
 
-# data_type = "eucl_motion"
-# n_data = 12
+# # data_type = "eucl_motion"
+# # n_data = 12
 
-# # data_type = "quat"
-# # n_data = 7
+# # # data_type = "quat"
+# # # n_data = 7
 
-# # data_type = "log_quat"
-# # n_data = 7
+# # # data_type = "log_quat"
+# # # n_data = 7
 
-# # data_type = "pos_diff"
+# # # data_type = "pos_diff"
+# # # n_data = 24
+
+# # data_type = "pos_diff_start"
 # # n_data = 24
 
-# data_type = "pos_diff_start"
-# n_data = 24
 
 
+# sims = {i for i in range(n_sims)}
+# train_sims = set(random.sample(sims, int(0.8 * n_sims)))
+# test_sims = sims - train_sims
+# print("DATATYPE", data_type)
+
+# batch_size = 128
+
+# model = LSTM(n_data, 96)
+# model.to(device)
+
+# data_set_train = MyDataset(sims=train_sims, n_frames=n_frames, n_data=n_data, data_type=data_type)
+# data_set_test = MyDataset(sims=test_sims, n_frames=n_frames, n_data=n_data, data_type=data_type)
+
+# train_data_loader = data.DataLoader(data_set_train, batch_size=batch_size, shuffle=True)
+# test_data_loader = data.DataLoader(data_set_test, batch_size=batch_size, shuffle=True, drop_last=False)
+
+# # # exit()
+# # lrs = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
+# # for lr in lrs:
+#     # print("Testing lr ", lr, "Datatype ", data_type)
+# num_epochs = 400
+
+# loss = "L1"
+# wandb.config = {
+#     "loss_module": loss,
+#     # "learning_rate": lr,
+#     "epochs": num_epochs,
+#     "batch_size": batch_size
+#     }
+# loss_module = nn.L1Loss()
+
+# # f = open(f"results/{data_type}/{num_epochs}_{lr}_{loss}.txt", "w")
+# # f.write(f"Data type: {data_type}, num_epochs: {num_epochs}, \t lr: {lr} \n")
+
+# optimizer = torch.optim.Adam(model.parameters())
 
+# train_model(model, optimizer, train_data_loader, test_data_loader, loss_module, num_epochs=num_epochs, loss_type=loss)
+
+# test_data_loader = data.DataLoader(data_set_test, batch_size=batch_size, shuffle=False, drop_last=False)
+# eval_model(model, test_data_loader, loss_module)
+# print("-------------------------")
+
+
+n_sims = 750
 sims = {i for i in range(n_sims)}
 train_sims = set(random.sample(sims, int(0.8 * n_sims)))
 test_sims = sims - train_sims
-print("DATATYPE", data_type)
 
-batch_size = 128
 
-model = LSTM(n_data, 96)
-model.to(device)
 
-data_set_train = MyDataset(sims=train_sims, n_frames=n_frames, n_data=n_data, data_type=data_type)
-data_set_test = MyDataset(sims=test_sims, n_frames=n_frames, n_data=n_data, data_type=data_type)
+config = dict(
+    learning_rate = 0.1,
+    epochs = 400,
+    batch_size = 128,
+    loss_type = "L1",
+    loss_reduction_type = "mean",
+    optimizer = "Adam",
+    data_type = "pos",
+    architecture = "lstm",
+    train_sims = list(train_sims),
+    test_sims = list(test_sims),
+    n_frames = 20,
+    n_sims = n_sims,
+    n_layers = 2,
+    hidden_size = 96,
+    # activation_func = ["Tanh", "Tanh", "ReLU"],
+    # dropout = [0, 0, 0],
+    # batch_norm = [True, True, True]
+    )
+
+loss_dict = {'L1': nn.L1Loss,
+                'L2': nn.MSELoss}
+
+optimizer_dict = {'Adam': torch.optim.Adam}
+
+ndata_dict = {"pos": 24,
+                "eucl_motion": 12,
+                "quat": 7,
+                "log_quat": 7,
+                "pos_diff": 24,
+                "pos_diff_start": 24,
+            }
+
+def model_pipeline(hyperparameters, ndata_dict, loss_dict, optimizer_dict):
+    # print("hyperparams", hyperparameters)
+    # tell wandb to get started
+    with wandb.init(project="thesis", config=hyperparameters):
+      # access all HPs through wandb.config, so logging matches execution!
+      config = wandb.config
+    #   print("model_pipeline", config)
+
+      # make the model, data, and optimization problem
+      model, train_loader, test_loader, criterion, optimizer = make(config, ndata_dict, loss_dict, optimizer_dict)
+      print(model)
+
+      # and use them to train the model
+    #   model, optimizer, data_loader, test_loader, loss_module, num_epochs=100, loss_type="L1"):
+      train_model(model, optimizer, train_loader, test_loader, criterion, config.epochs, config)
+
+      # and test its final performance
+      eval_model(model, test_loader, criterion)
+
+    return model
+
+def make(config, ndata_dict, loss_dict, optimizer_dict):
+    # print("make",config)
+    # Make the data
+    data_set_train = MyDataset(sims=config.train_sims, n_frames=config.n_frames, n_data=ndata_dict[config.data_type], data_type=config.data_type)
+    data_set_test = MyDataset(sims=config.test_sims, n_frames=config.n_frames, n_data=ndata_dict[config.data_type], data_type=config.data_type)
 
-train_data_loader = data.DataLoader(data_set_train, batch_size=batch_size, shuffle=True)
-test_data_loader = data.DataLoader(data_set_test, batch_size=batch_size, shuffle=True, drop_last=False)
+    train_data_loader = data.DataLoader(data_set_train, batch_size=config.batch_size, shuffle=True)
+    test_data_loader = data.DataLoader(data_set_test, batch_size=config.batch_size, shuffle=True, drop_last=False)
 
-# # exit()
-# lrs = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
-# for lr in lrs:
-    # print("Testing lr ", lr, "Datatype ", data_type)
-num_epochs = 400
 
-loss = "L1"
-wandb.config = {
-    "loss_module": loss,
-    # "learning_rate": lr,
-    "epochs": num_epochs,
-    "batch_size": batch_size
-    }
-loss_module = nn.L1Loss()
+    # Make the model
+    model = LSTM(ndata_dict[config.data_type], config).to(device)
 
-# f = open(f"results/{data_type}/{num_epochs}_{lr}_{loss}.txt", "w")
-# f.write(f"Data type: {data_type}, num_epochs: {num_epochs}, \t lr: {lr} \n")
 
-optimizer = torch.optim.Adam(model.parameters())
+    # Make the loss and optimizer
+    criterion = loss_dict[config.loss_type](reduction=config.loss_reduction_type)
+    optimizer = optimizer_dict[config.optimizer](
+        model.parameters(), lr=config.learning_rate)
 
-train_model(model, optimizer, train_data_loader, test_data_loader, loss_module, num_epochs=num_epochs, loss_type=loss)
+    return model, train_data_loader, test_data_loader, criterion, optimizer
 
-test_data_loader = data.DataLoader(data_set_test, batch_size=batch_size, shuffle=False, drop_last=False)
-eval_model(model, test_data_loader, loss_module)
-print("-------------------------")
\ No newline at end of file
+model = model_pipeline(config, ndata_dict, loss_dict, optimizer_dict)
\ No newline at end of file
diff --git a/code/results/pos/400_0.01_L1.txt b/code/results/pos/400_0.01_L1.txt
index 89e3872..ee678e0 100644
--- a/code/results/pos/400_0.01_L1.txt
+++ b/code/results/pos/400_0.01_L1.txt
@@ -324,3 +324,83 @@
 
 [390, 1.6703281403, 8.2675772274, 8.2675772274] 
 
+[0, 36.5237319049, 9.2150618609, 9.2150618609] 
+
+[10, 5.3468511245, 4.8384507123, 4.8384507123] 
+
+[20, 5.4063586067, 5.2613413194, 5.2613413194] 
+
+[30, 4.9190400067, 2.2500125661, 2.2500125661] 
+
+[40, 5.0168367274, 4.766270357, 4.766270357] 
+
+[50, 4.7632298189, 7.176248887, 7.176248887] 
+
+[60, 4.506536596, 6.2670566334, 6.2670566334] 
+
+[70, 4.2527865242, 13.3396803912, 13.3396803912] 
+
+[80, 3.8294583489, 7.7678438074, 7.7678438074] 
+
+[90, 3.566488827, 11.8808405259, 11.8808405259] 
+
+[100, 3.3312391393, 9.2335833381, 9.2335833381] 
+
+[110, 2.9158165876, 8.5312365364, 8.5312365364] 
+
+[120, 2.6973939783, 7.1518034094, 7.1518034094] 
+
+[130, 2.5546480067, 7.5539101993, 7.5539101993] 
+
+[140, 2.361920525, 9.1656054328, 9.1656054328] 
+
+[150, 2.4865296308, 10.1174989588, 10.1174989588] 
+
+[160, 2.2225646973, 2.9666833317, 2.9666833317] 
+
+[170, 2.1558635936, 6.9072045719, 6.9072045719] 
+
+[180, 2.0641820571, 7.3058251774, 7.3058251774] 
+
+[190, 2.072555542, 9.5022690717, 9.5022690717] 
+
+[200, 2.0464253145, 7.6550436581, 7.6550436581] 
+
+[210, 1.9037314022, 9.011056339, 9.011056339] 
+
+[220, 2.0456327551, 15.4685166303, 15.4685166303] 
+
+[230, 1.8686871248, 8.7038412655, 8.7038412655] 
+
+[240, 1.8289211498, 6.5400179695, 6.5400179695] 
+
+[250, 1.8090469136, 12.9318560432, 12.9318560432] 
+
+[260, 1.7593319837, 17.6739771226, 17.6739771226] 
+
+[270, 1.7998726789, 11.5392608643, 11.5392608643] 
+
+[280, 1.8028114543, 9.7677253274, 9.7677253274] 
+
+[290, 1.8306621103, 7.4363816205, 7.4363816205] 
+
+[300, 1.6836441264, 12.169128418, 12.169128418] 
+
+[310, 1.7390679752, 18.6371280446, 18.6371280446] 
+
+[320, 1.7341372546, 19.1457340016, 19.1457340016] 
+
+[330, 1.8315695594, 14.2336874569, 14.2336874569] 
+
+[340, 1.730618533, 8.9501926198, 8.9501926198] 
+
+[350, 1.71355988, 12.937612197, 12.937612197] 
+
+[360, 1.6308326721, 11.7496589212, 11.7496589212] 
+
+[370, 1.8025890799, 9.5446104162, 9.5446104162] 
+
+[380, 1.5566432055, 9.1804890352, 9.1804890352] 
+
+[390, 1.6937774209, 9.1715123794, 9.1715123794] 
+
diff --git a/code/torch_nn.py b/code/torch_nn.py
index b3e540d..564fa3f 100644
--- a/code/torch_nn.py
+++ b/code/torch_nn.py
@@ -115,7 +115,7 @@ class MyDataset(data.Dataset):
 
 
 def train_log(loss, epoch):
-    wandb.log({"epoch": epoch, "loss": loss}, step=epoch)
+    wandb.log({"Epoch": epoch, "Train loss": loss}, step=epoch)
     print(f"Loss after " + f" examples: {loss:.3f}")
 
 def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epochs, config):
@@ -170,7 +170,7 @@ def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epo
             model.train()
             print(epoch, round(loss_epoch.item()/len(data_loader), 10), "\t", round(true_loss, 10), '\t', round(convert_loss, 10))
 
-            f = open(f"results/{data_type}/{num_epochs}_{config.learning_rate}_{loss_type}.txt", "a")
+            f = open(f"results/{config.data_type}/{num_epochs}_{learning_rate}_{loss_type}.txt", "a")
             f.write(f"{[epoch, round(loss_epoch.item()/len(data_loader), 10), round(true_loss, 10), round(convert_loss, 10)]} \n")
             f.write("\n")
             f.close()
@@ -197,16 +197,16 @@ def eval_model(model, data_loader, loss_module):
             total_loss += loss_module(preds, data_labels)
             total_convert_loss += loss_module(alt_preds, alt_labels)
 
-        wandb.log({"total converted loss": total_convert_loss/len(data_loader)})
+        wandb.log({"Converted test loss": total_convert_loss/len(data_loader)})
 
     return total_loss.item()/len(data_loader), total_convert_loss.item()/len(data_loader)
 
 
-n_frames = 20
-n_sims = 750
+# n_frames = 20
+# n_sims = 750
 
-data_type = "pos"
-n_data = 24 # xyz * 8
+# data_type = "pos"
+# n_data = 24 # xyz * 8
 
 # data_type = "eucl_motion"
 # n_data = 12
@@ -291,7 +291,7 @@ test_sims = sims - train_sims
 
 
 config = dict(
-    learning_rate = 0.01,
+    learning_rate = 0.1,
     epochs = 400,
     batch_size = 128,
     loss_type = "L1",
diff --git a/code/wandb/latest-run b/code/wandb/latest-run
index 55d0c7e..ba5b286 120000
--- a/code/wandb/latest-run
+++ b/code/wandb/latest-run
@@ -1 +1 @@
-run-20220809_160450-oynhbmc5
\ No newline at end of file
+run-20220813_162732-o2vgtlze
\ No newline at end of file
