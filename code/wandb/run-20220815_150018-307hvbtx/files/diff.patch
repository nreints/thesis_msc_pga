diff --git a/code/convert.py b/code/convert.py
index 5b42665..90f8a79 100644
--- a/code/convert.py
+++ b/code/convert.py
@@ -1,6 +1,6 @@
 import torch
 import numpy as np
-from new_mujoco import own_rotVecQuat
+from new_mujoco import fast_rotVecQuat, own_rotVecQuat
 
 def eucl2pos(eucl_motion, start_pos):
     """
@@ -29,7 +29,6 @@ def eucl2pos(eucl_motion, start_pos):
         start_pos = start_pos.astype('float64')
         frames = eucl_motion.shape[1]
         # print(start_pos[0].shape)
-        
         for batch in range(out.shape[0]):
             # out[batch] =  (eucl_motion[batch,:9].reshape(3,3) @ start_pos[batch].T + np.vstack([eucl_motion[batch, 9:]]*8).T).T
 
@@ -67,16 +66,30 @@ def quat2pos(quat, start_pos):
     Output:
         Converted quaternion to current position
     """
+    batch, vert_num, dim = start_pos.shape
     out = torch.empty_like(start_pos)
-    # print(quat.shape)
-    # if not isinstance(quat, np.ndarray):
-    #     quat = quat.astype('float64')
-    # if not isinstance(start_pos, np.ndarray):
-    #     start_pos = start_pos.astype('float64')
-    for batch in range(out.shape[0]):
-        for vert in range(out.shape[1]):
-            out[batch, vert] = own_rotVecQuat(start_pos[batch, vert, :], quat[batch, :4]) + quat[batch, 4:]
+#     # print(quat.shape)
+#     # if not isinstance(quat, np.ndarray):
+#     #     quat = quat.astype('float64')
+#     # if not isinstance(start_pos, np.ndarray):
+#     #     start_pos = start_pos.astype('float64')
 
+
+    # for batch in range(out.shape[0]):
+    #     for vert in range(out.shape[1]):
+    #         out[batch, vert] = own_rotVecQuat(start_pos[batch, vert, :], quat[batch, :4]) + quat[batch, 4:]
+
+    rotated_start = fast_rotVecQuat(start_pos, quat[:,:4])
+    # 1024x3
+
+    rot_start = rotated_start.reshape((batch, vert_num, dim))
+    
+    repeated_trans = torch.repeat_interleave(quat[:, 4:], repeats=8, dim=0)
+    out = rotated_start + repeated_trans
+    print("here", out.shape)
+
+
+    return out.reshape((batch, vert_num, dim))
     return out.reshape((out.shape[0], -1))
 
 def log_quat2pos(log_quat, start_pos):
@@ -105,10 +118,9 @@ def log_quat2pos(log_quat, start_pos):
 
     return quat2pos(quat, start_pos)
 
+
 def diff_pos_start2pos(true_preds, start_pos):
     """
-
-
     Input:
         true_preds: Original predictions (difference compared to start)
             Shape [batch_size, frames, datapoints]
diff --git a/code/new_mujoco.py b/code/new_mujoco.py
index 52d690c..0b6da91 100644
--- a/code/new_mujoco.py
+++ b/code/new_mujoco.py
@@ -37,18 +37,28 @@ def rot_quaternions(q1, q2):
 #     return rot_quaternions(q_prime, part1)
 
 def fast_rotVecQuat(v, q):
+    print(v.shape, q.shape)
     # Batch of v batchx8x3
     # Batch of q batchx4
+
+    # print(v.reshape((v.shape[0]*v.shape[1], -1)))
+    v_reshaped = v.reshape((v.shape[0]*v.shape[1], -1))
+
     q_new = torch.empty_like(q)
+    # q_new = q
     q_new[:, 0:3] = q[:, 1:4]
     q_new[:, 3] = q[:, 0]
-    v_new = torch.hstack((v, torch.zeros(v.shape[0],1)))
 
+    q_new = torch.repeat_interleave(q_new, repeats=8, dim=0)
+    # print(torch.zeros(v_reshaped.shape[0],1).shape)
+    v_new = torch.hstack((v_reshaped, torch.zeros(v_reshaped.shape[0],1)))
+    print(v_new.shape)
+    print(q_new.shape)
     mult = roma.quat_product(v_new, q_new)
     q_conj = roma.quat_conjugation(q_new)
     mult2 = roma.quat_product(q_conj, mult)
 
-    return mult2
+    return mult2[:, :3]
 
 
 def own_rotVecQuat(v, q):
@@ -68,10 +78,18 @@ def rotVecQuat(v, q):
     return res
 
 # testing rotVecQuat vs own_rotVecQuat
-v_big = torch.tensor([[1, 0, 0], [1, 0, 0]])
-q_big = torch.tensor([[0.3,  0.87, 0.0, 0.707], [0.3,  0.87, 0.0, 0.707]])
-v = torch.tensor([1,0,0])
+v_big = torch.tensor([[[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]],
+                        [[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]])
+
+q_big = torch.tensor([[0.3,  0.87, 0.0, 0.707], 
+                        [0.3,  0.87, 0.0, 0.707]])
+
+# q_big = torch.repeat_interleave(q_big, repeats=8, dim=0)
+
+
+v = torch.tensor([1, 0, 0])
 q = torch.tensor([0.3,  0.87, 0.0, 0.707])
+
 print("fast",fast_rotVecQuat(v_big, q_big))
 print("own",own_rotVecQuat(v, q))
 # print("ori", rotVecQuat(v.astype(np.float64), q.astype(np.float64)))
diff --git a/code/torch_nn.py b/code/torch_nn.py
index 34f4033..dd3d61b 100644
--- a/code/torch_nn.py
+++ b/code/torch_nn.py
@@ -8,7 +8,6 @@ import random
 from convert import *
 import wandb
 
-# wandb.init(project="thesis_linearNN")
 
 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
 
@@ -121,11 +120,11 @@ def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epo
 
             ## Step 3: Calculate the loss
 
-            # alt_preds = convert(preds, start_pos, data_loader.dataset.data_type)
-            # alt_labels = convert(data_labels, start_pos, data_loader.dataset.data_type)
-            # loss = loss_module(alt_preds, alt_labels)
+            alt_preds = convert(preds, start_pos, data_loader.dataset.data_type)
+            alt_labels = convert(data_labels, start_pos, data_loader.dataset.data_type)
+            loss = loss_module(alt_preds, alt_labels)
 
-            loss = loss_module(preds, data_labels)
+            # loss = loss_module(preds, data_labels)
             loss_epoch += loss
 
             ## Step 4: Perform backpropagation
@@ -147,7 +146,7 @@ def train_model(model, optimizer, data_loader, test_loader, loss_module, num_epo
             model.train()
             print(epoch, round(loss_epoch.item()/len(data_loader), 10), "\t", round(true_loss, 10), '\t', round(convert_loss, 10))
 
-            f = open(f"results/{config.data_type}/{num_epochs}_{learning_rate}_{loss_type}.txt", "a")
+            f = open(f"results/{config.data_type}/{num_epochs}_{config.learning_rate}_{loss_type}.txt", "a")
             f.write(f"{[epoch, round(loss_epoch.item()/len(data_loader), 10), round(true_loss, 10), round(convert_loss, 10)]} \n")
             f.write("\n")
             f.close()
@@ -233,7 +232,7 @@ config = dict(
     loss_type = "L1",
     loss_reduction_type = "mean",
     optimizer = "Adam",
-    data_type = "pos",
+    data_type = "quat",
     architecture = "fcnn",
     train_sims = list(train_sims),
     test_sims = list(test_sims),
diff --git a/code/wandb/latest-run b/code/wandb/latest-run
index ecfa6a2..8cb26d2 120000
--- a/code/wandb/latest-run
+++ b/code/wandb/latest-run
@@ -1 +1 @@
-run-20220815_115546-2m1jqldd
\ No newline at end of file
+run-20220815_150018-307hvbtx
\ No newline at end of file
