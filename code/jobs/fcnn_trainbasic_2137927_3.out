wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121535-my0avhyf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-lantern-1179
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/my0avhyf
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▃▃▂▃▂▂▂▂▁▂▁▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▂▃▃▂▁▂▂▂▂▁▁▂▂▃▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▃▃▂▃▂▂▂▂▁▂▁▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▃▂▅▃▂▁▂▂▃▂▁▂▄▄▇▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.06936
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22696
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.88595
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10044
wandb:                         Train loss 1.33164
wandb: 
wandb: 🚀 View run flashing-lantern-1179 at: https://wandb.ai/nreints/thesis/runs/my0avhyf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121535-my0avhyf/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122808-cqb2f13p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-fireworks-1185
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/cqb2f13p
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2401009052991867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7368497848510742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.647427558898926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.430839538574219
0 4.3442019164 	 12.4308395798 	 12.4920740076
epoch_time;  36.194302797317505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16104505956172943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49515876173973083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.372981071472168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.539271831512451
1 1.7293822664 	 6.5392716691 	 6.5409430427
epoch_time;  34.76519465446472
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1300411820411682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41151267290115356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.83287239074707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.104019641876221
2 1.5658758185 	 5.1040194125 	 5.1048514701
epoch_time;  34.8528778553009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13688059151172638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3658781349658966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8467631340026855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.228931427001953
3 1.5005704328 	 5.2289313239 	 5.229679252
epoch_time;  34.614927768707275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12563923001289368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31380975246429443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.204374313354492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.501032829284668
4 1.4584436953 	 5.5010326489 	 5.5016430004
epoch_time;  34.85175275802612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18553254008293152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3874277174472809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.173278331756592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.480065822601318
5 1.4313941702 	 5.480065588 	 5.4804925042
epoch_time;  34.50498008728027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13813140988349915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34499868750572205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.515669345855713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.807775497436523
6 1.4087692602 	 4.807775549 	 4.8084871859
epoch_time;  34.37541890144348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11092687398195267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2723044753074646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.967525005340576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.246532917022705
7 1.400121029 	 5.2465328732 	 5.2471904033
epoch_time;  34.58755326271057
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09450667351484299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21816237270832062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.369780540466309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.562351226806641
8 1.3871633934 	 4.5623512062 	 4.5630585542
epoch_time;  34.48438334465027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1122647374868393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26595014333724976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.655956268310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.884827136993408
9 1.3828843207 	 4.8848269901 	 4.8852697424
epoch_time;  34.18185567855835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12506258487701416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.276598185300827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.213072776794434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.402204990386963
10 1.3672576677 	 4.4022051837 	 4.4025562183
epoch_time;  34.2397198677063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13861578702926636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32430267333984375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.804215669631958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.085976600646973
11 1.3655032272 	 4.0859767605 	 4.0861615155
epoch_time;  34.74170804023743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1099148616194725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2568318247795105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0349209308624268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2526426315307617
12 1.3532423098 	 3.2526426573 	 3.2532576277
epoch_time;  34.54589128494263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1010017991065979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2514498233795166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.958547592163086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.171520709991455
13 1.3549450956 	 4.1715206662 	 4.172109573
epoch_time;  34.95721793174744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10772581398487091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.251369833946228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1822619438171387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4358272552490234
14 1.3475879832 	 3.4358273068 	 3.4362502639
epoch_time;  34.84334945678711
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16002069413661957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32160842418670654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6670920848846436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9465038776397705
15 1.3444070474 	 3.9465038403 	 3.9468423379
epoch_time;  34.58940887451172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1603214144706726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3241550624370575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.116236448287964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3247127532958984
16 1.3386319728 	 3.3247126399 	 3.3249663482
epoch_time;  34.23773193359375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21077106893062592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3628467321395874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4117493629455566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7130112648010254
17 1.3381522042 	 3.7130113756 	 3.7132472683
epoch_time;  34.41693925857544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11218702793121338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24790453910827637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4401285648345947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.664422035217285
18 1.3344050506 	 3.6644221125 	 3.6649235906
epoch_time;  34.258026123046875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10041721165180206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2270219326019287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8865458965301514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0705697536468506
19 1.3316356992 	 3.0705698374 	 3.0708908493
epoch_time;  34.40230631828308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10044315457344055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2269635945558548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8859496116638184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0693607330322266
It took 752.2839317321777 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▄▄▃▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▇▆▂▅▄▂▃▅▃▂▁▃▂▃▃▃▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▃▄▃▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▇█▅▂▅▆▁▅▇▃▁▂▃▁▆▂▄▁▄▄
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.01827
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29652
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.68158
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13452
wandb:                         Train loss 1.327
wandb: 
wandb: 🚀 View run brilliant-fireworks-1185 at: https://wandb.ai/nreints/thesis/runs/cqb2f13p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122808-cqb2f13p/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124029-anurj41l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-chrysanthemum-1191
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/anurj41l
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16732126474380493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4653190076351166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.45176887512207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.789042472839355
0 3.1636479403 	 9.7890427048 	 9.7907899599
epoch_time;  34.08028841018677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16153135895729065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43957582116127014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.147299766540527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.490819931030273
1 1.5815049509 	 8.4908196527 	 8.4927239495
epoch_time;  34.03135919570923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17432120442390442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4160275459289551
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.724149703979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.073424339294434
2 1.4875572582 	 7.0734243032 	 7.0755067568
epoch_time;  33.92942452430725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14720496535301208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40281975269317627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.535286903381348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.936399459838867
3 1.4391549329 	 5.9363993877 	 5.9385498047
epoch_time;  35.44503831863403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10975994169712067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24915573000907898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.755560398101807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.999538898468018
4 1.4149565803 	 5.9995387722 	 6.0016370619
epoch_time;  35.388219356536865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14581002295017242
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3529621958732605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8713765144348145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.17329740524292
5 1.4003028534 	 5.173297614 	 5.1758122625
epoch_time;  34.4745078086853
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15843777358531952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3462155759334564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.62260103225708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.939838409423828
6 1.3825691032 	 4.9398384713 	 4.9420080236
epoch_time;  34.42900729179382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10193386673927307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26585641503334045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.177814483642578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.483787536621094
7 1.3777519954 	 4.4837874129 	 4.4856451911
epoch_time;  34.274038553237915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14541102945804596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30882981419563293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0316057205200195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.343490123748779
8 1.3656831508 	 4.3434903531 	 4.3453326251
epoch_time;  34.0311553478241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16027548909187317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.359332412481308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0435686111450195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.391146659851074
9 1.3613525887 	 4.3911466031 	 4.3928674646
epoch_time;  34.05381226539612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12682244181632996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2910245358943939
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.848945140838623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.100015640258789
10 1.3563863022 	 4.1000158361 	 4.1015776763
epoch_time;  33.98972535133362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10391099005937576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2487177848815918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.577974557876587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8249683380126953
11 1.3473355778 	 3.8249683277 	 3.8265087231
epoch_time;  34.30425500869751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11121553182601929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22890031337738037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.447775363922119
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6396708488464355
12 1.3445330848 	 3.639670872 	 3.6412567963
epoch_time;  34.06431245803833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11928757280111313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29127296805381775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3978607654571533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7258377075195312
13 1.3392367043 	 3.7258376663 	 3.7276350692
epoch_time;  34.30632019042969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10513642430305481
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.262147456407547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6091434955596924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8308475017547607
14 1.3357128642 	 3.8308474979 	 3.8323294975
epoch_time;  33.91072988510132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1498485803604126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3087739944458008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8397700786590576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.0723090171813965
15 1.3334327858 	 4.0723091744 	 4.0734259528
epoch_time;  34.142770528793335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11688544601202011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2959366738796234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.876119375228882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3272175788879395
16 1.3279633415 	 4.3272173907 	 4.3286977407
epoch_time;  34.33013916015625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13118614256381989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28227493166923523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8204123973846436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.089229583740234
17 1.3281497116 	 4.0892297693 	 4.0904092325
epoch_time;  34.47683095932007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10520832240581512
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2463042438030243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4962716102600098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6754684448242188
18 1.3243810696 	 3.6754684861 	 3.6766255147
epoch_time;  34.396161794662476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13448083400726318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29672971367836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.680449962615967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.019204616546631
19 1.3269966967 	 4.0192046294 	 4.0204454577
epoch_time;  34.18342900276184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13451750576496124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2965191602706909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.681584119796753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.018272876739502
It took 741.2196383476257 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.121 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.121 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▃▂▁▂▂▁▂▁▂▂▂▁▂▂▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▆▃▂▁▂▄▁▁▁▃▄▃▂▂▂▁▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.39326
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26148
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.10502
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11812
wandb:                         Train loss 1.33176
wandb: 
wandb: 🚀 View run luminous-chrysanthemum-1191 at: https://wandb.ai/nreints/thesis/runs/anurj41l
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124029-anurj41l/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125251-d2m3cu3n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-festival-1198
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/d2m3cu3n
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22898192703723907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7202509641647339
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.911748886108398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.857023239135742
0 4.1030438732 	 13.8570233319 	 13.891800834
epoch_time;  34.662522315979004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.222031831741333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5712615847587585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.326069831848145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.896265983581543
1 1.675240405 	 8.8962659681 	 8.8986466691
epoch_time;  34.41625094413757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18739023804664612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4915893077850342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.111461639404297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.605752944946289
2 1.5439408495 	 6.6057531408 	 6.6076429212
epoch_time;  34.39674353599548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14489419758319855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35339003801345825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.482081413269043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.852205276489258
3 1.4824461734 	 5.8522051837 	 5.853762405
epoch_time;  34.33568501472473
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1196804940700531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3038714528083801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.965258598327637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.247717380523682
4 1.4421799956 	 5.2477176151 	 5.2496638118
epoch_time;  34.59801506996155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0985470563173294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2310294508934021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.387696743011475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.576420307159424
5 1.4229616489 	 4.5764203046 	 4.5779748865
epoch_time;  34.29501819610596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10938647389411926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2903815507888794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.119425296783447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.386707305908203
6 1.4062347929 	 4.3867072028 	 4.387767235
epoch_time;  34.27398419380188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1554287075996399
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3296285569667816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7320332527160645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.9701337814331055
7 1.3885779875 	 4.9701340134 	 4.9711607237
epoch_time;  34.11569261550903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10481810569763184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2589089572429657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.57305383682251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.832846164703369
8 1.3821435514 	 4.8328461518 	 4.8339051943
epoch_time;  35.19347882270813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10535888373851776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26852425932884216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.127623558044434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.369309902191162
9 1.3658696861 	 4.3693098738 	 4.370205606
epoch_time;  35.02870965003967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10488678514957428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2573870122432709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.318600654602051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.617314338684082
10 1.3612138235 	 4.6173141892 	 4.6179096944
epoch_time;  34.47540211677551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13691005110740662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3154221475124359
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.042854309082031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.294285297393799
11 1.3540570486 	 4.2942851299 	 4.2950017156
epoch_time;  34.32275438308716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.148676335811615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3348378837108612
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7252817153930664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.046525478363037
12 1.3535990989 	 4.046525615 	 4.047125739
epoch_time;  34.18206238746643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14333248138427734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3125172257423401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.686657428741455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.026302814483643
13 1.3487964545 	 4.0263028531 	 4.0267785974
epoch_time;  34.53715252876282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11082801967859268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2559985816478729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2854700088500977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5723352432250977
14 1.3388010695 	 3.5723352381 	 3.5727614944
epoch_time;  34.54091215133667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.117035873234272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28130412101745605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3305768966674805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5572125911712646
15 1.34019682 	 3.5572127059 	 3.5576953785
epoch_time;  34.36589956283569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10993525385856628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2714788615703583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2978904247283936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.611337423324585
16 1.3356511265 	 3.6113373628 	 3.6118606155
epoch_time;  34.720982789993286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10041376203298569
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24485713243484497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3096323013305664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5640645027160645
17 1.3358824568 	 3.5640644795 	 3.5647949219
epoch_time;  34.65601205825806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12607210874557495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3227497339248657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.17276930809021
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4924702644348145
18 1.3326646602 	 3.4924702412 	 3.4929832665
epoch_time;  34.31230020523071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.118137888610363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26149457693099976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.105856418609619
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3923606872558594
19 1.3317554554 	 3.3923607079 	 3.3929911845
epoch_time;  34.41637420654297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11811627447605133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2614758312702179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.105018138885498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3932573795318604
It took 741.6348943710327 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▆▃▃▁▂▂▁▂▂▁▂▂▃▁▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▅▄▃▃▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▄▃█▂▃▁▁▃▂▃▂▂▂▃▅▁▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.75853
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27177
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.45691
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11189
wandb:                         Train loss 1.33986
wandb: 
wandb: 🚀 View run enchanting-festival-1198 at: https://wandb.ai/nreints/thesis/runs/d2m3cu3n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125251-d2m3cu3n/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_130510-08pix4r5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-envelope-1205
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/08pix4r5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17713186144828796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5895735621452332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.127165794372559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.400979042053223
0 3.0545347675 	 12.4009792019 	 12.404186022
epoch_time;  34.63100075721741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1679861694574356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5126745104789734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.27325439453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.52388858795166
1 1.5855064108 	 10.5238881704 	 10.5256928315
epoch_time;  34.67513465881348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14087322354316711
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3781391382217407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.677767276763916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.636788368225098
2 1.4972164332 	 8.6367880332 	 8.6388658678
epoch_time;  34.364157915115356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23017889261245728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49088191986083984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.9583587646484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.848804950714111
3 1.454175284 	 6.8488050306 	 6.8506050728
epoch_time;  34.47380328178406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12078240513801575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34249353408813477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.2498931884765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.862796783447266
4 1.4287039951 	 5.8627969278 	 5.8647256387
epoch_time;  34.46589708328247
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1496010422706604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3768055737018585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.164330005645752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.829440116882324
5 1.4143669733 	 5.8294400602 	 5.8312532992
epoch_time;  34.3396213054657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11326252669095993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2845367193222046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.524073123931885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.058272838592529
6 1.3994401791 	 5.058273068 	 5.0594766153
epoch_time;  34.35701513290405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1138436421751976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2984049320220947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8398385047912598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.213104248046875
7 1.391368166 	 4.2131040831 	 4.2141647751
epoch_time;  34.38177967071533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14454485476016998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3241618275642395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9014782905578613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.453050136566162
8 1.3821425596 	 4.4530501082 	 4.4540540541
epoch_time;  34.12791728973389
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12402597069740295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28899821639060974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9838109016418457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4035990238189697
9 1.3717114572 	 3.4035990947 	 3.4044007997
epoch_time;  34.22988939285278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13937385380268097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3128894567489624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.905052661895752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.244269847869873
10 1.3662677163 	 3.2442699535 	 3.2450145825
epoch_time;  34.155460834503174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13125230371952057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3167872726917267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8962514400482178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1735029220581055
11 1.3606073564 	 3.1735028241 	 3.1741761904
epoch_time;  34.31999611854553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12533609569072723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2837616503238678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.777432441711426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.088397979736328
12 1.3560081074 	 3.0883980416 	 3.0890446843
epoch_time;  35.720638036727905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12664392590522766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3205648362636566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.777876138687134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1190743446350098
13 1.3469178775 	 3.1190743111 	 3.119724583
epoch_time;  35.20984196662903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14637139439582825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32265034317970276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.950934410095215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2333264350891113
14 1.3443927494 	 3.233326515 	 3.233659404
epoch_time;  34.61939191818237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17928975820541382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3729785680770874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.697009325027466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1615350246429443
15 1.341847042 	 3.1615349847 	 3.1618889886
epoch_time;  34.205641746520996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11157896369695663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27321964502334595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.619309902191162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9104104042053223
16 1.341353554 	 2.9104102882 	 2.9110424145
epoch_time;  34.26899170875549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1298789381980896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28799915313720703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2722582817077637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.594588041305542
17 1.3401807037 	 2.5945879962 	 2.5953270165
epoch_time;  34.25062417984009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1215725988149643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3051062226295471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4855027198791504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.815744400024414
18 1.332068524 	 2.815744431 	 2.8161139411
epoch_time;  34.64918494224548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11187948286533356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27170330286026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4580376148223877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7592263221740723
19 1.339859453 	 2.7592262062 	 2.760045067
epoch_time;  34.58519911766052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11189261823892593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2717680037021637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.456912040710449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.758525848388672
It took 739.8814806938171 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▂▂▁▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▄▂▂▂▂▂▃▁▂▁▂▁▁▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▅▃▂▂▂▅▃▁▃▁▂▁▁▂▂▂▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.13716
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28361
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.85628
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13095
wandb:                         Train loss 1.3355
wandb: 
wandb: 🚀 View run enchanting-envelope-1205 at: https://wandb.ai/nreints/thesis/runs/08pix4r5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_130510-08pix4r5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131736-ozgmpfgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-monkey-1212
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ozgmpfgw
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21236945688724518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.685042679309845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.55417537689209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.67918586730957
0 4.2555793654 	 13.679186022 	 13.7732487859
epoch_time;  34.650707960128784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17184898257255554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.543893575668335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.487919807434082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.555517196655273
1 1.7109791543 	 8.5555175781 	 8.560370038
epoch_time;  34.80501437187195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1600160002708435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44428038597106934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.860884666442871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.118901252746582
2 1.5696380966 	 6.1189011033 	 6.120690588
epoch_time;  34.7384238243103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16327470541000366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4285488724708557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.931997299194336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.336675643920898
3 1.5043998207 	 5.3366758604 	 5.338725322
epoch_time;  34.76723051071167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12824051082134247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3343173861503601
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.433195114135742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.68623685836792
4 1.4709075948 	 4.6862367372 	 4.6876920133
epoch_time;  34.657965898513794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11751115322113037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3125338852405548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.576903820037842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.715856075286865
5 1.4420875957 	 4.7158562738 	 4.7176401499
epoch_time;  34.5511531829834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11721070110797882
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3229607939720154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.824008464813232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.000452518463135
6 1.4244010373 	 5.0004526499 	 5.0017208615
epoch_time;  34.753884077072144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11277791857719421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28557199239730835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.983439922332764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.032252788543701
7 1.4090795188 	 5.0322529561 	 5.033581543
epoch_time;  34.49474263191223
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.166891410946846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3316570818424225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.609465599060059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.736991882324219
8 1.3971281988 	 4.7369919236 	 4.7377540382
epoch_time;  34.86151647567749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.131124347448349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35229140520095825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.753222942352295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8696064949035645
9 1.3889327104 	 4.8696064717 	 4.8704896669
epoch_time;  34.86646866798401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10205249488353729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25901877880096436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.937156677246094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.950517177581787
10 1.3806374665 	 4.9505173142 	 4.951759792
epoch_time;  34.78646469116211
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.129709392786026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2858749330043793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.443338394165039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5931396484375
11 1.3707572095 	 4.5931396484 	 4.5940383499
epoch_time;  34.6764817237854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10298901796340942
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2705956697463989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.392337799072266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.6232733726501465
12 1.3652878479 	 4.6232735299 	 4.6246347788
epoch_time;  34.858696937561035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12097933143377304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2914975881576538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5277180671691895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.723622798919678
13 1.3582233075 	 4.7236229149 	 4.7246400575
epoch_time;  35.33830976486206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09846708178520203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2397414743900299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.322843074798584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.566308498382568
14 1.3565469689 	 4.5663085937 	 4.5671568175
epoch_time;  34.91396236419678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10079415142536163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24548137187957764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5084381103515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.765710353851318
15 1.3501062516 	 4.7657101193 	 4.766644782
epoch_time;  34.615981578826904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10671280324459076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2756883502006531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.194391250610352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.457753658294678
16 1.3463819913 	 4.4577537743 	 4.4584403373
epoch_time;  34.46617937088013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11925376951694489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2963739037513733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.085605144500732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.332489967346191
17 1.343319324 	 4.3324901684 	 4.3333271748
epoch_time;  35.276612520217896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11180119216442108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28707975149154663
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1209611892700195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.368439197540283
18 1.3378235153 	 4.3684392156 	 4.36940786
epoch_time;  35.46196126937866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1310248225927353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28363558650016785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8569912910461426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1388959884643555
19 1.3354994684 	 4.1388962204 	 4.1395121806
epoch_time;  34.85785698890686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13094887137413025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.283612996339798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8562793731689453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.13716459274292
It took 745.4008615016937 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▄▂▂▂▃▂▂▁▂▃▂▂▁▃▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▆▅▆▂▂▃▄▃▂▂▂▄▃▂▁▄▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.49821
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20685
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.2491
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.08912
wandb:                         Train loss 1.31789
wandb: 
wandb: 🚀 View run thriving-monkey-1212 at: https://wandb.ai/nreints/thesis/runs/ozgmpfgw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131736-ozgmpfgw/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132956-ptnmubhq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-dragon-1219
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ptnmubhq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23554272949695587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6609896421432495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.16057300567627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.300585746765137
0 3.031976306 	 13.3005859375 	 13.3028861381
epoch_time;  34.63003134727478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1482502520084381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4752766788005829
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.087681770324707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.869983673095703
1 1.5821783625 	 10.8699832401 	 10.8718024177
epoch_time;  34.54228973388672
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1888611614704132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.431443452835083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.487187385559082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.13368034362793
2 1.4876602369 	 9.133680189 	 9.1354802312
epoch_time;  34.689661741256714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1734006553888321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3897041380405426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.973116874694824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.70547866821289
3 1.4399147938 	 8.7054786476 	 8.7073235589
epoch_time;  34.63691449165344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19751577079296112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39453816413879395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.501158714294434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.1719331741333
4 1.4138317483 	 8.1719330659 	 8.173754223
epoch_time;  34.36179494857788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11485093832015991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2848707139492035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.60779333114624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.241357326507568
5 1.3905628491 	 7.2413574219 	 7.2432340055
epoch_time;  34.25013566017151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11492393910884857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29488369822502136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.1300225257873535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.743375301361084
6 1.3805390334 	 6.7433752111 	 6.7455381651
epoch_time;  34.06579327583313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12850016355514526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2911846339702606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.867042541503906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.3871541023254395
7 1.3730446435 	 6.3871542441 	 6.3894518053
epoch_time;  34.42316269874573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14379550516605377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3259967565536499
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.551564693450928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.192783355712891
8 1.3648969254 	 6.1927833351 	 6.1943187817
epoch_time;  34.64081430435181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13010916113853455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2898404598236084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.075021266937256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.571847438812256
9 1.3594437627 	 5.5718472867 	 5.5744213207
epoch_time;  34.76994299888611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11329420655965805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25619232654571533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.111594200134277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.542807102203369
10 1.3499455023 	 5.5428070893 	 5.5449126372
epoch_time;  34.589598178863525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10622632503509521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23773817718029022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.099781036376953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.532356262207031
11 1.3464808377 	 5.532356221 	 5.5339012352
epoch_time;  34.68297219276428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10685938596725464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2506360709667206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.043960094451904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.440871238708496
12 1.341734972 	 5.4408711201 	 5.4425223686
epoch_time;  34.703927516937256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15333472192287445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3408409655094147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.922583103179932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.428536415100098
13 1.3384013864 	 5.4285364099 	 5.4301170555
epoch_time;  34.27847981452942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12316403537988663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28123143315315247
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.235576629638672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.67368745803833
14 1.3365159496 	 5.6736875792 	 5.6750052787
epoch_time;  34.349719762802124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10230418294668198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2510157525539398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.875425815582275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.268261909484863
15 1.3342902533 	 5.2682617187 	 5.2696480416
epoch_time;  34.48167824745178
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08597619831562042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20687676966190338
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.36832332611084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.678925037384033
16 1.3269813243 	 4.6789250554 	 4.6803760425
epoch_time;  34.59061622619629
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16015203297138214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.321797251701355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.88448429107666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.317725658416748
17 1.3255525975 	 5.3177255991 	 5.3187803526
epoch_time;  34.807594776153564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09670048952102661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23601356148719788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.237513065338135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.623811721801758
18 1.3207634246 	 4.623811629 	 4.6250963366
epoch_time;  34.5509238243103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08912169188261032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2068963497877121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.248845100402832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.495757579803467
19 1.3178879538 	 4.4957575618 	 4.4968439875
epoch_time;  34.357659101486206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08912365138530731
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20685291290283203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.24909782409668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.498208045959473
It took 739.7838969230652 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▃▃▄▃▂▃▂▂▂▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▃▆▆▁▃▃▁▁▂▃▁▂▄▂▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▅▇▄▃▇█▁▄▄▁▂▃▄▁▁▆▂▆▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.10857
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29092
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.76627
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13144
wandb:                         Train loss 1.3326
wandb: 
wandb: 🚀 View run chromatic-dragon-1219 at: https://wandb.ai/nreints/thesis/runs/ptnmubhq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132956-ptnmubhq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134219-rr21bvnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-fish-1226
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rr21bvnp
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15834318101406097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5460705757141113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.763618469238281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.490357398986816
0 3.0885990791 	 10.4903571052 	 10.4933085674
epoch_time;  35.225714445114136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16372886300086975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45720362663269043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.696030616760254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.266691207885742
1 1.5871858074 	 8.2666913007 	 8.2681079761
epoch_time;  35.452805042266846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19603359699249268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41201725602149963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.164399147033691
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.752353668212891
2 1.4952352081 	 6.7523536476 	 6.7538561022
epoch_time;  34.44023084640503
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14390237629413605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3892735242843628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.729955196380615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.3277788162231445
3 1.453879902 	 6.3277785842 	 6.3298399229
epoch_time;  34.44713616371155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12685437500476837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3337388038635254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.390112400054932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.84970760345459
4 1.4252886711 	 5.8497076911 	 5.8518350137
epoch_time;  34.50187039375305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2000608742237091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4434209167957306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.544341087341309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.305029392242432
5 1.4107420553 	 6.3050292969 	 6.3063714105
epoch_time;  34.22748017311096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2219500094652176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47444185614585876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.845748424530029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.444558143615723
6 1.3946415017 	 6.4445583034 	 6.4458555479
epoch_time;  34.094297885894775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10579295456409454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24566885828971863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.062061309814453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.479639053344727
7 1.3855074687 	 5.4796390018 	 5.4807340055
epoch_time;  34.480005979537964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1606486737728119
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3497670292854309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.900688171386719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.36362886428833
8 1.3762791349 	 5.3636286555 	 5.3646408493
epoch_time;  34.417816400527954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1445312798023224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33615410327911377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9253668785095215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.436253547668457
9 1.3667478981 	 5.4362535631 	 5.4371773385
epoch_time;  34.41228151321411
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09936182200908661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25852957367897034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.541736602783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.894113540649414
10 1.3656680263 	 4.8941135716 	 4.8950112833
epoch_time;  34.29302167892456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.110310398042202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25310778617858887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4027509689331055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.789112567901611
11 1.3563459702 	 4.7891126478 	 4.7901829075
epoch_time;  34.61941170692444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13051865994930267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27425912022590637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.870486259460449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.308534145355225
12 1.352196902 	 5.3085343644 	 5.3092192779
epoch_time;  34.36647820472717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.156768798828125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32311883568763733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.500484943389893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.020068645477295
13 1.3466267105 	 5.0200686893 	 5.0205378352
epoch_time;  34.06746053695679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09946785867214203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25801554322242737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9102954864501953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3160834312438965
14 1.3466115033 	 4.3160832586 	 4.3169130068
epoch_time;  34.390013694763184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10590053349733353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26808324456214905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6793737411499023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.05702018737793
15 1.3433421347 	 4.0570200327 	 4.0579349002
epoch_time;  34.41157841682434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19292627274990082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3599354922771454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.096117973327637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.471586227416992
16 1.342524586 	 4.4715863202 	 4.472060085
epoch_time;  34.55075144767761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1116693988442421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27807146310806274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.729808807373047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.061738014221191
17 1.3386532958 	 4.0617382153 	 4.0626092034
epoch_time;  34.20711064338684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18435920774936676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33036452531814575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9611988067626953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3444905281066895
18 1.3343314117 	 4.3444903399 	 4.3450363572
epoch_time;  34.25412631034851
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13151387870311737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29083889722824097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7659292221069336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.111484050750732
19 1.3326013522 	 4.1114838471 	 4.1121994431
epoch_time;  34.31859874725342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13143786787986755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29091787338256836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.766266345977783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.108565330505371
It took 743.2029633522034 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▃▃▁▂▃▂▂▄▃▂▂▂▁▂▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▆▄▅▅▂▃▄▂▄▅▄▂▂▃▁▃▅▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.94206
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28968
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.54548
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11688
wandb:                         Train loss 1.3394
wandb: 
wandb: 🚀 View run golden-fish-1226 at: https://wandb.ai/nreints/thesis/runs/rr21bvnp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134219-rr21bvnp/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135439-vebont40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-snake-1232
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/vebont40
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19528800249099731
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.626243531703949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.666951179504395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.00786304473877
0 3.6163458182 	 13.0078626478 	 13.0161502323
epoch_time;  34.6249213218689
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20244546234607697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5415400862693787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.765227317810059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.387125968933105
1 1.6324625706 	 10.387125871 	 10.3886877111
epoch_time;  34.054890871047974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17324264347553253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4643919765949249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.973956108093262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.421903610229492
2 1.5235950261 	 7.4219033731 	 7.4240287162
epoch_time;  34.49559235572815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13516156375408173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3886480927467346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.7972941398620605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.383424758911133
3 1.469006703 	 6.3834248311 	 6.3854432802
epoch_time;  34.64051795005798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15079626441001892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3804004490375519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.615107536315918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.1913275718688965
4 1.4383853197 	 6.1913277291 	 6.1931765995
epoch_time;  34.433669090270996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15994606912136078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3540956974029541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.289280414581299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.823764324188232
5 1.4212475404 	 5.8237641206 	 5.8253286001
epoch_time;  35.2303626537323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1060892790555954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27092093229293823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.846001148223877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.459619045257568
6 1.4062466941 	 5.4596191406 	 5.4608913112
epoch_time;  34.620410680770874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11731158941984177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31832456588745117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.927652359008789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.5513129234313965
7 1.3991173581 	 5.5513130807 	 5.5525423617
epoch_time;  34.27604269981384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13053730130195618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.360584020614624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.838538885116577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.275105953216553
8 1.3886695609 	 4.2751059042 	 4.2764704524
epoch_time;  34.064467906951904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11082586646080017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31006190180778503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.330796241760254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.773551940917969
9 1.3792703948 	 4.7735519822 	 4.7746595228
epoch_time;  34.090864181518555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1366989016532898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31611931324005127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5250983238220215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.970152854919434
10 1.3725110715 	 4.9701528188 	 4.9709518845
epoch_time;  34.090333223342896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1575261801481247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3937230706214905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.317988395690918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.7191009521484375
11 1.3726250423 	 4.7191010346 	 4.7197189743
epoch_time;  34.027867555618286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1358497142791748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3290714621543884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.749784231185913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1030802726745605
12 1.3621332766 	 4.1030801309 	 4.1039788323
epoch_time;  34.525383710861206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10964249074459076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2880256772041321
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5318377017974854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7994842529296875
13 1.3650019703 	 3.7994843354 	 3.8001679292
epoch_time;  33.95284867286682
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.107454814016819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2794429361820221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.742311954498291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.043880462646484
14 1.3554806512 	 4.0438806482 	 4.044506506
epoch_time;  34.390300035476685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11696600914001465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.278759241104126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.368211269378662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.670490264892578
15 1.3530088307 	 3.6704903268 	 3.6711396088
epoch_time;  34.206218957901
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08927030116319656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24734631180763245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3392205238342285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.583827257156372
16 1.3483801592 	 3.5838273332 	 3.5847200961
epoch_time;  34.733317375183105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11369608342647552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29960697889328003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.318887948989868
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6567788124084473
17 1.3443693601 	 3.6567788614 	 3.6575848554
epoch_time;  34.40337157249451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15725629031658173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3622457981109619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.470111846923828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.701990842819214
18 1.34487227 	 3.7019907359 	 3.7025872308
epoch_time;  33.991117000579834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11689381301403046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2895217537879944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.544109344482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9422054290771484
19 1.3393998504 	 3.9422053157 	 3.9430169183
epoch_time;  34.305819034576416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11687947064638138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2896795868873596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5454843044281006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9420602321624756
It took 739.9005284309387 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▅▃▃▂▂▁▂▃▂▂▂▃▃▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▄▆█▃▄▃▂▁▂▅▂▃▂▃▅▂▁▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.29733
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23403
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.14317
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.103
wandb:                         Train loss 1.33715
wandb: 
wandb: 🚀 View run red-snake-1232 at: https://wandb.ai/nreints/thesis/runs/vebont40
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135439-vebont40/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140658-ovu7k7q3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-orchid-1239
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ovu7k7q3
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21788670122623444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6396766304969788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.029524803161621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.27710247039795
0 4.1678742013 	 12.2771022487 	 12.3285261824
epoch_time;  34.226049184799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15155626833438873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5109623670578003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.418686866760254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.438411235809326
1 1.7089020581 	 6.4384112384 	 6.4412538271
epoch_time;  34.26294302940369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19648131728172302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5025777816772461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.757639408111572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.00929594039917
2 1.5684322701 	 6.0092958193 	 6.010756704
epoch_time;  34.182432651519775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23022514581680298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46988019347190857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.466468811035156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.630171775817871
3 1.5040409654 	 5.6301718222 	 5.6312368032
epoch_time;  34.09413409233093
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14047814905643463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.360259473323822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.999586582183838
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1407151222229
4 1.466801338 	 5.1407150681 	 5.1417071368
epoch_time;  34.11698269844055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1576198935508728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35949793457984924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.067030906677246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.211578369140625
5 1.4395384172 	 5.2115785341 	 5.2124171901
epoch_time;  33.89362573623657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13082215189933777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3189754784107208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.023098945617676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.170852184295654
6 1.4225378327 	 5.1708522487 	 5.1720709723
epoch_time;  34.394912004470825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11097120493650436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28133484721183777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.728387355804443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.837796688079834
7 1.403784626 	 4.8377965979 	 4.8390054239
epoch_time;  34.53651762008667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09941644221544266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2406640499830246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.549229145050049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.609330654144287
8 1.391587843 	 4.6093307908 	 4.6099256361
epoch_time;  34.43539619445801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11871074140071869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29203736782073975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.886105537414551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.067876815795898
9 1.3819376765 	 5.0678767024 	 5.0687793629
epoch_time;  35.27299356460571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1738707274198532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36908990144729614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.469237327575684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.700506210327148
10 1.3715650059 	 4.7005060969 	 4.7010240709
epoch_time;  35.53603410720825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11708181351423264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28590548038482666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.147830009460449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.302757740020752
11 1.3669615418 	 4.3027577993 	 4.3036657385
epoch_time;  34.578019857406616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12977612018585205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3071172833442688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1831536293029785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.371237754821777
12 1.363025952 	 4.3712379249 	 4.3716849662
epoch_time;  34.02485752105713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10927575826644897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2686620056629181
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.265710353851318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.535948753356934
13 1.3547652547 	 4.5359487173 	 4.5370341533
epoch_time;  34.18407082557678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1360359787940979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3214775621891022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.270219326019287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.417315483093262
14 1.3508766087 	 4.4173155089 	 4.4179984428
epoch_time;  34.777342796325684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17361702024936676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3387610614299774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.239367485046387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.331947326660156
15 1.3493701106 	 4.3319471205 	 4.332294526
epoch_time;  34.326385498046875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11822604387998581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29470008611679077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9979264736175537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.131173133850098
16 1.3441556338 	 4.1311731287 	 4.1316079629
epoch_time;  34.62502384185791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10409805923700333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2651456296443939
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8703253269195557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.99884033203125
17 1.3409778386 	 3.998840332 	 3.9992213894
epoch_time;  33.97367715835571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13573545217514038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2915676534175873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9099574089050293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.067868232727051
18 1.3385035386 	 4.0678681245 	 4.0681116053
epoch_time;  34.172892570495605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10301835089921951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23398035764694214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.142853260040283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.29683256149292
19 1.3371456407 	 4.2968327703 	 4.2973048855
epoch_time;  34.02765083312988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10300278663635254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2340296357870102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.143174171447754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.29733419418335
It took 738.805545091629 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▃▃▄▃▁▂▄▃▁▁▂▁▁▂▂▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▇▆▄▃▅▅▁▂▅▃▁▁▃▁▂▂▂▁██
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.19383
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34978
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.005
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20097
wandb:                         Train loss 1.33739
wandb: 
wandb: 🚀 View run scintillating-orchid-1239 at: https://wandb.ai/nreints/thesis/runs/ovu7k7q3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140658-ovu7k7q3/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18919801712036133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6050059795379639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.29237174987793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.670010566711426
0 3.7665975954 	 9.6700102935 	 9.6786693676
epoch_time;  34.423470973968506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18485496938228607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5708066821098328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.263640880584717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.887024402618408
1 1.6423571257 	 7.8870242557 	 7.888260795
epoch_time;  34.14054775238037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16679826378822327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4419333338737488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.619899272918701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.060359477996826
2 1.5274784654 	 6.0603594806 	 6.0614059861
epoch_time;  34.20950651168823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14221705496311188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3421909809112549
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0104851722717285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.441644668579102
3 1.4756009793 	 5.441644782 	 5.4429129936
epoch_time;  34.58261561393738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12902827560901642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33518338203430176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.908809661865234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2043633460998535
4 1.4433359031 	 5.2043635188 	 5.2057594093
epoch_time;  34.51663088798523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16281355917453766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38052478432655334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.846688747406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.202369689941406
5 1.4222839397 	 5.2023694837 	 5.2033790382
epoch_time;  34.389065742492676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15958623588085175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33714422583580017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.866570472717285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1067423820495605
6 1.4093173782 	 5.1067425702 	 5.1078692462
epoch_time;  34.56902360916138
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10517878085374832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2649458944797516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.576521396636963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.829774856567383
7 1.3932146758 	 4.8297749287 	 4.8308385901
epoch_time;  34.34372925758362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11545953154563904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27802518010139465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.651284694671631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.863427639007568
8 1.3878338297 	 4.8634277344 	 4.8643330342
epoch_time;  34.508893966674805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.165143683552742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38569462299346924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.640602111816406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.818392276763916
9 1.3767186541 	 4.818392367 	 4.8191808752
epoch_time;  34.14253878593445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13000257313251495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3476813733577728
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.634891510009766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.863903045654297
10 1.3695920455 	 4.8639028188 	 4.8646332612
epoch_time;  34.11362409591675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10466005653142929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25921955704689026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4996538162231445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.786365032196045
11 1.3651175941 	 4.786365076 	 4.7869025153
epoch_time;  34.333507776260376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10562802851200104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2665807604789734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.178009033203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.381432056427002
12 1.3604337141 	 4.3814321157 	 4.381813503
epoch_time;  34.131906270980835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13360443711280823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2818220555782318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9079949855804443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.089869976043701
13 1.3580897492 	 4.0898701436 	 4.0902980495
epoch_time;  34.21196722984314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10367181897163391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2449846863746643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7388665676116943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.002651691436768
14 1.351387618 	 4.0026518951 	 4.0031411661
epoch_time;  34.916229486465454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11286100745201111
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2579942047595978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.900651693344116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.054018974304199
15 1.3483531335 	 4.0540187526 	 4.0543618692
epoch_time;  35.04221725463867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11522488296031952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27732396125793457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.048855304718018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.296658515930176
16 1.3470442455 	 4.2966585726 	 4.2969815641
epoch_time;  34.42400598526001
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1146412342786789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2763855457305908
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.191701412200928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.420222282409668
17 1.3459104049 	 4.420222102 	 4.4208604967
epoch_time;  34.1955406665802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10383761674165726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.248041033744812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7689950466156006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.012928009033203
18 1.3445610196 	 4.0129282359 	 4.0134739231
epoch_time;  34.05598711967468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20096561312675476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34977343678474426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.006776332855225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.192152976989746
19 1.3373930046 	 4.1921531883 	 4.1922254408
epoch_time;  34.17996120452881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20096753537654877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34978359937667847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0049967765808105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1938252449035645
It took 741.1673259735107 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137933
Array Job ID: 2137927_3
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-13:26:06 core-walltime
Job Wall-clock time: 02:04:47
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
