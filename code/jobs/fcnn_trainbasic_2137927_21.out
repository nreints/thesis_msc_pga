wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162837-vwx2prjl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-tiger-1321
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/vwx2prjl
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▄▄▃▃▃▂▃▂▂▃▂▂▂▁▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▄▄▃▃▂▂▃▂▂▂▂▂▂▁▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▂▄▃▃▂▃▁▂▂▁▁▂▂▂▁▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▂▃▃▃▂▃▂▂▄▂▁▂▂▂▁▂▃▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.04258
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.60292
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25977
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10281
wandb:                         Train loss 2.00545
wandb: 
wandb: 🚀 View run vibrant-tiger-1321 at: https://wandb.ai/nreints/thesis/runs/vwx2prjl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162837-vwx2prjl/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_164025-7r0i6twa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run abundant-rat-1328
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7r0i6twa
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29961368441581726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5144739151000977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5648052096366882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.287693977355957
0 5.0521925382 	 4.2876939928 	 4.2876939928
epoch_time;  32.8114755153656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21910594403743744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.816592216491699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4468333125114441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.55301833152771
1 2.3622344372 	 3.5530184359 	 3.5530184359
epoch_time;  31.713463306427002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11861637979745865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.280949831008911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3091977834701538
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.962191581726074
2 2.2775680552 	 2.962191525 	 2.962191525
epoch_time;  31.42596960067749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16642890870571136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.352323532104492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3777155578136444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9844770431518555
3 2.2258280946 	 2.9844769452 	 2.9844769452
epoch_time;  31.895123720169067
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17069974541664124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.173076629638672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3281022608280182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.715733289718628
4 2.1928922515 	 2.7157332137 	 2.7157332137
epoch_time;  31.501895666122437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15752680599689484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9184751510620117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33368179202079773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4361114501953125
5 2.1597524345 	 2.4361115327 	 2.4361115327
epoch_time;  31.512982845306396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1320633441209793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9984691143035889
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28871962428092957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.606022596359253
6 2.1359669886 	 2.6060226853 	 2.6060226853
epoch_time;  31.821025371551514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16284668445587158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8259449005126953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33562198281288147
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.275719404220581
7 2.1162124252 	 2.27571939 	 2.27571939
epoch_time;  31.74798011779785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12435039132833481
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7049633264541626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2658553421497345
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.186859369277954
8 2.0965231532 	 2.1868594608 	 2.1868594608
epoch_time;  31.622878551483154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1296597123146057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8533122539520264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3084080219268799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.342137098312378
9 2.082904999 	 2.3421370223 	 2.3421370223
epoch_time;  31.69277310371399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17482565343379974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8012887239456177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3128456771373749
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.207287073135376
10 2.0729284577 	 2.2072871028 	 2.2072871028
epoch_time;  31.358789205551147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1278848648071289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.784316897392273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2803148031234741
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.251227378845215
11 2.0579943906 	 2.2512274665 	 2.2512274665
epoch_time;  31.694881200790405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10838750004768372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8203730583190918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2658929228782654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2816643714904785
12 2.0519920354 	 2.2816643792 	 2.2816643792
epoch_time;  31.488268852233887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14488057792186737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.658969521522522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30059629678726196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0688138008117676
13 2.040496103 	 2.0688138395 	 2.0688138395
epoch_time;  31.820194482803345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13595935702323914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6247957944869995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3101719915866852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.003234624862671
14 2.0326298822 	 2.0032345334 	 2.0032345334
epoch_time;  31.505656003952026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11729437112808228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.535722017288208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2881701588630676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9160187244415283
15 2.0260258535 	 1.9160187592 	 1.9160187592
epoch_time;  31.5954692363739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10288874804973602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4402761459350586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2604527175426483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8308517932891846
16 2.0221721672 	 1.8308517868 	 1.8308517868
epoch_time;  31.446940898895264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13662853837013245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5278565883636475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3064342737197876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.931135654449463
17 2.0138860972 	 1.9311356828 	 1.9311356828
epoch_time;  31.559358596801758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14648476243019104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3674885034561157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.291216105222702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.721604824066162
18 2.0093456106 	 1.7216047957 	 1.7216047957
epoch_time;  31.77734661102295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10280625522136688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6027342081069946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2598211169242859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0424787998199463
19 2.0054469641 	 2.0424788192 	 2.0424788192
epoch_time;  31.81701970100403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10280654579401016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6029161214828491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2597738206386566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0425803661346436
It took 708.4815442562103 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▅▂▂▃▂▁▂▂▂▂▂▂▁▁▁▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▁▂▄▂▁▂▂▃▁▁▁▂▂▂▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.79039
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.44979
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.31905
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15081
wandb:                         Train loss 2.01569
wandb: 
wandb: 🚀 View run abundant-rat-1328 at: https://wandb.ai/nreints/thesis/runs/7r0i6twa
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_164025-7r0i6twa/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165159-fz9b67w5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-ox-1333
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/fz9b67w5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24635373055934906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8044443130493164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4855342209339142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.37493896484375
0 5.2062665956 	 4.3749389648 	 4.3749389648
epoch_time;  31.346802711486816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18181884288787842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7282283306121826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40042391419410706
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4195547103881836
1 2.3660649821 	 3.4195546743 	 3.4195546743
epoch_time;  31.66946291923523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.165170356631279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.510647773742676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33918607234954834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1420726776123047
2 2.2891818429 	 3.1420726879 	 3.1420726879
epoch_time;  31.536168098449707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17234660685062408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2702536582946777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40176260471343994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8643670082092285
3 2.2407287722 	 2.8643670159 	 2.8643670159
epoch_time;  33.64637470245361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11967596411705017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.257248878479004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2928526699542999
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8583924770355225
4 2.2046437299 	 2.8583924989 	 2.8583924989
epoch_time;  32.509427309036255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13678228855133057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9995530843734741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31915149092674255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5433707237243652
5 2.1755746313 	 2.5433707572 	 2.5433707572
epoch_time;  31.51550602912903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16875684261322021
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8632302284240723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3351132273674011
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.347850799560547
6 2.1472270452 	 2.3478507377 	 2.3478507377
epoch_time;  31.396270036697388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13490796089172363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.893126130104065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2952248454093933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3645482063293457
7 2.1280614276 	 2.3645481419 	 2.3645481419
epoch_time;  31.663378715515137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11695478111505508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.876787781715393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2841557264328003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2971417903900146
8 2.1062730752 	 2.2971419051 	 2.2971419051
epoch_time;  31.928373098373413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13209962844848633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.722395658493042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29371196031570435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1172568798065186
9 2.0930721255 	 2.1172567832 	 2.1172567832
epoch_time;  31.56260108947754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12412667274475098
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6800721883773804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.295223593711853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0948493480682373
10 2.0785681557 	 2.0948494576 	 2.0948494576
epoch_time;  31.778575658798218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1517094522714615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.588426947593689
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3121437430381775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9863802194595337
11 2.0678215053 	 1.9863802523 	 1.9863802523
epoch_time;  31.568073511123657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11128439754247665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5793993473052979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30240321159362793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0143070220947266
12 2.0557619171 	 2.0143069705 	 2.0143069705
epoch_time;  31.98944401741028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11964407563209534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6735073328018188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2977416515350342
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0256800651550293
13 2.0488805235 	 2.0256801296 	 2.0256801296
epoch_time;  31.70707941055298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11970910429954529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5795379877090454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29290714859962463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.977942705154419
14 2.0379236179 	 1.9779427193 	 1.9779427193
epoch_time;  31.580392122268677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1221121996641159
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4051930904388428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2753417491912842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7766127586364746
15 2.0318062175 	 1.7766128128 	 1.7766128128
epoch_time;  31.311349868774414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13701561093330383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4344427585601807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27789199352264404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7734962701797485
16 2.025303184 	 1.7734962257 	 1.7734962257
epoch_time;  31.40418767929077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12328676879405975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4412256479263306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28212806582450867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8041980266571045
17 2.0216304957 	 1.804198064 	 1.804198064
epoch_time;  31.48707103729248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1451893448829651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3979188203811646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3215574622154236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.763957142829895
18 2.0147505898 	 1.7639570906 	 1.7639570906
epoch_time;  31.45909881591797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15082110464572906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4495402574539185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31906402111053467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.791053056716919
19 2.0156868203 	 1.7910530709 	 1.7910530709
epoch_time;  31.66633653640747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15080751478672028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4497894048690796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31904906034469604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7903928756713867
It took 694.2562901973724 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▃▃▃▃▂▂▂▁▂▁▂▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▃▂▂▂▂▁▁▁▂▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▅▆▂▃▂▃▄▂▁▂▁▂▂▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▃▅█▂▃▃▃▅▂▂▃▂▃▁▁▂▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.70565
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.35537
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.2665
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.09756
wandb:                         Train loss 2.00854
wandb: 
wandb: 🚀 View run crimson-ox-1333 at: https://wandb.ai/nreints/thesis/runs/fz9b67w5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165159-fz9b67w5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_170329-0qrprwsu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-rat-1339
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/0qrprwsu
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22028405964374542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.787590742111206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48357102274894714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.399227142333984
0 5.1500233396 	 4.399226998 	 4.399226998
epoch_time;  31.632712602615356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2032632827758789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.749260425567627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4067390263080597
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3244872093200684
1 2.3613149014 	 3.3244873047 	 3.3244873047
epoch_time;  31.380692720413208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14048123359680176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.301678419113159
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3487304747104645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9155757427215576
2 2.2803871604 	 2.915575842 	 2.915575842
epoch_time;  31.75272011756897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16060800850391388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2788240909576416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3724000155925751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.862626075744629
3 2.2314787021 	 2.8626260293 	 2.8626260293
epoch_time;  31.581756830215454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22454789280891418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8837928771972656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40800413489341736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.395277738571167
4 2.1931373839 	 2.3952776935 	 2.3952776935
epoch_time;  31.54260516166687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11917340755462646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8957983255386353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30084967613220215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.454108238220215
5 2.1605203948 	 2.4541083259 	 2.4541083259
epoch_time;  31.345515966415405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12877756357192993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9070910215377808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3365132808685303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4469387531280518
6 2.1370296698 	 2.4469388395 	 2.4469388395
epoch_time;  31.361624479293823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.132425457239151
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.854326844215393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.295736700296402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2987465858459473
7 2.114208224 	 2.2987466348 	 2.2987466348
epoch_time;  31.625548839569092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1268860250711441
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.604577660560608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32506102323532104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.065593719482422
8 2.0964755483 	 2.0655938226 	 2.0655938226
epoch_time;  31.297733545303345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16928783059120178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6352912187576294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3398531377315521
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.116708993911743
9 2.0817530586 	 2.1167089514 	 2.1167089514
epoch_time;  31.728416681289673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11322085559368134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5453323125839233
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29810959100723267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9789025783538818
10 2.0718586689 	 1.9789026209 	 1.9789026209
epoch_time;  34.2170512676239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11109548807144165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5103991031646729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2593139708042145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8664343357086182
11 2.0580649715 	 1.8664342932 	 1.8664342932
epoch_time;  32.132914543151855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12391743063926697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.51683509349823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27802807092666626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9197555780410767
12 2.0527469374 	 1.9197555954 	 1.9197555954
epoch_time;  31.582541942596436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11839897930622101
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4742845296859741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27279090881347656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.867323875427246
13 2.0410984378 	 1.8673239218 	 1.8673239218
epoch_time;  31.42610192298889
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13376688957214355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.607027292251587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.282977432012558
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.016970634460449
14 2.0348783892 	 2.0169705778 	 2.0169705778
epoch_time;  31.64163088798523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10157722234725952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4179022312164307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28735777735710144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8214983940124512
15 2.0310447037 	 1.8214983966 	 1.8214983966
epoch_time;  31.4137544631958
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09441829472780228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.467058777809143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2884572744369507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8832099437713623
16 2.0217091761 	 1.8832098884 	 1.8832098884
epoch_time;  31.341626167297363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11940909922122955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5696465969085693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.296054482460022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9478436708450317
17 2.018212527 	 1.9478436444 	 1.9478436444
epoch_time;  31.472528219223022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12626458704471588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5003020763397217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30071642994880676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8337966203689575
18 2.0110417019 	 1.8337966507 	 1.8337966507
epoch_time;  31.543762683868408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09759201854467392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3555850982666016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2664744257926941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7048242092132568
19 2.0085445913 	 1.7048242517 	 1.7048242517
epoch_time;  31.711395263671875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0975559875369072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3553670644760132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2665040194988251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7056506872177124
It took 689.5927741527557 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▄▃▃▄▂▃▂▁▂▂▂▁▂▁▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▅▅▃▃▂▂▂▄▄▂▂▂▁▃▂▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.75634
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.48928
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28649
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11762
wandb:                         Train loss 2.00567
wandb: 
wandb: 🚀 View run sparkling-rat-1339 at: https://wandb.ai/nreints/thesis/runs/0qrprwsu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_170329-0qrprwsu/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171456-mi2z1ow4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-firecracker-1346
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/mi2z1ow4
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25517764687538147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.029793739318848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5225061178207397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.569142818450928
0 5.1311185575 	 4.5691426045 	 4.5691426045
epoch_time;  31.441118717193604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15238071978092194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.839883327484131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3416966497898102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.322615385055542
1 2.3556808234 	 3.3226153399 	 3.3226153399
epoch_time;  31.265159130096436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14221857488155365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.607027769088745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3721185624599457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.110564947128296
2 2.2713098235 	 3.1105650206 	 3.1105650206
epoch_time;  31.542662858963013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1787363886833191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.260904550552368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3414572775363922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6229796409606934
3 2.2253128703 	 2.6229795714 	 2.6229795714
epoch_time;  31.31560206413269
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17903906106948853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.253016233444214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34145867824554443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.538297653198242
4 2.1876105823 	 2.538297581 	 2.538297581
epoch_time;  31.503972053527832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14667829871177673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0613551139831543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3911420702934265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4768452644348145
5 2.1623791363 	 2.4768452412 	 2.4768452412
epoch_time;  31.237643003463745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13445909321308136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9942041635513306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30906084179878235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.344799518585205
6 2.1354590373 	 2.3447994748 	 2.3447994748
epoch_time;  31.44703722000122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12194778025150299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0751521587371826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3353200852870941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4565353393554688
7 2.1170265027 	 2.4565353806 	 2.4565353806
epoch_time;  31.238630294799805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11910616606473923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.007960081100464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29752102494239807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3276400566101074
8 2.1026359253 	 2.3276400179 	 2.3276400179
epoch_time;  31.456879138946533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12006471306085587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9195040464401245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29059121012687683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.232633113861084
9 2.083304021 	 2.2326331886 	 2.2326331886
epoch_time;  31.54246211051941
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15638476610183716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6725932359695435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31900274753570557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9828157424926758
10 2.0734712863 	 1.9828157992 	 1.9828157992
epoch_time;  31.361491203308105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1646205633878708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.843793511390686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31524476408958435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1128499507904053
11 2.0601503324 	 2.1128498799 	 2.1128498799
epoch_time;  31.562073469161987
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1276811808347702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.665329098701477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2946244478225708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9211292266845703
12 2.0531335623 	 1.9211292164 	 1.9211292164
epoch_time;  31.314424991607666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11382217705249786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6367897987365723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2792637050151825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8895752429962158
13 2.0427912232 	 1.8895751953 	 1.8895751953
epoch_time;  31.35856032371521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11313847452402115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.640794038772583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.296470046043396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9343634843826294
14 2.0365389422 	 1.9343634528 	 1.9343634528
epoch_time;  31.601974964141846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09493590891361237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.668108582496643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27634188532829285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9781441688537598
15 2.0267114969 	 1.9781441353 	 1.9781441353
epoch_time;  31.312475442886353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13117368519306183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6318037509918213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28950536251068115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.852856159210205
16 2.0218450486 	 1.8528561154 	 1.8528561154
epoch_time;  32.356059312820435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12868864834308624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3317512273788452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2971191108226776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5874931812286377
17 2.014032218 	 1.5874932366 	 1.5874932366
epoch_time;  33.15642189979553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12616999447345734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4844683408737183
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2860095798969269
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7280919551849365
18 2.0107033432 	 1.728092008 	 1.728092008
epoch_time;  31.567084074020386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11758922040462494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4897851943969727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28652891516685486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7566338777542114
19 2.0056706082 	 1.7566338616 	 1.7566338616
epoch_time;  31.249488353729248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11761949211359024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4892759323120117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2864938974380493
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7563443183898926
It took 687.1046285629272 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▅▄▃▃▃▂▂▁▂▂▁▂▁▁▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▄▅▂▂▂▂▁▃▃▂▃▃▂▄▂▄▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.87312
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.55547
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27553
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1114
wandb:                         Train loss 2.01111
wandb: 
wandb: 🚀 View run auspicious-firecracker-1346 at: https://wandb.ai/nreints/thesis/runs/mi2z1ow4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171456-mi2z1ow4/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_172618-mhdny9dk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-ox-1353
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/mhdny9dk
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22508448362350464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7048561573028564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.534504234790802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.31895112991333
0 5.1355616854 	 4.3189512511 	 4.3189512511
epoch_time;  31.604445695877075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1574905961751938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.666222333908081
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35234758257865906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2445335388183594
1 2.3609657985 	 3.2445335594 	 3.2445335594
epoch_time;  31.157796144485474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16457384824752808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.529322385787964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4193100333213806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1016924381256104
2 2.2758473349 	 3.1016924884 	 3.1016924884
epoch_time;  31.444308519363403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15432418882846832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.296490430831909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37480682134628296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8282833099365234
3 2.2280488321 	 2.8282833615 	 2.8282833615
epoch_time;  31.726702451705933
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1807498186826706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.098134994506836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36153221130371094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.508709669113159
4 2.1907186231 	 2.5087095518 	 2.5087095518
epoch_time;  31.740906238555908
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12454438209533691
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1299026012420654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.333347886800766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5902018547058105
5 2.1611290107 	 2.5902018779 	 2.5902018779
epoch_time;  31.336865663528442
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12122511118650436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.024522542953491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33231282234191895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4407100677490234
6 2.136073108 	 2.4407099543 	 2.4407099543
epoch_time;  31.351394414901733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12708601355552673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9582349061965942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3026502728462219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.36456036567688
7 2.1144756712 	 2.3645603489 	 2.3645603489
epoch_time;  31.331568241119385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12240159511566162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9229836463928223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29502183198928833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2677924633026123
8 2.0989293516 	 2.2677924079 	 2.2677924079
epoch_time;  31.450523853302002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10975663363933563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.915467619895935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2910972535610199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2946083545684814
9 2.0857343525 	 2.2946084512 	 2.2946084512
epoch_time;  31.380775928497314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14354576170444489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8155772686004639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3161117136478424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.172880172729492
10 2.0752723399 	 2.1728801006 	 2.1728801006
epoch_time;  31.30786395072937
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13644815981388092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.792372703552246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3026564419269562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0635056495666504
11 2.0653513377 	 2.0635057604 	 2.0635057604
epoch_time;  31.30446696281433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11928722262382507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6278996467590332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28237879276275635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9760394096374512
12 2.0562417683 	 1.9760394122 	 1.9760394122
epoch_time;  31.586794137954712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1366487592458725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6618674993515015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3156784474849701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9916961193084717
13 2.0459427365 	 1.9916960845 	 1.9916960845
epoch_time;  31.417351722717285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.139302060008049
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5766782760620117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29365310072898865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9310855865478516
14 2.0403174192 	 1.931085535 	 1.931085535
epoch_time;  31.337663412094116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1268068552017212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6139986515045166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2916634976863861
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9280061721801758
15 2.0316954699 	 1.9280062289 	 1.9280062289
epoch_time;  31.41422462463379
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15350939333438873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.562552809715271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3169686198234558
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8460630178451538
16 2.0272331677 	 1.8460630675 	 1.8460630675
epoch_time;  31.06377935409546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13188815116882324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3895578384399414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3039172887802124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7018940448760986
17 2.0209785725 	 1.7018940694 	 1.7018940694
epoch_time;  31.47659683227539
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15117068588733673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4622493982315063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3165600597858429
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7721282243728638
18 2.0128718475 	 1.7721282134 	 1.7721282134
epoch_time;  31.19625473022461
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11136418581008911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.555671215057373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2755363881587982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8727748394012451
19 2.0111119523 	 1.8727748562 	 1.8727748562
epoch_time;  31.720121383666992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11139721423387527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5554730892181396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27552899718284607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8731249570846558
It took 682.0985472202301 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▃▄▂▃▂▂▂▃▂▃▁▂▂▂▃▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▅▃▃▂▃▁▂▂▃▂▂▁▁▂▂▂▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.0312
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.67916
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28491
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12641
wandb:                         Train loss 2.01359
wandb: 
wandb: 🚀 View run vivid-ox-1353 at: https://wandb.ai/nreints/thesis/runs/mhdny9dk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_172618-mhdny9dk/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173748-kzaw8j60
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-dog-1360
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kzaw8j60
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2762795388698578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.173751354217529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5501652956008911
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.738520622253418
0 5.1241606323 	 4.7385204418 	 4.7385204418
epoch_time;  31.675456762313843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1827773153781891
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0002975463867188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37111589312553406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.589905023574829
1 2.3617995462 	 3.5899051151 	 3.5899051151
epoch_time;  32.78708052635193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2000301331281662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6039371490478516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3597184419631958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0877110958099365
2 2.2779679037 	 3.0877111486 	 3.0877111486
epoch_time;  33.6510751247406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1410907506942749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4963252544403076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34911665320396423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9998600482940674
3 2.2320058276 	 2.999860114 	 2.999860114
epoch_time;  31.50598692893982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.142057865858078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.491178035736084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36810198426246643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.020169258117676
4 2.194348004 	 3.0201693148 	 3.0201693148
epoch_time;  31.461146116256714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1274229884147644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1827335357666016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2925946116447449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6182284355163574
5 2.16418581 	 2.6182283969 	 2.6182283969
epoch_time;  31.437463521957397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15433640778064728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.290734052658081
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32331836223602295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7215006351470947
6 2.142677302 	 2.7215005411 	 2.7215005411
epoch_time;  31.231565952301025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11200418323278427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0727412700653076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30160775780677795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.510207176208496
7 2.1221660183 	 2.5102070576 	 2.5102070576
epoch_time;  31.645798444747925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12632696330547333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.185593605041504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28021541237831116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.546531915664673
8 2.1096903827 	 2.5465318834 	 2.5465318834
epoch_time;  31.445470094680786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12622764706611633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9698262214660645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3001290559768677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.307854175567627
9 2.0920387015 	 2.3078542349 	 2.3078542349
epoch_time;  31.47655701637268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.146565243601799
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9330083131790161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31776168942451477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.271359443664551
10 2.0808090592 	 2.2713593354 	 2.2713593354
epoch_time;  31.49181604385376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11794981360435486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9060497283935547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2750854790210724
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.250279664993286
11 2.0669484174 	 2.250279772 	 2.250279772
epoch_time;  31.63491201400757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13311296701431274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9718666076660156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32403209805488586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.318685293197632
12 2.0661814489 	 2.3186853357 	 2.3186853357
epoch_time;  31.374172687530518
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10052286833524704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9236218929290771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.249043807387352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.288039445877075
13 2.0520907165 	 2.2880394188 	 2.2880394188
epoch_time;  31.968417406082153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1114063486456871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7953391075134277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28000107407569885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1523499488830566
14 2.0445645073 	 2.1523498535 	 2.1523498535
epoch_time;  31.258477687835693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11367689818143845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.855078101158142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2856435477733612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.195855140686035
15 2.0365432398 	 2.195855218 	 2.195855218
epoch_time;  31.443110942840576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11893846839666367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7728830575942993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29078009724617004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1391966342926025
16 2.0326645941 	 2.1391966124 	 2.1391966124
epoch_time;  31.20993161201477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12373704463243484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8898937702178955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3147740662097931
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2745351791381836
17 2.0253795502 	 2.2745351431 	 2.2745351431
epoch_time;  31.41001534461975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12643611431121826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7908991575241089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2919063866138458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1118712425231934
18 2.0190252822 	 2.1118711729 	 2.1118711729
epoch_time;  31.364707231521606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12638366222381592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.679928183555603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2849006652832031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.03145432472229
19 2.0135865828 	 2.0314542203 	 2.0314542203
epoch_time;  31.430408239364624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12641137838363647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.679161548614502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28490912914276123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0311970710754395
It took 689.6166844367981 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▄▄▃▂▂▂▂▂▂▁▂▁▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▃▂▂▂▂▂▂▁▂▁▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▅▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▂▅▂▃▂▄▁▂▂▃▂▁▄▁▁▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.69327
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.45731
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.3045
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14258
wandb:                         Train loss 2.01856
wandb: 
wandb: 🚀 View run scintillating-dog-1360 at: https://wandb.ai/nreints/thesis/runs/kzaw8j60
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173748-kzaw8j60/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174916-vgcncljk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-wonton-1366
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/vgcncljk
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23102229833602905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6811256408691406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5108613967895508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.139923572540283
0 5.1922975203 	 4.1399235906 	 4.1399235906
epoch_time;  31.293282747268677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16132047772407532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.70969820022583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40039610862731934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2424256801605225
1 2.3644877378 	 3.2424257021 	 3.2424257021
epoch_time;  31.341415405273438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15421733260154724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3667614459991455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35707318782806396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7958199977874756
2 2.2819546523 	 2.7958199166 	 2.7958199166
epoch_time;  31.386454105377197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13375219702720642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3066985607147217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3422486186027527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8339688777923584
3 2.2389592898 	 2.8339688688 	 2.8339688688
epoch_time;  31.881296157836914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17704381048679352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1607320308685303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39097627997398376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.606259346008301
4 2.195174644 	 2.6062594027 	 2.6062594027
epoch_time;  31.578704833984375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13342039287090302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.111387252807617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2987337112426758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5739448070526123
5 2.1676940298 	 2.5739449166 	 2.5739449166
epoch_time;  31.581349849700928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14134620130062103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.146791934967041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29044461250305176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.495847702026367
6 2.1430826052 	 2.4958477948 	 2.4958477948
epoch_time;  31.374151706695557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12450024485588074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.686949372291565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29255664348602295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1036159992218018
7 2.1221575882 	 2.1036160856 	 2.1036160856
epoch_time;  31.339664220809937
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1536792516708374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6936941146850586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3325701057910919
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.01271653175354
8 2.1044892122 	 2.0127165923 	 2.0127165923
epoch_time;  33.24621248245239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10885082930326462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7662333250045776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25612378120422363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1359379291534424
9 2.0921119272 	 2.1359378299 	 2.1359378299
epoch_time;  32.620545864105225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12616637349128723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7922165393829346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2981242835521698
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.182265281677246
10 2.0770798401 	 2.1822653281 	 2.1822653281
epoch_time;  31.789731979370117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1336398720741272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.618757963180542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29074475169181824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.925477385520935
11 2.071233854 	 1.9254773939 	 1.9254773939
epoch_time;  31.86511540412903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14872108399868011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6287829875946045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2879110872745514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9346567392349243
12 2.0596946143 	 1.9346567515 	 1.9346567515
epoch_time;  31.79155683517456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13168716430664062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4519925117492676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2862933576107025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.75062096118927
13 2.053674242 	 1.750620909 	 1.750620909
epoch_time;  31.47461986541748
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11614616215229034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7288458347320557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27538055181503296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0067451000213623
14 2.0449797282 	 2.0067452096 	 2.0067452096
epoch_time;  32.0371994972229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15318705141544342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.544511079788208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27793872356414795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8342187404632568
15 2.0373731857 	 1.834218783 	 1.834218783
epoch_time;  31.718966722488403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11007460206747055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6603678464889526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2911972999572754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.973758339881897
16 2.0326027738 	 1.973758347 	 1.973758347
epoch_time;  31.588149785995483
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1079668179154396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6744325160980225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26088637113571167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9785891771316528
17 2.0269110078 	 1.9785891971 	 1.9785891971
epoch_time;  31.67748713493347
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1344088315963745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4429924488067627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2832708954811096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7173899412155151
18 2.022447218 	 1.7173899058 	 1.7173899058
epoch_time;  31.529308080673218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14255942404270172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4569191932678223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30452650785446167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6929223537445068
19 2.0185636134 	 1.6929223963 	 1.6929223963
epoch_time;  31.447655200958252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14257660508155823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4573148488998413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30450358986854553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6932685375213623
It took 688.4831540584564 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▃▃▃▂▂▂▂▁▁▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▃▂▂▂▂▁▁▂▂▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▄▅▄▃▄▃▁▄▂▂▂▂▃▁▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄█▃▄▃▄▃▄▁▃▂▂▃▂▂▁▃▂▃▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.88814
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.58038
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27253
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11442
wandb:                         Train loss 2.01398
wandb: 
wandb: 🚀 View run golden-wonton-1366 at: https://wandb.ai/nreints/thesis/runs/vgcncljk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174916-vgcncljk/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180050-1fxs90f2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-laughter-1372
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1fxs90f2
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17427152395248413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8146562576293945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4019937813282013
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.234500408172607
0 5.1816531874 	 4.2345003695 	 4.2345003695
epoch_time;  31.824355363845825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.264270544052124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9171664714813232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44007059931755066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.448411703109741
1 2.3676076797 	 3.4484117663 	 3.4484117663
epoch_time;  31.790873289108276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15519605576992035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4282124042510986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33912649750709534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9171040058135986
2 2.2845527582 	 2.9171040303 	 2.9171040303
epoch_time;  31.49340295791626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1620260775089264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1885244846343994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3561866879463196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.648411512374878
3 2.2413091227 	 2.6484116013 	 2.6484116013
epoch_time;  31.60669231414795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15287567675113678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1808383464813232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33396995067596436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6221253871917725
4 2.20147453 	 2.6221254091 	 2.6221254091
epoch_time;  31.306371450424194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16043341159820557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.035391092300415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31657224893569946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.437310218811035
5 2.1729214823 	 2.4373101312 	 2.4373101312
epoch_time;  31.60972023010254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15114662051200867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.997613549232483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3226880133152008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3834707736968994
6 2.1466179333 	 2.3834706899 	 2.3834706899
epoch_time;  31.56347346305847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15928363800048828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.861312747001648
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3172350227832794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2489144802093506
7 2.1230298496 	 2.248914399 	 2.248914399
epoch_time;  31.492407083511353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10952791571617126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7543747425079346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2643106281757355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.131542205810547
8 2.1091926797 	 2.1315423089 	 2.1315423089
epoch_time;  31.282713890075684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14256401360034943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8239738941192627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3451766073703766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.23193097114563
9 2.0907745258 	 2.2319309544 	 2.2319309544
epoch_time;  31.398781061172485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11518353968858719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7265827655792236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2705709636211395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.126009941101074
10 2.0806014488 	 2.1260100494 	 2.1260100494
epoch_time;  31.217575311660767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11514747142791748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4957493543624878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28114402294158936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8324637413024902
11 2.0691660143 	 1.8324637748 	 1.8324637748
epoch_time;  31.52709674835205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15385942161083221
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4686274528503418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27938979864120483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7728129625320435
12 2.0595904785 	 1.7728129619 	 1.7728129619
epoch_time;  31.274904012680054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12546290457248688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.62562894821167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2734370827674866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9681681394577026
13 2.0468683882 	 1.9681681865 	 1.9681681865
epoch_time;  31.71770977973938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12706854939460754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.751615285873413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3026031255722046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1193366050720215
14 2.0425703892 	 2.1193365973 	 2.1193365973
epoch_time;  31.63786482810974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09801136702299118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6102635860443115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25200554728507996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.993492603302002
15 2.0356822185 	 1.9934926626 	 1.9934926626
epoch_time;  33.377673625946045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1450183093547821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5962402820587158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2867436110973358
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8907796144485474
16 2.0336958692 	 1.8907795674 	 1.8907795674
epoch_time;  32.49686408042908
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12691473960876465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.503952980041504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27914348244667053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8104971647262573
17 2.0248530859 	 1.8104972221 	 1.8104972221
epoch_time;  31.439940214157104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13373002409934998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4446609020233154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27130499482154846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7702765464782715
18 2.0193368632 	 1.7702765387 	 1.7702765387
epoch_time;  31.489744186401367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11441843956708908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5800905227661133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2725048363208771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.890270471572876
19 2.0139770095 	 1.8902705012 	 1.8902705012
epoch_time;  31.43813729286194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11442343890666962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5803831815719604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2725349962711334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.888139009475708
It took 693.5999853610992 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▄▃▃▃▂▃▁▁▂▂▂▂▂▂▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▂▅▂▂▂▂▃▁▁▂▃▁▂▂▂▁▃▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.8098
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.52481
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26387
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12278
wandb:                         Train loss 2.00385
wandb: 
wandb: 🚀 View run beaming-laughter-1372 at: https://wandb.ai/nreints/thesis/runs/1fxs90f2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180050-1fxs90f2/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_181217-phqezjty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-moon-1378
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/phqezjty
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2819792926311493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.651364326477051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5583350658416748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.21043062210083
0 5.0756766514 	 4.2104304133 	 4.2104304133
epoch_time;  31.55992031097412
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1506783664226532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.78545880317688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4061909317970276
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3631608486175537
1 2.3549690634 	 3.3631608293 	 3.3631608293
epoch_time;  31.549193382263184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1411171406507492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.419675350189209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35389527678489685
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9960553646087646
2 2.2747418424 	 2.9960554793 	 2.9960554793
epoch_time;  31.577647924423218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20583215355873108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2998180389404297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3916718363761902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7478551864624023
3 2.2237709991 	 2.7478551916 	 2.7478551916
epoch_time;  31.40122413635254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1427062749862671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.193341016769409
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3593832552433014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.634981155395508
4 2.1878563759 	 2.6349812276 	 2.6349812276
epoch_time;  31.506935596466064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1349443942308426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1928064823150635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3400559723377228
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6430623531341553
5 2.1566202607 	 2.6430624472 	 2.6430624472
epoch_time;  31.38116216659546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1436234414577484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.103799343109131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32937750220298767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.494288206100464
6 2.1303550601 	 2.4942880991 	 2.4942880991
epoch_time;  31.436797380447388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12032879143953323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0971028804779053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2972434461116791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4690937995910645
7 2.11091621 	 2.4690937764 	 2.4690937764
epoch_time;  31.511650800704956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1607261300086975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8205937147140503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32639992237091064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1528120040893555
8 2.0966097677 	 2.1528119061 	 2.1528119061
epoch_time;  31.435128211975098
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11012523621320724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9237658977508545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.268841415643692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.291292190551758
9 2.0787185741 	 2.2912922627 	 2.2912922627
epoch_time;  31.87890934944153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10906577110290527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.834854006767273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2616642415523529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.195502519607544
10 2.0689453456 	 2.1955025338 	 2.1955025338
epoch_time;  31.608237266540527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1425451934337616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7844756841659546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28795483708381653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1427924633026123
11 2.0561338307 	 2.1427924079 	 2.1427924079
epoch_time;  31.766000747680664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15215787291526794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7293206453323364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3099037706851959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1149959564208984
12 2.0473736948 	 2.114995843 	 2.114995843
epoch_time;  31.40383505821228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1077696830034256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6603639125823975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31560733914375305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0783956050872803
13 2.0402489871 	 2.0783955342 	 2.0783955342
epoch_time;  31.51041579246521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14236599206924438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.617661714553833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32466015219688416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9611774682998657
14 2.0328052601 	 1.9611775166 	 1.9611775166
epoch_time;  31.46616530418396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13806086778640747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4975404739379883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29760557413101196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8393751382827759
15 2.0255795571 	 1.839375099 	 1.839375099
epoch_time;  31.51685404777527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12511031329631805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5673179626464844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3191477060317993
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9068946838378906
16 2.0177306922 	 1.9068946632 	 1.9068946632
epoch_time;  31.78536319732666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1194104477763176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.493647575378418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26691707968711853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7833613157272339
17 2.0122863728 	 1.7833613215 	 1.7833613215
epoch_time;  31.38524556159973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15715262293815613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4921213388442993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30779343843460083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.790816068649292
18 2.008811212 	 1.7908160235 	 1.7908160235
epoch_time;  31.237094402313232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1228107139468193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5243701934814453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2638871371746063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8097034692764282
19 2.0038510415 	 1.8097034351 	 1.8097034351
epoch_time;  31.655735969543457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12278153002262115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5248138904571533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26387134194374084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8098015785217285
It took 687.1756772994995 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▃▃▃▂▂▂▂▂▁▂▂▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▂▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▆▃▂▃▃▃▃▃▃▁▁▃▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▃▇▃▂▃▄▂▄▃▄▁▁▄▂▃▄▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.73587
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.46477
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27619
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11253
wandb:                         Train loss 2.01938
wandb: 
wandb: 🚀 View run cheerful-moon-1378 at: https://wandb.ai/nreints/thesis/runs/phqezjty
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_181217-phqezjty/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2474159598350525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7992422580718994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48461002111434937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.251389980316162
0 5.1720085583 	 4.251389952 	 4.251389952
epoch_time;  33.185805320739746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16066643595695496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6857380867004395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4004594087600708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1421895027160645
1 2.3652152008 	 3.1421894795 	 3.1421894795
epoch_time;  32.33141613006592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14598993957042694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4105708599090576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3561619520187378
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8597452640533447
2 2.2851641842 	 2.85974517 	 2.85974517
epoch_time;  31.245489597320557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13294178247451782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.295762777328491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3246290981769562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7653026580810547
3 2.2378430531 	 2.7653026684 	 2.7653026684
epoch_time;  31.94017505645752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2154507339000702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.018646478652954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4319697320461273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4673421382904053
4 2.2025102681 	 2.4673422324 	 2.4673422324
epoch_time;  31.442225456237793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1324305683374405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9104340076446533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30813300609588623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3075554370880127
5 2.1737163856 	 2.3075553275 	 2.3075553275
epoch_time;  31.747236967086792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12230781465768814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0192840099334717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.299173504114151
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.400432825088501
6 2.1512782588 	 2.4004328547 	 2.4004328547
epoch_time;  31.277867317199707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14378304779529572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8201848268508911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.313662052154541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1836044788360596
7 2.1288622815 	 2.1836044724 	 2.1836044724
epoch_time;  31.447893857955933
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15794400870800018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7611981630325317
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3091566860675812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1446733474731445
8 2.1119410386 	 2.1446734454 	 2.1446734454
epoch_time;  31.20402431488037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11717046052217484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8214571475982666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31740760803222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.213610887527466
9 2.1010725658 	 2.2136108398 	 2.2136108398
epoch_time;  31.981404542922974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1657300740480423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7188225984573364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3164259195327759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0433192253112793
10 2.0914165313 	 2.0433191248 	 2.0433191248
epoch_time;  31.23543691635132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1268431842327118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6040407419204712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30133822560310364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9083938598632812
11 2.0774889451 	 1.9083938186 	 1.9083938186
epoch_time;  31.605807065963745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16707460582256317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.55155611038208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3138011693954468
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.855453610420227
12 2.065648538 	 1.8554535737 	 1.8554535737
epoch_time;  31.367802619934082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09087839722633362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.571148157119751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24976526200771332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8713593482971191
13 2.0580938981 	 1.8713593354 	 1.8713593354
epoch_time;  31.651443004608154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10188575834035873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5711251497268677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2658047676086426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9142967462539673
14 2.0528783469 	 1.914296743 	 1.914296743
epoch_time;  31.390775442123413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1656501144170761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4603596925735474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3038853704929352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7016875743865967
15 2.0439129147 	 1.7016875396 	 1.7016875396
epoch_time;  31.47456192970276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11580844223499298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5728858709335327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28881189227104187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8621159791946411
16 2.0339494316 	 1.8621159734 	 1.8621159734
epoch_time;  31.57615065574646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12605875730514526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.637446403503418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2951962947845459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9097028970718384
17 2.0318708478 	 1.9097029402 	 1.9097029402
epoch_time;  31.73162865638733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15416598320007324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4596004486083984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2921516001224518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6861921548843384
18 2.0249016826 	 1.6861921981 	 1.6861921981
epoch_time;  31.671180248260498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11255450546741486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4640276432037354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27614259719848633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7360105514526367
19 2.0193839722 	 1.7360105772 	 1.7360105772
epoch_time;  31.357657432556152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11253302544355392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4647719860076904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2761911153793335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7358675003051758
It took 694.1172308921814 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139506
Array Job ID: 2137927_21
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 04:10:35
CPU Efficiency: 12.05% of 1-10:39:18 core-walltime
Job Wall-clock time: 01:55:31
Memory Utilized: 8.07 GB
Memory Efficiency: 25.82% of 31.25 GB
