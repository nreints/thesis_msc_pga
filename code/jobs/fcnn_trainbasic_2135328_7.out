wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163841-wpi6ci10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-tiger-1133
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/wpi6ci10
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–‚â–…â–…â–„â–…â–„â–„â–ƒâ–ƒâ–â–ƒâ–„â–ƒâ–â–ƒâ–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–…â–â–…â–…â–…â–‡â–†â–…â–†â–†â–…â–†â–ˆâ–†â–„â–ˆâ–„â–„â–…â–…
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‡â–„â–ƒâ–â–ƒâ–‚â–†â–†â–…â–ƒâ–ˆâ–…â–ƒâ–„â–…â–ƒâ–‡â–…â–„â–„
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.25903
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.73323
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.75479
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26667
wandb:                         Train loss 0.57691
wandb: 
wandb: ðŸš€ View run radiant-tiger-1133 at: https://wandb.ai/nreints/thesis/runs/wpi6ci10
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163841-wpi6ci10/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21161572635173798
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 12.56609058380127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.437861442565918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.068897247314453
0 2.7232792466 	 27.0688978041 	 27.0688978041
epoch_time;  29.706698894500732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4014122188091278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.611969947814941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.840898513793945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.97709846496582
1 0.7768478652 	 24.9770982897 	 24.9770982897
epoch_time;  28.831785678863525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28619515895843506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.128003120422363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.703507423400879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.69589614868164
2 0.7169900221 	 22.6958957981 	 22.6958957981
epoch_time;  28.657124280929565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21793872117996216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.50957202911377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.945630073547363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.516483306884766
3 0.6802046665 	 24.5164827914 	 24.5164827914
epoch_time;  28.264093160629272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1526576280593872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.627153396606445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.958274841308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.964618682861328
4 0.6529161664 	 24.9646194046 	 24.9646194046
epoch_time;  28.276641607284546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22711962461471558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.90405559539795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.967170715332031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.94493293762207
5 0.6294186643 	 23.9449324324 	 23.9449324324
epoch_time;  28.53033185005188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1982814073562622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.969063758850098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.349035263061523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.47472381591797
6 0.6203715351 	 24.4747228674 	 24.4747228674
epoch_time;  28.281696796417236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3468564748764038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.788589477539062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.189298629760742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.12063217163086
7 0.6109724103 	 24.1206318623 	 24.1206318623
epoch_time;  28.425060510635376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3389962613582611
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.607545852661133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.791793823242188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.57567024230957
8 0.6050948942 	 23.575670397 	 23.575670397
epoch_time;  28.422667264938354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3142561912536621
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.027377128601074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.29497241973877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.197250366210938
9 0.599444452 	 23.1972497889 	 23.1972497889
epoch_time;  28.646057844161987
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21398334205150604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.359800338745117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.314706802368164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.41341781616211
10 0.59537737 	 23.4134184966 	 23.4134184966
epoch_time;  28.652231454849243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4284113645553589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.544023036956787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.959915161132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.82539939880371
11 0.5931254331 	 21.8253985431 	 21.8253985431
epoch_time;  28.610547304153442
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3170698583126068
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.963566780090332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.16409969329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.817636489868164
12 0.5878479376 	 22.8176361909 	 22.8176361909
epoch_time;  28.738342761993408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22910752892494202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.552773475646973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.71410083770752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.117076873779297
13 0.5864936348 	 24.117076647 	 24.117076647
epoch_time;  29.71082377433777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28169041872024536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.185346603393555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.27155590057373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.11382293701172
14 0.5866070272 	 23.1138223184 	 23.1138223184
epoch_time;  28.800602197647095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29479923844337463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.651613235473633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.648735046386719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.621007919311523
15 0.5818666989 	 21.6210079709 	 21.6210079709
epoch_time;  28.809126615524292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22514177858829498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.704344272613525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.791482925415039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.09174346923828
16 0.5825490777 	 23.0917440878 	 23.0917440878
epoch_time;  28.3757483959198
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.377011239528656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.323756694793701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.618374824523926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.541616439819336
17 0.582298036 	 21.541616079 	 21.541616079
epoch_time;  28.220314502716064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32990381121635437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.549072742462158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.677977561950684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.901020050048828
18 0.5779552008 	 21.9010201119 	 21.9010201119
epoch_time;  28.483678102493286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2667599320411682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.731488227844238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.751176834106445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.2609806060791
19 0.5769116938 	 22.2609797297 	 22.2609797297
epoch_time;  28.50759720802307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26667115092277527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.73322868347168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.754786491394043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.259031295776367
It took 630.2039964199066 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn59: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135335.0

JOB STATISTICS
==============
Job ID: 2135335
Array Job ID: 2135328_7
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:13:48 core-walltime
Job Wall-clock time: 00:10:46
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
