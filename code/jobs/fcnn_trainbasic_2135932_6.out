wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195346-wuz6at4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-envelope-1164
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/wuz6at4j
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▂▂▃▂▃▆▃▁▅▂▂▅▄▃▂▅▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▁▂▆▃▄▃▄▄▂▅▃▄▅▄▃▃▃▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▄▁▂▅▄▄▂▃▂▁▄▆▂▁▄▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▁▂▅▂▄▂▂▃▁▃▂▃▃▂▁▂▁▄▄
wandb:                         Train loss █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 36409.21875
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.43225
wandb:    Test loss t(0, 0)_r(-5, 5)_none 107.3698
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31853
wandb:                         Train loss 1.75
wandb: 
wandb: 🚀 View run fortuitous-envelope-1164 at: https://wandb.ai/nreints/thesis/runs/wuz6at4j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195346-wuz6at4j/logs
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5341231226921082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8010636568069458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 356.47393798828125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 911099.0
0 3.4798264139 	 911098.9837837837 	 911098.9837837837
epoch_time;  39.49643278121948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2529655992984772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0997955799102783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 174.94337463378906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 154903.21875
1 2.0928907969 	 154903.2216216216 	 154903.2216216216
epoch_time;  37.94274091720581
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21215109527111053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8638573884963989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 202.49581909179688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 177390.046875
2 1.9930323224 	 177390.0432432432 	 177390.0432432432
epoch_time;  37.9140784740448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22307731211185455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0302315950393677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 94.79467010498047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 225401.34375
3 1.9363834327 	 225401.3405405406 	 225401.3405405406
epoch_time;  37.749720335006714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37057745456695557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5319870710372925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 123.61145782470703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 161651.703125
4 1.9020303508 	 161651.6972972973 	 161651.6972972973
epoch_time;  37.59179902076721
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2663373649120331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1875559091567993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 254.08963012695312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 324162.65625
5 1.8756689486 	 324162.6594594595 	 324162.6594594595
epoch_time;  37.60392785072327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3515065312385559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.206575632095337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 188.97206115722656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 692961.25
6 1.8556765257 	 692961.254054054 	 692961.254054054
epoch_time;  37.74307727813721
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24766018986701965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1790587902069092
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 204.81045532226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 276779.1875
7 1.8441067065 	 276779.2 	 276779.2
epoch_time;  37.77411437034607
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24149058759212494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2524548768997192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 111.90933990478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 87964.3671875
8 1.8249004263 	 87964.3675675676 	 87964.3675675676
epoch_time;  37.647664308547974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30785489082336426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2721714973449707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 149.8800811767578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 493215.0
9 1.8187838524 	 493215.0054054054 	 493215.0054054054
epoch_time;  38.729795694351196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19699084758758545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0614560842514038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 110.24465942382812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 117699.234375
10 1.805803902 	 117699.2324324324 	 117699.2324324324
epoch_time;  38.33119797706604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27670058608055115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4002628326416016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 108.88834381103516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 161554.21875
11 1.7901180735 	 161554.2162162162 	 161554.2162162162
epoch_time;  37.47067070007324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26657912135124207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1358058452606201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 208.8250274658203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 568640.1875
12 1.7834778452 	 568640.2162162162 	 568640.2162162162
epoch_time;  37.39259719848633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2918596863746643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3232091665267944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 273.9311828613281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 408102.1875
13 1.7748350687 	 408102.1837837838 	 408102.1837837838
epoch_time;  37.68247604370117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2855580449104309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3929048776626587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 118.6355972290039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 227756.171875
14 1.7686781628 	 227756.172972973 	 227756.172972973
epoch_time;  37.57567095756531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23060618340969086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2342841625213623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 103.67996978759766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 140051.65625
15 1.7642492502 	 140051.6540540541 	 140051.6540540541
epoch_time;  37.83317732810974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.212342768907547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1286677122116089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 193.5641326904297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 526771.75
16 1.759105437 	 526771.7621621621 	 526771.7621621621
epoch_time;  37.96771836280823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24908410012722015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1628601551055908
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 90.45530700683594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48657.70703125
17 1.7533449084 	 48657.7081081081 	 48657.7081081081
epoch_time;  37.87498450279236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22106793522834778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1119762659072876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 118.89607238769531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 192047.4375
18 1.7484411034 	 192047.4378378378 	 192047.4378378378
epoch_time;  38.327245235443115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3184572756290436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4329193830490112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 102.88496398925781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36417.04296875
19 1.7500001653 	 36417.0432432432 	 36417.0432432432
epoch_time;  37.54891490936279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3185322880744934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4322483539581299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 107.36980438232422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36409.21875
It took 818.187379360199 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn39: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135938.0

JOB STATISTICS
==============
Job ID: 2135938
Array Job ID: 2135932_6
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 04:09:54 core-walltime
Job Wall-clock time: 00:13:53
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
