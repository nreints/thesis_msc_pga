wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142629-5b1ii786
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-lantern-1251
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5b1ii786
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▃▄▃▂▃▄▁▄▂▁▁▃▁▁▂▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▂▃▂▂▂▄▁▂▂▁▁▃▂▁▁▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.15442
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.33738
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.04091
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15057
wandb:                         Train loss 1.76661
wandb: 
wandb: 🚀 View run filigreed-lantern-1251 at: https://wandb.ai/nreints/thesis/runs/5b1ii786
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142629-5b1ii786/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_143810-sdh5beyd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-rabbit-1257
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sdh5beyd
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36223527789115906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5357719659805298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.632779121398926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.538437843322754
0 4.8795181719 	 8.5384382918 	 8.5384382918
epoch_time;  32.80490016937256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2134101241827011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3839057683944702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.157364368438721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.067019462585449
1 2.1072142646 	 7.0670192409 	 7.0670192409
epoch_time;  32.26788640022278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2219233512878418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36381709575653076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.785202503204346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.574362754821777
2 2.0152302757 	 6.574362595 	 6.574362595
epoch_time;  31.87218976020813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15876905620098114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3252789378166199
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.221035480499268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.082892894744873
3 1.9665839751 	 6.0828930004 	 6.0828930004
epoch_time;  32.035701513290405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17270439863204956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3629668056964874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.029231071472168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.933859825134277
4 1.9255903343 	 5.9338596653 	 5.9338596653
epoch_time;  32.35575580596924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15347006916999817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3329406678676605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.737221717834473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.700344562530518
5 1.9012843747 	 5.7003444362 	 5.7003444362
epoch_time;  33.1153929233551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1418539136648178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2975026071071625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.664640426635742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.58530855178833
6 1.8829749849 	 5.5853086729 	 5.5853086729
epoch_time;  31.68800663948059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1450233906507492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32376742362976074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.557958126068115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.487701892852783
7 1.8650956925 	 5.4877019109 	 5.4877019109
epoch_time;  32.248831033706665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2063237428665161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3599553406238556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.374146938323975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.323831558227539
8 1.8466563809 	 5.3238314242 	 5.3238314242
epoch_time;  31.90209650993347
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12957391142845154
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26587724685668945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.478182792663574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.3957319259643555
9 1.8393825771 	 5.395731828 	 5.395731828
epoch_time;  32.03957748413086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16085857152938843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38276445865631104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.222793102264404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.283034801483154
10 1.8274196865 	 5.2830348659 	 5.2830348659
epoch_time;  31.7776882648468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13973940908908844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2964000403881073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.107548713684082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.099366664886475
11 1.814734457 	 5.099366884 	 5.099366884
epoch_time;  32.029486417770386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12006533890962601
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2714426517486572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.011868953704834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.011390686035156
12 1.8021412968 	 5.0113908098 	 5.0113908098
epoch_time;  31.632771492004395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11952408403158188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2644501030445099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.869885444641113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.861546039581299
13 1.8005909955 	 4.861546202 	 4.861546202
epoch_time;  32.1540322303772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17025378346443176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3317316770553589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.698699951171875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.765725612640381
14 1.7921652513 	 4.7657256255 	 4.7657256255
epoch_time;  31.598629474639893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14435496926307678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2744458317756653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.508943557739258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.570406913757324
15 1.787946098 	 4.5704068571 	 4.5704068571
epoch_time;  31.94244623184204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12156761437654495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25577110052108765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.40652322769165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.457012176513672
16 1.7840142273 	 4.4570121147 	 4.4570121147
epoch_time;  31.897597074508667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11732673645019531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30104199051856995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.298819065093994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4094696044921875
17 1.7753757485 	 4.409469687 	 4.409469687
epoch_time;  32.2488534450531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11620768159627914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26311013102531433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.130548477172852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.21124267578125
18 1.7733246036 	 4.2112426758 	 4.2112426758
epoch_time;  32.051424741744995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15050920844078064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33735954761505127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.03996467590332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.153559684753418
19 1.7666070172 	 4.1535598342 	 4.1535598342
epoch_time;  32.116947412490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15056881308555603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.337380588054657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.040914535522461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.154421806335449
It took 700.4925229549408 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▄▃▄▂▂▁▁▁▁▂▁▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▃▂▂▃▂▄▁▂▁▁▁▁▂▁▁▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.1367
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23864
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.74693
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11486
wandb:                         Train loss 1.79211
wandb: 
wandb: 🚀 View run flashing-rabbit-1257 at: https://wandb.ai/nreints/thesis/runs/sdh5beyd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_143810-sdh5beyd/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144945-g3yykxsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-lamp-1264
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g3yykxsi
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33563166856765747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5187543034553528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.452052116394043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.015764236450195
0 4.8567279668 	 9.0157642261 	 9.0157642261
epoch_time;  31.7608380317688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22928258776664734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39627787470817566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.87666130065918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.353653430938721
1 2.0967712609 	 7.3536535315 	 7.3536535315
epoch_time;  31.443946838378906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18428878486156464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3600277006626129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.335875988006592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.784333229064941
2 2.020775425 	 6.7843334301 	 6.7843334301
epoch_time;  31.49162244796753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18411363661289215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3243557810783386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.8917131423950195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.304032325744629
3 1.9753084073 	 6.3040322793 	 6.3040322793
epoch_time;  31.691648721694946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15284651517868042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.316761314868927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.582289218902588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.984004974365234
4 1.9463738588 	 5.98400483 	 5.98400483
epoch_time;  31.971619844436646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1455180048942566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29132381081581116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.406625270843506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.827477931976318
5 1.9178571759 	 5.8274776974 	 5.8274776974
epoch_time;  31.830716371536255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1656743288040161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3466421663761139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.274510860443115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.689641952514648
6 1.9042181946 	 5.6896418391 	 5.6896418391
epoch_time;  31.703510999679565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1387341320514679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29328593611717224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1842756271362305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.552923679351807
7 1.8832685487 	 5.5529237489 	 5.5529237489
epoch_time;  31.734209775924683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19523686170578003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3614720106124878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.987344741821289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.448583602905273
8 1.8754970419 	 5.4485836545 	 5.4485836545
epoch_time;  32.04284334182739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12289765477180481
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28450989723205566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.761500835418701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.169512748718262
9 1.856456222 	 5.1695127745 	 5.1695127745
epoch_time;  32.02824020385742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13270039856433868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24922560155391693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.671342372894287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.013753890991211
10 1.8446346585 	 5.0137536951 	 5.0137536951
epoch_time;  31.886670112609863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11473526805639267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24446611106395721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.269955158233643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.667837619781494
11 1.8373345728 	 4.6678374419 	 4.6678374419
epoch_time;  32.0152633190155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11565347760915756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23625127971172333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.293872356414795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.65022611618042
12 1.8330048372 	 4.650225995 	 4.650225995
epoch_time;  31.472511291503906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11877484619617462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23645561933517456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.082581996917725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.497714519500732
13 1.8188721199 	 4.4977146458 	 4.4977146458
epoch_time;  31.808383464813232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11774858087301254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22553741931915283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.084255695343018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.462005615234375
14 1.8156510174 	 4.4620054503 	 4.4620054503
epoch_time;  31.632894039154053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1358284056186676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26657935976982117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.878507614135742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.286471366882324
15 1.8134091222 	 4.2864713102 	 4.2864713102
epoch_time;  32.1122567653656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12672311067581177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23595142364501953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8977420330047607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.292315483093262
16 1.8056457148 	 4.2923155089 	 4.2923155089
epoch_time;  31.89814829826355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11788585782051086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23939023911952972
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.881847381591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.264297008514404
17 1.7981882683 	 4.264297073 	 4.264297073
epoch_time;  31.921689987182617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15993596613407135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28325581550598145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.015565872192383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.39021635055542
18 1.7935138927 	 4.3902162294 	 4.3902162294
epoch_time;  31.513315439224243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11485414952039719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23859618604183197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.746148109436035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.136785507202148
19 1.7921087204 	 4.1367853938 	 4.1367853938
epoch_time;  31.740289449691772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11486072838306427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23863860964775085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.746931314468384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1367011070251465
It took 695.9520356655121 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▂▂▃▃▃▂▂▂▁▁▁▂▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.2275
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.2683
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.18153
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12937
wandb:                         Train loss 1.78372
wandb: 
wandb: 🚀 View run prosperous-lamp-1264 at: https://wandb.ai/nreints/thesis/runs/g3yykxsi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144945-g3yykxsi/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150122-abivdzva
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-horse-1271
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/abivdzva
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38074320554733276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6152710914611816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.853898048400879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.828068733215332
0 4.8203465508 	 8.8280689136 	 8.8280689136
epoch_time;  32.12695288658142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2160550057888031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39658594131469727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.497159481048584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.337199687957764
1 2.0954269172 	 7.337199773 	 7.337199773
epoch_time;  31.860960721969604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17575593292713165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.346096009016037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.910350799560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.764492511749268
2 2.0145309128 	 6.7644927154 	 6.7644927154
epoch_time;  31.7666277885437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16625991463661194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2995765507221222
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.423773288726807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.262243747711182
3 1.9737924874 	 6.2622439823 	 6.2622439823
epoch_time;  32.24075508117676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1674416959285736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30259713530540466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.267789363861084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.121109485626221
4 1.9366788147 	 6.1211095861 	 6.1211095861
epoch_time;  32.03968954086304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13867945969104767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35903599858283997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.818624496459961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.789358139038086
5 1.9172908755 	 5.7893581081 	 5.7893581081
epoch_time;  32.07417368888855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14597488939762115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3141268491744995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.747448444366455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.695883274078369
6 1.8938434578 	 5.6958832612 	 5.6958832612
epoch_time;  31.783021688461304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15344612300395966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3219762146472931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.603183269500732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.574779510498047
7 1.8808714415 	 5.5747796136 	 5.5747796136
epoch_time;  31.922502040863037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12514154613018036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2569115161895752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.609939098358154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.589484691619873
8 1.8709000243 	 5.5894847973 	 5.5894847973
epoch_time;  31.75470781326294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1473732888698578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2823733687400818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.544098377227783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.519964218139648
9 1.8545326848 	 5.5199641047 	 5.5199641047
epoch_time;  32.36319875717163
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13950660824775696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2936710715293884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.2888994216918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.280471324920654
10 1.8453523691 	 5.2804713894 	 5.2804713894
epoch_time;  31.774686813354492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12251467257738113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2204277664422989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.261750221252441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.215221405029297
11 1.8372631654 	 5.2152211782 	 5.2152211782
epoch_time;  32.20058059692383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12399065494537354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24796296656131744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.114772796630859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.096012592315674
12 1.8280365672 	 5.0960125897 	 5.0960125897
epoch_time;  31.826908349990845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12579330801963806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24624912440776825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.951848030090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.968470096588135
13 1.8155963048 	 4.9684698981 	 4.9684698981
epoch_time;  31.882845878601074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1254650056362152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2800079584121704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.781930446624756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.787644386291504
14 1.8066016154 	 4.7876445049 	 4.7876445049
epoch_time;  31.736029148101807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.126275435090065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25094619393348694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.592445373535156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.614521026611328
15 1.8059096908 	 4.6145210885 	 4.6145210885
epoch_time;  31.974380254745483
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12764164805412292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2740445137023926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.480754852294922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.512509822845459
16 1.79296032 	 4.5125098976 	 4.5125098976
epoch_time;  32.0623095035553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.137283056974411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2592717707157135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.501707077026367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.524899482727051
17 1.7937215032 	 4.5248997044 	 4.5248997044
epoch_time;  31.914520978927612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1323171854019165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27338922023773193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.361624717712402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.39372444152832
18 1.7845772218 	 4.3937245962 	 4.3937245962
epoch_time;  32.081055641174316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12939117848873138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26817506551742554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.183193683624268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.228347301483154
19 1.7837242999 	 4.2283473659 	 4.2283473659
epoch_time;  31.91476798057556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12937471270561218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2683034837245941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.181527137756348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.22749662399292
It took 696.1380083560944 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▄▂▄▂▂▂▂▁▂▂▁▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▃▃▄▂▄▂▂▂▂▂▂▂▂▂▃▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.94356
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24327
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.88178
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11726
wandb:                         Train loss 1.7724
wandb: 
wandb: 🚀 View run chromatic-horse-1271 at: https://wandb.ai/nreints/thesis/runs/abivdzva
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150122-abivdzva/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151302-gdqzkrbn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-fuse-1276
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/gdqzkrbn
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3303013741970062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5936919450759888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.035127639770508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.150870323181152
0 4.8996395248 	 9.1508703283 	 9.1508703283
epoch_time;  31.643547534942627
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2553020715713501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4350283741950989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.5541253089904785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.552734375
1 2.109443928 	 7.552734375 	 7.552734375
epoch_time;  31.48772382736206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18782731890678406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31935158371925354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.628396987915039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.64711332321167
2 2.0208691472 	 6.6471132021 	 6.6471132021
epoch_time;  31.63912034034729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19271157681941986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35498613119125366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.1340131759643555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.1722893714904785
3 1.9672001947 	 6.1722893792 	 6.1722893792
epoch_time;  31.578683853149414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19726434350013733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39154282212257385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.872048377990723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.89845085144043
4 1.9343792316 	 5.8984506968 	 5.8984506968
epoch_time;  31.782848119735718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15703286230564117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3033539056777954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.7078633308410645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.701955318450928
5 1.9010327958 	 5.7019551045 	 5.7019551045
epoch_time;  31.700557947158813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20972840487957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3925856649875641
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.568586826324463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.6151204109191895
6 1.8739303764 	 5.6151202228 	 5.6151202228
epoch_time;  31.651371240615845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15762709081172943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30386149883270264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.465624809265137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.48554801940918
7 1.8590951557 	 5.4855478648 	 5.4855478648
epoch_time;  31.57369351387024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15625299513339996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.289257287979126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.372404098510742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.387386798858643
8 1.8516927524 	 5.3873868375 	 5.3873868375
epoch_time;  31.800180673599243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14593078196048737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2830173075199127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.2646074295043945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.283736228942871
9 1.8378621943 	 5.2837362753 	 5.2837362753
epoch_time;  31.715954780578613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1526707410812378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27818575501441956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9797868728637695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.000577926635742
10 1.8266363538 	 5.0005780194 	 5.0005780194
epoch_time;  31.49093747138977
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13275447487831116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26210078597068787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.802035808563232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.826325416564941
11 1.8189393949 	 4.8263252877 	 4.8263252877
epoch_time;  31.389607191085815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13396011292934418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3085272014141083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.562553405761719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.665519714355469
12 1.8119208065 	 4.6655197556 	 4.6655197556
epoch_time;  31.713741540908813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1595858633518219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2932929992675781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.37728214263916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.448653697967529
13 1.8012890361 	 4.4486535974 	 4.4486535974
epoch_time;  31.826335906982422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13666070997714996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2609979510307312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.352246284484863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.397343635559082
14 1.7907423925 	 4.397343816 	 4.397343816
epoch_time;  31.970526933670044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1541973352432251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2579908072948456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.278411865234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3249592781066895
15 1.7901344377 	 4.3249594199 	 4.3249594199
epoch_time;  31.80811643600464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16350477933883667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29579249024391174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.023465633392334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.124167442321777
16 1.7868895327 	 4.1241676124 	 4.1241676124
epoch_time;  31.677940607070923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15507708489894867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2884235084056854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.141983509063721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.221299171447754
17 1.7806038621 	 4.2212989601 	 4.2212989601
epoch_time;  32.084277629852295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.149685800075531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29317978024482727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.071094512939453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.186038970947266
18 1.7756926189 	 4.1860387854 	 4.1860387854
epoch_time;  31.830312728881836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1173015609383583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24314971268177032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8819448947906494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.944324493408203
19 1.7724032496 	 3.9443243903 	 3.9443243903
epoch_time;  31.797035217285156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1172603890299797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24326851963996887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8817808628082275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.943563222885132
It took 699.9443795681 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▅▃▃▃▃▄▃▅▄▃▂▁▃▂▁▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▄▂▂▂▂▂▂▃▂▂▁▁▁▁▁▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.234
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22747
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.18072
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1271
wandb:                         Train loss 1.76796
wandb: 
wandb: 🚀 View run cheerful-fuse-1276 at: https://wandb.ai/nreints/thesis/runs/gdqzkrbn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151302-gdqzkrbn/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_152429-w4slo8gi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-dragon-1283
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/w4slo8gi
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3218872547149658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.478732705116272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.862131118774414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.790922164916992
0 4.908778682 	 8.7909225876 	 8.7909225876
epoch_time;  31.571701765060425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20879314839839935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37161484360694885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.380127429962158
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.313708305358887
1 2.1105709089 	 7.3137081662 	 7.3137081662
epoch_time;  31.68736982345581
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17623648047447205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35349947214126587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.790164947509766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.726513862609863
2 2.0170662066 	 6.7265136719 	 6.7265136719
epoch_time;  31.775506019592285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20559951663017273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3750981092453003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.4243597984313965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.373691082000732
3 1.9706335358 	 6.3736908784 	 6.3736908784
epoch_time;  31.557403802871704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16333001852035522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3151196539402008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.095281600952148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.0425028800964355
4 1.9266055759 	 6.0425029033 	 6.0425029033
epoch_time;  31.3392117023468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1662120670080185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3019394278526306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.962422847747803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.869686126708984
5 1.9012303233 	 5.8696863123 	 5.8696863123
epoch_time;  31.43162488937378
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15919366478919983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2925660312175751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.8405561447143555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.735649108886719
6 1.8730809258 	 5.7356491501 	 5.7356491501
epoch_time;  31.492387294769287
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14856398105621338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30298271775245667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.641724109649658
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.572399616241455
7 1.8617441724 	 5.5723995724 	 5.5723995724
epoch_time;  31.599834442138672
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16043947637081146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34284186363220215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4153828620910645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.417992115020752
8 1.8491599381 	 5.4179921743 	 5.4179921743
epoch_time;  31.387758255004883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15220347046852112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3091747462749481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.310044765472412
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.260632038116455
9 1.8347797742 	 5.2606319943 	 5.2606319943
epoch_time;  31.905989170074463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18198469281196594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3583143353462219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.11676549911499
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.127000331878662
10 1.8282268217 	 5.1270003035 	 5.1270003035
epoch_time;  31.655840635299683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14740784466266632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3214569389820099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.958590507507324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.968795299530029
11 1.8172684284 	 4.968795199 	 4.968795199
epoch_time;  32.062007188797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15102724730968475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28775495290756226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.700047016143799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.71696662902832
12 1.8126818244 	 4.7169664538 	 4.7169664538
epoch_time;  31.802144050598145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13217972218990326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2696668803691864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6705098152160645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.667187690734863
13 1.8028737186 	 4.6671875 	 4.6671875
epoch_time;  31.760260820388794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1300724446773529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23674677312374115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6582350730896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.634749889373779
14 1.7979945426 	 4.6347497889 	 4.6347497889
epoch_time;  31.437052488327026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13371258974075317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2825338840484619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.396852016448975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4258599281311035
15 1.7882075946 	 4.4258597709 	 4.4258597709
epoch_time;  31.5392062664032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13633523881435394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27735182642936707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.230641841888428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.326456546783447
16 1.7858483399 	 4.3264565958 	 4.3264565958
epoch_time;  31.29449462890625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13517947494983673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23437006771564484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.330881595611572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.354345798492432
17 1.7803995575 	 4.354346033 	 4.354346033
epoch_time;  31.463306665420532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17802610993385315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33397340774536133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0839972496032715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.186644077301025
18 1.7717010767 	 4.1866441881 	 4.1866441881
epoch_time;  31.35719895362854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12712952494621277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2275318205356598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.179315567016602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.234345436096191
19 1.7679647499 	 4.2343456371 	 4.2343456371
epoch_time;  31.7043936252594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12710030376911163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22747276723384857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.180724620819092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2339982986450195
It took 687.4736199378967 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▄▄▃▃▃▃▃▃▃▃▃▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▄▄▅▂▂▂▃▃▄▂▂▂▂▁▁▃▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▃▃▃▃▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▃▄▂▂▃▃▂▃▂▂▂▂▁▂▃▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.11237
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26562
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.83619
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11934
wandb:                         Train loss 1.77175
wandb: 
wandb: 🚀 View run lambent-dragon-1283 at: https://wandb.ai/nreints/thesis/runs/w4slo8gi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_152429-w4slo8gi/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_153559-ncghpbs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-lamp-1290
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ncghpbs9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32073551416397095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5069258213043213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.648988723754883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.895397186279297
0 4.9316062437 	 8.8953976193 	 8.8953976193
epoch_time;  31.56324791908264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22216781973838806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44994354248046875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.970888614654541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.227470397949219
1 2.1044548308 	 7.2274704392 	 7.2274704392
epoch_time;  31.846704483032227
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17936722934246063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35723450779914856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.3379387855529785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.474710464477539
2 2.016395605 	 6.4747103304 	 6.4747103304
epoch_time;  31.488083124160767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15856009721755981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.337917685508728
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.073115348815918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.2617411613464355
3 1.9655822878 	 6.2617411845 	 6.2617411845
epoch_time;  31.848936557769775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21324306726455688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38201743364334106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.751088619232178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.966126441955566
4 1.921747227 	 5.966126478 	 5.966126478
epoch_time;  31.22910714149475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13564038276672363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2735655605792999
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.6299357414245605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.814015865325928
5 1.8994618327 	 5.8140156514 	 5.8140156514
epoch_time;  31.365901231765747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1426246613264084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2865157127380371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.572561740875244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.746890068054199
6 1.8778400976 	 5.7468901763 	 5.7468901763
epoch_time;  31.109588861465454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15948936343193054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29385092854499817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.483725547790527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.670194625854492
7 1.8604862779 	 5.6701943887 	 5.6701943887
epoch_time;  31.6391339302063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.160090833902359
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32223454117774963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.482331275939941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.695249080657959
8 1.8478959277 	 5.6952491554 	 5.6952491554
epoch_time;  31.639928579330444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1388939619064331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31462037563323975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.379027366638184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.591688632965088
9 1.8358634479 	 5.5916886613 	 5.5916886613
epoch_time;  31.625540733337402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18163536489009857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.337690144777298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.362584590911865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.570362091064453
10 1.830452518 	 5.570361988 	 5.570361988
epoch_time;  31.22052526473999
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14053590595722198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28665387630462646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.269811153411865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.4908246994018555
11 1.8191436995 	 5.4908249314 	 5.4908249314
epoch_time;  31.48541808128357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1376352310180664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2679447531700134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.991237640380859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.199241638183594
12 1.8103085197 	 5.1992418444 	 5.1992418444
epoch_time;  31.72244644165039
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14853733777999878
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.274405300617218
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.965592384338379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.195681571960449
13 1.8005700031 	 5.1956813503 	 5.1956813503
epoch_time;  31.223459720611572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14199300110340118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2892184257507324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.384238243103027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.660490989685059
14 1.7990582156 	 4.6604911186 	 4.6604911186
epoch_time;  31.60881519317627
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11132911592721939
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24289047718048096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.158702850341797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4283623695373535
15 1.7935831513 	 4.4283622123 	 4.4283622123
epoch_time;  31.438551902770996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12970894575119019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2371794581413269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.119022369384766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.384998321533203
16 1.7905972635 	 4.3849985484 	 4.3849985484
epoch_time;  31.66073989868164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16780924797058105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3037486970424652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9086854457855225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.192502021789551
17 1.7824034281 	 4.1925022435 	 4.1925022435
epoch_time;  31.400394439697266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14692193269729614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3158716559410095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.885655164718628
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.192292213439941
18 1.7751058218 	 4.1922920846 	 4.1922920846
epoch_time;  31.289807081222534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11938370019197464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26571300625801086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.836927652359009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.111024379730225
19 1.7717503346 	 4.1110245988 	 4.1110245988
epoch_time;  31.21634340286255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11934228986501694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2656160593032837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8361895084381104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.112365245819092
It took 689.9955370426178 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▃▂▂▂▂▃▁▂▂▂▂▂▃▃▃▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▂▂▁▂▂▁▂▂▂▂▂▂▃▃▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.13822
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25096
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.91296
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13441
wandb:                         Train loss 1.7739
wandb: 
wandb: 🚀 View run golden-lamp-1290 at: https://wandb.ai/nreints/thesis/runs/ncghpbs9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_153559-ncghpbs9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154720-d88towfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-envelope-1297
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/d88towfu
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40870770812034607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5979855060577393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.983450889587402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.239994049072266
0 4.8747950343 	 9.2399941934 	 9.2399941934
epoch_time;  31.119831562042236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2922840416431427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5183330774307251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.310811996459961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.575158596038818
1 2.1082726481 	 7.5751583615 	 7.5751583615
epoch_time;  31.43441152572632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2065577507019043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3586491346359253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.538402080535889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.758376598358154
2 2.0183282334 	 6.7583766628 	 6.7583766628
epoch_time;  31.360533714294434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17934300005435944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3129165768623352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.038560390472412
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.25617790222168
3 1.9707895743 	 6.2561780775 	 6.2561780775
epoch_time;  31.641563653945923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14968356490135193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2684383690357208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.879815578460693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.062489986419678
4 1.9296246879 	 6.0624901024 	 6.0624901024
epoch_time;  31.506122827529907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1501390039920807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3086349666118622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.783550262451172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.0079827308654785
5 1.9045001878 	 6.0079827386 	 6.0079827386
epoch_time;  31.574883460998535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13659201562404633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2906937599182129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.585971355438232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.836724758148193
6 1.8770385825 	 5.8367246886 	 5.8367246886
epoch_time;  31.275797128677368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1411319524049759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26959922909736633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.373350620269775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.550344944000244
7 1.8663299499 	 5.5503450961 	 5.5503450961
epoch_time;  31.378589868545532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17743536829948425
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3515337109565735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.047175884246826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.3393402099609375
8 1.8519213553 	 5.3393402924 	 5.3393402924
epoch_time;  31.232008457183838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12026627361774445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23336148262023926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.832313537597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0409955978393555
9 1.8376405338 	 5.0409954999 	 5.0409954999
epoch_time;  31.142752647399902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14668352901935577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28728097677230835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7311930656433105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.922787666320801
10 1.8260437714 	 4.9227875581 	 4.9227875581
epoch_time;  31.289668798446655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1544031947851181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30443236231803894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.458673477172852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.723345756530762
11 1.8182241637 	 4.7233457823 	 4.7233457823
epoch_time;  31.320242643356323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15577766299247742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.286354660987854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.274980545043945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.507989406585693
12 1.8133388718 	 4.507989337 	 4.507989337
epoch_time;  31.42555332183838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15309596061706543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2909768223762512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.184463977813721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.417201042175293
13 1.8089063227 	 4.4172010267 	 4.4172010267
epoch_time;  31.143681526184082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1495756357908249
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2738785147666931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.127851963043213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.343967914581299
14 1.7975032861 	 4.343968077 	 4.343968077
epoch_time;  31.31393790245056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1803300976753235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34041520953178406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.102492809295654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.370995998382568
15 1.7915888678 	 4.3709957638 	 4.3709957638
epoch_time;  31.433149337768555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20030036568641663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3190856873989105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0707926750183105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2976484298706055
16 1.7875844327 	 4.2976486618 	 4.2976486618
epoch_time;  31.544508695602417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21041855216026306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3319986164569855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.082504749298096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3236870765686035
17 1.7821516839 	 4.3236872493 	 4.3236872493
epoch_time;  31.513770818710327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11834636330604553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2342473566532135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.023707389831543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.258363246917725
18 1.7759969268 	 4.2583631361 	 4.2583631361
epoch_time;  31.261008739471436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13447169959545135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2510153353214264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9125888347625732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.139002799987793
19 1.7739034664 	 4.1390027845 	 4.1390027845
epoch_time;  31.57575225830078
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13441243767738342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2509613633155823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.912959337234497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.138221263885498
It took 681.0322835445404 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▂▂▂▁▃▄▁▂▂▁▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▂▃▃▂▂▂▁▂▃▁▂▁▁▁▁▂▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.15472
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22563
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.07147
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12549
wandb:                         Train loss 1.78635
wandb: 
wandb: 🚀 View run brilliant-envelope-1297 at: https://wandb.ai/nreints/thesis/runs/d88towfu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154720-d88towfu/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155839-nruax2ib
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-ox-1303
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nruax2ib
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32430580258369446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5469180345535278
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.943201065063477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.91450023651123
0 4.8752135611 	 8.9145006334 	 8.9145006334
epoch_time;  31.411140203475952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1857767105102539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3557355999946594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.229887962341309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.063564300537109
1 2.103082387 	 7.0635643212 	 7.0635643212
epoch_time;  31.374472856521606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.148092582821846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3235468566417694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.664890766143799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.482117176055908
2 2.0149902674 	 6.4821170291 	 6.4821170291
epoch_time;  31.670768976211548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17794710397720337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31585636734962463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.243953227996826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.070633411407471
3 1.9653518666 	 6.070633182 	 6.070633182
epoch_time;  31.43152403831482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16750986874103546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3014940619468689
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.2187652587890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.974014759063721
4 1.9409931779 	 5.9740148596 	 5.9740148596
epoch_time;  31.163949251174927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13980264961719513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.278043657541275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.816658973693848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.682938098907471
5 1.9145651619 	 5.6829378695 	 5.6829378695
epoch_time;  30.963383674621582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14321811497211456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2766543924808502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.839433193206787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.728682994842529
6 1.8955220279 	 5.7286832242 	 5.7286832242
epoch_time;  31.335351705551147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15123774111270905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2866590917110443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.67369270324707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.5599164962768555
7 1.8854306065 	 5.5599167283 	 5.5599167283
epoch_time;  31.145753383636475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12929536402225494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23956520855426788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.324394702911377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.254968166351318
8 1.8689192951 	 5.2549682617 	 5.2549682617
epoch_time;  31.413320302963257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15585409104824066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30969536304473877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1063666343688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.081547737121582
9 1.859958987 	 5.0815479175 	 5.0815479175
epoch_time;  31.041967630386353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1802888810634613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3512066900730133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.73729133605957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.79410982131958
10 1.8447786304 	 4.7941096125 	 4.7941096125
epoch_time;  31.55696940422058
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12073540687561035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22637300193309784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.698916435241699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.68112325668335
11 1.8348004361 	 4.6811233108 	 4.6811233108
epoch_time;  31.475680589675903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16373518109321594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28350740671157837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.585139274597168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.605255126953125
12 1.8271352139 	 4.605254962 	 4.605254962
epoch_time;  31.333451509475708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1301278918981552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2490338534116745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.467688083648682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.500920295715332
13 1.8219294151 	 4.5009204761 	 4.5009204761
epoch_time;  31.24586772918701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12184647470712662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2436034083366394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.327550411224365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.386795520782471
14 1.8134329246 	 4.3867952914 	 4.3867952914
epoch_time;  31.241411924362183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12326546013355255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24522104859352112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.198590278625488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.275698661804199
15 1.8089140916 	 4.2756987701 	 4.2756987701
epoch_time;  31.33985662460327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12906166911125183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2537491023540497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.128338813781738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.208254814147949
16 1.7994788912 	 4.2082545925 	 4.2082545925
epoch_time;  31.049898147583008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14065338671207428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26717808842658997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.086030006408691
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.187360763549805
17 1.8000551093 	 4.1873607739 	 4.1873607739
epoch_time;  31.44748330116272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13065925240516663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26550015807151794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.959264039993286
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.080410957336426
18 1.7906600756 	 4.080411014 	 4.080411014
epoch_time;  31.303757905960083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12551289796829224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2256200611591339
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.070553779602051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.152204990386963
19 1.7863493489 	 4.1522048538 	 4.1522048538
epoch_time;  31.29742956161499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12549079954624176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2256256490945816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.071467399597168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.154723644256592
It took 678.4317154884338 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▄▃▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▂▃▂▃▃▄▃▂▃▁▂▂▁▃▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▂▃▂▃▃▄▃▂▃▁▃▁▁▃▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.87772
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23814
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.66292
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13317
wandb:                         Train loss 1.78348
wandb: 
wandb: 🚀 View run vibrant-ox-1303 at: https://wandb.ai/nreints/thesis/runs/nruax2ib
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155839-nruax2ib/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_160938-209y2gvz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-laughter-1309
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/209y2gvz
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2960449457168579
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5307732224464417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.767810821533203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.112039566040039
0 4.9915431803 	 9.112039432 	 9.112039432
epoch_time;  31.412134647369385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22928722202777863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37879762053489685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.97551155090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.166039943695068
1 2.120904322 	 7.1660400391 	 7.1660400391
epoch_time;  31.163139581680298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21996921300888062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3477199673652649
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.299012660980225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.39451265335083
2 2.0308622181 	 6.3945127745 	 6.3945127745
epoch_time;  31.233055591583252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1630784273147583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2894509434700012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.959251403808594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.056889533996582
3 1.9831433874 	 6.0568893845 	 6.0568893845
epoch_time;  30.921387434005737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1771293580532074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30098265409469604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.638443470001221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.748325347900391
4 1.9443587482 	 5.7483253273 	 5.7483253273
epoch_time;  31.08844256401062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1535215675830841
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29119038581848145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.490650177001953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.593917369842529
5 1.910061536 	 5.5939175992 	 5.5939175992
epoch_time;  30.876464366912842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17922359704971313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29721394181251526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.81196928024292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.856808185577393
6 1.8904896234 	 5.8568082242 	 5.8568082242
epoch_time;  30.963358640670776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17798465490341187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.306367427110672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.436625957489014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.515446186065674
7 1.8729660458 	 5.5154461835 	 5.5154461835
epoch_time;  31.437061071395874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20574624836444855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3400915861129761
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.285529136657715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.379189968109131
8 1.8588207661 	 5.379189981 	 5.379189981
epoch_time;  31.013044595718384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1832962930202484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3008398711681366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.146249294281006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2420830726623535
9 1.8488362906 	 5.2420829154 	 5.2420829154
epoch_time;  30.61210870742798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1644451916217804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27900493144989014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8641886711120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.959357738494873
10 1.8353483888 	 4.9593575143 	 4.9593575143
epoch_time;  29.657629013061523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17600390315055847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3095281720161438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.713449478149414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.857303142547607
11 1.8330254991 	 4.8573031039 	 4.8573031039
epoch_time;  29.608503818511963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13013333082199097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24846428632736206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.608203887939453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.738579750061035
12 1.8203787833 	 4.7385798274 	 4.7385798274
epoch_time;  29.61842131614685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17378288507461548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28924691677093506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.188322067260742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.373276233673096
13 1.814658256 	 4.3732761692 	 4.3732761692
epoch_time;  29.85506772994995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14146524667739868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2748783528804779
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.001000881195068
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.192812442779541
14 1.8031357111 	 4.192812368 	 4.192812368
epoch_time;  29.296141147613525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14114034175872803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23199495673179626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8647398948669434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.047747611999512
15 1.7996198878 	 4.0477476378 	 4.0477476378
epoch_time;  29.662628412246704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16777268052101135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31073155999183655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.859302520751953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.092646598815918
16 1.7936426575 	 4.0926464184 	 4.0926464184
epoch_time;  29.691348791122437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14647629857063293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.265383780002594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7300708293914795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.950237274169922
17 1.7906699933 	 3.9502372123 	 3.9502372123
epoch_time;  29.591672658920288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14299213886260986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2846234142780304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6818606853485107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9319896697998047
18 1.7873097125 	 3.9319896801 	 3.9319896801
epoch_time;  29.417073011398315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1331823468208313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23812907934188843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.661926507949829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.878002643585205
19 1.7834750352 	 3.8780025998 	 3.8780025998
epoch_time;  29.754340171813965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13316687941551208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23814497888088226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.662916421890259
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.877715587615967
It took 659.4769785404205 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▃▄▃▃▃▆▃▃▂▂▁▅▄▁▁▁▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▃▄▄▆▃▃▂▂▁▅▃▁▁▂▂▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.15009
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.33381
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.95215
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17705
wandb:                         Train loss 1.77552
wandb: 
wandb: 🚀 View run filigreed-laughter-1309 at: https://wandb.ai/nreints/thesis/runs/209y2gvz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_160938-209y2gvz/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30888453125953674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49992942810058594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.94424057006836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.07059383392334
0 4.9089932348 	 9.0705942515 	 9.0705942515
epoch_time;  29.760124683380127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24033993482589722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4475240409374237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.391628742218018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.509172439575195
1 2.1085376158 	 7.5091724293 	 7.5091724293
epoch_time;  29.68453359603882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19969618320465088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33287546038627625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.951935291290283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.981875419616699
2 2.021005185 	 6.9818755279 	 6.9818755279
epoch_time;  29.559526205062866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19794127345085144
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36033573746681213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.413447380065918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.536427974700928
3 1.9766768841 	 6.5364277608 	 6.5364277608
epoch_time;  29.793452501296997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19153890013694763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33228975534439087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.259848594665527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.32717752456665
4 1.9340540964 	 6.3271774704 	 6.3271774704
epoch_time;  29.34286379814148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19431741535663605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3340776264667511
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.9929399490356445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.096244812011719
5 1.9033723804 	 6.0962448533 	 6.0962448533
epoch_time;  29.611549139022827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1979740858078003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3221905827522278
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.7714996337890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.873195171356201
6 1.8807188743 	 5.8731953389 	 5.8731953389
epoch_time;  29.721691608428955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26372912526130676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4205017387866974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.650715351104736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.752532482147217
7 1.8656038092 	 5.7525324641 	 5.7525324641
epoch_time;  29.83251953125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16857734322547913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3200201094150543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.460069179534912
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.540328502655029
8 1.8526727861 	 5.5403287321 	 5.5403287321
epoch_time;  30.04312014579773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18058077991008759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33434778451919556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.213847637176514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.313564300537109
9 1.8436017304 	 5.3135643212 	 5.3135643212
epoch_time;  30.207249402999878
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15725107491016388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2760311961174011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.970912456512451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.062079906463623
10 1.8304596257 	 5.0620796822 	 5.0620796822
epoch_time;  29.617032289505005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14435140788555145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27402031421661377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6337199211120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.769845485687256
11 1.8246590957 	 4.7698453336 	 4.7698453336
epoch_time;  29.63943314552307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1269945353269577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2654990553855896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.44657564163208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.568747043609619
12 1.8158892074 	 4.5687470307 	 4.5687470307
epoch_time;  29.624698638916016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22519832849502563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3774213194847107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.253969192504883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.410676956176758
13 1.8061184583 	 4.4106771933 	 4.4106771933
epoch_time;  31.23529314994812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18505960702896118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35012131929397583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.121377944946289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.299088954925537
14 1.8035106 	 4.2990890915 	 4.2990890915
epoch_time;  29.80368661880493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1331651210784912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2488848865032196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.074316501617432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.214801788330078
15 1.7942246609 	 4.2148018502 	 4.2148018502
epoch_time;  29.77838134765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1322508454322815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2622879147529602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.004879951477051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.162542343139648
16 1.7897727724 	 4.1625422297 	 4.1625422297
epoch_time;  29.221212148666382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14401687681674957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2559652328491211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.951870918273926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.109803676605225
17 1.7891686194 	 4.1098035658 	 4.1098035658
epoch_time;  29.449755907058716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1496940553188324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2630186378955841
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.92010235786438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.063706874847412
18 1.7799762372 	 4.0637068465 	 4.0637068465
epoch_time;  29.49448275566101
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.177100270986557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33414536714553833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.952117681503296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.149138927459717
19 1.7755231915 	 4.1491389094 	 4.1491389094
epoch_time;  29.949278116226196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1770540326833725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33381208777427673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.952152729034424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.150087833404541
It took 653.459228515625 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138615
Array Job ID: 2137927_14
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 04:02:11
CPU Efficiency: 11.77% of 1-10:16:48 core-walltime
Job Wall-clock time: 01:54:16
Memory Utilized: 8.52 GB
Memory Efficiency: 27.28% of 31.25 GB
