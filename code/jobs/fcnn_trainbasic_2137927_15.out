wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_143223-rfkxjae5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-kumquat-1253
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rfkxjae5
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▅▃▃▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▅▄▃▃▂▂▂▂▂▁▂▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▄▄▄▃▂▂▂▃▃▂▂▂▂▁▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▅▅▆▃▂▃▁▂▄▂▃▁▂▂▂▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.42456
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.37234
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.34501
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2812
wandb:                         Train loss 2.74105
wandb: 
wandb: 🚀 View run cheerful-kumquat-1253 at: https://wandb.ai/nreints/thesis/runs/rfkxjae5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_143223-rfkxjae5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144401-nc6l7vhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-rooster-1260
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nc6l7vhn
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4344230890274048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9787870645523071
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5704476237297058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1060609817504883
0 5.5466416035 	 1.106060956 	 1.106060956
epoch_time;  32.434701442718506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37017545104026794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7913361191749573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47113457322120667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8814694881439209
1 3.1470529896 	 0.8814694791 	 0.8814694791
epoch_time;  31.526618719100952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35414454340934753
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6866822838783264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43266329169273376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.750901997089386
2 3.0334742087 	 0.7509020006 	 0.7509020006
epoch_time;  31.39593815803528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3745400905609131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7099868655204773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.454487144947052
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.772152304649353
3 2.9760593422 	 0.7721522976 	 0.7721522976
epoch_time;  31.41383409500122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37938016653060913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6185998320579529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43166810274124146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6643217206001282
4 2.9270566657 	 0.6643216932 	 0.6643216932
epoch_time;  31.556752920150757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3890135586261749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5764869451522827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4442313611507416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.632146418094635
5 2.8938280853 	 0.632146392 	 0.632146392
epoch_time;  31.36345076560974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3305836319923401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5061141848564148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3939242660999298
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5582842826843262
6 2.8703697582 	 0.5582842853 	 0.5582842853
epoch_time;  31.853310108184814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3137757480144501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48764824867248535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38115185499191284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5447210073471069
7 2.8391402547 	 0.5447209951 	 0.5447209951
epoch_time;  31.664633989334106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3169253468513489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4722842276096344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38622990250587463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5333332419395447
8 2.8262495636 	 0.5333332371 	 0.5333332371
epoch_time;  31.442419052124023
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2856869399547577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.433156818151474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37913060188293457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5156300067901611
9 2.8136666517 	 0.51562999 	 0.51562999
epoch_time;  31.536247730255127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30497604608535767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4475344717502594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40370267629623413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5379006266593933
10 2.7977497408 	 0.5379006051 	 0.5379006051
epoch_time;  31.643666744232178
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33637985587120056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45155251026153564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4057678282260895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5084546208381653
11 2.7902023739 	 0.5084546476 	 0.5084546476
epoch_time;  32.4902868270874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2970481812953949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3903636634349823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3665561079978943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44886213541030884
12 2.7807706447 	 0.448862148 	 0.448862148
epoch_time;  31.690694570541382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3198612630367279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42899084091186523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37923741340637207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47281938791275024
13 2.7729073991 	 0.4728193953 	 0.4728193953
epoch_time;  32.351645946502686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2873150408267975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38369882106781006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37240365147590637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45960527658462524
14 2.7677202786 	 0.459605284 	 0.459605284
epoch_time;  31.618120908737183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3082277774810791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40878865122795105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36408761143684387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4573287069797516
15 2.7608797129 	 0.4573287139 	 0.4573287139
epoch_time;  31.68415403366089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29433873295783997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3995697796344757
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35182493925094604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4465743899345398
16 2.754963807 	 0.4465744019 	 0.4465744019
epoch_time;  31.582625150680542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2963746190071106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4225315451622009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3911168575286865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5127292275428772
17 2.7516589 	 0.5127292118 	 0.5127292118
epoch_time;  31.74564838409424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2953847348690033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4068334102630615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36162126064300537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4581705927848816
18 2.7507193636 	 0.4581705867 	 0.4581705867
epoch_time;  31.671104907989502
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2811470627784729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3724164068698883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34504586458206177
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42463138699531555
19 2.7410466343 	 0.4246313971 	 0.4246313971
epoch_time;  31.573781967163086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28119993209838867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3723445534706116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3450060784816742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42456215620040894
It took 697.7126700878143 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▄▄▂▃▄▃▂▂▃▂▂▂▂▁▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▇▄▄▂▃▄▂▁▂▃▃▂▁▂▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▃▄▂▂▄▂▁▁▃▂▂▂▃▂▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▇▃▅▂▃▄▂▁▂▄▄▂▁▃▃▂▃▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.53271
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.48211
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.36315
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31818
wandb:                         Train loss 2.76838
wandb: 
wandb: 🚀 View run thriving-rooster-1260 at: https://wandb.ai/nreints/thesis/runs/nc6l7vhn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144401-nc6l7vhn/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_145525-2syyuef9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-monkey-1267
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2syyuef9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4622547924518585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9433172345161438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5461077690124512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0261595249176025
0 5.5440213482 	 1.026159503 	 1.026159503
epoch_time;  31.271787405014038
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3722265660762787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7910412549972534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45427969098091125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8970996737480164
1 3.1493810035 	 0.8970996754 	 0.8970996754
epoch_time;  31.167937755584717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4442428648471832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8689375519752502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4789387583732605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.895134687423706
2 3.0405356085 	 0.8951346732 	 0.8951346732
epoch_time;  31.50190043449402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32178187370300293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6311715841293335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3968721926212311
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7107081413269043
3 2.9815534154 	 0.7107081233 	 0.7107081233
epoch_time;  31.38128662109375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3747125267982483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6393797993659973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.419727623462677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7107240557670593
4 2.9335355662 	 0.7107240419 	 0.7107240419
epoch_time;  31.361082077026367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2975955009460449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5053838491439819
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37192800641059875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5816214680671692
5 2.9158784302 	 0.5816214484 	 0.5816214484
epoch_time;  31.375573873519897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31793951988220215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5595231056213379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38660159707069397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6385930180549622
6 2.8880883839 	 0.6385930242 	 0.6385930242
epoch_time;  31.35344648361206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3612370193004608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.631373405456543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42402273416519165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6961854696273804
7 2.8711248254 	 0.6961854677 	 0.6961854677
epoch_time;  31.514681816101074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.291334867477417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5245903730392456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36879998445510864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.610045313835144
8 2.8522729376 	 0.6100453145 	 0.6100453145
epoch_time;  31.010373830795288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2693479061126709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4600120782852173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3466745615005493
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5453065633773804
9 2.8390668638 	 0.5453065614 	 0.5453065614
epoch_time;  31.315571308135986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29689913988113403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4917144477367401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.357585072517395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5504725575447083
10 2.8314593295 	 0.5504725688 	 0.5504725688
epoch_time;  31.30462145805359
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3601941168308258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5889334678649902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41057026386260986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6319940090179443
11 2.818960718 	 0.6319940103 	 0.6319940103
epoch_time;  30.964922666549683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36267709732055664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.555659830570221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3872429430484772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5834945440292358
12 2.8145268465 	 0.5834945266 	 0.5834945266
epoch_time;  31.18780779838562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3092506527900696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5106073021888733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3844773471355438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5872713923454285
13 2.7985629919 	 0.5872714068 	 0.5872714068
epoch_time;  31.027864694595337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27324578166007996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4565456807613373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3629131019115448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5439990162849426
14 2.7962782853 	 0.5439990069 	 0.5439990069
epoch_time;  31.16111421585083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3249158561229706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5015431046485901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4092254638671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5828943252563477
15 2.792948584 	 0.5828943201 	 0.5828943201
epoch_time;  31.135154724121094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3307861089706421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49155929684638977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3730446398258209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.52435302734375
16 2.7871027632 	 0.5243530273 	 0.5243530273
epoch_time;  31.227747678756714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29532018303871155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4418688714504242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3578062355518341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5019712448120117
17 2.7798661508 	 0.5019712706 	 0.5019712706
epoch_time;  31.54824137687683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3221130073070526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4584220349788666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35545986890792847
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49056950211524963
18 2.7712184154 	 0.490569491 	 0.490569491
epoch_time;  32.96782684326172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31824764609336853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4820762574672699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36317092180252075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5326504111289978
19 2.7683839378 	 0.5326504269 	 0.5326504269
epoch_time;  31.764751434326172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31817781925201416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4821135103702545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3631454408168793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5327093005180359
It took 684.4907329082489 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.118 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▅▄▃▄▄▂▃▃▃▃▂▂▁▂▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▅▅▄▄▄▃▂▂▃▃▂▂▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▇▆▄▃▅▃▂▃▃▃▂▂▁▁▂▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▆▇▅▅▆▄▃▂▃▄▃▂▁▁▃▂▃▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49908
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.42553
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.32305
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27539
wandb:                         Train loss 2.76028
wandb: 
wandb: 🚀 View run scintillating-monkey-1267 at: https://wandb.ai/nreints/thesis/runs/2syyuef9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_145525-2syyuef9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150705-cmkhpsab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-rabbit-1274
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/cmkhpsab
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4222910702228546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8789469003677368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5064519643783569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0029999017715454
0 5.5350058316 	 1.0029998779 	 1.0029998779
epoch_time;  31.133991479873657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3787551820278168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7591301798820496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45773860812187195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9066291451454163
1 3.1479181433 	 0.9066291603 	 0.9066291603
epoch_time;  31.35491371154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38437405228614807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7330008149147034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4696815013885498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8663240075111389
2 3.0390287798 	 0.8663240175 	 0.8663240175
epoch_time;  31.165573835372925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39216065406799316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6966578960418701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4486083984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7980257868766785
3 2.9785179391 	 0.7980257601 	 0.7980257601
epoch_time;  31.349384784698486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3658831715583801
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6536362767219543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4138556718826294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7274050116539001
4 2.9414561691 	 0.7274050326 	 0.7274050326
epoch_time;  31.082283973693848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35247424244880676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6186812520027161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38070836663246155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6669620275497437
5 2.9078087313 	 0.6669620411 	 0.6669620411
epoch_time;  31.36065101623535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3698541522026062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6452798247337341
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4363129138946533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7446408867835999
6 2.8863904743 	 0.7446408658 	 0.7446408658
epoch_time;  30.92143440246582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3371742367744446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5973752737045288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38698792457580566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.681922197341919
7 2.8610497683 	 0.6819221703 	 0.6819221703
epoch_time;  31.377200603485107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.315252423286438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5245934724807739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35246655344963074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5777226090431213
8 2.8491247302 	 0.5777225804 	 0.5777225804
epoch_time;  31.201328992843628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29202327132225037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5027029514312744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37693294882774353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6099991798400879
9 2.828868166 	 0.6099992082 	 0.6099992082
epoch_time;  31.118241548538208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3173978328704834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5219801068305969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37869614362716675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6164644956588745
10 2.821574031 	 0.6164644808 	 0.6164644808
epoch_time;  31.23061442375183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3385113477706909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5634219646453857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3633151054382324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6177941560745239
11 2.8129496022 	 0.61779414 	 0.61779414
epoch_time;  31.23708701133728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3151283860206604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5520759224891663
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35363760590553284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6166093349456787
12 2.8045159239 	 0.6166093569 	 0.6166093569
epoch_time;  31.04252314567566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29276353120803833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48211249709129333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33851784467697144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5573990941047668
13 2.7950775828 	 0.5573991105 	 0.5573991105
epoch_time;  31.284815549850464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2709854245185852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46126407384872437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33574867248535156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5463182926177979
14 2.788096847 	 0.5463183016 	 0.5463183016
epoch_time;  31.413639545440674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27337315678596497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4488939344882965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32538312673568726
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5157750248908997
15 2.7833029631 	 0.515775031 	 0.515775031
epoch_time;  31.442134380340576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31065699458122253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4831952452659607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3590003252029419
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.551090657711029
16 2.7801944265 	 0.5510906735 	 0.5510906735
epoch_time;  31.17294144630432
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30261820554733276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.465228796005249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3372117877006531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5195379853248596
17 2.7709326205 	 0.5195380134 	 0.5195380134
epoch_time;  31.497857809066772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3050861656665802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4699927270412445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36753779649734497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5558270215988159
18 2.7651922578 	 0.5558270429 	 0.5558270429
epoch_time;  31.118942737579346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2754037380218506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.425594300031662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32306888699531555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49885380268096924
19 2.7602821717 	 0.4988538175 	 0.4988538175
epoch_time;  31.056084871292114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27538999915122986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42553338408470154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3230484426021576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.499082088470459
It took 700.0399985313416 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▆▄▅▄▃▄▃▄▃▃▃▃▂▁▁▂▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▆▅▅▄▃▄▃▄▃▃▃▃▂▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▅▄▆▄▄▄▅▅▃▃▃▃▂▁▁▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▆▅▇▅▄▅▅▅▃▄▄▃▁▂▂▃▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47313
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39288
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33181
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26715
wandb:                         Train loss 2.76324
wandb: 
wandb: 🚀 View run dancing-rabbit-1274 at: https://wandb.ai/nreints/thesis/runs/cmkhpsab
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150705-cmkhpsab/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151829-1hztlqbo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-moon-1279
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1hztlqbo
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4096883535385132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8535229563713074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4817662537097931
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9589008092880249
0 5.5248471352 	 0.9589008228 	 0.9589008228
epoch_time;  31.461567878723145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4012748897075653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8005384206771851
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4818142056465149
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9450918436050415
1 3.1445600113 	 0.9450918662 	 0.9450918662
epoch_time;  31.254119873046875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36764344573020935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7018676996231079
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42086347937583923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.786578893661499
2 3.0331231222 	 0.786578864 	 0.786578864
epoch_time;  31.789775371551514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35075414180755615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6343807578086853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3938450515270233
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7016056180000305
3 2.9727517905 	 0.701605637 	 0.701605637
epoch_time;  31.960339546203613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38430508971214294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6542624831199646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4258096516132355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7382238507270813
4 2.9325348706 	 0.738223844 	 0.738223844
epoch_time;  32.10735082626343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35141271352767944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6137367486953735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39493510127067566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.699668824672699
5 2.9037844607 	 0.699668843 	 0.699668843
epoch_time;  31.426690816879272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32671472430229187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5431625843048096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3771377503871918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6321234703063965
6 2.8813863353 	 0.6321234626 	 0.6321234626
epoch_time;  31.179492950439453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35680460929870605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5988247990608215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39338868856430054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6581985950469971
7 2.8668645137 	 0.6581985886 	 0.6581985886
epoch_time;  30.859857320785522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35254520177841187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5530509948730469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4076484739780426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6344886422157288
8 2.8447520179 	 0.6344886574 	 0.6344886574
epoch_time;  31.13857340812683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35580840706825256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.586858332157135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4065021872520447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6580262780189514
9 2.8364185083 	 0.658026288 	 0.658026288
epoch_time;  30.85710573196411
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30905550718307495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5082706212997437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36511915922164917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5894039869308472
10 2.8254868928 	 0.589403967 	 0.589403967
epoch_time;  31.129589319229126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3346518278121948
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5322659015655518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37471866607666016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5931422710418701
11 2.8155480387 	 0.5931422878 	 0.5931422878
epoch_time;  30.9145724773407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32038673758506775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5535954236984253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35509032011032104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6156032085418701
12 2.8025489141 	 0.6156032253 	 0.6156032253
epoch_time;  31.02605175971985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3113510012626648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5139359831809998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36883842945098877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5981630682945251
13 2.7939142371 	 0.598163048 	 0.598163048
epoch_time;  31.08698058128357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2751515805721283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.440937340259552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3364773988723755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5212093591690063
14 2.7842084488 	 0.5212093456 	 0.5212093456
epoch_time;  31.514925241470337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2798342704772949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4074608087539673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3177933692932129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4647176265716553
15 2.7837681031 	 0.4647176382 	 0.4647176382
epoch_time;  31.190248727798462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28299495577812195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43043455481529236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32795512676239014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4876505732536316
16 2.7789193414 	 0.4876505671 	 0.4876505671
epoch_time;  31.288182735443115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2993479073047638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4470713436603546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3379461467266083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5066488981246948
17 2.7710222104 	 0.506648873 	 0.506648873
epoch_time;  31.478320837020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28020307421684265
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4310648441314697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3446319103240967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5183393955230713
18 2.7609597156 	 0.5183394149 	 0.5183394149
epoch_time;  31.290741443634033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26717159152030945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3929401636123657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33181196451187134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4733220636844635
19 2.7632401246 	 0.4733220693 	 0.4733220693
epoch_time;  31.032395124435425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26715412735939026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39288169145584106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3318091332912445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47312799096107483
It took 683.5902910232544 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▃▄▃▃▃▂▃▂▃▂▁▂▂▃▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▅▃▄▃▃▂▁▃▂▃▂▁▂▁▃▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▅▃▄▃▄▃▂▅▂▄▂▂▂▂▄▁▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▃▃▄▃▄▂▂▄▂▄▂▂▂▁▃▁▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.5888
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.53027
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.3498
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29873
wandb:                         Train loss 2.78359
wandb: 
wandb: 🚀 View run lunar-moon-1279 at: https://wandb.ai/nreints/thesis/runs/1hztlqbo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151829-1hztlqbo/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_152954-38v463l8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-peony-1286
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/38v463l8
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5068338513374329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9482313990592957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5464642643928528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0021743774414062
0 5.5272270518 	 1.0021744187 	 1.0021744187
epoch_time;  31.23109745979309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3365975618362427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7209144234657288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42357656359672546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8146094083786011
1 3.1563495076 	 0.8146094245 	 0.8146094245
epoch_time;  31.121532440185547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3501843810081482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7246528267860413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4562472105026245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8282392621040344
2 3.0394777209 	 0.8282392347 	 0.8282392347
epoch_time;  31.37251377105713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33837950229644775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6098732948303223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40454983711242676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6738886833190918
3 2.9853413142 	 0.6738886653 	 0.6738886653
epoch_time;  31.40333914756775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.361911416053772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6871269941329956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43038174510002136
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7571253776550293
4 2.9366275733 	 0.7571253596 	 0.7571253596
epoch_time;  31.367311000823975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3563586175441742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.622373104095459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3878616690635681
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6536184549331665
5 2.910170796 	 0.6536184362 	 0.6536184362
epoch_time;  31.10227608680725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3728155493736267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6162088513374329
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.409837931394577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6535460352897644
6 2.8862483206 	 0.6535460601 	 0.6535460601
epoch_time;  31.223901510238647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30767497420310974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5757554173469543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40300092101097107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6798638701438904
7 2.8729586075 	 0.6798638421 	 0.6798638421
epoch_time;  31.28593420982361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.301448792219162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5196051001548767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3773510158061981
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5981367826461792
8 2.8617879755 	 0.5981367781 	 0.5981367781
epoch_time;  31.65309453010559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37401556968688965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6091634631156921
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43933042883872986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6926645636558533
9 2.8458423033 	 0.692664564 	 0.692664564
epoch_time;  31.964573860168457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3189646601676941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5330007672309875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36264362931251526
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.593672513961792
10 2.8336511404 	 0.5936725101 	 0.5936725101
epoch_time;  31.75863790512085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37395769357681274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6092423796653748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41807058453559875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6572951674461365
11 2.8223409995 	 0.6572951446 	 0.6572951446
epoch_time;  31.718326807022095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3202621638774872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5443878769874573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36303940415382385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5937482118606567
12 2.8167751883 	 0.5937481854 	 0.5937481854
epoch_time;  31.479037284851074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3013803958892822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5073062181472778
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34979528188705444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5623152852058411
13 2.8130679534 	 0.5623152862 	 0.5623152862
epoch_time;  31.109131574630737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3024437725543976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5546156167984009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3627467751502991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6132741570472717
14 2.8045453464 	 0.6132741567 	 0.6132741567
epoch_time;  31.051595211029053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2873760163784027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5272964239120483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3579048812389374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6114950776100159
15 2.8050805714 	 0.6114950644 	 0.6114950644
epoch_time;  30.995353937149048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3516392111778259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6037257313728333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42943552136421204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6938280463218689
16 2.7970998011 	 0.693828026 	 0.693828026
epoch_time;  31.38222050666809
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2846323251724243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.500667154788971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3319025933742523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5458217263221741
17 2.7969222744 	 0.5458217312 	 0.5458217312
epoch_time;  31.51339364051819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2789384424686432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5192100405693054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35007983446121216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5891395211219788
18 2.7857370964 	 0.5891395363 	 0.5891395363
epoch_time;  31.422290086746216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2987927198410034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5302994847297668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34983029961586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5889060497283936
19 2.7835925599 	 0.5889060356 	 0.5889060356
epoch_time;  31.19173574447632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29872649908065796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5302691459655762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34979644417762756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5888020992279053
It took 684.9928543567657 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▅▄▃▄▄▂▁▂▂▁▁▁▁▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▆▅▄▃▃▄▂▂▂▂▂▁▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▆▄▄▃▄▅▁▁▁▂▁▂▂▂▃▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▇▃▄▁▃▆▁▂▁▂▂▂▂▂▂▃▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47906
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.40841
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.36067
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30366
wandb:                         Train loss 2.76288
wandb: 
wandb: 🚀 View run red-peony-1286 at: https://wandb.ai/nreints/thesis/runs/38v463l8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_152954-38v463l8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154118-g90vy7rr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run abundant-wish-1292
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g90vy7rr
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4832392930984497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.975593090057373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5680612921714783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0865943431854248
0 5.5347721046 	 1.0865942878 	 1.0865942878
epoch_time;  31.419715404510498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34814396500587463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7647184729576111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44207218289375305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9066673517227173
1 3.1512613987 	 0.9066673485 	 0.9066673485
epoch_time;  31.37777853012085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4608052372932434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7742515206336975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5165565609931946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8665894269943237
2 3.0430547034 	 0.8665894379 	 0.8665894379
epoch_time;  31.414535760879517
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3588137924671173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7199445366859436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4278765320777893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8235690593719482
3 2.9886019222 	 0.8235690555 	 0.8235690555
epoch_time;  31.117750883102417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3723967969417572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.669747531414032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4361523985862732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.776667594909668
4 2.9457706976 	 0.7766675794 	 0.7766675794
epoch_time;  31.27695894241333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30117031931877136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5484319925308228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.394977331161499
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.680381178855896
5 2.9158787608 	 0.6803811975 	 0.6803811975
epoch_time;  31.160703420639038
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3490014672279358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5902518630027771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42780420184135437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7130124568939209
6 2.8900957256 	 0.7130124479 	 0.7130124479
epoch_time;  31.292927026748657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43368101119995117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6865794658660889
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4805198609828949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7605801224708557
7 2.871101023 	 0.7605801144 	 0.7605801144
epoch_time;  31.354188919067383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29302412271499634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45264938473701477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3590976297855377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5452142953872681
8 2.8544617732 	 0.5452142664 	 0.5452142664
epoch_time;  31.293384552001953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32149192690849304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46088847517967224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35842952132225037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5167481303215027
9 2.8390109941 	 0.5167481294 	 0.5167481294
epoch_time;  31.064475774765015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2971290051937103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.461883008480072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.357473224401474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5422380566596985
10 2.8239000614 	 0.5422380602 	 0.5422380602
epoch_time;  31.45811414718628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3295568823814392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4619588851928711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3794686794281006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5255136489868164
11 2.8130160508 	 0.5255136438 	 0.5255136438
epoch_time;  31.316580533981323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3149245083332062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45204195380210876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34361532330513
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49079465866088867
12 2.8074905716 	 0.4907946612 	 0.4907946612
epoch_time;  31.681726694107056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30684056878089905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4200725555419922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3686504065990448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4980408251285553
13 2.7924688981 	 0.4980408127 	 0.4980408127
epoch_time;  31.295557498931885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31873607635498047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43729960918426514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3678203225135803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49998417496681213
14 2.7883927249 	 0.4999841639 	 0.4999841639
epoch_time;  31.140706062316895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3095633089542389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4263923466205597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36306363344192505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4992634952068329
15 2.7786760272 	 0.4992634954 	 0.4992634954
epoch_time;  31.369543075561523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3231066167354584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44753918051719666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39445236325263977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5439367890357971
16 2.7768938172 	 0.5439367758 	 0.5439367758
epoch_time;  31.960960865020752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3376787602901459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47372791171073914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3873905539512634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5475753545761108
17 2.7710327892 	 0.5475753372 	 0.5475753372
epoch_time;  31.60871410369873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32017138600349426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43344762921333313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3579246699810028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4881852865219116
18 2.7662792374 	 0.4881852846 	 0.4881852846
epoch_time;  32.395848751068115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3036311864852905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4082927405834198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3606213629245758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4790949523448944
19 2.7628849058 	 0.4790949641 	 0.4790949641
epoch_time;  31.556857585906982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30366066098213196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.408407598733902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36066922545433044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47905829548835754
It took 684.6133258342743 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▁▂▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▅▄▃▃▃▃▃▂▂▃▂▂▂▂▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▅▅▃▃▄▃▃▂▂▂▂▂▁▂▂▁▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▇▅▆▅▅▅▃▃▂▃▄▂▃▂▃▂▁▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.5565
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.47991
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.37971
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.3087
wandb:                         Train loss 2.76856
wandb: 
wandb: 🚀 View run abundant-wish-1292 at: https://wandb.ai/nreints/thesis/runs/g90vy7rr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154118-g90vy7rr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155243-w2y1hiqd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-rabbit-1299
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/w2y1hiqd
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40296000242233276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9729568958282471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5233065485954285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0886751413345337
0 5.4859757162 	 1.0886751742 	 1.0886751742
epoch_time;  31.007790088653564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4098760783672333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8412930965423584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4622131288051605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9053192138671875
1 3.1552112866 	 0.9053192139 	 0.9053192139
epoch_time;  31.394583225250244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38091835379600525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7546479105949402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45552578568458557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.832010805606842
2 3.0510136547 	 0.832010795 	 0.832010795
epoch_time;  31.4553542137146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3562052845954895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.688456118106842
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42485496401786804
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7533539533615112
3 2.9796783096 	 0.7533539643 	 0.7533539643
epoch_time;  31.756890296936035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37302151322364807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6430374979972839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4411698579788208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7257082462310791
4 2.9423038015 	 0.7257082553 	 0.7257082553
epoch_time;  31.485353469848633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34274396300315857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5814826488494873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38873642683029175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6395525932312012
5 2.9126713778 	 0.6395525958 	 0.6395525958
epoch_time;  31.393197298049927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3446846306324005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5638189911842346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3820410370826721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6032558679580688
6 2.8843252793 	 0.6032558544 	 0.6032558544
epoch_time;  31.30736994743347
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3423832952976227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5565871000289917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3983590304851532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.616861879825592
7 2.8672255178 	 0.6168618692 	 0.6168618692
epoch_time;  31.465046167373657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29907652735710144
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5225688815116882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36650794744491577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5994065999984741
8 2.8565646554 	 0.5994065981 	 0.5994065981
epoch_time;  31.312499523162842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3129381239414215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5305855870246887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3675772547721863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5915660262107849
9 2.8396420902 	 0.5915660136 	 0.5915660136
epoch_time;  31.293912649154663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29557713866233826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46601203083992004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3378855288028717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5091595649719238
10 2.8308563336 	 0.5091595624 	 0.5091595624
epoch_time;  31.1113498210907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30521446466445923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4810941815376282
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3523438274860382
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5337656736373901
11 2.8175176932 	 0.5337656794 	 0.5337656794
epoch_time;  31.476343393325806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3225155472755432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.536414384841919
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36108142137527466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5779718160629272
12 2.8064627676 	 0.5779718347 	 0.5779718347
epoch_time;  31.345494031906128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2898425757884979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4706687033176422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35637158155441284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5391486883163452
13 2.8031033133 	 0.5391486915 	 0.5391486915
epoch_time;  31.165160417556763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3033990263938904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.490458220243454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3373998701572418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5231550335884094
14 2.7930527198 	 0.5231550474 	 0.5231550474
epoch_time;  31.364198684692383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2815391719341278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43441903591156006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32163116335868835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47812581062316895
15 2.7878419622 	 0.4781258248 	 0.4781258248
epoch_time;  31.467541694641113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3057418167591095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4669044613838196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34486761689186096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5119065642356873
16 2.785200549 	 0.5119065568 	 0.5119065568
epoch_time;  31.584057807922363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29677101969718933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43247854709625244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3365146219730377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4726041555404663
17 2.778814875 	 0.4726041639 	 0.4726041639
epoch_time;  31.690088748931885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2688380181789398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37940889596939087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32954180240631104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44350409507751465
18 2.7738765234 	 0.4435040861 	 0.4435040861
epoch_time;  31.32599973678589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.308676153421402
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.480054646730423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37967807054519653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5566443204879761
19 2.7685598116 	 0.5566442954 	 0.5566442954
epoch_time;  31.440555810928345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3087047338485718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4799139201641083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.379712849855423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5564988851547241
It took 684.5891969203949 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▂▃▁▁▁▁▂▂▂▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▄▄▂▃▄▂▃▁▁▂▁▃▂▃▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▇▄▃▄▄▄▂▃▂▁▁▁▃▂▃▂▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▇▄▄▂▄▆▂▃▂▁▂▁▃▃▄▄▃▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.59841
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.53814
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.37401
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32656
wandb:                         Train loss 2.77736
wandb: 
wandb: 🚀 View run filigreed-rabbit-1299 at: https://wandb.ai/nreints/thesis/runs/w2y1hiqd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155243-w2y1hiqd/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_160404-kt9v9skq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-firecracker-1306
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kt9v9skq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40005233883857727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8944706320762634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4769633412361145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0033742189407349
0 5.5963160386 	 1.0033742544 	 1.0033742544
epoch_time;  31.04469609260559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4060809910297394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8059830665588379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4713028073310852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9003278613090515
1 3.1551339286 	 0.9003278578 	 0.9003278578
epoch_time;  32.535093784332275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39624857902526855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7646703720092773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4596409201622009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8349007964134216
2 3.0466257352 	 0.8349008096 	 0.8349008096
epoch_time;  31.59553551673889
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3443267345428467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6749957799911499
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39218857884407043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7367241382598877
3 2.9805011478 	 0.7367241112 	 0.7367241112
epoch_time;  32.27821588516235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3333820104598999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6616752743721008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38259875774383545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7266787886619568
4 2.9418998207 	 0.7266787967 	 0.7266787967
epoch_time;  31.298413276672363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30442506074905396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.575872004032135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39547106623649597
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6802878975868225
5 2.9092451443 	 0.6802879127 	 0.6802879127
epoch_time;  31.297510385513306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3276532292366028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6032453179359436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39150217175483704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6891998648643494
6 2.8858886388 	 0.6891998703 	 0.6891998703
epoch_time;  30.942382335662842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.370882511138916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6551544070243835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38949039578437805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6750726699829102
7 2.8693419542 	 0.6750726648 	 0.6750726648
epoch_time;  31.47194266319275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29553669691085815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5595956444740295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3556811213493347
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6309716701507568
8 2.8581561151 	 0.6309716714 	 0.6309716714
epoch_time;  31.245768070220947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3171943426132202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6010278463363647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.388253778219223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6916007399559021
9 2.8396120065 	 0.6916007377 	 0.6916007377
epoch_time;  31.437292337417603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2958855628967285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5173177123069763
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3609864115715027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5913302302360535
10 2.8257235951 	 0.5913302447 	 0.5913302447
epoch_time;  31.206724166870117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28428027033805847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4912227690219879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34591296315193176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5740486979484558
11 2.8212083986 	 0.5740486764 	 0.5740486764
epoch_time;  30.992877960205078
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2997226119041443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5530199408531189
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3448465168476105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5987097024917603
12 2.8138888081 	 0.598709725 	 0.598709725
epoch_time;  31.16792321205139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2831021845340729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5011293888092041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3402876555919647
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5712674260139465
13 2.8066578157 	 0.5712674528 	 0.5712674528
epoch_time;  31.08806562423706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3123158812522888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5813615322113037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37490415573120117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6637305021286011
14 2.8034666316 	 0.6637305182 	 0.6637305182
epoch_time;  31.364104509353638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31709715723991394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5688458681106567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36466681957244873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.616975724697113
15 2.8001911471 	 0.6169757328 	 0.6169757328
epoch_time;  31.162501573562622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3367213308811188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6052915453910828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37363162636756897
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6547279357910156
16 2.7905527992 	 0.6547279564 	 0.6547279564
epoch_time;  31.13094186782837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3280266523361206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5535215139389038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35743629932403564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5891192555427551
17 2.7862114929 	 0.5891192462 	 0.5891192462
epoch_time;  31.201855182647705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3185506761074066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5634838938713074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38682833313941956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6469441652297974
18 2.7777579791 	 0.6469441594 	 0.6469441594
epoch_time;  30.88490653038025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32660651206970215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5381259322166443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37411069869995117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5985317826271057
19 2.7773642466 	 0.5985317746 	 0.5985317746
epoch_time;  30.8439519405365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3265577256679535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.538144052028656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3740144968032837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5984079837799072
It took 680.6112034320831 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▆▄▃▄▄▂▂▃▂▂▂▂▂▂▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▆▅▄▂▃▄▂▂▃▂▁▂▂▁▂▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▆▄▃▄▄▂▂▃▂▂▂▂▂▂▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▆▅▄▂▃▃▂▂▃▁▁▂▁▁▃▂▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.5522
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.50524
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.36905
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32144
wandb:                         Train loss 2.75951
wandb: 
wandb: 🚀 View run brilliant-firecracker-1306 at: https://wandb.ai/nreints/thesis/runs/kt9v9skq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_160404-kt9v9skq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_161528-yx4k6ycz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-orchid-1313
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/yx4k6ycz
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.542951762676239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0557172298431396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5875320434570312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0922826528549194
0 5.534560527 	 1.0922826819 	 1.0922826819
epoch_time;  31.27711582183838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37423890829086304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8094009160995483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46082383394241333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.888970673084259
1 3.1538198338 	 0.8889706998 	 0.8889706998
epoch_time;  31.668012857437134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46448785066604614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8271004557609558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5251476168632507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8867013454437256
2 3.0399666633 	 0.8867013467 	 0.8867013467
epoch_time;  31.46039056777954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42462703585624695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7906857132911682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5300130248069763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8984233140945435
3 2.9756219718 	 0.8984233135 	 0.8984233135
epoch_time;  31.485520601272583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41562777757644653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6931306719779968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4299362599849701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7223299741744995
4 2.9338780573 	 0.7223299594 	 0.7223299594
epoch_time;  31.101744413375854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31703487038612366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5389382243156433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38588157296180725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6254401803016663
5 2.9046122577 	 0.6254401954 	 0.6254401954
epoch_time;  31.380900859832764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37678590416908264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6249048113822937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44133469462394714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6960449814796448
6 2.8796087535 	 0.6960450044 	 0.6960450044
epoch_time;  31.66341233253479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3801553547382355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6546226143836975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4434211254119873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7187167406082153
7 2.8618861607 	 0.7187167606 	 0.7187167606
epoch_time;  31.47591519355774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3484477400779724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.526900053024292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37949132919311523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5684287548065186
8 2.8449391318 	 0.5684287819 	 0.5684287819
epoch_time;  31.90762734413147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3388194739818573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5176655650138855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3682316839694977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5476022362709045
9 2.8301739961 	 0.5476022256 	 0.5476022256
epoch_time;  31.44631266593933
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3844827711582184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5808773636817932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3986704647541046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5996761322021484
10 2.8154812595 	 0.5996761425 	 0.5996761425
epoch_time;  32.348915338516235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3048662841320038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4824502468109131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3681146204471588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5452529788017273
11 2.8094043564 	 0.5452529907 	 0.5452529907
epoch_time;  31.176017999649048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30807626247406006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44396552443504333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3638336956501007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.50655597448349
12 2.8002169331 	 0.5065559593 	 0.5065559593
epoch_time;  31.190428733825684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3324427604675293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5219476819038391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37210866808891296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5692593455314636
13 2.7926791533 	 0.5692593549 	 0.5692593549
epoch_time;  30.960695028305054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30198609828948975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4633292853832245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3605944514274597
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5304887294769287
14 2.7831637848 	 0.5304887101 	 0.5304887101
epoch_time;  31.074265956878662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31103187799453735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4505045413970947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3565931022167206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5087084174156189
15 2.7779417871 	 0.5087083971 	 0.5087083971
epoch_time;  31.38407802581787
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3512016236782074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5377486944198608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38444799184799194
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5769302248954773
16 2.771765707 	 0.5769302368 	 0.5769302368
epoch_time;  31.23068642616272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3438783586025238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.487067848443985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3599872887134552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5072952508926392
17 2.7663218835 	 0.5072952683 	 0.5072952683
epoch_time;  31.462790966033936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2970747947692871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4076521694660187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33050864934921265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4501989781856537
18 2.7646529005 	 0.4501989829 	 0.4501989829
epoch_time;  31.172377109527588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32140520215034485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.505213737487793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36911314725875854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5521780848503113
19 2.7595130543 	 0.552178089 	 0.552178089
epoch_time;  31.359896659851074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3214375078678131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5052414536476135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3690479099750519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5521970987319946
It took 684.5457098484039 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▆▅▅▅▄▃▃▃▃▃▂▂▂▁▁▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ██▇▆▅▅▄▃▄▃▃▂▂▂▂▁▁▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇█▇▅█▃▁▃▃▂▂▂▂▁▁▁▂▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▇█▇▄█▃▂▄▃▃▁▂▂▂▁▂▃▃▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.51933
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.45146
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.39867
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32914
wandb:                         Train loss 2.75297
wandb: 
wandb: 🚀 View run radiant-orchid-1313 at: https://wandb.ai/nreints/thesis/runs/yx4k6ycz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_161528-yx4k6ycz/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41172972321510315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8291048407554626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5030385851860046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9114435911178589
0 5.509355363 	 0.9114435969 	 0.9114435969
epoch_time;  31.062788486480713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4211476445198059
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8451595902442932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48259466886520386
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9015771746635437
1 3.1431183089 	 0.9015771814 	 0.9015771814
epoch_time;  31.26274085044861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43721091747283936
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7633879780769348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49610501527786255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8131247758865356
2 3.0321217655 	 0.8131247856 	 0.8131247856
epoch_time;  31.21994686126709
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.415316641330719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7007521390914917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4789932668209076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.746708869934082
3 2.9719828384 	 0.7467088854 	 0.7467088854
epoch_time;  31.084238290786743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3494832217693329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6394824385643005
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44231855869293213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7338529229164124
4 2.9258480292 	 0.733852902 	 0.733852902
epoch_time;  31.122310161590576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43656548857688904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6834156513214111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4974710941314697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7453230619430542
5 2.8928776394 	 0.7453230574 	 0.7453230574
epoch_time;  31.160119771957397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3233862817287445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5654601454734802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3884505331516266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6261731386184692
6 2.8672913052 	 0.6261731534 	 0.6261731534
epoch_time;  31.51648736000061
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31481021642684937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5287983417510986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36514583230018616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5736264586448669
7 2.8504626275 	 0.5736264615 	 0.5736264615
epoch_time;  31.458372831344604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3482043743133545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5669358372688293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39012980461120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.599047839641571
8 2.8347602694 	 0.5990478103 	 0.5990478103
epoch_time;  31.10640048980713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3224375247955322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5326087474822998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3930952250957489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5940659046173096
9 2.8260505485 	 0.5940658982 	 0.5940658982
epoch_time;  31.179525136947632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33667221665382385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.545962393283844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36900949478149414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5762803554534912
10 2.8133767244 	 0.5762803774 	 0.5762803774
epoch_time;  30.923888683319092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2969808578491211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49287039041519165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3703558146953583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5663152933120728
11 2.8035426673 	 0.5663153159 	 0.5663153159
epoch_time;  31.342519521713257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32045963406562805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48821988701820374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37892118096351624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5355240106582642
12 2.7928950285 	 0.5355239868 	 0.5355239868
epoch_time;  31.152546167373657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30787548422813416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47175532579421997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3712961673736572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5300762057304382
13 2.7873126878 	 0.530076228 	 0.530076228
epoch_time;  31.470144987106323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.317184180021286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4608679711818695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36282801628112793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4987313747406006
14 2.7789447968 	 0.498731376 	 0.498731376
epoch_time;  31.16883635520935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2904813289642334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4052903950214386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3547717034816742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46636542677879333
15 2.7753939309 	 0.4663654224 	 0.4663654224
epoch_time;  32.213785886764526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30436769127845764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4311889111995697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35713306069374084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4714798331260681
16 2.7643054505 	 0.4714798386 	 0.4714798386
epoch_time;  31.36722159385681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.338471382856369
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4887247383594513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3790823519229889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5247217416763306
17 2.7631847508 	 0.5247217127 	 0.5247217127
epoch_time;  31.73590898513794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3258802890777588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47117140889167786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.385562539100647
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.526434600353241
18 2.7618415641 	 0.5264346149 	 0.5264346149
epoch_time;  31.0693678855896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3291154205799103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4513869881629944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39873388409614563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5192853212356567
19 2.7529673747 	 0.5192852948 	 0.5192852948
epoch_time;  30.943785190582275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3291354477405548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4514601528644562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39866673946380615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5193328261375427
It took 680.5950939655304 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138681
Array Job ID: 2137927_15
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-10:24:18 core-walltime
Job Wall-clock time: 01:54:41
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
