wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_171504-d4aswp31
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-moon-1156
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/d4aswp31
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇█▄█▅▃▆▇▂▁▅▇▇▃▂▅▅▅▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▆█▄▇▄▂▆▇▂▁▄▇▇▃▂▅▅▄▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ███▅█▅▃▆▇▂▁▅▇▇▃▂▅▆▅▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▆█▄▇▄▂▅▇▂▁▄▇▇▃▂▄▅▄▇▇
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 89.87814
wandb:  Test loss t(-10, 10)_r(0, 0)_none 61.78072
wandb:    Test loss t(0, 0)_r(-5, 5)_none 61.96622
wandb:     Test loss t(0, 0)_r(0, 0)_none 37.73076
wandb:                         Train loss 21.79612
wandb: 
wandb: 🚀 View run chromatic-moon-1156 at: https://wandb.ai/nreints/thesis/runs/d4aswp31
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_171504-d4aswp31/logs
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 42.7569694519043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 65.07577514648438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.65706634521484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.02726745605469
0 40.6015942366 	 101.027269848 	 101.027269848
epoch_time;  36.34981608390808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.4886589050293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 54.522132873535156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.67460632324219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.7107925415039
1 28.3926085393 	 90.7107896959 	 90.7107896959
epoch_time;  34.431052684783936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 46.74155807495117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 71.69002532958984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.66748809814453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.873291015625
2 26.2491960054 	 101.8732896959 	 101.8732896959
epoch_time;  34.59009337425232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 19.51809310913086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 34.01323699951172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 41.19479751586914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 59.66035079956055
3 25.1575141757 	 59.6603515625 	 59.6603515625
epoch_time;  35.25224733352661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 42.699302673339844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 65.51919555664062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.22760009765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 97.18502044677734
4 24.9280795108 	 97.1850190034 	 97.1850190034
epoch_time;  34.478384017944336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 21.937471389770508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.99715042114258
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.406700134277344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 70.24093627929688
5 24.5280736925 	 70.2409364443 	 70.2409364443
epoch_time;  34.93935418128967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 12.118945121765137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 22.949920654296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 31.812660217285156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.7421760559082
6 24.2048467121 	 46.7421769426 	 46.7421769426
epoch_time;  34.813805103302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.072101593017578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 51.04346466064453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.22023391723633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.20414733886719
7 23.6166215301 	 82.2041437922 	 82.2041437922
epoch_time;  34.79171633720398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 39.90581130981445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 63.62924575805664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.66622924804688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.92726135253906
8 23.58758516 	 93.9272592905 	 93.9272592905
epoch_time;  34.61828541755676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 9.89807415008545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 19.44472885131836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.857728958129883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.228511810302734
9 23.3073877581 	 32.2285129856 	 32.2285129856
epoch_time;  34.71084141731262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.335801601409912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.0711669921875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.50510025024414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.818355560302734
10 23.1304163846 	 17.818355416 	 17.818355416
epoch_time;  34.3116991519928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 20.253210067749023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 35.369728088378906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 42.959964752197266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.135780334472656
11 22.7699094448 	 61.1357791385 	 61.1357791385
epoch_time;  34.21098017692566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 40.40717315673828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 62.70301818847656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 64.91244506835938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 92.35729217529297
12 22.6189356593 	 92.3572951858 	 92.3572951858
epoch_time;  34.414257526397705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 39.40633010864258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 63.11244583129883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.30498504638672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 94.22002410888672
13 22.8426074814 	 94.2200274493 	 94.2200274493
epoch_time;  34.82507514953613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 15.131107330322266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 28.009931564331055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 31.610200881958008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.32514953613281
14 22.6936066562 	 46.3251478041 	 46.3251478041
epoch_time;  34.55811953544617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 10.979260444641113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 21.262065887451172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.9943904876709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.26579284667969
15 22.4558305476 	 35.2657939189 	 35.2657939189
epoch_time;  34.341572761535645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 24.39830780029297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 42.82211685180664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.161041259765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 66.82575225830078
16 22.0299686336 	 66.8257548564 	 66.8257548564
epoch_time;  34.38273334503174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 28.30183219909668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 47.70264434814453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.40043640136719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 71.6400375366211
17 22.2161026363 	 71.6400337838 	 71.6400337838
epoch_time;  34.689614057540894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 23.77627182006836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 40.30413818359375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.699039459228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 66.48863220214844
18 22.2616764345 	 66.488634924 	 66.488634924
epoch_time;  34.22237229347229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.72380828857422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.789424896240234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.969730377197266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.91629028320312
19 21.7961238998 	 89.9162901182 	 89.9162901182
epoch_time;  34.450058460235596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.73076248168945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.780723571777344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.966217041015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.87814331054688
It took 748.2366483211517 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn57: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135479.0

JOB STATISTICS
==============
Job ID: 2135479
Array Job ID: 2135328_27
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:58:12
CPU Efficiency: 25.46% of 03:48:36 core-walltime
Job Wall-clock time: 00:12:42
Memory Utilized: 3.88 GB
Memory Efficiency: 12.40% of 31.25 GB
