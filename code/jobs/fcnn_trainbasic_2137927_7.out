wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121457-ip1o4itv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-rocket-1176
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ip1o4itv
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▂▂▁▂▂▁▁▁▁▂▁▁▁▃▃▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▂▂▁▂▃▂▂▂▂▃▃▁▁▄▅▃▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▁▂▃▁▃▃▂█▃▃▃▁▄▄▅▃▃▂▅▅▅
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 21.31804
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.95271
wandb:    Test loss t(0, 0)_r(-5, 5)_none 9.40964
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31927
wandb:                         Train loss 0.57739
wandb: 
wandb: 🚀 View run thriving-rocket-1176 at: https://wandb.ai/nreints/thesis/runs/ip1o4itv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121457-ip1o4itv/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122751-y7pxz7f8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-festival-1184
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/y7pxz7f8
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18690311908721924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 12.353928565979004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.948948860168457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.977581024169922
0 2.7152764922 	 26.9775812922 	 26.9775812922
epoch_time;  36.0749237537384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20947980880737305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.64323902130127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.681175231933594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.786426544189453
1 0.7791771188 	 22.7864257812 	 22.7864257812
epoch_time;  35.631667375564575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2559411823749542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.901748657226562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.031835556030273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.841842651367188
2 0.7196101949 	 21.8418417441 	 21.8418417441
epoch_time;  34.82760000228882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.189349964261055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.869413375854492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.072081565856934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.954029083251953
3 0.6867689831 	 21.9540289802 	 21.9540289802
epoch_time;  35.28521203994751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25789597630500793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.248419761657715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.882153511047363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.13551902770996
4 0.6605832482 	 21.1355191617 	 21.1355191617
epoch_time;  34.71021819114685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27324149012565613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.216446876525879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.109816551208496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.29585075378418
5 0.6437794473 	 21.2958509291 	 21.2958509291
epoch_time;  34.71134161949158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20430439710617065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.074403762817383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.364935874938965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.147253036499023
6 0.6294665585 	 21.1472524282 	 21.1472524282
epoch_time;  35.73548984527588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4446703791618347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.057005882263184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.128232955932617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.991567611694336
7 0.6167142606 	 20.9915672508 	 20.9915672508
epoch_time;  34.798720359802246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2510504722595215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.996394634246826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.046072959899902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.900726318359375
8 0.6064893636 	 20.9007258235 	 20.9007258235
epoch_time;  34.38430953025818
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2681398391723633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.966041564941406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.079723358154297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.035818099975586
9 0.5996367726 	 21.0358174092 	 21.0358174092
epoch_time;  34.300105810165405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2640282213687897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.060574531555176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.180071830749512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.0322208404541
10 0.5951222373 	 21.0322199641 	 21.0322199641
epoch_time;  34.35675549507141
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17946839332580566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.018095016479492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.291903495788574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.261547088623047
11 0.5903068651 	 21.2615471917 	 21.2615471917
epoch_time;  34.47441482543945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29432013630867004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.770572662353516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.32597827911377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.69536781311035
12 0.5873138696 	 20.6953679265 	 20.6953679265
epoch_time;  34.50431799888611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28653714060783386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.041230201721191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.82903003692627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.82082176208496
13 0.5832987729 	 20.8208218961 	 20.8208218961
epoch_time;  34.63510322570801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32041940093040466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.004805564880371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.954339981079102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.843942642211914
14 0.58332865 	 20.8439426731 	 20.8439426731
epoch_time;  34.78251051902771
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27011460065841675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.495041847229004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.703003883361816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.592334747314453
15 0.5813841616 	 22.5923353041 	 22.5923353041
epoch_time;  33.96789264678955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26444369554519653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.273737907409668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.893065452575684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.326173782348633
16 0.5805366532 	 22.3261745144 	 22.3261745144
epoch_time;  34.60406231880188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22178156673908234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.937725067138672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.382572174072266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.33265495300293
17 0.5786657624 	 21.3326554582 	 21.3326554582
epoch_time;  34.056530714035034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3277376890182495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.831869602203369
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.327371597290039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.975690841674805
18 0.5770031846 	 20.9756915118 	 20.9756915118
epoch_time;  34.407958984375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3193913996219635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.9523515701293945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.41232967376709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.316434860229492
19 0.5773882391 	 21.3164339633 	 21.3164339633
epoch_time;  34.06101894378662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3192656338214874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.952713966369629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.40964412689209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.318035125732422
It took 773.7077977657318 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▂▃▃▂▁▂▄▁▂▃▂▃▄▄▃▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▂▂▁▂▂▁▁▁▂▂▃▃▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▄▁▃▆▄▂▄█▃▅▇▅▅██▅▃▆▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃█▄▃▄▅▆▂▆▃▆▃▄▃▁▂▅▆▃▃
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 20.11897
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.56069
wandb:    Test loss t(0, 0)_r(-5, 5)_none 9.55178
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22151
wandb:                         Train loss 0.58085
wandb: 
wandb: 🚀 View run cheerful-festival-1184 at: https://wandb.ai/nreints/thesis/runs/y7pxz7f8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122751-y7pxz7f8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124035-lirs1xyi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-pig-1192
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/lirs1xyi
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24966031312942505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.23430347442627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.006814956665039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.611188888549805
0 2.5959563568 	 24.611188239 	 24.611188239
epoch_time;  33.59931540489197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2253931760787964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.585243225097656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.872851371765137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.834415435791016
1 0.7731611269 	 22.8344145904 	 22.8344145904
epoch_time;  34.00312781333923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45243310928344727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.279401779174805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.05925464630127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.362302780151367
2 0.7193568804 	 20.3623033678 	 20.3623033678
epoch_time;  33.737802267074585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2593357264995575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.14561653137207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.465025901794434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.962251663208008
3 0.6838779746 	 20.9622519003 	 20.9622519003
epoch_time;  33.949307918548584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2299399971961975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.566910266876221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.338948249816895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.81517791748047
4 0.6568058869 	 20.8151776288 	 20.8151776288
epoch_time;  34.36017894744873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25668150186538696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.642478942871094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.814476013183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.49940299987793
5 0.6401294871 	 20.4994021854 	 20.4994021854
epoch_time;  34.97525072097778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30525586009025574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.261507511138916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.281564712524414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.46927261352539
6 0.6207960538 	 19.4692725929 	 19.4692725929
epoch_time;  34.03181076049805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3414632976055145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.336877346038818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.90369701385498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.115690231323242
7 0.6135628299 	 20.115690984 	 20.115690984
epoch_time;  34.47364330291748
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17557525634765625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.816251277923584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.705039978027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.521987915039062
8 0.6063877072 	 21.5219884924 	 21.5219884924
epoch_time;  34.940823793411255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3436289131641388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.989569187164307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.602416038513184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.454505920410156
9 0.6024210832 	 19.4545053843 	 19.4545053843
epoch_time;  34.62432074546814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23040787875652313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.2324066162109375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.965812683105469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.124605178833008
10 0.600093152 	 20.124605416 	 20.124605416
epoch_time;  34.47914171218872
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33065691590309143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.239331245422363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.480125427246094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.660707473754883
11 0.5944758927 	 20.6607065562 	 20.6607065562
epoch_time;  34.25702428817749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2233627736568451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.504796504974365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.979265213012695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.449922561645508
12 0.5939845535 	 20.4499221389 	 20.4499221389
epoch_time;  35.005826234817505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2398425191640854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.663345813751221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.180318832397461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.8057804107666
13 0.5890720293 	 20.8057801943 	 20.8057801943
epoch_time;  34.72088956832886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19784541428089142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.9727253913879395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.810242652893066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.983051300048828
14 0.5882774565 	 21.9830513619 	 21.9830513619
epoch_time;  34.34851884841919
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10981014370918274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.949580192565918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.759617805480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.619482040405273
15 0.5884068411 	 21.6194824219 	 21.6194824219
epoch_time;  34.89786076545715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1619984358549118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.614504814147949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.095699310302734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.871301651000977
16 0.5847384092 	 20.8713009396 	 20.8713009396
epoch_time;  33.925638914108276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2845277488231659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.140620231628418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.51689624786377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.42319679260254
17 0.5846370421 	 19.4231973184 	 19.4231973184
epoch_time;  34.41154098510742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3662080466747284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.273781776428223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.201549530029297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.5035457611084
18 0.5827540848 	 20.5035459776 	 20.5035459776
epoch_time;  34.05661225318909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2214449644088745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.567241668701172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.549311637878418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.125028610229492
19 0.5808548046 	 20.1250290329 	 20.1250290329
epoch_time;  34.473329067230225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22151227295398712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.560685634613037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.551783561706543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.11897087097168
It took 764.3312518596649 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▄▂▅▃▃▃▂▄▄▄▅▃▁▁▂▂▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▄▄▂▃▂▂▂▁▂▂▂▂▂▁▁▁▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▃▃▂▁▇▆▆▅▅▇▇▆█▅▁▁▃▂▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▅▁▄▇▁█▆▅▇▆▆▇▅████▆▃▃
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.40421
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.87019
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.80863
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.23196
wandb:                         Train loss 0.57715
wandb: 
wandb: 🚀 View run dazzling-pig-1192 at: https://wandb.ai/nreints/thesis/runs/lirs1xyi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124035-lirs1xyi/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125315-16islq47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-noodles-1199
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/16islq47
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3376614451408386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.786561012268066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.149726867675781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.00811004638672
0 2.6151631329 	 25.0081107475 	 25.0081107475
epoch_time;  34.581865072250366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28694629669189453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.301156044006348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.483404159545898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.91309356689453
1 0.7809222201 	 23.9130938556 	 23.9130938556
epoch_time;  34.1462619304657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14965230226516724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.488164901733398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.3433256149292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.943775177001953
2 0.7212362012 	 21.9437750739 	 21.9437750739
epoch_time;  33.94722604751587
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26571694016456604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.407214164733887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.089614868164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.238019943237305
3 0.6874866524 	 22.2380199535 	 22.2380199535
epoch_time;  34.313175678253174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35360288619995117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.402844429016113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.887284278869629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.632980346679688
4 0.6622090892 	 20.6329800992 	 20.6329800992
epoch_time;  34.32337045669556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16553165018558502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.053194046020508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.427392959594727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.00597381591797
5 0.6409409613 	 23.0059728674 	 23.0059728674
epoch_time;  34.6675078868866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3859459161758423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.362624168395996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.195449829101562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.83635902404785
6 0.6223741246 	 21.8363597973 	 21.8363597973
epoch_time;  34.43623924255371
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32583656907081604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.3816986083984375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.087174415588379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.496826171875
7 0.6139222224 	 21.4968261719 	 21.4968261719
epoch_time;  34.57975244522095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2776551842689514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.455475807189941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.736686706542969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.44287872314453
8 0.6066197813 	 21.4428790118 	 21.4428790118
epoch_time;  34.61639595031738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34538233280181885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.106601238250732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.921379089355469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.054136276245117
9 0.6012201329 	 21.0541358742 	 21.0541358742
epoch_time;  34.62892961502075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3251187801361084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.682159423828125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.386490821838379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.435321807861328
10 0.5975047985 	 22.4353225296 	 22.4353225296
epoch_time;  34.64403033256531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.311540812253952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.633130073547363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.387458801269531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.382783889770508
11 0.5917130705 	 22.3827834671 	 22.3827834671
epoch_time;  34.27253603935242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3673447072505951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.40317440032959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.079272270202637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.884965896606445
12 0.5874385433 	 21.8849662162 	 21.8849662162
epoch_time;  33.718894481658936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2956264913082123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.665699005126953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.578341484069824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.912826538085938
13 0.5882069169 	 22.9128272804 	 22.9128272804
epoch_time;  34.18287467956543
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3869010806083679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.359762191772461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.867873191833496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.586246490478516
14 0.5852255334 	 21.5862463049 	 21.5862463049
epoch_time;  34.90694713592529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3877362608909607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.032137393951416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.955570220947266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.114654541015625
15 0.5800712239 	 20.1146537162 	 20.1146537162
epoch_time;  34.969879150390625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.385633260011673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.123523712158203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.995515823364258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.46381378173828
16 0.5801564335 	 20.4638144003 	 20.4638144003
epoch_time;  34.38683533668518
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37390783429145813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.20602560043335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.46847915649414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.11803436279297
17 0.5808934423 	 21.1180334143 	 21.1180334143
epoch_time;  33.775073289871216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32141679525375366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.35043478012085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.065171241760254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.891836166381836
18 0.5780596259 	 20.8918364654 	 20.8918364654
epoch_time;  33.58720397949219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23194751143455505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.867563724517822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.816166877746582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.399110794067383
19 0.5771482309 	 22.3991105363 	 22.3991105363
epoch_time;  33.640188217163086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23195824027061462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.870185375213623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.808625221252441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.404207229614258
It took 759.6181063652039 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▂▄▅▆▅▆▃▃▃▄▄▃▄▃▃▄▃▃▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▄▃▂▃▁▁▁▂▂▂▂▁▁▂▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▁▄▆▆▆█▆▇▇█▇▅▇▆▇▆▅▄▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃█▁▁▃▅▁▃▆▅█▂▄▄▇▃▄▅▄██
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 20.72104
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.81759
wandb:    Test loss t(0, 0)_r(-5, 5)_none 9.69006
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.43497
wandb:                         Train loss 0.57783
wandb: 
wandb: 🚀 View run fortuitous-noodles-1199 at: https://wandb.ai/nreints/thesis/runs/16islq47
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125315-16islq47/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_130552-7nupgfqm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-monkey-1206
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7nupgfqm
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2515643239021301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.600262641906738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.922001838684082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.555395126342773
0 2.7028625116 	 25.555394848 	 25.555394848
epoch_time;  33.44741940498352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4358997941017151
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.408953666687012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.46034049987793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.32817268371582
1 0.7784505649 	 21.3281725084 	 21.3281725084
epoch_time;  33.49146819114685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1634567379951477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.441641807556152
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.420034408569336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.050270080566406
2 0.7203663366 	 23.0502692145 	 23.0502692145
epoch_time;  33.39415216445923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1666065901517868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.06396484375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.056988716125488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.82281494140625
3 0.6840038054 	 23.8228146115 	 23.8228146115
epoch_time;  33.59473419189453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23972122371196747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.728772163391113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.304539680480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.85144805908203
4 0.6551539293 	 23.8514490076 	 23.8514490076
epoch_time;  34.44890332221985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31851497292518616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.352149963378906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.127410888671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.468603134155273
5 0.6315759286 	 23.4686021959 	 23.4686021959
epoch_time;  34.520678758621216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1550549566745758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.713798522949219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.913393020629883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.327861785888672
6 0.6202854577 	 24.3278610642 	 24.3278610642
epoch_time;  34.71376442909241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22844502329826355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.768009185791016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.157126426696777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.86541748046875
7 0.6085462525 	 21.8654178104 	 21.8654178104
epoch_time;  34.3667254447937
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3421681523323059
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.678845405578613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.576192855834961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.087419509887695
8 0.6028177498 	 22.0874194996 	 22.0874194996
epoch_time;  34.900041341781616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29822060465812683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.631523132324219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.648492813110352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.168445587158203
9 0.5991178292 	 22.1684464738 	 22.1684464738
epoch_time;  34.56716179847717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43751096725463867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.944000720977783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.789260864257812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.84891700744629
10 0.5930102638 	 22.8489178632 	 22.8489178632
epoch_time;  34.24384164810181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21436761319637299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.025575637817383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.667274475097656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.550018310546875
11 0.5915670324 	 22.5500184755 	 22.5500184755
epoch_time;  33.897149324417114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2770552933216095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.257588386535645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.871042251586914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.041236877441406
12 0.5883628726 	 22.0412360114 	 22.0412360114
epoch_time;  34.66525435447693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2583377957344055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.302905082702637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.417770385742188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.800281524658203
13 0.588418701 	 22.8002824113 	 22.8002824113
epoch_time;  35.00115752220154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3929296135902405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.891135215759277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.108819007873535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.0566349029541
14 0.5851291251 	 22.0566353463 	 22.0566353463
epoch_time;  34.351484537124634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2556438446044922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.901592254638672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.413516998291016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.10734748840332
15 0.5824277926 	 22.107347973 	 22.107347973
epoch_time;  34.36491680145264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27332186698913574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.215575218200684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.252655029296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.82191276550293
16 0.5829342149 	 22.821911951 	 22.821911951
epoch_time;  34.02116513252258
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29830479621887207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.895968914031982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.87667179107666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.919416427612305
17 0.5819765787 	 21.9194164379 	 21.9194164379
epoch_time;  34.480833768844604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2687561511993408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.125712394714355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.682523727416992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.31451988220215
18 0.5781329755 	 22.314519109 	 22.314519109
epoch_time;  34.574174880981445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43493854999542236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.8291425704956055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.689952850341797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.721757888793945
19 0.57783218 	 20.7217575486 	 20.7217575486
epoch_time;  34.529226303100586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4349704682826996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.817590713500977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.690062522888184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.721038818359375
It took 756.9109644889832 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▄▅▅▃▅▃▇▄▄▂▄▄▄▁▄▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▃▂▃▂▃▂▃▁▂▂▂▁▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▂▃▂▄▅▃▆▅█▅▅▄▇▃▅▁▅▄▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▁▁▁▄▁▅▁▇▂▆▄▅▅▅▄█▅▃▅▅
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 21.77301
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.88661
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.14522
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31204
wandb:                         Train loss 0.57451
wandb: 
wandb: 🚀 View run lunar-monkey-1206 at: https://wandb.ai/nreints/thesis/runs/7nupgfqm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_130552-7nupgfqm/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131812-m47awlft
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run virtuous-kumquat-1213
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/m47awlft
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18786334991455078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.09394359588623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.71837043762207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.444005966186523
0 2.6704331322 	 25.4440060177 	 25.4440060177
epoch_time;  34.08307075500488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17353306710720062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.012594223022461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.974422454833984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.87788963317871
1 0.7745122477 	 22.8778900971 	 22.8778900971
epoch_time;  33.378718852996826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17165018618106842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.573410034179688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.272303581237793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.03913116455078
2 0.7162066068 	 23.0391311233 	 23.0391311233
epoch_time;  33.77745294570923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16324377059936523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.459955215454102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.898326873779297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.987071990966797
3 0.6872181721 	 22.987072424 	 22.987072424
epoch_time;  33.747092485427856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2787613272666931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.632749557495117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.397284507751465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.476837158203125
4 0.6575252505 	 23.4768369932 	 23.4768369932
epoch_time;  34.254809617996216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1568404734134674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.39587116241455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.827229499816895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.557783126831055
5 0.6364702291 	 23.5577834671 	 23.5577834671
epoch_time;  34.23947715759277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.332370400428772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.9715576171875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.071852684020996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.143814086914062
6 0.6252521161 	 22.1438133446 	 22.1438133446
epoch_time;  33.20987796783447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17587243020534515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.34154987335205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.151573181152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.426254272460938
7 0.6146686944 	 23.4262536951 	 23.4262536951
epoch_time;  33.3885281085968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41310635209083557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.881688117980957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.705455780029297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.45267677307129
8 0.6053230425 	 22.4526763091 	 22.4526763091
epoch_time;  33.210811614990234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21463505923748016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.751065254211426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.765721321105957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.495311737060547
9 0.601216579 	 24.4953125 	 24.4953125
epoch_time;  33.08514475822449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35502979159355164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.083104133605957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.891287803649902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.79238510131836
10 0.5964162487 	 22.7923854519 	 22.7923854519
epoch_time;  33.130146741867065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.285727858543396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.338582992553711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.862005233764648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.792680740356445
11 0.5946761062 	 22.79268106 	 22.79268106
epoch_time;  33.30814552307129
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3381139039993286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.452650547027588
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.510159492492676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.5065975189209
12 0.5905281537 	 21.5065983953 	 21.5065983953
epoch_time;  33.008042335510254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3174596428871155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.100741386413574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.355094909667969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.085140228271484
13 0.5895937827 	 23.0851404139 	 23.0851404139
epoch_time;  32.9291832447052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33512499928474426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.175797462463379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.312381744384766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.518625259399414
14 0.5854802116 	 22.5186259502 	 22.5186259502
epoch_time;  32.743725061416626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2776944637298584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.011358261108398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.883502006530762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.903078079223633
15 0.5834314634 	 22.9030774916 	 22.9030774916
epoch_time;  32.77203369140625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45183297991752625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.615439414978027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.581687927246094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.830265045166016
16 0.5814785037 	 20.8302641997 	 20.8302641997
epoch_time;  32.89306664466858
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3434743881225586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.146024703979492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.715727806091309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.801246643066406
17 0.5792383027 	 22.801245777 	 22.801245777
epoch_time;  34.08850049972534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22996558248996735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.904992580413818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.55237102508545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.12282943725586
18 0.5760821198 	 22.122829128 	 22.122829128
epoch_time;  33.29016137123108
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3120158314704895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.894864559173584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.141799926757812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.76882553100586
19 0.5745141321 	 21.7688252217 	 21.7688252217
epoch_time;  33.25834345817566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3120419383049011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.886605739593506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.145218849182129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.773006439208984
It took 740.0432195663452 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▃▃▆▄▃▂▂▂▃▂▂▃▃▃▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▃▄▃▂▂▁▂▂▁▁▂▂▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▆▄▁▂█▅▅▃▄▅▆▆▄▇▅▅▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▃▁▆▆▂▃▅██▅▅▅▅▄▆▅▆▇▇▆
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 19.95364
wandb:  Test loss t(-10, 10)_r(0, 0)_none 8.04343
wandb:    Test loss t(0, 0)_r(-5, 5)_none 8.69735
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.36768
wandb:                         Train loss 0.57391
wandb: 
wandb: 🚀 View run virtuous-kumquat-1213 at: https://wandb.ai/nreints/thesis/runs/m47awlft
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131812-m47awlft/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_133027-sb2f5my5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-festival-1220
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sb2f5my5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3100343942642212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 12.781961441040039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.192431449890137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.517961502075195
0 2.6502332642 	 26.5179608319 	 26.5179608319
epoch_time;  33.29100561141968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21145394444465637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.374566078186035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.863117218017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.536636352539062
1 0.7766414945 	 24.5366369299 	 24.5366369299
epoch_time;  33.71588087081909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1201910674571991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.81200885772705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.340399742126465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.73368263244629
2 0.7187742157 	 23.7336834882 	 23.7336834882
epoch_time;  33.42107844352722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33195966482162476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.664138793945312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.586621284484863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.75165557861328
3 0.6864895934 	 21.7516548775 	 21.7516548775
epoch_time;  33.33812713623047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34641867876052856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.829760551452637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.966808319091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.382226943969727
4 0.6653867604 	 21.3822265625 	 21.3822265625
epoch_time;  33.158005475997925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14534048736095428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.74232292175293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.47379207611084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.126209259033203
5 0.6461508098 	 24.126208826 	 24.126208826
epoch_time;  33.007081270217896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20138758420944214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.136858940124512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.757038116455078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.90127182006836
6 0.6268822548 	 22.9012721706 	 22.9012721706
epoch_time;  33.08546257019043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3143686354160309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.380695343017578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.751585960388184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.74050521850586
7 0.6180772413 	 21.7405049092 	 21.7405049092
epoch_time;  32.793771266937256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43521323800086975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.227033615112305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.19534969329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.910873413085938
8 0.608192356 	 20.9108741554 	 20.9108741554
epoch_time;  32.95169377326965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4325841963291168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.079822540283203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.491209983825684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.940187454223633
9 0.6014927869 	 20.9401868666 	 20.9401868666
epoch_time;  32.93065071105957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30391284823417664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.187657356262207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.718993186950684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.097320556640625
10 0.5972421862 	 21.0973210515 	 21.0973210515
epoch_time;  32.780462980270386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30828574299812317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.185479164123535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.997675895690918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.590484619140625
11 0.5917575348 	 21.5904837943 	 21.5904837943
epoch_time;  32.986412048339844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2787124216556549
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.07874584197998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.822991371154785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.08800506591797
12 0.589711762 	 21.0880041174 	 21.0880041174
epoch_time;  32.70192909240723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31394311785697937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.867581367492676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.310065269470215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.402599334716797
13 0.5883089865 	 20.4025997677 	 20.4025997677
epoch_time;  33.00746512413025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2547125220298767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.361847877502441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.101040840148926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.993833541870117
14 0.5810257196 	 21.9938331398 	 21.9938331398
epoch_time;  32.651118993759155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36567190289497375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.378836631774902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.604373931884766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.78190803527832
15 0.5826594121 	 21.7819072002 	 21.7819072002
epoch_time;  33.136091232299805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29623836278915405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.437105178833008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.67353343963623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.590890884399414
16 0.5805279339 	 21.5908915752 	 21.5908915752
epoch_time;  33.17791485786438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34094929695129395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.19896411895752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.943719863891602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.54938316345215
17 0.5769495877 	 20.5493837099 	 20.5493837099
epoch_time;  33.22215127944946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3805259168148041
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.748506546020508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.830647468566895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.730819702148438
18 0.579687657 	 19.7308197846 	 19.7308197846
epoch_time;  33.146286487579346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36781740188598633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.042856216430664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.697463989257812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.95125389099121
19 0.5739139048 	 19.9512536951 	 19.9512536951
epoch_time;  33.03685259819031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36767688393592834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.043426513671875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.697352409362793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.953643798828125
It took 735.0662138462067 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▆▁▅▃▃▃▄█▂▅▅▅▂▃▂▂▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▅▃▃▂▂▂▁▃▁▁▂▂▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▁▃▁▅▃▄▄▆█▄▇▇▅▄▄▄▃▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▅▁▃▁▃▃▃▃▂█▃▅▃▅▄▆▅▄▄▄
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.14136
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.76082
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.83985
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27129
wandb:                         Train loss 0.58195
wandb: 
wandb: 🚀 View run alight-festival-1220 at: https://wandb.ai/nreints/thesis/runs/sb2f5my5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_133027-sb2f5my5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134246-dqn609xr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-peony-1227
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/dqn609xr
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22691717743873596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.485560417175293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.673226356506348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.074527740478516
0 2.6871084163 	 25.0745275549 	 25.0745275549
epoch_time;  33.937114238739014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2736777067184448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.33169174194336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.028308868408203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.269880294799805
1 0.7827256706 	 22.2698796453 	 22.2698796453
epoch_time;  33.36388564109802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11626415699720383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.777122497558594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.75947093963623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.661725997924805
2 0.7237943057 	 23.6617266681 	 23.6617266681
epoch_time;  33.303393602371216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20248274505138397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.397931098937988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.81835651397705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.026866912841797
3 0.6932597435 	 21.0268660262 	 21.0268660262
epoch_time;  33.5852997303009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12131529301404953
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.76033878326416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.823911666870117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.615985870361328
4 0.6682565698 	 23.6159865921 	 23.6159865921
epoch_time;  33.38629484176636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18541210889816284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.267159461975098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.949891090393066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.998680114746094
5 0.6472158051 	 21.9986803209 	 21.9986803209
epoch_time;  33.00813889503479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19274719059467316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.094490051269531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.197588920593262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.105609893798828
6 0.6348172797 	 22.1056099557 	 22.1056099557
epoch_time;  33.3803186416626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22552582621574402
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.963753700256348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.242565155029297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.046653747558594
7 0.6266086503 	 22.0466532939 	 22.0466532939
epoch_time;  33.611812114715576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2159949690103531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.650946140289307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.416088104248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.479768753051758
8 0.6156693073 	 22.4797693201 	 22.4797693201
epoch_time;  33.69001293182373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14545658230781555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.64293098449707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.332732200622559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.927915573120117
9 0.6110373299 	 24.9279164907 	 24.9279164907
epoch_time;  33.316086292266846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.426745742559433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.517648220062256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.240615844726562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.402854919433594
10 0.6049387731 	 21.4028544658 	 21.4028544658
epoch_time;  33.31261968612671
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20874299108982086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.6816229820251465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.04120922088623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.316326141357422
11 0.599790373 	 23.3163270693 	 23.3163270693
epoch_time;  33.236932039260864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2853417694568634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.839308738708496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.637694358825684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.36673355102539
12 0.5989727415 	 23.3667335304 	 23.3667335304
epoch_time;  33.125083446502686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.194697305560112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.063719749450684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.011434555053711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.118906021118164
13 0.5939635197 	 23.1189057221 	 23.1189057221
epoch_time;  33.17586302757263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30203893780708313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.624148845672607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.189188003540039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.86095428466797
14 0.5909351512 	 21.8609546558 	 21.8609546558
epoch_time;  32.92437529563904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2306160181760788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.687862396240234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.285624504089355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.996627807617188
15 0.588157411 	 21.9966269003 	 21.9966269003
epoch_time;  33.16965413093567
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35605692863464355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.721131801605225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.080394744873047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.663713455200195
16 0.5869798086 	 21.6637141047 	 21.6637141047
epoch_time;  32.937888622283936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29672595858573914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.583614349365234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.969768524169922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.73508644104004
17 0.5852963209 	 21.735086307 	 21.735086307
epoch_time;  32.85827302932739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2358866035938263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.760770797729492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.462835311889648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.378108978271484
18 0.5810117108 	 21.3781091639 	 21.3781091639
epoch_time;  32.87156391143799
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.271289199590683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.7681884765625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.84512996673584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.14352798461914
19 0.5819493877 	 22.1435282939 	 22.1435282939
epoch_time;  33.04084849357605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27128878235816956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.7608160972595215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.839851379394531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.1413631439209
It took 738.9778966903687 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▄▄▄▂▄▃▂▁▃▃▃▃▄▃▂▄▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▁▂▂▁▁▂▂▂▂▂▂▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▁▂▅▆▄▆▆▃▂▅▅▅▅█▆▃▇▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▆▂▁▁▄▂▁▂▆▄▃▃▄▅█▅▃▇▆▆
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 19.93395
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.8115
wandb:    Test loss t(0, 0)_r(-5, 5)_none 9.47494
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.40231
wandb:                         Train loss 0.5772
wandb: 
wandb: 🚀 View run luminous-peony-1227 at: https://wandb.ai/nreints/thesis/runs/dqn609xr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134246-dqn609xr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135508-0thejvf5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-mandu-1233
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/0thejvf5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.186374232172966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 12.86426067352295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.450172424316406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.945777893066406
0 2.759211887 	 25.945777027 	 25.945777027
epoch_time;  33.338996171951294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3851096034049988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.653002738952637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.35977840423584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.71247673034668
1 0.7813723183 	 21.7124775655 	 21.7124775655
epoch_time;  33.062928199768066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20495620369911194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.365924835205078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.680364608764648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.089859008789062
2 0.7260205805 	 22.0898582665 	 22.0898582665
epoch_time;  33.00510787963867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16150306165218353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.023653030395508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.230289459228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.252166748046875
3 0.6894578359 	 22.252166913 	 22.252166913
epoch_time;  33.02291560173035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18488596379756927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.066831588745117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.430609703063965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.639795303344727
4 0.6699480742 	 22.6397962416 	 22.6397962416
epoch_time;  34.19673156738281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3295793831348419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.933777809143066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.04947566986084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.943195343017578
5 0.6490480995 	 20.9431944151 	 20.9431944151
epoch_time;  33.63762331008911
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.207834392786026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.697452545166016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.611318588256836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.23056983947754
6 0.6300748026 	 22.2305690456 	 22.2305690456
epoch_time;  33.46458053588867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16281723976135254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.505698204040527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.509567260742188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.05752944946289
7 0.6238186371 	 22.0575300887 	 22.0575300887
epoch_time;  33.50661635398865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19860367476940155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.060224533081055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.738402366638184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.68935775756836
8 0.6089593659 	 20.6893581081 	 20.6893581081
epoch_time;  33.49143648147583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39301797747612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.798339366912842
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.507802963256836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.00594711303711
9 0.6023011618 	 20.0059477935 	 20.0059477935
epoch_time;  33.68297505378723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.292589396238327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.287039756774902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.254204750061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.578231811523438
10 0.600022819 	 21.578231894 	 21.578231894
epoch_time;  33.151450634002686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24787048995494843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.582738876342773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.375173568725586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.948793411254883
11 0.592728312 	 21.9487938133 	 21.9487938133
epoch_time;  34.01842021942139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24102719128131866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.665247917175293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.370159149169922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.004474639892578
12 0.5901771499 	 22.0044750317 	 22.0044750317
epoch_time;  33.300363063812256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2867078483104706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.50204849243164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.222929000854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.799400329589844
13 0.5868594325 	 21.799399546 	 21.799399546
epoch_time;  33.33520317077637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33930137753486633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.520444869995117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.976910591125488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.37047004699707
14 0.5882960522 	 22.3704708615 	 22.3704708615
epoch_time;  33.43997097015381
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.508950412273407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.379303932189941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.428153991699219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.690397262573242
15 0.5849896575 	 21.6903980152 	 21.6903980152
epoch_time;  32.84358763694763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3712032735347748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.080550193786621
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.735135078430176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.60651969909668
16 0.5831111632 	 20.6065192145 	 20.6065192145
epoch_time;  32.99513554573059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26872602105140686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.623074531555176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.680988311767578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.2316951751709
17 0.5825341599 	 22.2316960515 	 22.2316960515
epoch_time;  32.88452124595642
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46669459342956543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.089926719665527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.712930679321289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.710189819335938
18 0.5800970099 	 20.7101905617 	 20.7101905617
epoch_time;  33.14452314376831
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4020792841911316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.816660404205322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.472731590270996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.93182945251465
19 0.5772003401 	 19.9318293391 	 19.9318293391
epoch_time;  32.93691945075989
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.402309775352478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.81150484085083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.474943161010742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.933948516845703
It took 742.1376786231995 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆█▅▅▇▄▅▆▅▄▃▃▁▂▁▅▃▆▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▂▄▂▄█▅▆▇▇▅▅▄▂▂▁▄▃▅▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▁▃▃▃▁▅▃▄▄▆▇▄▆▇█▃▆▄▄▄
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 21.28289
wandb:  Test loss t(-10, 10)_r(0, 0)_none 8.18872
wandb:    Test loss t(0, 0)_r(-5, 5)_none 9.57723
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27543
wandb:                         Train loss 0.5776
wandb: 
wandb: 🚀 View run lambent-mandu-1233 at: https://wandb.ai/nreints/thesis/runs/0thejvf5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135508-0thejvf5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140726-e8jncp2e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-peony-1240
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/e8jncp2e
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20357708632946014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.731561660766602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.014315605163574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.510231018066406
0 2.6184391133 	 24.510230152 	 24.510230152
epoch_time;  33.09181356430054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14793427288532257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.4507474899292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.905004501342773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.630632400512695
1 0.7793113383 	 23.6306323902 	 23.6306323902
epoch_time;  32.87318420410156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21056920289993286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.281786918640137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.294249534606934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.328428268432617
2 0.722434011 	 24.3284285262 	 24.3284285262
epoch_time;  32.887882232666016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21400655806064606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.43132495880127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.949007987976074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.051715850830078
3 0.6863538036 	 23.0517155828 	 23.0517155828
epoch_time;  33.02425670623779
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23616786301136017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.03311824798584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.273096084594727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.842607498168945
4 0.6671126049 	 22.8426071579 	 22.8426071579
epoch_time;  32.85547423362732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13376542925834656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.937508583068848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.650331497192383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.801939010620117
5 0.6431883939 	 23.8019399282 	 23.8019399282
epoch_time;  33.09115743637085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3157375454902649
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.197364807128906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.795051574707031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.17194366455078
6 0.6253654258 	 22.1719436233 	 22.1719436233
epoch_time;  32.53106188774109
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23309361934661865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.67646312713623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.002099990844727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.151779174804688
7 0.6127754062 	 23.1517789274 	 23.1517789274
epoch_time;  33.22623562812805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2563483417034149
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.531712532043457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.20456314086914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.23348617553711
8 0.6044043746 	 23.2334855363 	 23.2334855363
epoch_time;  32.944780588150024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.279198557138443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.222575187683105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.197196960449219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.70538902282715
9 0.5987446759 	 22.7053895693 	 22.7053895693
epoch_time;  33.86961793899536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3586386740207672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.066343307495117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.844832420349121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.172237396240234
10 0.594586475 	 22.1722365921 	 22.1722365921
epoch_time;  33.407039642333984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36876818537712097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.871084690093994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.613041877746582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.784618377685547
11 0.5893314183 	 21.7846191406 	 21.7846191406
epoch_time;  33.36130142211914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2704521119594574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.9788079261779785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.560521125793457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.743953704833984
12 0.5886792885 	 21.7439545503 	 21.7439545503
epoch_time;  32.829299211502075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32773905992507935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.912822723388672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.734403610229492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.846467971801758
13 0.5846327444 	 20.8464672192 	 20.8464672192
epoch_time;  33.661123275756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38155773282051086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.059857368469238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.727302551269531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.48164939880371
14 0.5839803253 	 21.4816485431 	 21.4816485431
epoch_time;  33.19748330116272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42475825548171997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.987503528594971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.48708438873291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.985645294189453
15 0.581743017 	 20.9856445312 	 20.9856445312
epoch_time;  33.462021350860596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22207111120224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.4625244140625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.321257591247559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.789228439331055
16 0.579956096 	 22.7892287796 	 22.7892287796
epoch_time;  33.79248809814453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32167619466781616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.162477493286133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.247475624084473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.912324905395508
17 0.5787093176 	 21.9123258024 	 21.9123258024
epoch_time;  33.36215424537659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24892657995224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.528711318969727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.741727828979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.338191986083984
18 0.5819064523 	 23.3381915118 	 23.3381915118
epoch_time;  33.47153449058533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2754559814929962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.188554763793945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.576305389404297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.288087844848633
19 0.5776028746 	 21.2880872572 	 21.2880872572
epoch_time;  33.04376578330994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27542731165885925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.18872356414795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.577225685119629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.282888412475586
It took 737.6457352638245 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▅▅▆▄▃▆▁▄▃▄▂▂▃▄▃▃▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▂▂▃▁▂▂▂▂▂▂▃▂▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▃▁▅▄▆▅▅█▃▅▅▅▃▃▃▄▃▁▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃▁▆▇▃▃▅▁█▃▃▄▆▅▅▆█▄▂▂
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.68074
wandb:  Test loss t(-10, 10)_r(0, 0)_none 8.40908
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.55924
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20701
wandb:                         Train loss 0.57368
wandb: 
wandb: 🚀 View run dazzling-peony-1240 at: https://wandb.ai/nreints/thesis/runs/e8jncp2e
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140726-e8jncp2e/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28504136204719543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.6337308883667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.883017539978027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.936969757080078
0 2.701694207 	 24.936969489 	 24.936969489
epoch_time;  32.70100975036621
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22624242305755615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.509869575500488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.17125129699707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.227985382080078
1 0.7781507199 	 22.227985114 	 22.227985114
epoch_time;  32.92942953109741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1486634910106659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.094290733337402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.512069702148438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.968420028686523
2 0.7199515289 	 21.9684200802 	 21.9684200802
epoch_time;  32.80556511878967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3332464396953583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.590302467346191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.025710105895996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.145734786987305
3 0.6891029063 	 23.1457347973 	 23.1457347973
epoch_time;  33.280210971832275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39422017335891724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.625800132751465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.591056823730469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.880416870117188
4 0.6644037927 	 22.8804159628 	 22.8804159628
epoch_time;  33.026843309402466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23615753650665283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.64452838897705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.479532241821289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.393720626831055
5 0.6447122893 	 23.3937209671 	 23.3937209671
epoch_time;  32.7845721244812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23202760517597198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.949924945831299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.974672317504883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.074161529541016
6 0.6283066426 	 22.0741606841 	 22.0741606841
epoch_time;  32.931843996047974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3013966977596283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.687087059020996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.056206703186035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.641672134399414
7 0.6131629401 	 21.6416728252 	 21.6416728252
epoch_time;  32.91701531410217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15882979333400726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.39819049835205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.079352378845215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.52199363708496
8 0.6042967262 	 23.5219937711 	 23.5219937711
epoch_time;  32.8681857585907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4269723892211914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.282047271728516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.316143989562988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.27863121032715
9 0.5990921258 	 20.2786317568 	 20.2786317568
epoch_time;  32.92641258239746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22629956901073456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.906850337982178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.901237487792969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.95473861694336
10 0.5943834102 	 21.9547389675 	 21.9547389675
epoch_time;  32.77810835838318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24201317131519318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.62167501449585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.890813827514648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.422256469726562
11 0.5891130224 	 21.4222563872 	 21.4222563872
epoch_time;  32.9089674949646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2530365586280823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.030277252197266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.001391410827637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.48591423034668
12 0.5867177334 	 22.4859137458 	 22.4859137458
epoch_time;  32.750370025634766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3463578522205353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.627547740936279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.21615982055664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.8847713470459
13 0.583440348 	 20.8847722234 	 20.8847722234
epoch_time;  33.004319190979004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30385899543762207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.855423927307129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.131053924560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.143959045410156
14 0.5825217214 	 21.143959829 	 21.143959829
epoch_time;  33.98073959350586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31493762135505676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.006828308105469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.296307563781738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.82352638244629
15 0.5799636169 	 21.8235259185 	 21.8235259185
epoch_time;  33.4018132686615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33583319187164307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.246140480041504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.485025405883789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.31053352355957
16 0.5780009461 	 22.3105336782 	 22.3105336782
epoch_time;  33.008854150772095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42836108803749084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.6070637702941895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.25252628326416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.34459686279297
17 0.5756599566 	 21.3445959143 	 21.3445959143
epoch_time;  33.146928787231445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2740364074707031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.376458168029785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.677824974060059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.786455154418945
18 0.5729437062 	 21.7864548142 	 21.7864548142
epoch_time;  32.77654051780701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20706859230995178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.400876998901367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.553123474121094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.673070907592773
19 0.5736816241 	 22.6730706292 	 22.6730706292
epoch_time;  32.89859986305237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20701470971107483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.409075736999512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.559237480163574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.680740356445312
It took 731.251880645752 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137937
Array Job ID: 2137927_7
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 04:41:21
CPU Efficiency: 12.50% of 1-13:31:30 core-walltime
Job Wall-clock time: 02:05:05
Memory Utilized: 8.76 GB
Memory Efficiency: 28.04% of 31.25 GB
