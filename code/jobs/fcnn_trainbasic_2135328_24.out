wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170832-1fp46siv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-mandu-1153
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1fp46siv
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▄▂▂▃▂▃▄▂▄▅▂▁▂▂▁▃▃▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▂▃▃▁▃▅▂▄▆▃▂▁▄▂▃▄▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▃▁▂▄▂▃▄▃▃▅▃▁▂▂▂▃▃▄▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▅▂▁▃▄▁▃▆▅▆█▅▃▁▅▄▄▆▆▆
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.30755
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28073
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.20942
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20975
wandb:                         Train loss 3.37932
wandb: 
wandb: 🚀 View run lucky-mandu-1153 at: https://wandb.ai/nreints/thesis/runs/1fp46siv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170832-1fp46siv/logs
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21468977630138397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3820502758026123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30115383863449097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4484134912490845
0 8.7253135314 	 0.4484134983 	 0.4667525833
epoch_time;  37.693755865097046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1970803588628769
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2902391254901886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19087375700473785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2927141487598419
1 4.1013049705 	 0.292714155 	 0.3062155852
epoch_time;  36.589542865753174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16796879470348358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2909870445728302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19087165594100952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30549535155296326
2 3.8316636341 	 0.3054953498 	 0.3186343013
epoch_time;  36.50135087966919
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1496737003326416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22858989238739014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15847255289554596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2377985417842865
3 3.7106756728 	 0.2377985361 	 0.2522234736
epoch_time;  36.067912340164185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1778082698583603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24512654542922974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18141469359397888
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2551714777946472
4 3.6430326861 	 0.2551714923 	 0.2700517603
epoch_time;  36.44549369812012
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18874013423919678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2407626211643219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2186039686203003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2879739999771118
5 3.5901161561 	 0.2879740122 	 0.3011868451
epoch_time;  36.33091354370117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15045304596424103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2093278467655182
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1765073984861374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2419011890888214
6 3.5487001867 	 0.2419011915 	 0.2564124649
epoch_time;  36.073217153549194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1718115210533142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2548734247684479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19948329031467438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2728058993816376
7 3.5225273861 	 0.2728058892 	 0.2867416382
epoch_time;  36.23204064369202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21236442029476166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3070269525051117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2296312004327774
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3284410536289215
8 3.4973308173 	 0.3284410425 	 0.3411839459
epoch_time;  36.553497314453125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19625224173069
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23624688386917114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1988065093755722
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24059875309467316
9 3.4742731651 	 0.2405987508 	 0.2547030268
epoch_time;  36.28767943382263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21235103905200958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2887321412563324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20893989503383636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.299749493598938
10 3.4572041485 	 0.2997494878 	 0.3126702798
epoch_time;  36.34588646888733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23950640857219696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33810168504714966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23328137397766113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34244340658187866
11 3.4497757939 	 0.3424434146 	 0.3542947305
epoch_time;  36.78614115715027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19600380957126617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24367207288742065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19415150582790375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25218167901039124
12 3.4322092396 	 0.2521816769 	 0.2631851196
epoch_time;  36.741780042648315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17533883452415466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21911297738552094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16735486686229706
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22000505030155182
13 3.4207473579 	 0.220005056 	 0.2336194219
epoch_time;  36.78840494155884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15380308032035828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19783703982830048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17421141266822815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2341269850730896
14 3.4132462736 	 0.2341269828 	 0.2477870632
epoch_time;  36.63776183128357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1981712132692337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27150866389274597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17555031180381775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25130248069763184
15 3.4064295103 	 0.251302482 	 0.2648815258
epoch_time;  36.59505319595337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19297023117542267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2149065136909485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17505080997943878
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21708588302135468
16 3.3942423144 	 0.2170858847 	 0.2260747549
epoch_time;  36.325777769088745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18843084573745728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2582911550998688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19292354583740234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28265395760536194
17 3.3855653219 	 0.2826539529 	 0.296000259
epoch_time;  36.26301145553589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21667546033859253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28736886382102966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2048121839761734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29517707228660583
18 3.3858350833 	 0.2951770679 	 0.3060982163
epoch_time;  36.00095891952515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2096950262784958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2808603346347809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20944727957248688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3075210452079773
19 3.3793168426 	 0.3075210365 	 0.3202370515
epoch_time;  36.452552318573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20975226163864136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28072524070739746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2094152718782425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3075464963912964
It took 790.9453029632568 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn36: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135443.0

JOB STATISTICS
==============
Job ID: 2135443
Array Job ID: 2135328_24
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 04:02:06 core-walltime
Job Wall-clock time: 00:13:27
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
