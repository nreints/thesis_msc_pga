wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230703_151102-jbgtn624
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-vortex-450
wandb: ‚≠êÔ∏è View project at https://wandb.ai/nreints/ThesisFinal2
wandb: üöÄ View run at https://wandb.ai/nreints/ThesisFinal2/runs/jbgtn624
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run dashing-vortex-450 at: https://wandb.ai/nreints/ThesisFinal2/runs/jbgtn624
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230703_151102-jbgtn624/logs
Training on dataset: data_t(5,20)_r(5,20)_combiR_pNone_gNone
Testing on 7 datasets: ['bigBlocks_data_t(5,20)_r(5,20)_tennis_pNone_gNone', 'bigBlocks_data_t(5,20)_r(5,20)_semi_pNone_gNone', 'data_t(5,20)_r(5,20)_none_pNone_gNone', 'bigBlocks_data_t(5,20)_r(5,20)_none_pNone_gNone', 'bigBlocks_data_t(5,20)_r(5,20)_combiR_pNone_gNone', 'bigBlocks_data_t(5,20)_r(5,20)_full_pNone_gNone', 'data_t(5,20)_r(5,20)_combiR_pNone_gNone']
Focussing on identity: False
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 65.5652186870575 seconds.
-- Finished Train Dataloader --
The dataloader for data/bigBlocks_data_t(5,20)_r(5,20)_tennis_pNone_gNone took 16.716989040374756 seconds.
The dataloader for data/bigBlocks_data_t(5,20)_r(5,20)_semi_pNone_gNone took 16.32244110107422 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_none_pNone_gNone took 16.413687229156494 seconds.
The dataloader for data/bigBlocks_data_t(5,20)_r(5,20)_none_pNone_gNone took 16.53723430633545 seconds.
The dataloader for data/bigBlocks_data_t(5,20)_r(5,20)_combiR_pNone_gNone took 16.534167289733887 seconds.
The dataloader for data/bigBlocks_data_t(5,20)_r(5,20)_full_pNone_gNone took 16.543790578842163 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 16.297909259796143 seconds.
-- Finished Test Dataloader(s) --
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/gru.py", line 295, in <module>
    model = model_pipeline(
  File "/gpfs/home2/nreints/MScThesis/code/utils.py", line 325, in model_pipeline
    ) = make(
  File "/gpfs/home2/nreints/MScThesis/code/utils.py", line 433, in make
    model = model_class(n_datapoints, config).to(device)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 187, in _apply
    ret = super(RNNBase, self)._apply(fn)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
srun: error: gcn23: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=3008125.0

JOB STATISTICS
==============
Job ID: 3008125
Array Job ID: 3008110_4
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:03:17
CPU Efficiency: 4.93% of 01:06:36 core-walltime
Job Wall-clock time: 00:03:42
Memory Utilized: 6.80 GB
Memory Efficiency: 0.00% of 0.00 MB
