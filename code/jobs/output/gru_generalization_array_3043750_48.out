wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_170033-6n8tp6o4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-resonance-1249
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/6n8tp6o4
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–†â–ƒâ–‚â–â–â–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run sleek-resonance-1249 at: https://wandb.ai/nreints/ThesisFinal2/runs/6n8tp6o4
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_170033-6n8tp6o4/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_170610-3tjfpuf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-forest-1264
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/3tjfpuf6
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run robust-forest-1264 at: https://wandb.ai/nreints/ThesisFinal2/runs/3tjfpuf6
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_170610-3tjfpuf6/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_171139-yes771cz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-pine-1275
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/yes771cz
Training on dataset: data_tennis_pNone_gNone_tennisEffect
Testing on 1 datasets: ['data_tennis_pNone_gNone_tennisEffect']
Focussing on identity: False
Using extra input: True
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 60.31447386741638 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 15.003312826156616 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.002049265 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.35406e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 25.733149766921997
Epoch 1/9
	 Logging train Loss: 5.11322e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.37377e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.86954379081726
Epoch 2/9
	 Logging train Loss: 3.97148e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.82712e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.480417251586914
Epoch 3/9
	 Logging train Loss: 3.38296e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.27089e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.823103189468384
Epoch 4/9
	 Logging train Loss: 2.83662e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.29514e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.633825540542603
Epoch 5/9
	 Logging train Loss: 2.3407e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.69755e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.54976534843445
Epoch 6/9
	 Logging train Loss: 1.80719e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.62081e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.627086639404297
Epoch 7/9
	 Logging train Loss: 1.4842e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.82563e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.492886543273926
Epoch 8/9
	 Logging train Loss: 1.34091e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.56794e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.652748823165894
Epoch 9/9
	 Logging train Loss: 1.22148e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.61367e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.836801052093506
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  338.41240215301514  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 54.38325095176697 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.854966878890991 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0020939473 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 7.33038e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.98361349105835
Epoch 1/9
	 Logging train Loss: 5.85707e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.81466e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.739355087280273
Epoch 2/9
	 Logging train Loss: 4.77019e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 5.22093e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.730005502700806
Epoch 3/9
	 Logging train Loss: 3.98831e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.91348e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.757704257965088
Epoch 4/9
	 Logging train Loss: 3.35936e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.83652e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.736513376235962
Epoch 5/9
	 Logging train Loss: 2.72162e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.99151e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.57437038421631
Epoch 6/9
	 Logging train Loss: 2.22187e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.06572e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.895719289779663
Epoch 7/9
	 Logging train Loss: 1.79325e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.55505e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.810312032699585
Epoch 8/9
	 Logging train Loss: 1.48534e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.00393e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.571961641311646
Epoch 9/9
	 Logging train Loss: 1.33576e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.9651e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.627442598342896
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  328.74257612228394  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 53.246681690216064 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.327553510665894 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0021213896 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.36812e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.742039442062378
Epoch 1/9
	 Logging train Loss: 5.61104e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 5.41848e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.733485460281372
Epoch 2/9
	 Logging train Loss: 4.29846e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.9486e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.789173126220703
Epoch 3/9
	 Logging train Loss: 3.68949e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.11393e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.651774883270264
Epoch 4/9
	 Logging train Loss: 2.95657e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.54779e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.68605327606201
Epoch 5/9
	 Logging train Loss: 2.4251e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.88732e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.70962405204773
Epoch 6/9
	 Logging train Loss: 1.92861e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.41877e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.91475558280945
Epoch 7/9
	 Logging train Loss: 1.59994e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–‡â–…â–…â–ƒâ–‚â–â–‚â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run swept-pine-1275 at: https://wandb.ai/nreints/ThesisFinal2/runs/yes771cz
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_171139-yes771cz/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_171703-90ihjrxt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-bush-1289
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/90ihjrxt
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–â–‚
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run lunar-bush-1289 at: https://wandb.ai/nreints/ThesisFinal2/runs/90ihjrxt
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_171703-90ihjrxt/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_172229-2fn56uog
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-tree-1304
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/2fn56uog
wandb: Waiting for W&B process to finish... (success).
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–†â–…â–…â–„â–‚â–ƒâ–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run different-tree-1304 at: https://wandb.ai/nreints/ThesisFinal2/runs/2fn56uog
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_172229-2fn56uog/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_172755-thkixli8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-mountain-1317
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/thkixli8
	 Logging test loss: 1.85444e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.494988441467285
Epoch 8/9
	 Logging train Loss: 1.41391e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22046e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.822609663009644
Epoch 9/9
	 Logging train Loss: 1.28341e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.2946e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.444040060043335
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  324.01578092575073  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 53.25760841369629 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.314397096633911 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.002417621 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 7.52183e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.80217480659485
Epoch 1/9
	 Logging train Loss: 5.90276e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 7.24717e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.48427677154541
Epoch 2/9
	 Logging train Loss: 4.40268e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.2911e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.727193355560303
Epoch 3/9
	 Logging train Loss: 3.83739e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.28034e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.723466873168945
Epoch 4/9
	 Logging train Loss: 3.21353e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.94318e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.51642656326294
Epoch 5/9
	 Logging train Loss: 2.63449e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.91409e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.81760311126709
Epoch 6/9
	 Logging train Loss: 2.13935e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.50368e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.634169578552246
Epoch 7/9
	 Logging train Loss: 1.7798e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.66274e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.633204698562622
Epoch 8/9
	 Logging train Loss: 1.46109e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.18898e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.81794261932373
Epoch 9/9
	 Logging train Loss: 1.37759e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.37879e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 25.057573795318604
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  326.0973880290985  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 53.172183990478516 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.289174556732178 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0030282242 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.5288e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.504454851150513
Epoch 1/9
	 Logging train Loss: 5.53985e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.76925e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.5494487285614
Epoch 2/9
	 Logging train Loss: 4.4545e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.1975e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.201619625091553
Epoch 3/9
	 Logging train Loss: 3.91987e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.92372e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.496121168136597
Epoch 4/9
	 Logging train Loss: 3.29388e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.1286e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.121145486831665
Epoch 5/9
	 Logging train Loss: 2.67207e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.24561e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.369874238967896
Epoch 6/9
	 Logging train Loss: 2.29183e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.26655e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.235185623168945
Epoch 7/9
	 Logging train Loss: 1.74813e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.19314e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.18110990524292
Epoch 8/9
	 Logging train Loss: 1.49313e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.09249e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.170631408691406
Epoch 9/9
	 Logging train Loss: 1.35968e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.18883e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.276634693145752
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  326.2582986354828  seconds.
----- ITERATION 6/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 53.304123401641846 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.343109607696533 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0028604059 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 7.24806e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.134669542312622
Epoch 1/9
	 Logging train Loss: 5.67381e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.76583e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.239713430404663
Epoch 2/9
	 Logging train Loss: 4.40957e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.57156e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.17981719970703
Epoch 3/9
	 Logging train Loss: 3.70817e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.11696e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.422136068344116
Epoch 4/9
	 Logging train Loss: 3.15224e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.55707e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.136878728866577
Epoch 5/9
	 Logging train Loss: 2.66019e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run earthy-mountain-1317 at: https://wandb.ai/nreints/ThesisFinal2/runs/thkixli8
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_172755-thkixli8/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_173323-p0nn6t3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-jazz-1330
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/p0nn6t3c
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–„â–ƒâ–„â–ƒâ–‚â–‚â–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run vague-jazz-1330 at: https://wandb.ai/nreints/ThesisFinal2/runs/p0nn6t3c
wandb: Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_173323-p0nn6t3c/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_173848-3yb88qnx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-smoke-1347
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/3yb88qnx
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run electric-smoke-1347 at: https://wandb.ai/nreints/ThesisFinal2/runs/3yb88qnx
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_173848-3yb88qnx/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_174415-rfo7kron
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-aardvark-1358
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/rfo7kron
	 Logging test loss: 3.61726e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.2407443523407
Epoch 6/9
	 Logging train Loss: 2.22952e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.79485e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.117615461349487
Epoch 7/9
	 Logging train Loss: 1.80892e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.76895e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.26785445213318
Epoch 8/9
	 Logging train Loss: 1.57663e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.21652e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.3351149559021
Epoch 9/9
	 Logging train Loss: 1.42227e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.56811e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.0352840423584
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  327.4692690372467  seconds.
----- ITERATION 7/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 52.95715355873108 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.197948694229126 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0020129497 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 8.28825e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.341376304626465
Epoch 1/9
	 Logging train Loss: 5.12044e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.96523e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.140409469604492
Epoch 2/9
	 Logging train Loss: 4.08076e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.38066e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.35669445991516
Epoch 3/9
	 Logging train Loss: 3.47757e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.81674e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.138253688812256
Epoch 4/9
	 Logging train Loss: 2.95847e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.96712e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.187801122665405
Epoch 5/9
	 Logging train Loss: 2.59252e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.60387e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.478163242340088
Epoch 6/9
	 Logging train Loss: 2.05435e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.98615e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 25.260188579559326
Epoch 7/9
	 Logging train Loss: 1.66731e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.1513e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.82456874847412
Epoch 8/9
	 Logging train Loss: 1.47265e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22151e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.91795516014099
Epoch 9/9
	 Logging train Loss: 1.31453e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22909e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.41223168373108
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  325.21186995506287  seconds.
----- ITERATION 8/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 55.843931913375854 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.938296794891357 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0029874565 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.41522e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.578809022903442
Epoch 1/9
	 Logging train Loss: 5.35905e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.00366e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.407432079315186
Epoch 2/9
	 Logging train Loss: 4.10408e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.36023e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.165998697280884
Epoch 3/9
	 Logging train Loss: 3.65791e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.76839e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.488279819488525
Epoch 4/9
	 Logging train Loss: 3.0804e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.21808e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.162384271621704
Epoch 5/9
	 Logging train Loss: 2.72336e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.88676e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.52642297744751
Epoch 6/9
	 Logging train Loss: 2.25128e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.67388e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.236047744750977
Epoch 7/9
	 Logging train Loss: 1.87494e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.16733e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.334342002868652
Epoch 8/9
	 Logging train Loss: 1.53309e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.55103e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.480698347091675
Epoch 9/9
	 Logging train Loss: 1.38045e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.24833e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.43649458885193
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  326.8623697757721  seconds.
----- ITERATION 9/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 54.526158809661865 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.761199951171875 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0024011724 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.1815e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.06240153312683
Epoch 1/9
	 Logging train Loss: 4.76987e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.43971e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.40341830253601
Epoch 2/9
	 Logging train Loss: 3.92554e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.72477e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.351722717285156
Epoch 3/9
	 Logging train Loss: 3.39223e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–†â–…â–„â–„â–ƒâ–‚â–‚â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run sleek-aardvark-1358 at: https://wandb.ai/nreints/ThesisFinal2/runs/rfo7kron
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_174415-rfo7kron/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_174938-rxhq1uix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-dragon-1365
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/rxhq1uix
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–ƒâ–ƒâ–‚â–ƒâ–â–â–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run good-dragon-1365 at: https://wandb.ai/nreints/ThesisFinal2/runs/rxhq1uix
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_174938-rxhq1uix/logs
	 Logging test loss: 3.2417e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.352380990982056
Epoch 4/9
	 Logging train Loss: 2.89063e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.54085e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.27262020111084
Epoch 5/9
	 Logging train Loss: 2.41288e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.75564e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.426002264022827
Epoch 6/9
	 Logging train Loss: 2.02338e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.06415e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.297416925430298
Epoch 7/9
	 Logging train Loss: 1.67789e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.5686e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.27515697479248
Epoch 8/9
	 Logging train Loss: 1.44541e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.09889e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.313291311264038
Epoch 9/9
	 Logging train Loss: 1.32103e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.14492e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 23.962213039398193
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  322.99248909950256  seconds.
----- ITERATION 10/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 53.29406476020813 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.29969048500061 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0019104671 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 0.0001073126 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.35203242301941
Epoch 1/9
	 Logging train Loss: 4.75834e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.90354e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.266147136688232
Epoch 2/9
	 Logging train Loss: 3.94598e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.30705e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.38667345046997
Epoch 3/9
	 Logging train Loss: 3.37719e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.95799e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 23.967459201812744
Epoch 4/9
	 Logging train Loss: 2.83544e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.95056e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.217454195022583
Epoch 5/9
	 Logging train Loss: 2.42621e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.73948e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.623697757720947
Epoch 6/9
	 Logging train Loss: 1.968e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.80551e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.4879789352417
Epoch 7/9
	 Logging train Loss: 1.6012e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.33669e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.273202419281006
Epoch 8/9
	 Logging train Loss: 1.41265e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.23707e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.161202430725098
Epoch 9/9
	 Logging train Loss: 1.29842e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.75671e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 23.93982243537903
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'True'.pth
It took  322.2086191177368  seconds.

JOB STATISTICS
==============
Job ID: 3043759
Array Job ID: 3043750_48
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:55:53
CPU Efficiency: 5.67% of 16:25:30 core-walltime
Job Wall-clock time: 00:54:45
Memory Utilized: 5.90 GB
Memory Efficiency: 0.00% of 0.00 MB
