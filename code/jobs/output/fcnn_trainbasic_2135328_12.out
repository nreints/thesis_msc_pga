wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165127-t9qv2pyc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-snake-1142
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/t9qv2pyc
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▁▃▃▃▂▅▃▃▁▄▁▂▃▂▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▂▃▂▂▂▂▁▂▂▃▁▂▁▃▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▁▁▃▃▃▂▆▃▃▁▅▂▂▃▂▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▂▂▂▂▁▂▂▂▁▂▂▂▁▁▁▂▂
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 8.43464
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.6264
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.69344
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30723
wandb:                         Train loss 3.00608
wandb: 
wandb: 🚀 View run lambent-snake-1142 at: https://wandb.ai/nreints/thesis/runs/t9qv2pyc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165127-t9qv2pyc/logs
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7060213088989258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3375952243804932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.999300956726074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.613304138183594
0 7.923802339 	 14.6133036845 	 14.6190139358
epoch_time;  37.25459861755371
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5308395624160767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0060389041900635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.049350738525391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.907039642333984
1 4.7499388409 	 10.9070391681 	 10.9078210779
epoch_time;  36.35005807876587
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3782646059989929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6818317174911499
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.9387407302856445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.110189437866211
2 3.9834083552 	 9.110189242 	 9.1104089685
epoch_time;  37.56386399269104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3428257703781128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7091522216796875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.115591526031494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.180381774902344
3 3.622498757 	 8.1803816512 	 8.1806812183
epoch_time;  36.406296730041504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3496461510658264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6377918720245361
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1364827156066895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.817931175231934
4 3.4343560903 	 7.8179311391 	 7.8185282939
epoch_time;  36.34950256347656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3077506721019745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.651710033416748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.288983345031738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.748991966247559
5 3.3090294675 	 9.748992425 	 9.7495981577
epoch_time;  37.04718589782715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33212006092071533
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6127359867095947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.276678085327148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.51466178894043
6 3.2528681978 	 9.5146616343 	 9.5155946474
epoch_time;  36.096980571746826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35765472054481506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6712481379508972
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.248985767364502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.305564880371094
7 3.1954093628 	 9.3055644267 	 9.3065515467
epoch_time;  36.37953996658325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2954900860786438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5447342991828918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4852728843688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.263957977294922
8 3.1599787232 	 8.2639582454 	 8.2653109164
epoch_time;  36.38228678703308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33418092131614685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6582309007644653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.686184883117676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.626776695251465
9 3.1253940631 	 11.626776288 	 11.6282820418
epoch_time;  36.287741899490356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32547855377197266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.675901472568512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.092245101928711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.994141578674316
10 3.1134078659 	 8.9941412848 	 8.9962441934
epoch_time;  36.45077323913574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3237353265285492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.703610897064209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.3256049156188965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.566876411437988
11 3.0941883622 	 9.5668767156 	 9.5689670872
epoch_time;  36.75886845588684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26901721954345703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5568243265151978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.011928081512451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.44887638092041
12 3.0702758045 	 7.4488762933 	 7.4514846389
epoch_time;  36.38847517967224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31844764947891235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6380909085273743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.362053394317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.013253211975098
13 3.0501590798 	 11.0132535367 	 11.0158051362
epoch_time;  36.216941356658936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3048582077026367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5738111138343811
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.354623794555664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.8318657875061035
14 3.053087445 	 7.8318656303 	 7.8349820524
epoch_time;  36.13051700592041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35736554861068726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7230256795883179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.776790618896484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.622339248657227
15 3.0379199814 	 8.6223388672 	 8.6253305796
epoch_time;  36.552836894989014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2706787884235382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5771103501319885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.1705498695373535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.203012466430664
16 3.019713405 	 9.2030121674 	 9.2061708193
epoch_time;  36.08107376098633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27620092034339905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5510328412055969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.766198635101318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.620158195495605
17 3.0240229747 	 8.6201580976 	 8.6235232791
epoch_time;  36.19118142127991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2689266800880432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5301142930984497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.248844623565674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.131141662597656
18 3.0209154298 	 9.1311417863 	 9.134638276
epoch_time;  36.671935081481934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30720055103302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6262168884277344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.695226192474365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.431434631347656
19 3.0060848373 	 8.4314347551 	 8.4350467166
epoch_time;  36.69024443626404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30723297595977783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.626401960849762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.693441867828369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.434639930725098
It took 793.8494598865509 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn30: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135376.0

JOB STATISTICS
==============
Job ID: 2135376
Array Job ID: 2135328_12
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 04:02:06 core-walltime
Job Wall-clock time: 00:13:27
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
