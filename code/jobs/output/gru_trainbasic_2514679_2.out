wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_124733-y9jhc9x2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-wind-481
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/y9jhc9x2
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: / 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–†â–„â–‚â–â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.00079
wandb:                                             Train loss 0.00082
wandb: 
wandb: ðŸš€ View run fine-wind-481 at: https://wandb.ai/nreints/test/runs/y9jhc9x2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_124733-y9jhc9x2/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_125522-6l0lranz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-firebrand-493
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/6l0lranz
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.00079
wandb:                                             Train loss 0.00082
wandb: 
wandb: ðŸš€ View run dashing-firebrand-493 at: https://wandb.ai/nreints/test/runs/6l0lranz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_125522-6l0lranz/logs
Running for data type: eucl_motion
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(12, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=12, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 0.3818548429 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.015107388608157635 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.996232986450195
Epoch 1
	 Logging train Loss: 0.0125369987 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.011451752856373787 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 40.88726496696472
Epoch 2
	 Logging train Loss: 0.0099261154 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.007265504449605942 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.624715089797974
Epoch 3
	 Logging train Loss: 0.0048292559 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0027244132943451405 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.0419921875
Epoch 4
	 Logging train Loss: 0.0020905316 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0016977547202259302 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.19613313674927
Epoch 5
	 Logging train Loss: 0.0014119212 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0012707120040431619 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 38.976356744766235
Epoch 6
	 Logging train Loss: 0.0011216242 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0010092511074617505 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.25245642662048
Epoch 7
	 Logging train Loss: 0.0009604899 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0009103872580453753 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.26826596260071
Epoch 8
	 Logging train Loss: 0.0008755176 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0008785544196143746 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.55085372924805
Epoch 9
	 Logging train Loss: 0.0008188524 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0007917723851278424 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.19447088241577
	 Logging test loss: 0.0007920119096525013 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  470.2049562931061  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(12, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=12, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 0.3436409679 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.013488021679222584 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.388728618621826
Epoch 1
	 Logging train Loss: 0.0119573112 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.009289774112403393 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.503347396850586
Epoch 2
	 Logging train Loss: 0.0076314127 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.004983358550816774 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.24865102767944
Epoch 3
	 Logging train Loss: 0.0034959565 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0023909558076411486 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.31243348121643
Epoch 4
	 Logging train Loss: 0.0020671399 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0015849637566134334 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.4614143371582
Epoch 5
	 Logging train Loss: 0.0014397091 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0012250336585566401 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.203001499176025
Epoch 6
	 Logging train Loss: 0.0011391224 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0010049643460661173 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.4131646156311
Epoch 7
	 Logging train Loss: 0.0009784059 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0008519431576132774 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.04522681236267
Epoch 8
	 Logging train Loss: 0.0008808563 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0008681028266437352 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.01923203468323
Epoch 9
	 Logging train Loss: 0.0008156788 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0007936176843941212 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 39.324076890945435
	 Logging test loss: 0.0007933562737889588 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  453.57792615890503  seconds.

JOB STATISTICS
==============
Job ID: 2514681
Array Job ID: 2514679_2
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:03:14
CPU Efficiency: 64.70% of 04:43:12 core-walltime
Job Wall-clock time: 00:15:44
Memory Utilized: 25.58 GB
Memory Efficiency: 81.87% of 31.25 GB
