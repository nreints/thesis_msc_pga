wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165129-gbmkubn0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-fuse-1143
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/gbmkubn0
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.58288
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24089
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.38492
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12436
wandb:                         Train loss 1.77685
wandb: 
wandb: ðŸš€ View run sparkling-fuse-1143 at: https://wandb.ai/nreints/thesis/runs/gbmkubn0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165129-gbmkubn0/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3815353810787201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5608083605766296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.904501914978027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.146306991577148
0 4.8434184184 	 9.1463068782 	 9.1463068782
epoch_time;  32.40681004524231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20800518989562988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3822212815284729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.134688377380371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.1938323974609375
1 2.1085572859 	 7.1938324799 	 7.1938324799
epoch_time;  31.202540397644043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1748289167881012
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35555484890937805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.623810291290283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.678674697875977
2 2.0185659276 	 6.6786746463 	 6.6786746463
epoch_time;  31.240356922149658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18489524722099304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32703733444213867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.31807279586792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.388094425201416
3 1.9756090788 	 6.3880945154 	 6.3880945154
epoch_time;  31.365725994110107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17818275094032288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35827332735061646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.054491996765137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.149486541748047
4 1.9255483494 	 6.1494866448 	 6.1494866448
epoch_time;  31.71351647377014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19754500687122345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39300215244293213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.917063236236572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.050267696380615
5 1.9010980873 	 6.0502678948 	 6.0502678948
epoch_time;  31.669678211212158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18194176256656647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37186121940612793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.778074741363525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.926534175872803
6 1.8834454143 	 5.9265341269 	 5.9265341269
epoch_time;  32.005675077438354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19189657270908356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34309443831443787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.8155341148376465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.938982009887695
7 1.8656871178 	 5.9389819996 	 5.9389819996
epoch_time;  31.076068878173828
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17530645430088043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3258403241634369
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.64540433883667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.75587272644043
8 1.852822378 	 5.7558725718 	 5.7558725718
epoch_time;  30.866567850112915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1623091846704483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30301809310913086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.55579137802124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.669423580169678
9 1.8410809826 	 5.6694236962 	 5.6694236962
epoch_time;  31.208611249923706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1650061160326004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32922717928886414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.459416389465332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.624966144561768
10 1.8302711895 	 5.6249663482 	 5.6249663482
epoch_time;  31.006258249282837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14009034633636475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2881060540676117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.441732406616211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.572754383087158
11 1.8216224624 	 5.5727545661 	 5.5727545661
epoch_time;  31.13213014602661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17101427912712097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.338771253824234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.244806289672852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.453685760498047
12 1.811887417 	 5.4536855337 	 5.4536855337
epoch_time;  31.038468599319458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17077770829200745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.317274808883667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.126044273376465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.331027507781982
13 1.8057604294 	 5.3310273042 	 5.3310273042
epoch_time;  31.1160728931427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12485641241073608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2440561205148697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.035602569580078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.191099643707275
14 1.804165168 	 5.1910994246 	 5.1910994246
epoch_time;  31.06876015663147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1491551548242569
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29861560463905334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.946927547454834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.176686763763428
15 1.7912448891 	 5.1766868798 	 5.1766868798
epoch_time;  31.506877660751343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12345336377620697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2914276719093323
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7271037101745605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.935389518737793
16 1.7916110174 	 4.9353895033 	 4.9353895033
epoch_time;  31.174580097198486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11943516135215759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2403556853532791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.555546760559082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.730420112609863
17 1.7880067612 	 4.7304202518 	 4.7304202518
epoch_time;  31.586708068847656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12959854304790497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2654916048049927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.451670169830322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.667322635650635
18 1.7759050229 	 4.6673224372 	 4.6673224372
epoch_time;  30.84946370124817
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12426186352968216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24073639512062073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.386770725250244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.582550048828125
19 1.7768539811 	 4.5825498839 	 4.5825498839
epoch_time;  31.398047924041748
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12436345964670181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24088996648788452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.384923934936523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.582881927490234
It took 691.8737616539001 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn48: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135378.0

JOB STATISTICS
==============
Job ID: 2135378
Array Job ID: 2135328_14
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:32:24 core-walltime
Job Wall-clock time: 00:11:48
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
