wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_135639-fmqy7ooc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-smoke-542
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/fmqy7ooc
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–‚â–â–‚â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 0.03035
wandb:                                             Train loss 0.03593
wandb: 
wandb: ðŸš€ View run hearty-smoke-542 at: https://wandb.ai/nreints/test/runs/fmqy7ooc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_135639-fmqy7ooc/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_140508-5sxk3a8o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-water-552
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/5sxk3a8o
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: | 0.078 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: / 0.078 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 0.03337
wandb:                                             Train loss 0.03602
wandb: 
wandb: ðŸš€ View run bumbling-water-552 at: https://wandb.ai/nreints/test/runs/5sxk3a8o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_140508-5sxk3a8o/logs
Running for data type: dual_quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 8.0849068289 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.25206172466278076 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 45.64140772819519
Epoch 1
	 Logging train Loss: 0.1397881298 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.09826118499040604 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.47448444366455
Epoch 2
	 Logging train Loss: 0.1016506811 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.07790704071521759 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.18771004676819
Epoch 3
	 Logging train Loss: 0.0936276037 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.06019692122936249 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.472591400146484
Epoch 4
	 Logging train Loss: 0.08300357 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.05213163420557976 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.192068338394165
Epoch 5
	 Logging train Loss: 0.0642064277 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.043829355388879776 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 44.05816388130188
Epoch 6
	 Logging train Loss: 0.0565792995 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.06285187602043152 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.40978741645813
Epoch 7
	 Logging train Loss: 0.0475928099 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.033832792192697525 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 45.08603572845459
Epoch 8
	 Logging train Loss: 0.0438776717 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.04658212885260582 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 44.83627414703369
Epoch 9
	 Logging train Loss: 0.0359284461 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.030351875349879265 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 45.92393469810486
	 Logging test loss: 0.030353110283613205 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  509.8158869743347  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 13.795997528 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.34399136900901794 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 46.60236072540283
Epoch 1
	 Logging train Loss: 0.1470599512 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.1063646748661995 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 45.132880210876465
Epoch 2
	 Logging train Loss: 0.1093709357 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.07966092973947525 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.570316791534424
Epoch 3
	 Logging train Loss: 0.0920648206 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.07237059623003006 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.704622983932495
Epoch 4
	 Logging train Loss: 0.0768837158 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.07406321167945862 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.85787320137024
Epoch 5
	 Logging train Loss: 0.0659542517 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.05382007732987404 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.65934991836548
Epoch 6
	 Logging train Loss: 0.0574286426 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.04139934480190277 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.99331712722778
Epoch 7
	 Logging train Loss: 0.0492570111 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.043225135654211044 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.37975287437439
Epoch 8
	 Logging train Loss: 0.0424453496 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.03197845071554184 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.68734908103943
Epoch 9
	 Logging train Loss: 0.036022059 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.033388759940862656 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 43.75630807876587
	 Logging test loss: 0.03336705267429352 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  504.8037574291229  seconds.

JOB STATISTICS
==============
Job ID: 2514853
Array Job ID: 2514848_5
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:33:21
CPU Efficiency: 69.11% of 05:08:42 core-walltime
Job Wall-clock time: 00:17:09
Memory Utilized: 25.14 GB
Memory Efficiency: 80.44% of 31.25 GB
