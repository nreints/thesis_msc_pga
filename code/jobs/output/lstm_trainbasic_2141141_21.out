/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_081446-1diuwweo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-dragon-1598
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/1diuwweo
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548b48528f0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad9244f0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad924670>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad924610>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013726524077355862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3720195293426514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04615115001797676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5903797149658203
0 1.8859573122 	 2.5903797899
epoch_time;  34.75907897949219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010437274351716042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0521762371063232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032403405755758286
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2407915592193604
1 0.075061928 	 2.2407916204
epoch_time;  34.02994179725647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004360440652817488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5488135814666748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015423329547047615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.665450096130371
2 0.0235463142 	 1.6654500817
epoch_time;  34.251171827316284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014554565772414207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1035232543945312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.034001126885414124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.205662965774536
3 0.0654453159 	 2.2056630299
epoch_time;  34.2901074886322
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033417404629290104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7643884420394897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013260004110634327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.86322820186615
4 0.0210745145 	 1.8632281439
epoch_time;  34.279969692230225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01867518201470375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5141665935516357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05084213614463806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6579601764678955
5 0.0657978679 	 2.6579602751
epoch_time;  34.1708447933197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014401193708181381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7846301794052124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02629934437572956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.866847276687622
6 0.0215977027 	 1.8668472889
epoch_time;  34.010467529296875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005529207177460194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5547239780426025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014500465244054794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5984563827514648
7 0.0129834824 	 1.5984563453
epoch_time;  34.35899090766907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003052386222407222
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3336668014526367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008460216224193573
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4001377820968628
8 0.0111079581 	 1.4001378178
epoch_time;  33.94333624839783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004661634098738432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.482612133026123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015258037485182285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6112587451934814
9 0.0474817332 	 1.611258792
epoch_time;  34.04786229133606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003290381981059909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2032614946365356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008638046681880951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2511956691741943
10 0.0122011923 	 1.2511957174
epoch_time;  34.22730731964111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0079575814306736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9969585537910461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014578153379261494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0672403573989868
11 0.0090774028 	 1.0672403664
epoch_time;  34.01512670516968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034142732620239258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2441562414169312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009590549394488335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.362168550491333
12 0.0162620718 	 1.3621685109
epoch_time;  33.850223779678345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014373379526659846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9254568815231323
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004430051427334547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9839563369750977
13 0.0079314371 	 0.9839563283
epoch_time;  33.96500253677368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006908933166414499
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.321682095527649
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016088563948869705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4848166704177856
14 0.0225502465 	 1.4848167212
epoch_time;  33.755030155181885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004438262432813644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1233175992965698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009697625413537025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2022348642349243
15 0.008792317 	 1.2022348271
epoch_time;  33.44531607627869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026372128631919622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0130088329315186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00593811459839344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0868936777114868
16 0.0074685571 	 1.0868936867
epoch_time;  33.91012787818909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004693356342613697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2424849271774292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011214127764105797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3395823240280151
17 0.0162846957 	 1.3395822761
epoch_time;  33.97594666481018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002680698409676552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9662821292877197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00590582937002182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0657780170440674
18 0.0062189543 	 1.065778012
epoch_time;  34.07995820045471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032805369701236486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.938569962978363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00735418451949954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0302098989486694
19 0.0060442929 	 1.030209913
epoch_time;  33.520623207092285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030134606640785933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4120306968688965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00890736747533083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4887659549713135
20 0.0229857659 	 1.4887659355
epoch_time;  34.20937776565552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014210643712431192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0755985975265503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004308759234845638
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0997625589370728
21 0.0067877131 	 1.0997626083
epoch_time;  33.91691708564758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011123589938506484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9476233720779419
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0032886306289583445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9842468500137329
22 0.006222587 	 0.9842468446
epoch_time;  34.001057147979736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004026179667562246
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–†â–…â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–ˆâ–ƒâ–‚â–‡â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–†â–„â–†â–…â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–â–â–â–‡â–ƒâ–‚â–‡â–ƒâ–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–„â–‚â–…â–‚â–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–ˆâ–‚â–â–…â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–ƒâ–‚â–„â–‚â–…â–„â–‚â–â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–ˆâ–â–â–…â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.36152
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.1977
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.02022
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01016
wandb:                         Train loss 0.0134
wandb: 
wandb: ğŸš€ View run filigreed-dragon-1598 at: https://wandb.ai/nreints/thesis/runs/1diuwweo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_081446-1diuwweo/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_083313-r6y6gy9m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-firecracker-1606
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/r6y6gy9m
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9141577482223511
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007266409229487181
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9552178978919983
23 0.0053237894 	 0.9552178974
epoch_time;  34.35339832305908
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025248134043067694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8402971625328064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005118640139698982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8591229319572449
24 0.0051630917 	 0.8591229303
epoch_time;  33.941486120224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030823195353150368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.328887939453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06392836570739746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.606477975845337
25 0.0372597422 	 2.6064779506
epoch_time;  34.24933457374573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029876576736569405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3746964931488037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00891209114342928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4687858819961548
26 0.0173140893 	 1.4687858651
epoch_time;  34.26110315322876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027260228525847197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0608147382736206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0071855392307043076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0976852178573608
27 0.0092440063 	 1.0976852002
epoch_time;  34.272847175598145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01787545159459114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2085742950439453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04162781685590744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3758203983306885
28 0.0254947249 	 2.3758203789
epoch_time;  33.86701416969299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01016424223780632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1993975639343262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020218385383486748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3599070310592651
29 0.0134000439 	 1.3599069832
epoch_time;  33.984943866729736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010164747014641762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1976990699768066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02022157981991768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3615163564682007
It took  1108.4157102108002  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548b4871a80>, <torch.utils.data.dataloader.DataLoader object at 0x1548825ae4a0>, <torch.utils.data.dataloader.DataLoader object at 0x1548b4966350>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae307c70>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013809596188366413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3517212867736816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0434516966342926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.508512020111084
0 1.7785241635 	 2.5085119149
epoch_time;  34.06022524833679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00783614907413721
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9344522953033447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025102773681282997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.090078592300415
1 0.0627401671 	 2.0900784864
epoch_time;  34.14384865760803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008747488260269165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9687323570251465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028526436537504196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1881096363067627
2 0.0746851769 	 2.188109614
epoch_time;  34.30429673194885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008379599079489708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.48176908493042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020598510280251503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6350758075714111
3 0.0215499872 	 1.6350757795
epoch_time;  34.22407793998718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007681760936975479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9399762153625488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022139616310596466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1679069995880127
4 0.0613032189 	 2.1679069773
epoch_time;  34.07664132118225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004026757087558508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5356882810592651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012549595907330513
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7347017526626587
5 0.0180254062 	 1.7347017501
epoch_time;  33.9076189994812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008724273182451725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.419910192489624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029393965378403664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6245856285095215
6 0.0565481309 	 2.6245856616
epoch_time;  33.94603896141052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005683696363121271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.560538411140442
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014685061760246754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6570913791656494
7 0.0176420797 	 1.6570914001
epoch_time;  33.45877194404602
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00929374247789383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7323020696640015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022145910188555717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9509795904159546
8 0.0263962901 	 1.9509795497
epoch_time;  33.96539068222046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002545893657952547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.346569299697876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009122735820710659
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4961366653442383
9 0.014271978 	 1.4961367143
epoch_time;  33.9446382522583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006694677751511335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1040798425674438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014819309115409851
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.177067518234253
10 0.0109605537 	 1.177067506
epoch_time;  33.812090158462524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007158714346587658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.571308970451355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021505948156118393
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7509567737579346
11 0.0558154318 	 1.7509568321
epoch_time;  34.136889696121216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0052511077374219894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1941587924957275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01338533777743578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.337536096572876
12 0.0155773148 	 1.3375361232
epoch_time;  33.98658204078674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029583508148789406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0395668745040894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00802022684365511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.147466778755188
13 0.0110805903 	 1.1474668382
epoch_time;  33.90880060195923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003573612542822957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0565557479858398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008960424922406673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1813067197799683
14 0.0177495698 	 1.1813067756
epoch_time;  33.947509765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024914611130952835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9323055744171143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04748622328042984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0633894205093384
15 0.0088071137 	 1.0633894353
epoch_time;  33.671125173568726
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–ˆâ–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–ˆâ–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.3457
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.36674
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00956
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00315
wandb:                         Train loss 0.0121
wandb: 
wandb: ğŸš€ View run floating-firecracker-1606 at: https://wandb.ai/nreints/thesis/runs/r6y6gy9m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_083313-r6y6gy9m/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_085117-sb72jh66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-kumquat-1613
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/sb72jh66
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002675684867426753
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0406551361083984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008683247491717339
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1865979433059692
16 0.0372795348 	 1.1865979336
epoch_time;  33.66218709945679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035485930275171995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8922722935676575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008300740271806717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0221202373504639
17 0.0099244295 	 1.0221202654
epoch_time;  34.00992202758789
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003972012083977461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.865064263343811
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008811133913695812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9612635970115662
18 0.0079173933 	 0.9612636048
epoch_time;  33.99459171295166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002834443701431155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7738744020462036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006117789074778557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8584434390068054
19 0.0069152311 	 0.8584434302
epoch_time;  34.147319078445435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030775584746152163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6935750842094421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006806713528931141
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8036965727806091
20 0.0063458334 	 0.8036965546
epoch_time;  34.013211488723755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019136009505018592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6630603075027466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004036385100334883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7573486566543579
21 0.0057537489 	 0.7573486513
epoch_time;  33.93117928504944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025645208079367876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6776443719863892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004783041309565306
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7415322065353394
22 0.0054551192 	 0.7415322019
epoch_time;  34.123806953430176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000947724562138319
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.687734067440033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002723388373851776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7447130084037781
23 0.0052062918 	 0.7447129909
epoch_time;  33.73312044143677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12705743312835693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.320813179016113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.296687513589859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.8442912101745605
24 0.2052495178 	 5.8442913874
epoch_time;  33.85277557373047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012287753634154797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1934778690338135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0376819409430027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.406182289123535
25 0.0798990162 	 2.4061823266
epoch_time;  33.55126714706421
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007759853266179562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7194631099700928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02217968739569187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7757437229156494
26 0.0273715733 	 1.7757437438
epoch_time;  33.76720595359802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053369090892374516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6377819776535034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01727987267076969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7154860496520996
27 0.0266993203 	 1.7154860021
epoch_time;  33.878459215164185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033679138869047165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4200917482376099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011479605920612812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4397896528244019
28 0.0152588122 	 1.4397896481
epoch_time;  34.30946087837219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003146268194541335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.366167664527893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009559695608913898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3417872190475464
29 0.0120965783 	 1.3417871942
epoch_time;  34.00133752822876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00314749195240438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3667360544204712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009558663703501225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3457019329071045
It took  1083.9799757003784  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548825ae440>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae3078e0>, <torch.utils.data.dataloader.DataLoader object at 0x15488260c2e0>, <torch.utils.data.dataloader.DataLoader object at 0x15488260c5b0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01862511783838272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1729090213775635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04713013768196106
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.593480348587036
0 1.8467427094 	 2.5934804127
epoch_time;  33.86050629615784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008058718405663967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7962349653244019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025193816050887108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.205173969268799
1 0.0615095071 	 2.205174011
epoch_time;  33.77018737792969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007029849104583263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1961185932159424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02640175074338913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.560856580734253
2 0.0644019717 	 2.5608564763
epoch_time;  33.73661470413208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037772213108837605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.59798002243042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013462087139487267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8825241327285767
3 0.0201641374 	 1.8825241043
epoch_time;  34.04287838935852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005791110917925835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1191117763519287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020595375448465347
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.41464900970459
4 0.0579669268 	 2.4146490644
epoch_time;  34.418012857437134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0049269236624240875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5705031156539917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013393095694482327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7930939197540283
5 0.0173995157 	 1.793093955
epoch_time;  34.12940692901611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005137811414897442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7983852624893188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015584687702357769
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9943807125091553
6 0.0335430951 	 1.9943807089
epoch_time;  33.91439771652222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00274276128038764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4566186666488647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009152628481388092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6086952686309814
7 0.0133869827 	 1.6086953155
epoch_time;  33.96006917953491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003928587306290865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2151892185211182
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00985017791390419
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–ˆâ–…â–‡â–…â–†â–„â–ƒâ–„â–‚â–‚â–â–â–ƒâ–‚â–‚â–„â–ƒâ–‚â–â–â–â–„â–‚â–‚â–„â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–ˆâ–…â–ˆâ–…â–†â–„â–ƒâ–„â–ƒâ–‚â–‚â–â–„â–ƒâ–‚â–…â–ƒâ–‚â–â–â–â–„â–‚â–‚â–…â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–…â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–„â–‚â–â–â–‡â–‚â–„â–‚â–â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–â–â–ˆâ–ƒâ–ƒâ–â–â–‚â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.88559
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.7717
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00395
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0015
wandb:                         Train loss 0.00578
wandb: 
wandb: ğŸš€ View run alight-kumquat-1613 at: https://wandb.ai/nreints/thesis/runs/sb72jh66
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_085117-sb72jh66/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_090923-7c8zirle
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-snake-1619
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/7c8zirle
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3502229452133179
8 0.0120641931 	 1.3502229535
epoch_time;  33.95670461654663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003526258748024702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4635164737701416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011541495099663734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.585565447807312
9 0.0291529994 	 1.5855653884
epoch_time;  33.751450061798096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018355374922975898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1449130773544312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006523588206619024
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2128465175628662
10 0.0109054239 	 1.2128464621
epoch_time;  33.767807722091675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031470409594476223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0682733058929443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008522208780050278
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1519474983215332
11 0.0093578646 	 1.1519474825
epoch_time;  33.98752474784851
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036193362902849913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8799107670783997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008181583136320114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.992808997631073
12 0.0081355667 	 0.9928090075
epoch_time;  33.80710244178772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015511121600866318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8110876083374023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004084714222699404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9289016723632812
13 0.0074496749 	 0.9289016954
epoch_time;  33.98014187812805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00462435744702816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3410906791687012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01249849982559681
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4570709466934204
14 0.0290321275 	 1.4570708952
epoch_time;  33.90200877189636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028113527223467827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1109567880630493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007625472731888294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2119317054748535
15 0.0092869989 	 1.2119317645
epoch_time;  34.06236481666565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003866582876071334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9837091565132141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008535065688192844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.087632179260254
16 0.0072311983 	 1.0876321937
epoch_time;  33.98006248474121
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008112667128443718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5286054611206055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01971166767179966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7228764295578003
17 0.0524945891 	 1.7228764191
epoch_time;  34.05421996116638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004570887424051762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1681890487670898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010280788876116276
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.328019380569458
18 0.0122104547 	 1.328019341
epoch_time;  33.986942768096924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022448415402323008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9632472395896912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00627174461260438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.117490291595459
19 0.0088156463 	 1.1174902786
epoch_time;  34.20754361152649
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001455844147130847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8581072092056274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004569742362946272
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0073610544204712
20 0.0073250656 	 1.0073610058
epoch_time;  34.05738806724548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024725260213017464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7612047791481018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04011054337024689
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9435358643531799
21 0.0065070859 	 0.9435358422
epoch_time;  33.95058488845825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0069893947802484035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7846972942352295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011911172419786453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9802656173706055
22 0.0060670435 	 0.9802656375
epoch_time;  34.388232946395874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008026917465031147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4098612070083618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02034861221909523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.673500418663025
23 0.0322544927 	 1.6735004529
epoch_time;  34.301868200302124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002713700756430626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9824257493019104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007241475861519575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.165617823600769
24 0.0086897057 	 1.1656177901
epoch_time;  34.31362247467041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016540694050490856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9270249009132385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004641690291464329
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.080458164215088
25 0.0065987975 	 1.0804581657
epoch_time;  33.59362506866455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004181188996881247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4964674711227417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011603208258748055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7186552286148071
26 0.0215281429 	 1.7186552203
epoch_time;  33.734116077423096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002404002705588937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0310747623443604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006173699162900448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.188392162322998
27 0.0080727679 	 1.1883922012
epoch_time;  33.66841101646423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027332890313118696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8295756578445435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006161047145724297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.996114194393158
28 0.0064578782 	 0.996114218
epoch_time;  33.77336001396179
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001500226091593504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7723228931427002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003953968640416861
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8887816071510315
29 0.0057756848 	 0.8887815908
epoch_time;  33.89086723327637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00150105613283813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7716968059539795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0039518349803984165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8855878710746765
It took  1085.8638348579407  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x154882557e20>, <torch.utils.data.dataloader.DataLoader object at 0x1548b47eac50>, <torch.utils.data.dataloader.DataLoader object at 0x1548b47eab60>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2bac20>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020494254305958748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.675243854522705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06432708352804184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9025566577911377
0 1.883341995 	 2.9025567277
epoch_time;  34.23454999923706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020519785583019257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.833493947982788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.060162484645843506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.969343423843384
1 0.0655087652 	 2.9693433871
epoch_time;  33.7472870349884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009304307401180267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8551132678985596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024934925138950348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9622154235839844
2 0.0299797 	 1.962215366
epoch_time;  33.82889938354492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00670145358890295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8714293241500854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018548771739006042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.946713924407959
3 0.036242275 	 1.9467139114
epoch_time;  33.71855068206787
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007965738885104656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.076850175857544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02547086402773857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1956515312194824
4 0.0509571708 	 2.1956514203
epoch_time;  33.92448973655701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037386047188192606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5441266298294067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012519233860075474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6403930187225342
5 0.0191109922 	 1.6403930295
epoch_time;  33.8563597202301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009788681752979755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5945377349853516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022138001397252083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6644340753555298
6 0.0379425701 	 1.6644340584
epoch_time;  33.94118070602417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033433884382247925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3245887756347656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009974370710551739
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.375502347946167
7 0.0139336525 	 1.3755023876
epoch_time;  34.004987478256226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019625958520919085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.147768497467041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0065556662157177925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.207519292831421
8 0.011240591 	 1.2075192547
epoch_time;  33.742852210998535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007936164736747742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1545705795288086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015639035031199455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2123785018920898
9 0.0116759307 	 1.2123784644
epoch_time;  33.88926911354065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014973229262977839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9100862741470337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0048127262853085995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9355536103248596
10 0.0075183879 	 0.9355536055
epoch_time;  34.162057876586914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002873321296647191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3060075044631958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009676448069512844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3204044103622437
11 0.0379196098 	 1.3204044215
epoch_time;  33.805965423583984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003347571473568678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.969972550868988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008312668651342392
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0043867826461792
12 0.010097708 	 1.0043867866
epoch_time;  33.59754967689514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016755351796746254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8258947134017944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.044363249093294144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.133598804473877
13 0.0339549727 	 2.1335987656
epoch_time;  33.7650465965271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002134275622665882
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.122766375541687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0075856163166463375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.203494906425476
14 0.0146427804 	 1.2034948989
epoch_time;  33.811633586883545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018537373980507255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9770470261573792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005442280787974596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0329002141952515
15 0.0089973165 	 1.0329002542
epoch_time;  33.91544508934021
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010737417032942176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8807086944580078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003849913366138935
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9428039789199829
16 0.0080397293 	 0.9428039735
epoch_time;  33.87791967391968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003109570359811187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2549550533294678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009828677400946617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3421874046325684
17 0.0287096946 	 1.3421874262
epoch_time;  34.06205368041992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002236786764115095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0012952089309692
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005892943125218153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0064607858657837
18 0.0081673499 	 1.0064607833
epoch_time;  34.13978290557861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028636506758630276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8957327008247375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006570915225893259
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8899925351142883
19 0.0073805224 	 0.8899925209
epoch_time;  33.99488425254822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002951257862150669
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.475795865058899
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009285376407206059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3951387405395508
20 0.0467668081 	 1.3951387434
epoch_time;  33.52892875671387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003552992595359683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2436712980270386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00811411440372467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1277744770050049
21 0.0081575482 	 1.1277745181
epoch_time;  33.721590757369995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017926504369825125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0126429796218872
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004989372566342354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9586788415908813
22 0.007020323 	 0.9586788304
epoch_time;  33.837778091430664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001927976030856371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9150853753089905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004956420976668596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8696425557136536
23 0.0064124426 	 0.8696425516
epoch_time;  34.034706115722656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005088506732136011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5807546377182007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013720493763685226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6023290157318115
24 0.0699727407 	 1.6023290352
epoch_time;  34.42956757545471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0045212917029857635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3802520036697388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011229396797716618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3629319667816162
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ˆâ–…â–…â–…â–„â–„â–ƒâ–‚â–‚â–â–ƒâ–â–…â–‚â–‚â–â–ƒâ–â–â–ƒâ–‚â–â–â–ƒâ–ƒâ–‚â–„â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–ˆâ–„â–…â–…â–ƒâ–„â–ƒâ–‚â–‚â–â–ƒâ–â–„â–‚â–â–â–‚â–â–â–ƒâ–‚â–â–â–„â–ƒâ–‚â–„â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–ˆâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–†â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–â–ƒâ–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ˆâ–„â–ƒâ–ƒâ–‚â–„â–‚â–â–ƒâ–â–‚â–‚â–‡â–â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–ƒâ–â–‚â–‚
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.91897
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.9369
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00913
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00479
wandb:                         Train loss 0.0068
wandb: 
wandb: ğŸš€ View run glowing-snake-1619 at: https://wandb.ai/nreints/thesis/runs/7c8zirle
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_090923-7c8zirle/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_092729-2kd3zmrh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-tiger-1625
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/2kd3zmrh
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
25 0.0111068268 	 1.3629320035
epoch_time;  34.10720419883728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026112045161426067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2361918687820435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0066080898977816105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2421002388000488
26 0.0088357629 	 1.2421002806
epoch_time;  33.85524582862854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007405517157167196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7279218435287476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019317328929901123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.669227123260498
27 0.0293587563 	 1.66922707
epoch_time;  34.13786840438843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002351288450881839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.103069543838501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00620567100122571
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.059428095817566
28 0.0100459894 	 1.059428143
epoch_time;  33.81326341629028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004789778497070074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9362754225730896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009124652482569218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9220016002655029
29 0.0067985276 	 0.922001588
epoch_time;  33.805397748947144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0047876713797450066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9369012713432312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009129129350185394
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.918971598148346
It took  1085.893586397171  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15488260eda0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae306f50>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae307c10>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae307730>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018095115199685097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4334633350372314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05111023038625717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.509021282196045
0 1.8626064904 	 2.5090212174
epoch_time;  34.114882946014404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009489323012530804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.037900447845459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02995236963033676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1116368770599365
1 0.0682296233 	 2.1116368043
epoch_time;  34.265857219696045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009692861698567867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7254972457885742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025734135881066322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7732051610946655
2 0.0317950599 	 1.7732051607
epoch_time;  34.23327112197876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003677707863971591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3047899007797241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012068264186382294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3619028329849243
3 0.0173967146 	 1.3619028881
epoch_time;  33.87053608894348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006510326638817787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0199100971221924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021975761279463768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.096803665161133
4 0.0757599974 	 2.0968036018
epoch_time;  33.97043299674988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0055670300498604774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4737697839736938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01564916968345642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5212355852127075
5 0.0180381296 	 1.5212355323
epoch_time;  33.81026029586792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011325021274387836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2279651165008545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03220181167125702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3691256046295166
6 0.0468842639 	 2.3691255045
epoch_time;  33.92614269256592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015203245915472507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5851612091064453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.030431527644395828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6475778818130493
7 0.0192767915 	 1.6475778447
epoch_time;  34.128814697265625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003593140048906207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3420968055725098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009110493585467339
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3695330619812012
8 0.0125920659 	 1.3695330202
epoch_time;  34.29892086982727
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011251959949731827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6084622144699097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024566208943724632
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6127697229385376
9 0.0475021062 	 1.6127697348
epoch_time;  34.15536069869995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002363816602155566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3814685344696045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007499301340430975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2928234338760376
10 0.0117351444 	 1.2928234458
epoch_time;  34.092610359191895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033727996051311493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1615785360336304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009232984855771065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1628175973892212
11 0.0219417907 	 1.1628175488
epoch_time;  34.10285544395447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001954745501279831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.852957010269165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005715286359190941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8642504215240479
12 0.0091124808 	 0.8642504367
epoch_time;  34.32194924354553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030019329860806465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7958306670188904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00681378273293376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7816048860549927
13 0.0077997449 	 0.7816048706
epoch_time;  33.60641407966614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019261406268924475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7259888648986816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004793461877852678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.733241856098175
14 0.0070628578 	 0.7332418851
epoch_time;  33.918537616729736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014385937247425318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9623754024505615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005080452188849449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.97852623462677
15 0.0190051207 	 0.9785262278
epoch_time;  33.83004879951477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017192392842844129
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8223167657852173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0045585413463413715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8156724572181702
16 0.006471857 	 0.8156724452
epoch_time;  33.9512779712677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023370906710624695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7610657215118408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005267031025141478
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7402575612068176
17 0.0061128481 	 0.7402575628
epoch_time;  34.00568628311157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001771031180396676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.689403772354126
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–„â–†â–„â–‡â–…â–„â–…â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–â–â–â–„â–ƒâ–‚â–‚â–‚â–„â–ƒâ–‚â–„â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–…â–ƒâ–†â–„â–‡â–…â–„â–…â–„â–ƒâ–‚â–â–â–‚â–‚â–â–â–â–„â–ƒâ–‚â–‚â–â–…â–ƒâ–‚â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–‚â–„â–ƒâ–…â–…â–‚â–„â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–„â–‚â–ƒâ–ƒâ–…â–‡â–‚â–…â–â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–„â–„
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.89786
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.90666
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01429
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00932
wandb:                         Train loss 0.01025
wandb: 
wandb: ğŸš€ View run twinkling-tiger-1625 at: https://wandb.ai/nreints/thesis/runs/2kd3zmrh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_092729-2kd3zmrh/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_094536-ddf117tr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-rabbit-1632
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ddf117tr
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0046835909597575665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.682106614112854
18 0.005601588 	 0.6821065943
epoch_time;  33.877012968063354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033008302561938763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7044568657875061
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006850486621260643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6852401494979858
19 0.0058863934 	 0.6852401319
epoch_time;  33.996676445007324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004924818407744169
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5032074451446533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014032842591404915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5926462411880493
20 0.044240696 	 1.5926462041
epoch_time;  33.78259229660034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003950285725295544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0870187282562256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008890179917216301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1715550422668457
21 0.0103998711 	 1.1715550725
epoch_time;  34.17596936225891
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034614161122590303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.934785783290863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008156461641192436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0232346057891846
22 0.0076391948 	 1.0232346641
epoch_time;  34.318079233169556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004520738031715155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8613575100898743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009937237948179245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9210501313209534
23 0.0066506756 	 0.9210501034
epoch_time;  34.30903172492981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004086833447217941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7652651071548462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008293249644339085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8297972679138184
24 0.0060596213 	 0.8297972895
epoch_time;  33.941261529922485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009709203615784645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.626409649848938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02108755335211754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5520009994506836
25 0.071413547 	 1.5520010311
epoch_time;  33.89475750923157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004532719496637583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1155121326446533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011141741648316383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1029560565948486
26 0.0115608566 	 1.1029560746
epoch_time;  34.041131019592285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027250954881310463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9863182306289673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00688895583152771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9487601518630981
27 0.0088501143 	 0.9487601565
epoch_time;  34.03867983818054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00655875401571393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2756812572479248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01645483262836933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3386121988296509
28 0.0435881084 	 1.3386121675
epoch_time;  33.7263822555542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009316461160779
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9086225628852844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014288017526268959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8979933857917786
29 0.0102541178 	 0.8979933817
epoch_time;  33.7396445274353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009318922646343708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.906663715839386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014293204993009567
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8978573083877563
It took  1086.9434533119202  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15488260ccd0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae307100>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2ba380>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2b8880>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07383719086647034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8835670948028564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1330438107252121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.692321538925171
0 1.7865660189 	 3.6923215929
epoch_time;  34.014676332473755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007914845831692219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8899027109146118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024189559742808342
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.769053339958191
1 0.0384757066 	 1.7690532949
epoch_time;  34.430776596069336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007134320214390755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5531065464019775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026624931022524834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.354069232940674
2 0.0781935375 	 2.3540691825
epoch_time;  33.82483911514282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007824648171663284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9374510049819946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019724790006875992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.786461591720581
3 0.0217259975 	 1.786461591
epoch_time;  33.63651490211487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005185327026993036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1245503425598145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019394483417272568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9857598543167114
4 0.0602926793 	 1.9857598158
epoch_time;  33.7029447555542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006869950797408819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.616487741470337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017214631661772728
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5130661725997925
5 0.016529616 	 1.5130661333
epoch_time;  33.867738008499146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003600906813517213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8277984857559204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01228009071201086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7107881307601929
6 0.0377264203 	 1.710788139
epoch_time;  34.38644289970398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036442195996642113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4503583908081055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011286965571343899
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3352382183074951
7 0.0129797933 	 1.3352381773
epoch_time;  34.063857555389404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006724711041897535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2659763097763062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013240729458630085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1323637962341309
8 0.0165065921 	 1.1323637717
epoch_time;  33.94299864768982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006222830619663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9688923358917236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017317142337560654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8290146589279175
9 0.0299577908 	 1.8290147119
epoch_time;  33.628820180892944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002348831854760647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.444966435432434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007344639394432306
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3153438568115234
10 0.0120850057 	 1.315343851
epoch_time;  33.711888551712036
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–â–â–ƒâ–‚â–â–â–â–â–ƒâ–‚â–â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–â–â–â–â–ƒâ–‚â–â–â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–ƒâ–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–ƒâ–â–â–â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.86924
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.95843
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00973
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00496
wandb:                         Train loss 0.00649
wandb: 
wandb: ğŸš€ View run filigreed-rabbit-1632 at: https://wandb.ai/nreints/thesis/runs/ddf117tr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_094536-ddf117tr/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_100342-hrp6zx2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-paper-1638
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/hrp6zx2d
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003013618290424347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1926367282867432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00813465565443039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0566518306732178
11 0.0094014636 	 1.0566518732
epoch_time;  34.352821826934814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023375863675028086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.160054326057434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006294821854680777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.038265585899353
12 0.0098304915 	 1.0382655394
epoch_time;  34.23434090614319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037271128967404366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7537990808486938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01262618973851204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.614701271057129
13 0.0229416356 	 1.6147012855
epoch_time;  34.14462089538574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031363798771053553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2528102397918701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007442871108651161
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1194331645965576
14 0.0099260314 	 1.1194331696
epoch_time;  33.78206729888916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006666987203061581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9980188608169556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013018297962844372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9192782640457153
15 0.0076594046 	 0.9192782399
epoch_time;  33.651034116744995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026356359012424946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8869551420211792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005516645032912493
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8130437135696411
16 0.007109559 	 0.8130436923
epoch_time;  33.758267641067505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005832171533256769
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4803582429885864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013936441391706467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.423408031463623
17 0.0231663908 	 1.4234080704
epoch_time;  33.865522146224976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010318942368030548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1843681335449219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01610470935702324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1159530878067017
18 0.0087936836 	 1.1159530594
epoch_time;  34.3149311542511
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00174714345484972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0685263872146606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00468656700104475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9877220392227173
19 0.0071491087 	 0.9877220684
epoch_time;  33.808170318603516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017728692619130015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0576905012130737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004838838707655668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9535261392593384
20 0.0114537895 	 0.953526154
epoch_time;  33.5051064491272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003410459728911519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9323993921279907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006109125446528196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8317023515701294
21 0.0060624614 	 0.8317023793
epoch_time;  34.090224742889404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022306744940578938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8761761784553528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004982261452823877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7809028029441833
22 0.0056349583 	 0.7809027819
epoch_time;  33.97272825241089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018693773075938225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8640985488891602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03876833990216255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8175911903381348
23 0.0356254751 	 1.8175911803
epoch_time;  34.10322070121765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022427388466894627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1437121629714966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0068677868694067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0770758390426636
24 0.0112904311 	 1.0770758776
epoch_time;  34.071019411087036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002761690178886056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9991429448127747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006310786586254835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9217535853385925
25 0.0072273013 	 0.9217535751
epoch_time;  34.004891872406006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005416675936430693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8917568922042847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010061770677566528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8079049587249756
26 0.0064320756 	 0.8079049378
epoch_time;  34.006725549697876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012912660837173462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3840693235397339
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02651422657072544
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.318057656288147
27 0.0365623572 	 1.3180576105
epoch_time;  34.25535821914673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002019342966377735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0409172773361206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005342394579201937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9527379274368286
28 0.0085544362 	 0.9527379523
epoch_time;  34.19991612434387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004964329302310944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.958544135093689
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009731875732541084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8685176372528076
29 0.0064882016 	 0.8685176423
epoch_time;  33.99680185317993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00496120797470212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9584273099899292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009729347191751003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8692377805709839
It took  1085.9286897182465  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548ad925300>, <torch.utils.data.dataloader.DataLoader object at 0x1548b47eb9d0>, <torch.utils.data.dataloader.DataLoader object at 0x15488260dae0>, <torch.utils.data.dataloader.DataLoader object at 0x15488260c460>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05257280915975571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.247617483139038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11236456781625748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.020859241485596
0 1.868101595 	 4.0208592717
epoch_time;  34.329913854599
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008400753140449524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.628247618675232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02509528025984764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0547330379486084
1 0.0383045737 	 2.0547330459
epoch_time;  34.65090084075928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005868855863809586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5796592235565186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02017967402935028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9886447191238403
2 0.0519711583 	 1.988644695
epoch_time;  34.17565369606018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009600619785487652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2671284675598145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020319301635026932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6295466423034668
3 0.0176847705 	 1.6295466582
epoch_time;  34.072713136672974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011656868271529675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.549842119216919
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02622709795832634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9737741947174072
4 0.0874658912 	 1.9737742444
epoch_time;  34.10401177406311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014637621119618416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.879249095916748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03948909044265747
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.425673484802246
5 0.0357747681 	 2.4256733782
epoch_time;  33.83259177207947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00483661238104105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1502249240875244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012948916293680668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5135059356689453
6 0.0188677208 	 1.5135059184
epoch_time;  33.9965763092041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005585435312241316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.420982837677002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01622805930674076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7064805030822754
7 0.0358277407 	 1.7064804584
epoch_time;  33.98778796195984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004060701001435518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0521541833877563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010790643282234669
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3133832216262817
8 0.0135943278 	 1.3133831658
epoch_time;  34.183385133743286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006159198936074972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8663707375526428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012965782545506954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.136438012123108
9 0.010224795 	 1.1364380067
epoch_time;  34.156912326812744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018650931306183338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9740463495254517
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006080481223762035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2695869207382202
10 0.0189974725 	 1.2695869377
epoch_time;  34.21020317077637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007853660732507706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.862712025642395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013126314617693424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.159567952156067
11 0.008358097 	 1.1595679338
epoch_time;  34.03270363807678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010377447120845318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5314141511917114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02244636043906212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.946227788925171
12 0.0281067674 	 1.9462278429
epoch_time;  33.78172326087952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0039958469569683075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9685392379760742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00987037643790245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2695486545562744
13 0.0115957888 	 1.2695486754
epoch_time;  34.02090835571289
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001660919631831348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8450495600700378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005313629284501076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1175076961517334
14 0.0089669809 	 1.1175077041
epoch_time;  33.91433072090149
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012413891963660717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7255377769470215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003921278286725283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9889751076698303
15 0.0080358511 	 0.9889751331
epoch_time;  33.93993377685547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005246399901807308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8042585253715515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011667519807815552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0693198442459106
16 0.0090648758 	 1.0693198028
epoch_time;  33.953322410583496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006544861476868391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6037120223045349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011472268030047417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8381800055503845
17 0.0051926383 	 0.8381800349
epoch_time;  33.96706247329712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0079428069293499
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.604565978050232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02327634021639824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0811257362365723
18 0.1108987535 	 2.0811256801
epoch_time;  34.415220975875854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036529155913740396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2655940055847168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011762429028749466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.632959246635437
19 0.0176098497 	 1.6329592794
epoch_time;  34.040058851242065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003708859905600548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.070101261138916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009472863748669624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3740249872207642
20 0.0121068131 	 1.3740250049
epoch_time;  33.94217801094055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003930792678147554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9382894039154053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009035782888531685
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2107317447662354
21 0.009253678 	 1.2107317138
epoch_time;  33.89324474334717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003169833682477474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2828065156936646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010533096268773079
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5693446397781372
22 0.0313449978 	 1.5693446041
epoch_time;  33.715999126434326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024513883981853724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0436928272247314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007312965579330921
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2523919343948364
23 0.0098606813 	 1.2523918959
epoch_time;  33.80329442024231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002119524870067835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.956400990486145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006084775552153587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1811151504516602
24 0.0082803585 	 1.1811150957
epoch_time;  34.32779121398926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016760361613705754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7933629155158997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004092229530215263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0143178701400757
25 0.0069251468 	 1.014317815
epoch_time;  33.835790157318115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006832083221524954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7938031554222107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012065852992236614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0335420370101929
26 0.0063487153 	 1.0335420453
epoch_time;  33.96706819534302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002391169546172023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.781845211982727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005620602983981371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.012036681175232
27 0.0056613833 	 1.0120366491
epoch_time;  34.06307601928711
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031575392931699753
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–ƒâ–„â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.92858
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.53098
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.0286
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00943
wandb:                         Train loss 0.06228
wandb: 
wandb: ğŸš€ View run vibrant-paper-1638 at: https://wandb.ai/nreints/thesis/runs/hrp6zx2d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_100342-hrp6zx2d/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_102149-4yp6s2oh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-fish-1645
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/4yp6s2oh
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6946103572845459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005781295243650675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8938063979148865
28 0.0053390247 	 0.8938063884
epoch_time;  33.836724042892456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009424148127436638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5310351848602295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028603773564100266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9252831935882568
29 0.062278915 	 1.9252831957
epoch_time;  34.19900846481323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009425711818039417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5309821367263794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028602931648492813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9285768270492554
It took  1087.065298318863  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548b6264a30>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2bab90>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2b9db0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2b9bd0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014541910961270332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.270986795425415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04524572938680649
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4156970977783203
0 1.8211587679 	 2.4156969883
epoch_time;  34.186835527420044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01317488495260477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.275972366333008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03840471804141998
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4460289478302
1 0.0692720377 	 2.4460288794
epoch_time;  34.11757969856262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008410750888288021
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2209534645080566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02937842346727848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.396960496902466
2 0.0449552226 	 2.3969604861
epoch_time;  33.98043489456177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004853445570915937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5425399541854858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014989485964179039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6730488538742065
3 0.0202675848 	 1.6730488665
epoch_time;  34.3426730632782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006325694266706705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.808817744255066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019528433680534363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9427077770233154
4 0.0610616714 	 1.9427077187
epoch_time;  34.357638359069824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0056836968287825584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0478994846343994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02046891860663891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2125134468078613
5 0.0307355905 	 2.2125133503
epoch_time;  34.30063199996948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006594971753656864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.267480492591858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014900763519108295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.387512445449829
6 0.0150384751 	 1.3875123914
epoch_time;  34.31212782859802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005507166963070631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7314729690551758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017566194757819176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8868118524551392
7 0.0456765988 	 1.8868118701
epoch_time;  34.051589012145996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003504381747916341
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.299694299697876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011049112305045128
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4318863153457642
8 0.0144288383 	 1.431886333
epoch_time;  33.88020706176758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001919425674714148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1213502883911133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006956002674996853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2161465883255005
9 0.0120474045 	 1.2161466017
epoch_time;  33.879504442214966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02970917336642742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.038741111755371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05729388818144798
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.309342861175537
10 0.0205383012 	 2.3093429519
epoch_time;  33.95681285858154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003864225000143051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.160332441329956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00931451003998518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.284521460533142
11 0.0122613423 	 1.2845214659
epoch_time;  33.96037721633911
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035173005890101194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.382615327835083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01097037922590971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4970548152923584
12 0.0248106178 	 1.4970548232
epoch_time;  33.98937106132507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007023700512945652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1392698287963867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015148903243243694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2143312692642212
13 0.0093189733 	 1.2143312206
epoch_time;  34.19424223899841
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004742152988910675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0854555368423462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009628156200051308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1544474363327026
14 0.0080068712 	 1.1544474345
epoch_time;  33.85962796211243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010980371152982116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9342392086982727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003730907803401351
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.010080337524414
15 0.0081381534 	 1.0100802972
epoch_time;  33.54743456840515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003742384724318981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3750925064086914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010822073556482792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4355589151382446
16 0.01817747 	 1.4355588608
epoch_time;  33.776084899902344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002840896602720022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0695239305496216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007181196939200163
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1202305555343628
17 0.0087868548 	 1.1202305912
epoch_time;  33.94465160369873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0057143704034388065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9673996567726135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013106219470500946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0220825672149658
18 0.0071691849 	 1.0220825564
epoch_time;  33.98549270629883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028762940783053637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8058522343635559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006064427085220814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8810595273971558
19 0.0060218415 	 0.8810595371
epoch_time;  33.72017455101013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011026788270100951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7575955390930176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0033819330856204033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8091734051704407
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ˆâ–ˆâ–…â–†â–‡â–ƒâ–†â–„â–ƒâ–‡â–ƒâ–„â–ƒâ–‚â–‚â–„â–‚â–‚â–â–â–†â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ˆâ–ˆâ–…â–†â–‡â–ƒâ–…â–ƒâ–ƒâ–‡â–ƒâ–„â–ƒâ–ƒâ–‚â–„â–‚â–‚â–â–â–†â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–†â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–ˆâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–â–â–„â–‚â–‚â–â–…â–‚â–‚â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–ƒâ–‚â–‚â–â–…â–‚â–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.14892
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.01623
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01166
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00471
wandb:                         Train loss 0.03894
wandb: 
wandb: ğŸš€ View run red-fish-1645 at: https://wandb.ai/nreints/thesis/runs/4yp6s2oh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_102149-4yp6s2oh/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_103951-q77zxjtd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-moon-1651
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/q77zxjtd
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
20 0.0058670149 	 0.8091733996
epoch_time;  33.81739926338196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008928850293159485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.914473533630371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02521277777850628
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0590646266937256
21 0.0561145029 	 2.0590645136
epoch_time;  33.87126350402832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00432088365778327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.275696873664856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010825004428625107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3682023286819458
22 0.0156530861 	 1.3682023247
epoch_time;  33.811318159103394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003529458539560437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0456010103225708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008024579845368862
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1455026865005493
23 0.0099252164 	 1.1455027416
epoch_time;  33.949450969696045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020552407950162888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.96539306640625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005601434502750635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0437633991241455
24 0.0083402313 	 1.0437634056
epoch_time;  33.92036271095276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01831427589058876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0973268747329712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.031883757561445236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1871854066848755
25 0.0387164195 	 1.18718542
epoch_time;  33.88215398788452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005684762727469206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9656144380569458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01037011668086052
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0390623807907104
26 0.0168952538 	 1.0390624078
epoch_time;  33.93726205825806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036702342331409454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8164787292480469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007686623837798834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8675743341445923
27 0.0076907098 	 0.8675743633
epoch_time;  33.6324098110199
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005168614909052849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7700951099395752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011289509013295174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8087248802185059
28 0.0069383127 	 0.8087248557
epoch_time;  33.62925100326538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004709407687187195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0157378911972046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011636593379080296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1508866548538208
29 0.038936332 	 1.1508866509
epoch_time;  34.044437885284424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004710647743195295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0162259340286255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011664497666060925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1489187479019165
It took  1082.0667142868042  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548b47ea7a0>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad924c10>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad924610>, <torch.utils.data.dataloader.DataLoader object at 0x1548ad924670>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014707660302519798
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.201545000076294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04314644634723663
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.563804864883423
0 1.802406698 	 2.5638049722
epoch_time;  34.07420206069946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007500204257667065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7677652835845947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02419988624751568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.085331678390503
1 0.0638318982 	 2.0853317583
epoch_time;  34.07750129699707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007417366839945316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.915412425994873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025103306397795677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3001174926757812
2 0.0547795783 	 2.3001174235
epoch_time;  34.134995460510254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004139314871281385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3661227226257324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013349573127925396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6554824113845825
3 0.0203805919 	 1.6554823584
epoch_time;  34.08604454994202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006605522707104683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6891462802886963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01912144012749195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0604610443115234
4 0.0532005709 	 2.0604609464
epoch_time;  33.95747637748718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003398467320948839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2911570072174072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010870297439396381
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.555643916130066
5 0.0164931353 	 1.5556439633
epoch_time;  33.661484718322754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0172690711915493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9124062061309814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.034539710730314255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.191528081893921
6 0.0724185594 	 2.1915281359
epoch_time;  33.859673261642456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004352930001914501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4474438428878784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012267247773706913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6239992380142212
7 0.0170010026 	 1.6239992816
epoch_time;  33.85262131690979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003223474370315671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4057129621505737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01078883558511734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6327018737792969
8 0.0214332621 	 1.6327018623
epoch_time;  34.112247467041016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026673225220292807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0419563055038452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007351154927164316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2768621444702148
9 0.0101469583 	 1.276862107
epoch_time;  33.89598894119263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29204368591308594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5155229568481445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4153858721256256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.271061420440674
10 0.026643818 	 4.27106137
epoch_time;  33.69973707199097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041923243552446365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.380221962928772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011233938857913017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5685694217681885
11 0.0245621686 	 1.5685694023
epoch_time;  33.988133668899536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026262064930051565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2399348020553589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009191631339490414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4185166358947754
12 0.0139943382 	 1.4185165912
epoch_time;  34.01041626930237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018086644122377038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9916895627975464
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–„â–„â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–ƒâ–‚â–â–â–ƒâ–‚â–â–â–â–ƒâ–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–…â–„â–„â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–„â–‚â–‚â–â–â–ƒâ–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–â–â–â–â–â–‚â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.78782
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.70181
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00885
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00456
wandb:                         Train loss 0.00662
wandb: 
wandb: ğŸš€ View run vivid-moon-1651 at: https://wandb.ai/nreints/thesis/runs/q77zxjtd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_103951-q77zxjtd/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_105751-igmjxtwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-mandu-1656
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/igmjxtwl
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005489297676831484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1556371450424194
13 0.0078296279 	 1.1556371591
epoch_time;  34.00726938247681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020138330291956663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.013987421989441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006929886061698198
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.168884515762329
14 0.0119684437 	 1.1688845539
epoch_time;  33.72177028656006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015854970552027225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9228018522262573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004845179617404938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9728671312332153
15 0.0063599335 	 0.9728671071
epoch_time;  33.89517378807068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001177921541966498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7533338665962219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035910257138311863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8306902050971985
16 0.0075921052 	 0.8306902283
epoch_time;  34.080081939697266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008226428180932999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.435004711151123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021516947075724602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5669538974761963
17 0.0474817996 	 1.5669539068
epoch_time;  33.877737283706665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005175197497010231
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0429819822311401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011950583197176456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.215964913368225
18 0.0139458401 	 1.2159649714
epoch_time;  33.80981945991516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003285372629761696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8899518847465515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008019954897463322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.007026195526123
19 0.0097202933 	 1.0070261422
epoch_time;  33.78329038619995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003470087656751275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7908903956413269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008394964039325714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8883886337280273
20 0.0082701987 	 0.8883886424
epoch_time;  33.5627875328064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011773297563195229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7257524728775024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03065509721636772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9871461391448975
21 0.0602182681 	 1.9871461067
epoch_time;  33.60807824134827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00776462908834219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1235991716384888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016242973506450653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1984270811080933
22 0.0180862897 	 1.1984271369
epoch_time;  33.95372009277344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010468640364706516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9238693118095398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017686039209365845
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0043435096740723
23 0.012601864 	 1.0043435457
epoch_time;  33.555564403533936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002905979286879301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8454079031944275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006418357603251934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9291739463806152
24 0.0083433003 	 0.9291739565
epoch_time;  33.73297905921936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006956635508686304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.77886962890625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014265555888414383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9226008653640747
25 0.0071844632 	 0.9226008758
epoch_time;  33.728686571121216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010819077491760254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3435862064361572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020893722772598267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5623013973236084
26 0.037316723 	 1.5623014052
epoch_time;  34.024089097976685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002026275498792529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9619258642196655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005739448592066765
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0536752939224243
27 0.010914778 	 1.053675349
epoch_time;  34.018274784088135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005960020236670971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8187301158905029
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012794281356036663
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9110422730445862
28 0.0075691124 	 0.9110422739
epoch_time;  33.78977942466736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004563626833260059
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7014697194099426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008855074644088745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7874557375907898
29 0.0066223224 	 0.7874557633
epoch_time;  33.67610001564026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004563658032566309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7018104195594788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008851908147335052
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7878235578536987
It took  1080.2514584064484  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1548b47e9f60>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2bbb20>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2eac50>, <torch.utils.data.dataloader.DataLoader object at 0x1548ae2eaf80>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014547637663781643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3442769050598145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04638255015015602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5204455852508545
0 1.8780450121 	 2.520445671
epoch_time;  33.63740515708923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010351112112402916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1524200439453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03236174210906029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.23779559135437
1 0.0661880297 	 2.2377955503
epoch_time;  33.64317607879639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1698136180639267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.467494487762451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2760433852672577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.983022212982178
2 0.0584190652 	 4.9830222692
epoch_time;  33.73885488510132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0045342701487243176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8155666589736938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01857886277139187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9089082479476929
3 0.037046174 	 1.9089082562
epoch_time;  33.92284393310547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03468222916126251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6844545602798462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06135443225502968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7811930179595947
4 0.0182293106 	 1.7811930216
epoch_time;  33.87631678581238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005454865284264088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.017245054244995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018988344818353653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1070573329925537
5 0.0510981327 	 2.1070573236
epoch_time;  33.770328998565674
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–ƒâ–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–„â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–„â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.04073
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.99926
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00699
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00313
wandb:                         Train loss 0.00848
wandb: 
wandb: ğŸš€ View run crimson-mandu-1656 at: https://wandb.ai/nreints/thesis/runs/igmjxtwl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_105751-igmjxtwl/logs
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011442667804658413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5992273092269897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02239193581044674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.627001404762268
6 0.016636956 	 1.6270014368
epoch_time;  33.89475131034851
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038804132491350174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.822346568107605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014646688476204872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9486373662948608
7 0.0361111227 	 1.9486373486
epoch_time;  33.92833662033081
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00838887132704258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4098637104034424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017279157415032387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4868500232696533
8 0.0137913752 	 1.4868499664
epoch_time;  33.76151084899902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001610671286471188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.296618938446045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006335393991321325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.346776008605957
9 0.0120629347 	 1.3467760345
epoch_time;  33.67705416679382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003652114886790514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5852378606796265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013631584122776985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6980226039886475
10 0.0294715772 	 1.6980225716
epoch_time;  33.97756242752075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01091261487454176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1813883781433105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018357805907726288
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2618076801300049
11 0.0109505152 	 1.261807629
epoch_time;  34.157888889312744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002860459964722395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9900611639022827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007349252700805664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0726433992385864
12 0.009276868 	 1.0726434529
epoch_time;  34.23102784156799
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002730255713686347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3338232040405273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00880036223679781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4396458864212036
13 0.0245892902 	 1.4396459113
epoch_time;  34.02666783332825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023840097710490227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0515398979187012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006394284777343273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1308302879333496
14 0.0088544798 	 1.1308303326
epoch_time;  34.244292974472046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015172876883298159
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9549370408058167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004760118201375008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0361385345458984
15 0.0087975062 	 1.0361385288
epoch_time;  34.00229263305664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004366009961813688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8623237013816833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009087621234357357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9219101071357727
16 0.0067537569 	 0.9219101275
epoch_time;  33.82477807998657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003267992753535509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.231852412223816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00930787157267332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4094561338424683
17 0.0327557341 	 1.4094560975
epoch_time;  33.97981858253479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00706625496968627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0340746641159058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01294810138642788
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2047605514526367
18 0.0087014761 	 1.2047605947
epoch_time;  33.849472999572754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018562907353043556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9260326623916626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0049467128701508045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0654644966125488
19 0.0094408964 	 1.0654644462
epoch_time;  33.67760181427002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001324811833910644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8477817177772522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004057412035763264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9675018191337585
20 0.008513569 	 0.9675018218
epoch_time;  34.436118602752686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009574157884344459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8801986575126648
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002961738733574748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9390811324119568
21 0.0057486434 	 0.9390811056
epoch_time;  34.3619384765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034239136148244143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3848248720169067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012186624109745026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5875474214553833
22 0.0194960341 	 1.5875474636
epoch_time;  33.83531665802002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015930506633594632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8580324053764343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004831499420106411
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.981972873210907
23 0.007055775 	 0.9819728701
epoch_time;  34.16474461555481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009338111849501729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7240326404571533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0031164600513875484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8443121314048767
24 0.0057767797 	 0.8443121319
epoch_time;  34.146353006362915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014831434236839414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9948058128356934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005228497087955475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0962806940078735
25 0.0188056085 	 1.0962807462
epoch_time;  33.98735690116882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004553043283522129
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8387194871902466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009983936324715614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9476516842842102
26 0.0063660684 	 0.9476516585
epoch_time;  33.9005331993103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01949586533010006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.076734781265259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04230070486664772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.247237205505371
27 0.0419828543 	 2.2472371911
epoch_time;  33.68414855003357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004153742920607328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2047815322875977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010341127403080463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.247836947441101
28 0.0154539537 	 1.2478369399
epoch_time;  33.67204737663269
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031325265299528837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0022767782211304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006986989639699459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0420078039169312
29 0.0084806467 	 1.042007769
epoch_time;  33.74461507797241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031318552792072296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9992615580558777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00698967557400465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0407330989837646
It took  1081.8942019939423  seconds.

JOB STATISTICS
==============
Job ID: 2142321
Array Job ID: 2141141_21
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-06:26:24 core-walltime
Job Wall-clock time: 03:01:28
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
