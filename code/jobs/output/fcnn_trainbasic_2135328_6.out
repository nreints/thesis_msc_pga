wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163841-ez0gbws2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-orchid-1131
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ez0gbws2
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▂▂▂▂▁▂▂█▁▂▁▄▄▇▂▃▂▄▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆▇▅▆█▃▃▅▃▃▃▄▃▃▄▃▂▆▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▂▁▃▂▂▃▁▅▄▃▅▃▂█▁▃▄▇▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▄▆▃▅▃▁▂▃▂▂▂▂▅▃▁▂▃▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 301.02005
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.25502
wandb:    Test loss t(0, 0)_r(-5, 5)_none 89.58971
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25373
wandb:                         Train loss 1.74693
wandb: 
wandb: 🚀 View run brilliant-orchid-1131 at: https://wandb.ai/nreints/thesis/runs/ez0gbws2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163841-ez0gbws2/logs
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3942827880382538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.567222833633423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 118.82807159423828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 244.46507263183594
0 3.6805080241 	 244.4650760135 	 244.4650760135
epoch_time;  36.54673409461975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2544127404689789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.872135639190674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 107.5114974975586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 293.9905700683594
1 2.0763194834 	 293.9905827703 	 293.9905827703
epoch_time;  34.48731541633606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28053030371665955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3870849609375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 106.8439712524414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 297.3578186035156
2 1.9719096128 	 297.3578336149 	 297.3578336149
epoch_time;  34.25445628166199
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3365413248538971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6087846755981445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 149.90647888183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 295.8480224609375
3 1.9275411452 	 295.8480152027 	 295.8480152027
epoch_time;  34.099400758743286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2703794836997986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.112922191619873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 116.91708374023438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 255.01951599121094
4 1.8899735726 	 255.0195101351 	 255.0195101351
epoch_time;  34.088536739349365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30932119488716125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.841946601867676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 137.38180541992188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 224.9941864013672
5 1.8635419201 	 224.9941934122 	 224.9941934122
epoch_time;  34.12052059173584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2537844479084015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.882643222808838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 152.86038208007812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 291.2086181640625
6 1.8506371789 	 291.2086148649 	 291.2086148649
epoch_time;  34.143595695495605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20346572995185852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.287278652191162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 103.27896881103516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 291.9837341308594
7 1.8292554521 	 291.9837204392 	 291.9837204392
epoch_time;  34.09881591796875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23158565163612366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7776694297790527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 250.8002471923828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 633.7959594726562
8 1.8216156853 	 633.7959881757 	 633.7959881757
epoch_time;  34.103577613830566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24437175691127777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.694619655609131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 211.29051208496094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 225.99200439453125
9 1.8129610076 	 225.9919974662 	 225.9919974662
epoch_time;  34.13120412826538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23201774060726166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.758955240249634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 158.2529754638672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 304.9844970703125
10 1.8020085649 	 304.9845016892 	 304.9845016892
epoch_time;  35.50084447860718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2245546132326126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.151110887527466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 235.82923889160156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 215.7154998779297
11 1.7954443723 	 215.7154983108 	 215.7154983108
epoch_time;  35.2602002620697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.219253271818161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9141390323638916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 176.9047393798828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 397.9033508300781
12 1.7889967126 	 397.9033361486 	 397.9033361486
epoch_time;  34.383007287979126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22566233575344086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.915801763534546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 136.2090301513672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 408.8262634277344
13 1.7857423859 	 408.8262668919 	 408.8262668919
epoch_time;  35.056031465530396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32523202896118164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0727081298828125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 341.9073181152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 577.8414306640625
14 1.7714058599 	 577.8414273649 	 577.8414273649
epoch_time;  34.3225793838501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2633449137210846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.680204153060913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 88.77465057373047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 252.8744659423828
15 1.7658488093 	 252.8744721284 	 252.8744721284
epoch_time;  34.48084282875061
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20258072018623352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.564135789871216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 157.14248657226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 357.74090576171875
16 1.7536399599 	 357.7409206081 	 357.7409206081
epoch_time;  34.96234679222107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23081158101558685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.681926965713501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 203.11338806152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 269.9552001953125
17 1.7551489374 	 269.9551942568 	 269.9551942568
epoch_time;  35.383413791656494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2612307071685791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.886974334716797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 315.99505615234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 414.96343994140625
18 1.7513615344 	 414.9634290541 	 414.9634290541
epoch_time;  34.148589849472046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25375545024871826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2554574012756348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 89.06404876708984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 301.0390625
19 1.7469293159 	 301.0390625 	 301.0390625
epoch_time;  34.25964379310608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2537257671356201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.255018472671509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 89.58970642089844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 301.0200500488281
It took 746.8818039894104 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn59: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135334.0

JOB STATISTICS
==============
Job ID: 2135334
Array Job ID: 2135328_6
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:48:54 core-walltime
Job Wall-clock time: 00:12:43
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
