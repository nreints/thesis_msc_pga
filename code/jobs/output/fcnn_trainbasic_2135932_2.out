wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195344-x3u1j5we
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-cake-1158
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/x3u1j5we
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–â–‚â–ƒâ–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–ƒâ–„â–‚â–ƒâ–‚â–â–ƒâ–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–ƒâ–â–â–…â–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–„â–â–„â–ƒâ–„â–ƒâ–„â–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–„â–â–‚â–ƒâ–‚â–ƒâ–â–â–ƒâ–‚â–ƒâ–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.1941
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.33248
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.20063
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16043
wandb:                         Train loss 1.52702
wandb: 
wandb: ðŸš€ View run vermilion-cake-1158 at: https://wandb.ai/nreints/thesis/runs/x3u1j5we
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195344-x3u1j5we/logs
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28671008348464966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7973026037216187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.561751842498779
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.199960231781006
0 3.3036520925 	 6.1999604096 	 6.1999604096
epoch_time;  33.47297763824463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2701752483844757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.672741174697876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.093761444091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.372057914733887
1 1.8272372009 	 5.3720577755 	 5.3720577755
epoch_time;  32.4921338558197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1777917742729187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4510331153869629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.890768527984619
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.947645664215088
2 1.7218203535 	 4.9476456926 	 4.9476456926
epoch_time;  32.207640171051025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20619921386241913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4884166717529297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.933724880218506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.995828628540039
3 1.6688148948 	 4.9958284945 	 4.9958284945
epoch_time;  32.12620949745178
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18362779915332794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39150896668434143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.239540100097656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.228283882141113
4 1.6321266913 	 5.2282836914 	 5.2282836914
epoch_time;  32.0068085193634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18776212632656097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4101349413394928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.191514015197754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.194951057434082
5 1.6121266649 	 5.1949509079 	 5.1949509079
epoch_time;  32.1462676525116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16785860061645508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3736622631549835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.097558498382568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.087398529052734
6 1.5968387012 	 5.0873987146 	 5.0873987146
epoch_time;  31.736785411834717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1666373312473297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33339229226112366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.971157550811768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.920734882354736
7 1.5860520494 	 4.9207347973 	 4.9207347973
epoch_time;  31.742626905441284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20814010500907898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4313678741455078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.077094078063965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.111809253692627
8 1.5801219282 	 5.111809148 	 5.111809148
epoch_time;  31.756024599075317
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1553073227405548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30309608578681946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.95076322555542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.923092842102051
9 1.569713967 	 4.9230927338 	 4.9230927338
epoch_time;  31.773761987686157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17118175327777863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3469431400299072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.044670581817627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.02284049987793
10 1.5665717102 	 5.0228406751 	 5.0228406751
epoch_time;  31.8332839012146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1916625201702118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39535218477249146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.991831302642822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.005033493041992
11 1.5589565724 	 5.0050335858 	 5.0050335858
epoch_time;  31.917853832244873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1591867208480835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33551058173179626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.142401695251465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.142156600952148
12 1.552435687 	 5.1421568175 	 5.1421568175
epoch_time;  32.068952560424805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19402596354484558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3542163670063019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.924333572387695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8744330406188965
13 1.5468130144 	 4.8744331978 	 4.8744331978
epoch_time;  31.834948301315308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1507878601551056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3094075322151184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.180088043212891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.163891315460205
14 1.5439714291 	 5.1638912716 	 5.1638912716
epoch_time;  31.90634274482727
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14619454741477966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29946616291999817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.091800212860107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0693583488464355
15 1.5393820151 	 5.069358372 	 5.069358372
epoch_time;  31.760550260543823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18057774007320404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33931633830070496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.225982666015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.198597431182861
16 1.5366556404 	 5.1985975111 	 5.1985975111
epoch_time;  32.04815745353699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1641661673784256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3347732424736023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.108850002288818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.074188709259033
17 1.5339590188 	 5.0741887273 	 5.0741887273
epoch_time;  32.14750695228577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1894455999135971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35576438903808594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.221797943115234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.197548866271973
18 1.5339904248 	 5.1975490261 	 5.1975490261
epoch_time;  32.07019090652466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16041995584964752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.332307904958725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.199760437011719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.194111347198486
19 1.5270158048 	 5.1941112621 	 5.1941112621
epoch_time;  31.786308765411377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1604263335466385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33248263597488403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.200628280639648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.194103240966797
It took 701.5194170475006 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn48: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135934.0

JOB STATISTICS
==============
Job ID: 2135934
Array Job ID: 2135932_2
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:34:30 core-walltime
Job Wall-clock time: 00:11:55
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
