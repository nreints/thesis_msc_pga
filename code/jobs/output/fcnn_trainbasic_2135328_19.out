wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170327-hvxxe6fy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-rocket-1148
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/hvxxe6fy
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▅▅▄▃▃▄▃▁▄▃▄▂▄▃▅▄▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▄▅▆▇▃▅▄▁▃▆█▃▄▅▆▇▅▆▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▁▄▂▃▁▃▂▃▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▆▄▂▄▄▁▃▇▇▃▃▃▅▄▃▃▅▅
wandb:                         Train loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.86474
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.53109
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.77406
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.64987
wandb:                         Train loss 5.06068
wandb: 
wandb: 🚀 View run glowing-rocket-1148 at: https://wandb.ai/nreints/thesis/runs/hvxxe6fy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170327-hvxxe6fy/logs
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8794089555740356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.589327812194824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2112092971801758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.659421443939209
0 8.4902899141 	 3.6594215187 	 3.6803968275
epoch_time;  37.76432156562805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6719045639038086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.056572437286377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8609444499015808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0949931144714355
1 6.1088050631 	 3.0949931377 	 3.1126507733
epoch_time;  36.86665487289429
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.63689124584198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.246161699295044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8089556097984314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.91471004486084
2 5.7793213387 	 2.9147101325 	 2.9291497308
epoch_time;  36.97226619720459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7217789888381958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4390103816986084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8113619685173035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0898118019104004
3 5.6209423398 	 3.0898117478 	 3.1010250607
epoch_time;  36.87073040008545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6345208883285522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.461286783218384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7156857848167419
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1279983520507812
4 5.514705299 	 3.1279983108 	 3.1374392948
epoch_time;  36.57106423377991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4583952724933624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9745469093322754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7078940868377686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0393872261047363
5 5.4345514689 	 3.039387141 	 3.0470089474
epoch_time;  36.859875202178955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6300854682922363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2126026153564453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6773394346237183
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8034863471984863
6 5.3758433348 	 2.8034862621 	 2.809794328
epoch_time;  36.60510039329529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5860416889190674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.003997564315796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7117202281951904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7869246006011963
7 5.3218269322 	 2.7869246199 	 2.7921492293
epoch_time;  36.66056847572327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41357937455177307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5822360515594482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7238360643386841
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9885075092315674
8 5.2716956219 	 2.988507575 	 2.992734177
epoch_time;  36.48124551773071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5141000747680664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.874114513397217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.704984188079834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.744969606399536
9 5.2420152627 	 2.7449697134 	 2.7490115604
epoch_time;  36.516658306121826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7944331169128418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4437057971954346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6621284484863281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4884660243988037
10 5.2292227567 	 2.4884660051 	 2.4923988136
epoch_time;  37.15647315979004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8078173398971558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6841793060302734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8631114959716797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9383838176727295
11 5.1827993425 	 2.938383855 	 2.9416355442
epoch_time;  37.20608043670654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5665646195411682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9554879665374756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7453789114952087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.756998062133789
12 5.1592249783 	 2.7569980931 	 2.7604634053
epoch_time;  38.1800742149353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5773341655731201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0425312519073486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7679159641265869
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.960503101348877
13 5.1376850642 	 2.9605029957 	 2.9640997809
epoch_time;  37.06430387496948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5663050413131714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2424263954162598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6307042837142944
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.633366823196411
14 5.120988953 	 2.6333669302 	 2.6371519346
epoch_time;  37.07086896896362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6505106091499329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.409406900405884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8287098407745361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0490622520446777
15 5.1042944948 	 3.049062368 	 3.0529831345
epoch_time;  36.58361053466797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5993647575378418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4797730445861816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7261182069778442
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.78444504737854
16 5.1041973013 	 2.784444943 	 2.7887147646
epoch_time;  36.49535918235779
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.565282940864563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.236569881439209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8333575129508972
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.133960723876953
17 5.0842184326 	 3.1339606208 	 3.1383726378
epoch_time;  36.56418323516846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5690257549285889
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.416256904602051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7632216215133667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9436962604522705
18 5.0755791273 	 2.9436962231 	 2.9484935864
epoch_time;  36.87711954116821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6500266194343567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.527806520462036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7738662362098694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.863510847091675
19 5.0606794415 	 2.8635108742 	 2.8685411608
epoch_time;  36.51978516578674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6498711109161377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.531085968017578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7740606069564819
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.864741563796997
It took 797.0794155597687 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn53: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135410.0

JOB STATISTICS
==============
Job ID: 2135410
Array Job ID: 2135328_19
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:03:08
CPU Efficiency: 25.98% of 04:03:00 core-walltime
Job Wall-clock time: 00:13:30
Memory Utilized: 4.05 GB
Memory Efficiency: 12.95% of 31.25 GB
