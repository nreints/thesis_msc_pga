/gpfs/home2/nreints/MScThesis/code/gru.py:583: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train_total)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_100836-hexbtjv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-glade-456
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/hexbtjv7
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() █▆▅▃▂▂▁▁▁▁▁
wandb:                                             Train loss █▆▅▃▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.43071
wandb:                                             Train loss 0.42069
wandb: 
wandb: 🚀 View run silver-glade-456 at: https://wandb.ai/nreints/test/runs/hexbtjv7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_100836-hexbtjv7/logs
Running for data type: eucl_motion
----- ITERATION 1/2 ------
Number of train simulations: 160
Number of test simulations: 40
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(12, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=0, out_features=96, bias=True)
  )
  (fc): Linear(in_features=96, out_features=12, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 7.9203295898 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 7.3450727462768555 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 5.9477503299713135
Epoch 1
	 Logging train Loss: 5.8087320964 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 5.356588363647461 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.460369348526001
Epoch 2
	 Logging train Loss: 4.262347819 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 3.920152425765991 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.807109117507935
Epoch 3
	 Logging train Loss: 2.9731758626 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 2.6664862632751465 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.129681825637817
Epoch 4
	 Logging train Loss: 2.0117443848 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 1.82209312915802 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.28309965133667
Epoch 5
	 Logging train Loss: 1.3448921712 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 1.1691267490386963 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 5.02750039100647
Epoch 6
	 Logging train Loss: 0.8585566203 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.7445107102394104 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 5.072358131408691
Epoch 7
	 Logging train Loss: 0.5927081299 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.565764307975769 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 5.401890516281128
Epoch 8
	 Logging train Loss: 0.4819692485 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.48739463090896606 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.480247974395752
Epoch 9
	 Logging train Loss: 0.4206879679 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.4307199716567993 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 4.5515453815460205
	 Logging test loss: 0.4307060241699219 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  63.73786759376526  seconds.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/gru.py", line 642, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/gru does not exist.
srun: error: gcn22: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2512263.0

JOB STATISTICS
==============
Job ID: 2512263
Array Job ID: 2512261_2
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:11:41
CPU Efficiency: 49.30% of 00:23:42 core-walltime
Job Wall-clock time: 00:01:19
Memory Utilized: 2.99 GB
Memory Efficiency: 9.56% of 31.25 GB
