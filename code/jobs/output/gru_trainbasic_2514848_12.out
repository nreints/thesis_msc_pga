wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_135637-nnu9r6gh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-frog-538
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/nnu9r6gh
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5, 20)_r(0, 0)_semi_pNone_gNone, MSELoss() â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(5, 20)_r(0, 0)_semi_pNone_gNone, MSELoss() 0.00288
wandb:                                             Train loss 0.00331
wandb: 
wandb: ðŸš€ View run graceful-frog-538 at: https://wandb.ai/nreints/test/runs/nnu9r6gh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_135637-nnu9r6gh/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_140511-7es6zrku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-donkey-555
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/7es6zrku
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5, 20)_r(0, 0)_semi_pNone_gNone, MSELoss() â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(5, 20)_r(0, 0)_semi_pNone_gNone, MSELoss() 0.00164
wandb:                                             Train loss 0.00327
wandb: 
wandb: ðŸš€ View run young-donkey-555 at: https://wandb.ai/nreints/test/runs/7es6zrku
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_140511-7es6zrku/logs
Running for data type: dual_quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.9212478617 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.19518998265266418 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 45.57860088348389
Epoch 1
	 Logging train Loss: 0.1083263606 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.06872209161520004 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.28913760185242
Epoch 2
	 Logging train Loss: 0.0466113071 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.030762841925024986 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.4311957359314
Epoch 3
	 Logging train Loss: 0.0208286685 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.014378485269844532 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.46066737174988
Epoch 4
	 Logging train Loss: 0.0110407385 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.00900096632540226 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.58086609840393
Epoch 5
	 Logging train Loss: 0.0077796721 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.00692811468616128 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.07348895072937
Epoch 6
	 Logging train Loss: 0.0061134522 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.005176943726837635 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.3355233669281
Epoch 7
	 Logging train Loss: 0.0049578765 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.003675551153719425 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 45.23671221733093
Epoch 8
	 Logging train Loss: 0.0042387841 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.004770620726048946 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 45.68527626991272
Epoch 9
	 Logging train Loss: 0.0033136445 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.002879288513213396 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 47.17920732498169
	 Logging test loss: 0.0028751518111675978 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
It took  515.3873143196106  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.4142108097 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.14053188264369965 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.59916615486145
Epoch 1
	 Logging train Loss: 0.084366405 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.053990673273801804 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.64076328277588
Epoch 2
	 Logging train Loss: 0.0369496288 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.02458343654870987 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.911653995513916
Epoch 3
	 Logging train Loss: 0.0168974422 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.012211519293487072 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.01417374610901
Epoch 4
	 Logging train Loss: 0.0097196837 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.008127505891025066 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.08262658119202
Epoch 5
	 Logging train Loss: 0.0068088488 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.005889768712222576 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.824803590774536
Epoch 6
	 Logging train Loss: 0.0049566974 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.004300311207771301 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.119125843048096
Epoch 7
	 Logging train Loss: 0.0041049978 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.003034898778423667 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 44.13311147689819
Epoch 8
	 Logging train Loss: 0.0038347407 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.0021875472739338875 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.979936361312866
Epoch 9
	 Logging train Loss: 0.0032725573 (MSELoss(): data_t(5, 20)_r(0, 0)_semi_pNone_gNone)
	 Logging test loss: 0.0016438743332400918 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
     --> Epoch time; 43.87708783149719
	 Logging test loss: 0.0016424792120233178 (MSELoss(): t(5, 20)_r(0, 0)_semi_pNone_gNone)
It took  501.96050572395325  seconds.

JOB STATISTICS
==============
Job ID: 2514860
Array Job ID: 2514848_12
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 05:09:18 core-walltime
Job Wall-clock time: 00:17:11
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
