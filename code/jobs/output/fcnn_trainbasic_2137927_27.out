wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_182634-2yvb4v8s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-rabbit-1386
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2yvb4v8s
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▅▇█▇▇█▇▆▆▅▆▂▂▁▃▄█▂▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▄▆▆▆▆▇▅▅▅▄▅▁▃▁▃▃█▂▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▅▇▇▇▇█▇▆▆▅▆▂▃▁▃▃█▂▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▃▆▆▆▅▇▅▄▄▃▅▁▂▁▃▃█▂▃▃
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 56.73301
wandb:  Test loss t(-10, 10)_r(0, 0)_none 34.15948
wandb:    Test loss t(0, 0)_r(-5, 5)_none 38.1455
wandb:     Test loss t(0, 0)_r(0, 0)_none 19.02835
wandb:                         Train loss 22.22804
wandb: 
wandb: 🚀 View run enchanting-rabbit-1386 at: https://wandb.ai/nreints/thesis/runs/2yvb4v8s
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_182634-2yvb4v8s/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_184550-8dus4ymx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-monkey-1393
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/8dus4ymx
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 31.465038299560547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 50.40655517578125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.06497573852539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.54296112060547
0 41.0566895523 	 82.5429634713 	 82.5429634713
epoch_time;  39.786529779434204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 19.958772659301758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 33.986846923828125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.01570129394531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 62.97591781616211
1 28.3275431618 	 62.9759184966 	 62.9759184966
epoch_time;  40.619404792785645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 33.85975646972656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.39554977416992
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.66183090209961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 88.88970947265625
2 26.5713783218 	 88.8897065034 	 88.8897065034
epoch_time;  38.52819609642029
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.543914794921875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 59.23868179321289
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 64.3588638305664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 94.40501403808594
3 25.8911089836 	 94.4050147804 	 94.4050147804
epoch_time;  38.03285765647888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.08210754394531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 57.827613830566406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.570274353027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.3671646118164
4 25.3225922478 	 90.3671663851 	 90.3671663851
epoch_time;  38.27175998687744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 33.101383209228516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.04655456542969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.330726623535156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 85.13636779785156
5 24.9873317959 	 85.136365076 	 85.136365076
epoch_time;  38.17134070396423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.15984344482422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.4032211303711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.74744415283203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.46080017089844
6 24.4893788084 	 99.4608002534 	 99.4608002534
epoch_time;  38.14559459686279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 32.139854431152344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 51.31226348876953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.20921325683594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.90760040283203
7 24.009864802 	 82.9076013514 	 82.9076013514
epoch_time;  37.9351122379303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 25.046722412109375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 42.415313720703125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.10478210449219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 70.40756225585938
8 23.8577908133 	 70.4075591216 	 70.4075591216
epoch_time;  37.90000128746033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.3850154876709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 43.14980697631836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.553199768066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 74.32470703125
9 23.531890022 	 74.3247043919 	 74.3247043919
epoch_time;  37.67092275619507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 17.964845657348633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 32.1937370300293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 39.50068283081055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 58.77562713623047
10 23.6944000719 	 58.7756281672 	 58.7756281672
epoch_time;  38.21474099159241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 27.280969619750977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 46.08955001831055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.25862121582031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 74.12781524658203
11 23.3155652294 	 74.1278188345 	 74.1278188345
epoch_time;  37.87830948829651
	 Logging test loss: t(0, 0)_r(0, 0)_none => 5.439422607421875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.790997505187988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.617527961730957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.12194061279297
12 23.1783254697 	 21.121940984 	 21.121940984
epoch_time;  37.5288782119751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 12.157499313354492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 22.60379981994629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.795631408691406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.22038269042969
13 22.9551482101 	 32.2203811233 	 32.2203811233
epoch_time;  38.30850124359131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9724419116973877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.87023401260376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.099768161773682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.657050132751465
14 22.7175757448 	 13.6570497255 	 13.6570497255
epoch_time;  37.92885875701904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 13.668789863586426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 25.238637924194336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.183319091796875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.247161865234375
15 22.5725155509 	 42.24716269 	 42.24716269
epoch_time;  37.9501793384552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 17.348146438598633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 31.77713966369629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 29.651540756225586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.06407165527344
16 22.7168801836 	 45.0640730574 	 45.0640730574
epoch_time;  37.96264410018921
	 Logging test loss: t(0, 0)_r(0, 0)_none => 50.97014236450195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 75.66552734375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.399169921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.60105895996094
17 22.3871815758 	 100.6010557432 	 100.6010557432
epoch_time;  37.79233956336975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.172996520996094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 21.197322845458984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.267147064208984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.844236373901367
18 22.1713209314 	 31.8442356419 	 31.8442356419
epoch_time;  38.31704258918762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 19.03031349182129
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 34.14189147949219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.15291976928711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 56.723304748535156
19 22.2280435427 	 56.7233055321 	 56.7233055321
epoch_time;  38.08177638053894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 19.028352737426758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 34.159481048583984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.145503997802734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 56.733009338378906
It took 1155.8687000274658 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: - 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: \ 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: - 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: \ 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: - 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: \ 0.121 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ███▇██▇▁▇▄▅▆▃▇▇▆▆▁▇▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▇▅▇▇▆▁▆▃▄▅▃▇▇▆▅▁▇▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ███▇██▇▁▇▄▅▆▃▇▇▆▆▁▇▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇█▇▅▇▇▆▁▆▃▃▄▃▇▇▆▄▁▇▃▃
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 63.70082
wandb:  Test loss t(-10, 10)_r(0, 0)_none 37.90865
wandb:    Test loss t(0, 0)_r(-5, 5)_none 44.76315
wandb:     Test loss t(0, 0)_r(0, 0)_none 21.46867
wandb:                         Train loss 21.9737
wandb: 
wandb: 🚀 View run fortuitous-monkey-1393 at: https://wandb.ai/nreints/thesis/runs/8dus4ymx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_184550-8dus4ymx/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_185933-rl2dq0sz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run incandescent-rabbit-1396
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rl2dq0sz
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.90079116821289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.13235473632812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.40370178222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.44932556152344
0 41.1083726515 	 101.4493243243 	 101.4493243243
epoch_time;  37.50934839248657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 52.15193176269531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 77.26303100585938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 74.18002319335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 104.11181640625
1 28.1990203961 	 104.1118137669 	 104.1118137669
epoch_time;  37.50990033149719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 46.17325210571289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 70.21656036376953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.56327819824219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.46214294433594
2 26.615841338 	 100.4621410473 	 100.4621410473
epoch_time;  37.68223977088928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 34.28316879272461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 51.738407135009766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 64.40857696533203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.3238525390625
3 25.6221251904 	 89.3238492399 	 89.3238492399
epoch_time;  37.63971495628357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 48.19014358520508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 72.58029174804688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.06355285644531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.00253295898438
4 25.4937373053 	 101.0025337838 	 101.0025337838
epoch_time;  37.86725878715515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 46.363914489746094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.83683776855469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.64884185791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.81486511230469
5 24.8670103038 	 99.8148648649 	 99.8148648649
epoch_time;  37.84633660316467
	 Logging test loss: t(0, 0)_r(0, 0)_none => 40.66853332519531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 62.814579010009766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 67.4627456665039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.7414779663086
6 24.2910777759 	 93.741480152 	 93.741480152
epoch_time;  37.861589670181274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 6.4792585372924805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 13.896693229675293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.560898780822754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.026931762695312
7 23.8193762695 	 23.0269320101 	 23.0269320101
epoch_time;  38.10763072967529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.74249267578125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 58.601722717285156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.67621612548828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 91.57597351074219
8 23.5882569186 	 91.5759712838 	 91.5759712838
epoch_time;  37.97488570213318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 19.207416534423828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 33.341758728027344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 41.000125885009766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 58.204402923583984
9 23.247392307 	 58.2044024493 	 58.2044024493
epoch_time;  37.80752182006836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 21.8475399017334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.790306091308594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.315921783447266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 67.6236343383789
10 23.4466745303 	 67.6236328125 	 67.6236328125
epoch_time;  37.45215916633606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 27.067907333374023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 45.681060791015625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 53.21901321411133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 75.45999145507812
11 23.1580960139 	 75.4599926098 	 75.4599926098
epoch_time;  37.89307689666748
	 Logging test loss: t(0, 0)_r(0, 0)_none => 16.628128051757812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 30.173099517822266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 33.25624084472656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.32816696166992
12 22.5990738194 	 48.3281672297 	 48.3281672297
epoch_time;  38.083009243011475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.79127502441406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.6561050415039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.80804443359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.5248031616211
13 22.8291987559 	 96.5247994088 	 96.5247994088
epoch_time;  38.09469246864319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 42.493682861328125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 65.39071655273438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.81590270996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 92.8050537109375
14 22.4687182634 	 92.8050570101 	 92.8050570101
epoch_time;  38.06228947639465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.78110122680664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 57.60417556762695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.75346755981445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 84.28216552734375
15 22.4409302006 	 84.2821632179 	 84.2821632179
epoch_time;  37.98601794242859
	 Logging test loss: t(0, 0)_r(0, 0)_none => 29.207841873168945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 49.64252471923828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 53.51551055908203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77.04419708251953
16 22.2833631305 	 77.0441934122 	 77.0441934122
epoch_time;  38.01306176185608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.071800231933594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 14.76346492767334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.692667961120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.254344940185547
17 22.0359999154 	 23.2543443834 	 23.2543443834
epoch_time;  37.697564363479614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.90980911254883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.70970153808594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 67.32876586914062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 94.89884948730469
18 22.2380670278 	 94.8988492399 	 94.8988492399
epoch_time;  40.27405285835266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 21.47693634033203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.9077033996582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 44.7580451965332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.69757843017578
19 21.973700914 	 63.6975770693 	 63.6975770693
epoch_time;  38.56533122062683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 21.468666076660156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.90864562988281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 44.76314926147461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.70082473754883
It took 822.6831002235413 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▁▁▃▃▁▁▁▁▃▁▅▁▁▁▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▆▅▄▄▃█▁▂▁▂▂▇▇▂▄▆▄▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▆▅▄▄▃█▁▂▁▂▃▅▇▃▄▅▅▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▅▄▃▃▃█▂▁▁▂▂▇▆▂▄▅▄▁▁▁
wandb:                         Train loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.50038
wandb:  Test loss t(-10, 10)_r(0, 0)_none 4.08434
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.79915
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.16669
wandb:                         Train loss 18.27557
wandb: 
wandb: 🚀 View run incandescent-rabbit-1396 at: https://wandb.ai/nreints/thesis/runs/rl2dq0sz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_185933-rl2dq0sz/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_191305-ogztl3e5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-laughter-1399
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ogztl3e5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8520232439041138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.8539042472839355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.6306328773498535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 483.3291015625
0 34.882643238 	 483.3290962838 	 483.3290962838
epoch_time;  37.82082772254944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.3767120838165283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.500572204589844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.821510314941406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.12135314941406
1 23.0208562542 	 42.1213550465 	 42.1213550465
epoch_time;  37.70997357368469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.072841167449951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.054628849029541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.96859884262085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.24661636352539
2 21.6664665496 	 45.2466163429 	 45.2466163429
epoch_time;  37.66212582588196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5294979810714722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.268702030181885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.175963878631592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 150.1232147216797
3 20.7422879993 	 150.1232157939 	 150.1232157939
epoch_time;  37.693862438201904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7056536674499512
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.312196254730225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.95043420791626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 179.56906127929688
4 20.2303603165 	 179.569066723 	 179.569066723
epoch_time;  37.966679096221924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6712435483932495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.9536213874816895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.733201026916504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.25049591064453
5 19.9953611628 	 22.2504961993 	 22.2504961993
epoch_time;  37.75921106338501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.1906659603118896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.782837390899658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.934463500976562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.51435089111328
6 19.7948174086 	 30.51435019 	 30.51435019
epoch_time;  37.835973501205444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.297781229019165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5685510635375977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.974337577819824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.759675979614258
7 19.4576157329 	 23.7596758868 	 23.7596758868
epoch_time;  38.24881362915039
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1861153841018677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.12785005569458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.637484550476074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.911643981933594
8 19.3346852255 	 31.911644848 	 31.911644848
epoch_time;  37.77224111557007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1459922790527344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.712008476257324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.183166027069092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 130.00848388671875
9 19.0269073714 	 130.0084776182 	 130.0084776182
epoch_time;  38.33450388908386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2886667251586914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9096405506134033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6420722007751465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.353702545166016
10 19.0646197423 	 28.3537030194 	 28.3537030194
epoch_time;  38.08909249305725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4234240055084229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.363427639007568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.165229320526123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 313.4352722167969
11 18.8613082896 	 313.4352618243 	 313.4352618243
epoch_time;  37.93408179283142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7793073654174805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.381856441497803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.100127220153809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.6058349609375
12 19.0343443213 	 39.6058356208 	 39.6058356208
epoch_time;  38.125558614730835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5741841793060303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.023219108581543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.082796096801758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.716331481933594
13 18.5019663486 	 19.716332348 	 19.716332348
epoch_time;  38.04802632331848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3040354251861572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.36776065826416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.558354377746582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.032926559448242
14 18.3457110592 	 28.0329259924 	 28.0329259924
epoch_time;  37.785247564315796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8372842073440552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.486460208892822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.774973392486572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.698762893676758
15 18.5819505861 	 31.698762141 	 31.698762141
epoch_time;  37.68711185455322
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.3871424198150635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.514774322509766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.042397499084473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.611108779907227
16 18.4353128174 	 16.6111090583 	 16.6111090583
epoch_time;  37.89302921295166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8468533754348755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.530752182006836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.470058441162109
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 53.92498016357422
17 18.2468303042 	 53.9249788851 	 53.9249788851
epoch_time;  37.78059911727905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0328763723373413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.012630462646484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.688002109527588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.62586212158203
18 18.2563473257 	 30.6258630701 	 30.6258630701
epoch_time;  38.267749547958374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1665462255477905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.087969779968262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.791322708129883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.50273513793945
19 18.2755691435 	 37.502734375 	 37.502734375
epoch_time;  37.82563352584839
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.166694164276123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.084338188171387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.79914665222168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.50038146972656
It took 812.2561161518097 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▅▆▃█▃▃▃▂▃▂▁▂▁▂▂▅▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▆▂▆█▂█▁▅▂▅▁▄▄▃▄▅▁▅▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▆▂▆▇▁█▁▄▂▄▁▃▃▃▃▄▁▄▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▆▂▆█▁█▁▅▂▅▁▃▄▃▄▅▁▅▆▆
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 50.92835
wandb:  Test loss t(-10, 10)_r(0, 0)_none 16.59731
wandb:    Test loss t(0, 0)_r(-5, 5)_none 15.19478
wandb:     Test loss t(0, 0)_r(0, 0)_none 8.27867
wandb:                         Train loss 19.37598
wandb: 
wandb: 🚀 View run scintillating-laughter-1399 at: https://wandb.ai/nreints/thesis/runs/ogztl3e5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_191305-ogztl3e5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_192630-77u1gsf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-envelope-1402
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/77u1gsf8
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.770049571990967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.562701225280762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.62055492401123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.95040893554688
0 36.2117388499 	 90.9504117399 	 90.9504117399
epoch_time;  37.799898624420166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.507552146911621
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.896577835083008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.256893157958984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 157.96218872070312
1 25.2866134902 	 157.9621938345 	 157.9621938345
epoch_time;  38.0158486366272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5318093299865723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.870932579040527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.66986083984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 191.90570068359375
2 23.7937886129 	 191.9057010135 	 191.9057010135
epoch_time;  37.824146032333374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.37031364440918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.930862426757812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.294353485107422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.96602630615234
3 22.4242949179 	 101.9660261824 	 101.9660261824
epoch_time;  37.917651891708374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.628217697143555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 22.275671005249023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.48050880432129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 285.0456848144531
4 21.9740751418 	 285.0456925676 	 285.0456925676
epoch_time;  37.99285054206848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8205312490463257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.001168727874756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.675777435302734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.17757415771484
5 21.354122588 	 99.1775760135 	 99.1775760135
epoch_time;  37.56393551826477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 12.09395694732666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 23.06626319885254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.13994598388672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 110.4275894165039
6 21.156673155 	 110.4275865709 	 110.4275865709
epoch_time;  37.583001375198364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4369841814041138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.952610015869141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.528823375701904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 112.39175415039062
7 21.170223373 	 112.3917546453 	 112.3917546453
epoch_time;  37.64277625083923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.1206536293029785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 14.951991081237793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.337576866149902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 53.130374908447266
8 21.1035143026 	 53.1303737331 	 53.1303737331
epoch_time;  37.3393919467926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5665416717529297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.207460880279541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.4471516609191895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.10852813720703
9 20.2467602192 	 80.1085251267 	 80.1085251267
epoch_time;  37.35668182373047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.13139533996582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 14.642596244812012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.979598045349121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.24399948120117
10 20.0714497292 	 61.2439980997 	 61.2439980997
epoch_time;  38.00685119628906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5402741432189941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.755157470703125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.151088237762451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.550865173339844
11 19.9240423472 	 29.5508657095 	 29.5508657095
epoch_time;  37.62799787521362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 5.030078887939453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.451253890991211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.797917366027832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 64.18795013427734
12 19.9940057443 	 64.1879486909 	 64.1879486909
epoch_time;  37.90095853805542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 5.393550872802734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.75367546081543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.895672798156738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.70325469970703
13 19.8739936844 	 37.7032543285 	 37.7032543285
epoch_time;  37.682989835739136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.539204120635986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.240050315856934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.877799987792969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.63301467895508
14 19.3437870261 	 61.6330130912 	 61.6330130912
epoch_time;  37.51691770553589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 5.374475002288818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.77536392211914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.29287338256836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.637733459472656
15 19.4069455611 	 61.6377322635 	 61.6377322635
epoch_time;  37.49780488014221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.451776504516602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 15.447293281555176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.88216495513916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 188.7466278076172
16 19.6603387356 	 188.7466216216 	 188.7466216216
epoch_time;  37.82510495185852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2702431678771973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.584142208099365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.109463691711426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.45647430419922
17 19.3209485549 	 20.4564743454 	 20.4564743454
epoch_time;  37.579928159713745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.293635368347168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 15.057485580444336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.469490051269531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.619049072265625
18 19.2347161688 	 50.6190508868 	 50.6190508868
epoch_time;  37.60370445251465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.280426025390625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.598983764648438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.191554069519043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.947872161865234
19 19.3759811908 	 50.9478726774 	 50.9478726774
epoch_time;  37.53641653060913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.278665542602539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.597314834594727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.194784164428711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.92835235595703
It took 804.6821918487549 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▃▆█▇█▃▇▂▃▇█▇▆▃▃▁▁▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▂▂▄▇▆▆▂▆▂▃▅▇▇▆▂▂▁▁▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▃▆█▇█▃▇▂▄▇█▇▆▃▃▁▁▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▂▅▇▆▇▂▆▂▃▅▇▇▅▂▂▁▁▅▅
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 78.02645
wandb:  Test loss t(-10, 10)_r(0, 0)_none 52.16753
wandb:    Test loss t(0, 0)_r(-5, 5)_none 55.43195
wandb:     Test loss t(0, 0)_r(0, 0)_none 31.2632
wandb:                         Train loss 22.09598
wandb: 
wandb: 🚀 View run auspicious-envelope-1402 at: https://wandb.ai/nreints/thesis/runs/77u1gsf8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_192630-77u1gsf8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_193956-gmdbim1x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-wonton-1405
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/gmdbim1x
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 49.80215835571289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 75.65119934082031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.29755401611328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.17005920410156
0 41.6396147173 	 100.1700591216 	 100.1700591216
epoch_time;  37.50401496887207
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.63350772857666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 21.747100830078125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.933374404907227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.43042755126953
1 28.609695011 	 38.4304291596 	 38.4304291596
epoch_time;  39.64003348350525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.691884994506836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 20.821218490600586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.12394142150879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.633453369140625
2 26.8307908768 	 38.633453864 	 38.633453864
epoch_time;  38.08274960517883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 28.601221084594727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 44.7801513671875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.7401123046875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.20419311523438
3 25.7852038549 	 80.2041965794 	 80.2041965794
epoch_time;  37.703622817993164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 41.3408088684082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 64.1972885131836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.79369354248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.27571105957031
4 25.2846273062 	 96.275707348 	 96.275707348
epoch_time;  37.459030866622925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 38.02481460571289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 60.77963638305664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.0400161743164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.402099609375
5 25.4910000212 	 93.4021009291 	 93.4021009291
epoch_time;  37.64015007019043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 41.44677734375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.3547248840332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.77125549316406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.85059356689453
6 24.3483756136 	 96.8505912162 	 96.8505912162
epoch_time;  37.38519310951233
	 Logging test loss: t(0, 0)_r(0, 0)_none => 14.448396682739258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 25.78739356994629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 32.132911682128906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.891639709472656
7 24.5015709631 	 45.8916385135 	 45.8916385135
epoch_time;  37.79382610321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 39.25616455078125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 62.071346282958984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.2105484008789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.83384704589844
8 23.6716713566 	 90.8338471284 	 90.8338471284
epoch_time;  37.764042139053345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 9.779006004333496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 18.97010040283203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.325536727905273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.290346145629883
9 23.1822343644 	 30.290345228 	 30.290345228
epoch_time;  37.44650173187256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 16.15856170654297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 29.069913864135742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 34.974021911621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.400516510009766
10 23.2465089709 	 49.4005173142 	 49.4005173142
epoch_time;  37.760833978652954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 32.43840408325195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 53.365333557128906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.70903778076172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 84.1773910522461
11 23.0804259056 	 84.1773912584 	 84.1773912584
epoch_time;  37.556238651275635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.047691345214844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.02015686035156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.885009765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.54088592529297
12 22.6880527463 	 96.5408889358 	 96.5408889358
epoch_time;  37.45317721366882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 40.945133209228516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 64.02920532226562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.22277069091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.09385681152344
13 22.7051931703 	 93.0938555743 	 93.0938555743
epoch_time;  37.386595726013184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 34.023902893066406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 55.937042236328125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.223106384277344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.89241027832031
14 22.7522718136 	 82.8924092061 	 82.8924092061
epoch_time;  37.376487016677856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 13.621756553649902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 25.297420501708984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.249919891357422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.99034118652344
15 22.5709974822 	 39.9903399493 	 39.9903399493
epoch_time;  37.51804280281067
	 Logging test loss: t(0, 0)_r(0, 0)_none => 14.094503402709961
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 25.509416580200195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.193357467651367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.74925231933594
16 22.3476853419 	 39.7492504223 	 39.7492504223
epoch_time;  37.63985514640808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 6.538722038269043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 14.058905601501465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.256952285766602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.393198013305664
17 22.3854254824 	 21.3931983742 	 21.3931983742
epoch_time;  37.27368950843811
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.039207458496094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 15.756316184997559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.999895095825195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.94375228881836
18 22.2940848214 	 25.9437526394 	 25.9437526394
epoch_time;  37.39604830741882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 31.264310836791992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.15565872192383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 55.43590545654297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 78.027587890625
19 22.0959794981 	 78.0275865709 	 78.0275865709
epoch_time;  37.44974207878113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 31.263195037841797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.16753005981445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 55.43195343017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 78.02645111083984
It took 805.9097986221313 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▇▆▇▁█▇▇▄▇▁▆▇▅▅▆▂▂▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▂▇▅▆▁█▇▇▃▇▁▅▇▄▅▅▂▂▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▇▆▇▁█▇▇▄█▁▆▇▅▅▆▂▂▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▇▅▆▁█▇▇▃▇▁▅▇▄▄▅▂▁▅▅
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 81.05025
wandb:  Test loss t(-10, 10)_r(0, 0)_none 52.35864
wandb:    Test loss t(0, 0)_r(-5, 5)_none 57.60815
wandb:     Test loss t(0, 0)_r(0, 0)_none 30.78101
wandb:                         Train loss 22.27333
wandb: 
wandb: 🚀 View run glowing-wonton-1405 at: https://wandb.ai/nreints/thesis/runs/gmdbim1x
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_193956-gmdbim1x/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_195310-bt46wi0m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-goat-1408
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/bt46wi0m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 52.51253128051758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 79.33000183105469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.62940979003906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 107.00318908691406
0 40.3890328791 	 107.0031883446 	 107.0031883446
epoch_time;  37.46348547935486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 10.561968803405762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 20.61280632019043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.29450225830078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.121971130371094
1 28.6149130416 	 37.1219726563 	 37.1219726563
epoch_time;  37.87771821022034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.953033447265625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.35004425048828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.60588073730469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.54915618896484
2 26.8107597749 	 99.5491554054 	 99.5491554054
epoch_time;  37.667689085006714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.827659606933594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 51.21665954589844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.58831787109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 84.26884460449219
3 25.7874227742 	 84.2688450169 	 84.2688450169
epoch_time;  38.90398907661438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 38.47404098510742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.08761215209961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.21118927001953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 95.02330780029297
4 24.9997619753 	 95.0233108108 	 95.0233108108
epoch_time;  39.686508893966675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.4448094367980957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.976346969604492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.601088523864746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.22420883178711
5 24.860033535 	 16.2242095122 	 16.2242095122
epoch_time;  37.603797912597656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 52.17259216308594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 78.83418273925781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.3372802734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 105.69758605957031
6 24.5040411307 	 105.697582348 	 105.697582348
epoch_time;  37.09264254570007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 43.21652603149414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 67.5320816040039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.97956085205078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 98.7530746459961
7 23.8635853927 	 98.7530722128 	 98.7530722128
epoch_time;  37.04388928413391
	 Logging test loss: t(0, 0)_r(0, 0)_none => 43.464393615722656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 66.48567199707031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.98970794677734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 97.75556182861328
8 23.5770512441 	 97.7555637669 	 97.7555637669
epoch_time;  36.98576235771179
	 Logging test loss: t(0, 0)_r(0, 0)_none => 17.585277557373047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 30.364065170288086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.513763427734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 52.848758697509766
9 23.6263884775 	 52.8487595017 	 52.8487595017
epoch_time;  37.10875082015991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 47.10336685180664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 72.07756042480469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.53909301757812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.05989074707031
10 23.3979191351 	 100.0598923142 	 100.0598923142
epoch_time;  36.69412922859192
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4985506534576416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.299666881561279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.6360931396484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.566981315612793
11 23.4286957304 	 10.5669809702 	 10.5669809702
epoch_time;  36.94621658325195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 32.94643020629883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.85725402832031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.86988830566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 79.36419677734375
12 23.0079288676 	 79.3641997466 	 79.3641997466
epoch_time;  36.818079471588135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.037960052490234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.77059936523438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.56673431396484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 97.19269561767578
13 22.9427074518 	 97.1926942568 	 97.1926942568
epoch_time;  36.71648907661438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 21.07091522216797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.16200256347656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.533348083496094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.71642303466797
14 23.1107687669 	 61.7164220861 	 61.7164220861
epoch_time;  36.78394532203674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.118288040161133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 45.844810485839844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.878944396972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 67.09115600585938
15 22.8785888837 	 67.0911581503 	 67.0911581503
epoch_time;  36.4139940738678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 28.618860244750977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 48.04266357421875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 54.00654983520508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 74.99120330810547
16 22.4177941985 	 74.9912056588 	 74.9912056588
epoch_time;  36.61711144447327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.674890041351318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.09682846069336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.334555625915527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.87207794189453
17 22.6776775135 	 22.8720782306 	 22.8720782306
epoch_time;  36.67982888221741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.88132905960083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.532968521118164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.509678840637207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.920377731323242
18 22.2529779536 	 18.9203771643 	 18.9203771643
epoch_time;  36.769103050231934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.786216735839844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.351097106933594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.61349105834961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 81.09187316894531
19 22.2733317112 	 81.0918760557 	 81.0918760557
epoch_time;  36.67063879966736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.781007766723633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 52.358642578125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.608154296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 81.05025482177734
It took 794.3859784603119 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▂█▇█▅▇▁▇▆▂▆▆█▆▂▄▆▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▂█▆█▄▇▁▇▅▂▅▆█▆▂▃▅▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▂█▇█▅▇▁▇▆▂▆▆█▆▂▄▆▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▂▇▆▇▄▇▁▆▅▂▅▅█▅▂▃▅▄▄
wandb:                         Train loss █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 68.35831
wandb:  Test loss t(-10, 10)_r(0, 0)_none 41.38958
wandb:    Test loss t(0, 0)_r(-5, 5)_none 46.38161
wandb:     Test loss t(0, 0)_r(0, 0)_none 23.69637
wandb:                         Train loss 21.85225
wandb: 
wandb: 🚀 View run resplendent-goat-1408 at: https://wandb.ai/nreints/thesis/runs/bt46wi0m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_195310-bt46wi0m/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_200612-gf4tkxu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-wish-1411
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/gf4tkxu9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 47.658878326416016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 71.37843322753906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 73.06953430175781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 104.99304962158203
0 40.4449052133 	 104.9930532095 	 104.9930532095
epoch_time;  36.53190016746521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.87660789489746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 44.43733215332031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 51.150909423828125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 74.56535339355469
1 28.5166114802 	 74.5653557855 	 74.5653557855
epoch_time;  36.4398934841156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.004192352294922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 15.577258110046387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.170442581176758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.707735061645508
2 26.8130210096 	 28.7077359586 	 28.7077359586
epoch_time;  36.72342133522034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 43.77902603149414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.40396881103516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.22208404541016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.17662811279297
3 26.1524257363 	 99.1766258446 	 99.1766258446
epoch_time;  36.83569002151489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.54217529296875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 56.900146484375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.74821472167969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 86.32185363769531
4 25.1969892519 	 86.3218538851 	 86.3218538851
epoch_time;  36.5083966255188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.05496597290039
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.06840515136719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.00469207763672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 99.93164825439453
5 24.9457938389 	 99.9316511824 	 99.9316511824
epoch_time;  36.245296001434326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 20.250732421875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 35.817325592041016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 42.66849899291992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.459346771240234
6 24.547768915 	 63.4593486064 	 63.4593486064
epoch_time;  37.50241827964783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 38.49073791503906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 62.96303939819336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.67923355102539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.5001449584961
7 23.8769491579 	 93.5001478041 	 93.5001478041
epoch_time;  38.083003520965576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4188287258148193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.585750579833984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.62455940246582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.774038314819336
8 24.0188356889 	 9.774037954 	 9.774037954
epoch_time;  36.67391490936279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.353424072265625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 58.83123779296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.676692962646484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 91.72798919677734
9 23.5738537788 	 91.7279877534 	 91.7279877534
epoch_time;  36.633105754852295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 25.249223709106445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 42.62028884887695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.10906982421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 71.75379943847656
10 23.381767836 	 71.7538006757 	 71.7538006757
epoch_time;  36.531527280807495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.543789863586426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.845653533935547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.053625106811523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.18256187438965
11 22.9948666004 	 27.182561761 	 27.182561761
epoch_time;  36.469849824905396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 28.704832077026367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 47.217403411865234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 55.36602783203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.40214538574219
12 22.590182274 	 80.4021484375 	 80.4021484375
epoch_time;  36.46012282371521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.946447372436523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 51.87092971801758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.21085739135742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 83.14154052734375
13 22.8837011256 	 83.1415382179 	 83.1415382179
epoch_time;  36.17811894416809
	 Logging test loss: t(0, 0)_r(0, 0)_none => 46.095123291015625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 70.79827880859375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.1378402709961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.45812225341797
14 22.7102181364 	 100.4581186655 	 100.4581186655
epoch_time;  36.25176644325256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 29.420703887939453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 49.352508544921875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 55.0916633605957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.66459655761719
15 22.2440996319 	 80.6645956503 	 80.6645956503
epoch_time;  36.33808660507202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.74190616607666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 17.593538284301758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.041685104370117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.521961212158203
16 22.4869059961 	 27.5219620988 	 27.5219620988
epoch_time;  36.20892262458801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 15.61012077331543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 28.370447158813477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 29.602161407470703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.45402908325195
17 22.1810786222 	 44.4540276605 	 44.4540276605
epoch_time;  36.145914793014526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 24.962051391601562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 43.727474212646484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.5107307434082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 73.57595825195312
18 21.9572877877 	 73.5759607264 	 73.5759607264
epoch_time;  36.14575958251953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 23.692285537719727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 41.39253234863281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.39331817626953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 68.36658477783203
19 21.8522514493 	 68.3665857264 	 68.3665857264
epoch_time;  36.3040771484375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 23.696365356445312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 41.389583587646484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.38160705566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 68.35830688476562
It took 781.3776388168335 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆█▇▅█▆▇▇▇▁▇▆▁▄▇▇▇▂▇▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅█▆▄▇▅▇▇▇▁▆▅▁▃▇▆▇▂▆▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▇▅█▆▇▇▇▁▇▆▁▄▇▇▇▂▇▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▅▄▇▄▇▆▇▁▆▅▁▃▇▆▇▂▆▄▄
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 72.39922
wandb:  Test loss t(-10, 10)_r(0, 0)_none 46.84059
wandb:    Test loss t(0, 0)_r(-5, 5)_none 50.36126
wandb:     Test loss t(0, 0)_r(0, 0)_none 26.77712
wandb:                         Train loss 21.51338
wandb: 
wandb: 🚀 View run flashing-wish-1411 at: https://wandb.ai/nreints/thesis/runs/gf4tkxu9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_200612-gf4tkxu9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_201911-byl2dbty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run incandescent-noodles-1414
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/byl2dbty
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 32.53321838378906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 53.94143295288086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.19407272338867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 85.7679443359375
0 40.0848240733 	 85.7679476351 	 85.7679476351
epoch_time;  36.1604540348053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 53.82160568237305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 82.86720275878906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.12818145751953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 108.26549530029297
1 28.096328601 	 108.2654983108 	 108.2654983108
epoch_time;  36.35895776748657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 33.572021484375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 55.43513488769531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.77420425415039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.1761474609375
2 26.3256786349 	 89.1761507601 	 89.1761507601
epoch_time;  36.30849814414978
	 Logging test loss: t(0, 0)_r(0, 0)_none => 23.073984146118164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 40.26654052734375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.95484924316406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 67.79829406738281
3 25.4385367299 	 67.7982949747 	 67.7982949747
epoch_time;  36.58240270614624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 48.424251556396484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 76.42791748046875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.95789337158203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 102.95050811767578
4 24.9525828326 	 102.9505067568 	 102.9505067568
epoch_time;  36.33923101425171
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.25876235961914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 46.73161315917969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 51.96229553222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 75.35691833496094
5 23.962283662 	 75.356920397 	 75.356920397
epoch_time;  36.57494068145752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 47.688838958740234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 76.23145294189453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.39813995361328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.46561431884766
6 23.6600425271 	 100.4656144426 	 100.4656144426
epoch_time;  36.37678003311157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 41.97332763671875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 67.8928451538086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 67.68186950683594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.00709533691406
7 23.6578659233 	 96.0070945946 	 96.0070945946
epoch_time;  36.37184977531433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.00170135498047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 71.79900360107422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 67.23834228515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.58366394042969
8 23.4770649966 	 96.583667652 	 96.583667652
epoch_time;  36.1267294883728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7259491682052612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.5328049659729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.252377510070801
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.563291549682617
9 23.1677862644 	 8.5632911476 	 8.5632911476
epoch_time;  36.1971275806427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.7949104309082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 60.86748123168945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 63.18247604370117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.08934783935547
10 23.0545579088 	 90.0893475507 	 90.0893475507
epoch_time;  37.94241738319397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.764999389648438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 53.3311882019043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.36256408691406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.49811553955078
11 23.0175291977 	 82.4981154983 	 82.4981154983
epoch_time;  37.4705069065094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1822640895843506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.9434709548950195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.068839073181152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.074190139770508
12 22.6308580526 	 13.0741897171 	 13.0741897171
epoch_time;  36.52644634246826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 15.6812105178833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 29.83565330505371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 34.0518913269043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.27313232421875
13 22.3849467883 	 50.2731313345 	 50.2731313345
epoch_time;  36.40956354141235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 48.075618743896484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 75.74890899658203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.19487762451172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.7306900024414
14 22.3680126523 	 100.7306904561 	 100.7306904561
epoch_time;  36.43108034133911
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.56520080566406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.4608154296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.188804626464844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.04679870605469
15 22.0497577437 	 89.046801098 	 89.046801098
epoch_time;  36.48652958869934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.08110809326172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 71.12211608886719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.1937484741211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 97.41424560546875
16 22.2220294516 	 97.4142419764 	 97.4142419764
epoch_time;  36.247591733932495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 7.627740383148193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.310546875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.576781272888184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.10170555114746
17 22.1095746763 	 25.1017050253 	 25.1017050253
epoch_time;  36.39863562583923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.069698333740234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 61.78624725341797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.7685661315918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 91.17108154296875
18 21.7682472389 	 91.1710831926 	 91.1710831926
epoch_time;  36.42180109024048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.78449821472168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 46.84501647949219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.34413146972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 72.40172576904297
19 21.5133783112 	 72.4017261402 	 72.4017261402
epoch_time;  36.28707671165466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.777122497558594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 46.8405876159668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.36125564575195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 72.39921569824219
It took 779.3935301303864 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▂▇▄▃▆█▇███▄▅▇█▁▇▇▄▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▂▆▃▂▅▇▆███▃▅▆█▁▆▇▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▂▇▄▃▆█▇███▄▅▇█▁▇▇▄▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▁▆▃▂▅▇▆█▇█▃▄▆█▁▆▆▄▃▃
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 36.94516
wandb:  Test loss t(-10, 10)_r(0, 0)_none 23.50362
wandb:    Test loss t(0, 0)_r(-5, 5)_none 24.65433
wandb:     Test loss t(0, 0)_r(0, 0)_none 11.88687
wandb:                         Train loss 22.04432
wandb: 
wandb: 🚀 View run incandescent-noodles-1414 at: https://wandb.ai/nreints/thesis/runs/byl2dbty
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_201911-byl2dbty/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_203213-gg8vpcny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-lantern-1417
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/gg8vpcny
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 13.856854438781738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 25.537561416625977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.09404754638672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.740264892578125
0 39.8023918839 	 41.7402634079 	 41.7402634079
epoch_time;  36.480390310287476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.620065212249756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.78715991973877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.934001922607422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.664207458496094
1 28.0526431322 	 18.6642076647 	 18.6642076647
epoch_time;  36.55130052566528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 30.472553253173828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 49.443824768066406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.19804763793945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.40990447998047
2 26.5840465259 	 82.4099081503 	 82.4099081503
epoch_time;  36.24010229110718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 15.033924102783203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 26.49453353881836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 32.39777755737305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.63155746459961
3 25.5292691055 	 46.6315561655 	 46.6315561655
epoch_time;  36.2021222114563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.349706649780273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 16.614177703857422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.2656307220459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.70104217529297
4 25.0137075787 	 29.7010425465 	 29.7010425465
epoch_time;  36.340166330337524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 24.98529052734375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 42.985050201416016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.17036437988281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 71.95256042480469
5 24.4079928698 	 71.9525601774 	 71.9525601774
epoch_time;  36.13517355918884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 40.366844177246094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 65.15318298339844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.13985443115234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.415771484375
6 24.4008177471 	 93.4157728041 	 93.4157728041
epoch_time;  36.269875288009644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 34.183048248291016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 55.73521041870117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.976409912109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 83.5284652709961
7 23.9465581627 	 83.5284628378 	 83.5284628378
epoch_time;  36.157609939575195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 44.7440299987793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.98455047607422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.2783203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.91687774658203
8 23.4888789565 	 96.9168813345 	 96.9168813345
epoch_time;  36.08616065979004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 41.886932373046875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 67.55419158935547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 64.59956359863281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.24307250976562
9 23.6483105535 	 93.2430743243 	 93.2430743243
epoch_time;  36.51216101646423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.84263229370117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.92401885986328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.78437805175781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.13098907470703
10 23.4974848722 	 96.1309860642 	 96.1309860642
epoch_time;  36.46742868423462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 14.447409629821777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 27.105255126953125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 29.861766815185547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.64042282104492
11 23.3396982905 	 43.6404217694 	 43.6404217694
epoch_time;  36.29993772506714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 23.421220779418945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 37.71794128417969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 45.13545608520508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.2451057434082
12 22.9773479816 	 63.2451066301 	 63.2451066301
epoch_time;  36.29394340515137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 32.602542877197266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 54.82929611206055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.286495208740234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.6341552734375
13 22.6452849949 	 82.6341532939 	 82.6341532939
epoch_time;  38.18520736694336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 43.086158752441406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.52894592285156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.40870666503906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 94.98897552490234
14 22.6656519761 	 94.9889780405 	 94.9889780405
epoch_time;  37.454063415527344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5807708501815796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.316380500793457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.3852410316467285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.267778396606445
15 22.5470733539 	 10.2677780564 	 10.2677780564
epoch_time;  36.3745653629303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 33.12557601928711
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 54.844120025634766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.29608917236328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.99996948242188
16 22.7591507278 	 82.9999683277 	 82.9999683277
epoch_time;  36.58893084526062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.31514358520508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 59.039432525634766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.36342239379883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 88.7439193725586
17 22.2829135283 	 88.7439189189 	 88.7439189189
epoch_time;  36.21741843223572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 17.57339096069336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 32.00518798828125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 33.98253631591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.00173568725586
18 22.0117610655 	 49.0017366976 	 49.0017366976
epoch_time;  36.17444896697998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.891563415527344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 23.494293212890625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.656057357788086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.93485641479492
19 22.0443188791 	 36.9348553632 	 36.9348553632
epoch_time;  36.69235301017761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 11.886865615844727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 23.50362205505371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.654329299926758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.94516372680664
It took 781.87788438797 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▅█▅▇▇▇█▅▇█▆▁▇▃▃▇▅▇▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▄▇▄▇▅▆▇▄▆█▅▁▇▂▃▆▄▆▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▅█▅▇▇▇█▅▆█▆▁▇▃▃▆▅▇▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▄▇▄▇▅▆▇▄▅█▅▁▆▂▃▆▄▆▃▃
wandb:                         Train loss █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 55.12328
wandb:  Test loss t(-10, 10)_r(0, 0)_none 33.56861
wandb:    Test loss t(0, 0)_r(-5, 5)_none 37.6452
wandb:     Test loss t(0, 0)_r(0, 0)_none 20.17675
wandb:                         Train loss 21.70349
wandb: 
wandb: 🚀 View run brilliant-lantern-1417 at: https://wandb.ai/nreints/thesis/runs/gg8vpcny
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_203213-gg8vpcny/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 17.204343795776367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 29.583768844604492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.8095703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 57.62529373168945
0 37.2160470972 	 57.6252956081 	 57.6252956081
epoch_time;  36.37548208236694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 24.091636657714844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 39.766212463378906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.179588317871094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 70.4043960571289
1 27.7639191562 	 70.4043971706 	 70.4043971706
epoch_time;  36.44184756278992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 46.534236907958984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 69.19612884521484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.2583236694336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.98428344726562
2 26.2108475796 	 101.9842799831 	 101.9842799831
epoch_time;  36.48507761955261
	 Logging test loss: t(0, 0)_r(0, 0)_none => 22.138643264770508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 36.1357307434082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 45.78946304321289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 66.61905670166016
3 25.1423043966 	 66.6190561655 	 66.6190561655
epoch_time;  36.4407684803009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 42.59149932861328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 64.76876068115234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 67.65190887451172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 95.615234375
4 24.5163152717 	 95.615234375 	 95.615234375
epoch_time;  36.29372453689575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 34.138641357421875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 53.1834602355957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.526432037353516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 86.7280044555664
5 24.1630099018 	 86.7280035895 	 86.7280035895
epoch_time;  36.38931751251221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 37.34052276611328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 59.38976287841797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.69649887084961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.8660888671875
6 23.6666463905 	 89.866089527 	 89.866089527
epoch_time;  36.40965008735657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 45.40317916870117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 68.95721435546875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.75582885742188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 98.31835174560547
7 24.0369996192 	 98.3183488176 	 98.3183488176
epoch_time;  36.274118423461914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 24.9183406829834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 40.737876892089844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.93257141113281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 69.23421478271484
8 23.1603281567 	 69.2342113598 	 69.2342113598
epoch_time;  36.614910364151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 33.320186614990234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 54.42637252807617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.05809783935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 84.57542419433594
9 22.9656080738 	 84.5754222973 	 84.5754222973
epoch_time;  36.708889961242676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 50.92515182495117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 77.66739654541016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.6837387084961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.34184265136719
10 23.1544701041 	 100.3418391047 	 100.3418391047
epoch_time;  36.431705474853516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 28.415205001831055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 47.85293960571289
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 51.3914680480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 75.90038299560547
11 22.6097082346 	 75.9003853463 	 75.9003853463
epoch_time;  36.446343660354614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 5.16825532913208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.993448257446289
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.67961311340332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.880905151367188
12 22.696423282 	 17.8809042441 	 17.8809042441
epoch_time;  36.08526587486267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 40.225425720214844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 65.04773712158203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.81418991088867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 91.35521697998047
13 22.5042394846 	 91.3552153716 	 91.3552153716
epoch_time;  36.55621910095215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 12.863603591918945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 22.42227554321289
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.304887771606445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.2767219543457
14 22.432162957 	 40.2767208615 	 40.2767208615
epoch_time;  36.370991230010986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 16.43802833557129
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 29.190793991088867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 30.222566604614258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.85806655883789
15 22.0458462043 	 44.8580658784 	 44.8580658784
epoch_time;  36.00824856758118
	 Logging test loss: t(0, 0)_r(0, 0)_none => 34.689849853515625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 56.35382080078125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.379703521728516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 85.09254455566406
16 21.8308662513 	 85.0925464527 	 85.0925464527
epoch_time;  38.537628173828125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 26.5527400970459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 43.92951965332031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.01905059814453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 71.23320007324219
17 21.7110644465 	 71.233203125 	 71.233203125
epoch_time;  37.654176473617554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 36.916358947753906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 59.312679290771484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.2567253112793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 88.73828887939453
18 21.8728762906 	 88.7382918074 	 88.7382918074
epoch_time;  36.352400064468384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 20.169830322265625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 33.565818786621094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 37.63933563232422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 55.12348937988281
19 21.7034899712 	 55.1234902872 	 55.1234902872
epoch_time;  36.54806065559387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 20.176746368408203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 33.56861114501953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 37.64519500732422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 55.12327575683594
It took 783.4066021442413 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2140423
Array Job ID: 2137927_27
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 10:38:00
CPU Efficiency: 25.50% of 1-17:41:42 core-walltime
Job Wall-clock time: 02:18:59
Memory Utilized: 4.84 GB
Memory Efficiency: 15.48% of 31.25 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
