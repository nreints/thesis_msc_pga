wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_173523-c7mhxrsa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-bush-80
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/c7mhxrsa
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–…â–ƒâ–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–…â–ƒâ–â–â–â–â–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–…â–ƒâ–â–â–â–â–â–â–
wandb:                                   Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 2e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run jumping-bush-80 at: https://wandb.ai/nreints/ThesisFinal/runs/c7mhxrsa
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_173523-c7mhxrsa/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_174437-vfqkxh6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-glade-87
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/vfqkxh6u
Training on dataset: data_t(5,20)_r(5,20)_combiR_pNone_gNone
Testing on 4 datasets: ['data_t(5,20)_r(5,20)_combiR_pNone_gNone', 'data_t(5,20)_r(5,20)_tennis_pNone_gNone', 'data_t(5,20)_r(5,20)_semi_pNone_gNone', 'data_t(5,20)_r(5,20)_full_pNone_gNone']
Focussing on identity: False
Using extra input: None
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 21.717018127441406 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 5.477611541748047 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 5.474421739578247 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 5.509900331497192 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 5.538665771484375 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0024045496 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002303747 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.000265638 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0003015306 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 7.14866e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 56.452802419662476
Epoch 1/9
	 Logging train Loss: 0.0001624273 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001259641 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001435894 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.00016216 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 4.94899e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 50.273279428482056
Epoch 2/9
	 Logging train Loss: 8.40026e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.70907e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.38533e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 7.17986e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.58182e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.25905466079712
Epoch 3/9
	 Logging train Loss: 3.345e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.90827e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.16919e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.49638e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 7.8218e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.904571533203125
Epoch 4/9
	 Logging train Loss: 1.16873e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.2768e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.6561e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.17333e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.126e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.12102913856506
Epoch 5/9
	 Logging train Loss: 6.9377e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.0751e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.2399e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.03499e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.4957e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.30392599105835
Epoch 6/9
	 Logging train Loss: 6.1115e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.8634e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.3447e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.14995e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.3629e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.87711524963379
Epoch 7/9
	 Logging train Loss: 5.7796e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.2957e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.01294e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.23165e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.2676e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.02629733085632
Epoch 8/9
	 Logging train Loss: 5.7281e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.9999e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.0533e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.8385e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.2035e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.69685173034668
Epoch 9/9
	 Logging train Loss: 5.5872e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.05206e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.34364e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.53475e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.197e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.89708876609802
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  555.066910982132  seconds.
----- ITERATION 2/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.432714462280273 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.990890264511108 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 5.117318153381348 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 5.05962347984314 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 5.07726788520813 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0021062386 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.000207351 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002756218 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002878894 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 8.01392e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.46385931968689
Epoch 1/9
	 Logging train Loss: 0.0001621596 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001040379 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001329593 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001414282 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 5.49354e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.15587544441223
Epoch 2/9
	 Logging train Loss: 7.69027e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.56443e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.66221e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 6.03824e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.84858e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.04832100868225
Epoch 3/9
	 Logging train Loss: 3.02313e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 2e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 2e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run distinctive-glade-87 at: https://wandb.ai/nreints/ThesisFinal/runs/vfqkxh6u
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_174437-vfqkxh6u/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_175349-0p3cu4cr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-gorge-94
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/0p3cu4cr
	 Logging test loss: 1.80909e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.29581e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.41209e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.02431e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.213632106781006
Epoch 4/9
	 Logging train Loss: 1.25303e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.255e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.08272e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.17594e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.9103e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.91780066490173
Epoch 5/9
	 Logging train Loss: 7.9674e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.5904e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.08164e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.15279e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.4037e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.96528673171997
Epoch 6/9
	 Logging train Loss: 6.9334e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.5628e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.5851e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.2894e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.9394e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.82758617401123
Epoch 7/9
	 Logging train Loss: 6.6609e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.3022e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.9197e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.4751e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.7498e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 51.276084184646606
Epoch 8/9
	 Logging train Loss: 7.3641e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.3237e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.08051e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.13225e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6079e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 55.40252661705017
Epoch 9/9
	 Logging train Loss: 7.2062e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.9699e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.52254e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.50992e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6464e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.7445592880249
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  551.9682018756866  seconds.
----- ITERATION 3/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.41138482093811 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.914666652679443 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 5.103077411651611 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 5.06337571144104 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 5.05011773109436 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0010647057 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001838785 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.000244333 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002257912 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.06817e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.05849361419678
Epoch 1/9
	 Logging train Loss: 9.43462e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.24145e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.28417e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 5.89743e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.92531e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.23952531814575
Epoch 2/9
	 Logging train Loss: 2.76421e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.27919e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.89697e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.71866e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.03081e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.1337571144104
Epoch 3/9
	 Logging train Loss: 1.11849e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.05607e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.4718e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.37768e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.0278e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.34959650039673
Epoch 4/9
	 Logging train Loss: 9.1789e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.02173e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.89426e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.68439e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.6272e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.14334487915039
Epoch 5/9
	 Logging train Loss: 7.9299e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.8084e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.35335e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.29134e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.0119e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.154743671417236
Epoch 6/9
	 Logging train Loss: 8.6252e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.0496e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.47703e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.40253e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.8067e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.05440592765808
Epoch 7/9
	 Logging train Loss: 7.8589e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.10781e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.22835e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.79831e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.0823e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.246477127075195
Epoch 8/9
	 Logging train Loss: 7.7678e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.06161e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.58868e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.38485e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.4224e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.25441360473633
Epoch 9/9
	 Logging train Loss: 7.1836e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–ƒâ–‚â–â–â–â–â–‚â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–ƒâ–‚â–â–‚â–â–â–‚â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–ƒâ–‚â–â–‚â–â–â–‚â–â–
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 1e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run rare-gorge-94 at: https://wandb.ai/nreints/ThesisFinal/runs/0p3cu4cr
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_175349-0p3cu4cr/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_180253-n7f58hut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-pond-101
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/n7f58hut
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 2e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 2e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run lively-pond-101 at: https://wandb.ai/nreints/ThesisFinal/runs/n7f58hut
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_180253-n7f58hut/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_181205-ps34xj0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-lion-106
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/ps34xj0t
	 Logging test loss: 8.2646e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.21434e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.07865e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.2478e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.07451391220093
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  543.8084208965302  seconds.
----- ITERATION 4/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.269623517990112 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.8134925365448 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.990225553512573 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.999765396118164 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.996568202972412 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0018830036 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001960425 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002652133 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002710696 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.66545e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.0900719165802
Epoch 1/9
	 Logging train Loss: 0.0001566259 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.12887e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001209325 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001212828 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 4.15559e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.30306077003479
Epoch 2/9
	 Logging train Loss: 6.41629e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.27739e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.17479e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 4.1924e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.87385e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.156487464904785
Epoch 3/9
	 Logging train Loss: 2.35897e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.41841e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.76895e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.77557e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 8.1892e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 51.28233742713928
Epoch 4/9
	 Logging train Loss: 1.15463e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.2999e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.10171e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.09686e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.4258e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 56.2550265789032
Epoch 5/9
	 Logging train Loss: 7.3869e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.7574e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.7431e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 7.7261e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6791e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.98808813095093
Epoch 6/9
	 Logging train Loss: 6.2997e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.3782e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.383e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 7.3923e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.4208e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.00097846984863
Epoch 7/9
	 Logging train Loss: 6.5994e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.8789e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.2706e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.1265e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.2679e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.64991283416748
Epoch 8/9
	 Logging train Loss: 6.9167e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.4558e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.7221e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 7.5972e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.021e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.240314245224
Epoch 9/9
	 Logging train Loss: 6.7239e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.08811e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.64446e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.64001e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.1763e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.844765186309814
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  552.154058933258  seconds.
----- ITERATION 5/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.23423433303833 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.797202825546265 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.940162658691406 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.967941761016846 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.945193529129028 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0022418492 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002086397 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002831146 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002847573 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.52628e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.91460156440735
Epoch 1/9
	 Logging train Loss: 0.0001469886 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.78245e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001285985 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001279109 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.75865e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.09098148345947
Epoch 2/9
	 Logging train Loss: 6.19121e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.65529e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.84422e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 4.55937e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.5123e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–…â–ƒâ–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:                                   Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 1e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run giddy-lion-106 at: https://wandb.ai/nreints/ThesisFinal/runs/ps34xj0t
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_181205-ps34xj0t/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_182107-3ja4u0cb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-energy-110
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/3ja4u0cb
		--> Epoch time; 48.96508574485779
Epoch 3/9
	 Logging train Loss: 2.3178e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.72042e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.34701e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.23089e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 5.5334e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.99214696884155
Epoch 4/9
	 Logging train Loss: 1.06816e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.04308e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.4645e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.46022e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.1274e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.7491192817688
Epoch 5/9
	 Logging train Loss: 7.0628e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.4356e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.02299e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.07083e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.3211e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.206331729888916
Epoch 6/9
	 Logging train Loss: 6.1284e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.2418e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.5834e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.059e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.1631e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.02510476112366
Epoch 7/9
	 Logging train Loss: 5.8145e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.371e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.01938e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.08104e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.0047e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.04280471801758
Epoch 8/9
	 Logging train Loss: 5.822e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.6699e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.07e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.10885e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.1495e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.121034383773804
Epoch 9/9
	 Logging train Loss: 6.2138e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.8724e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.9559e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.5333e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.1306e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.91392493247986
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  541.9511826038361  seconds.
----- ITERATION 6/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.057279586791992 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.774686574935913 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.955091953277588 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.9527904987335205 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.9446611404418945 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0020533924 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002091092 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002598594 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002606947 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.98769e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 53.48872184753418
Epoch 1/9
	 Logging train Loss: 0.0001349812 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.71454e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001146132 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001162495 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 4.7728e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 53.94076728820801
Epoch 2/9
	 Logging train Loss: 5.93886e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.95646e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.44327e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 4.55571e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.63897e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.97899770736694
Epoch 3/9
	 Logging train Loss: 2.50002e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.69615e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.96857e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.02891e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.03031e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.903270959854126
Epoch 4/9
	 Logging train Loss: 1.12834e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.387e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.09623e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.15258e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.7097e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.89817953109741
Epoch 5/9
	 Logging train Loss: 7.1745e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.4342e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.6071e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.1114e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.0765e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.8374342918396
Epoch 6/9
	 Logging train Loss: 6.4901e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.2265e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.4826e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.9429e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6974e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.98549151420593
Epoch 7/9
	 Logging train Loss: 6.7537e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.70723e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.53033e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 6.46268e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.0885e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.929851770401
Epoch 8/9
	 Logging train Loss: 7.2272e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.38204e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.88967e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.8418e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6607e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.84882879257202
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–‚â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–†â–„â–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–ƒâ–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–ƒâ–â–
wandb:                                   Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 1e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run eager-energy-110 at: https://wandb.ai/nreints/ThesisFinal/runs/3ja4u0cb
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_182107-3ja4u0cb/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_183018-94vt8t05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-tree-114
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/94vt8t05
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–ƒâ–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–†â–„â–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–ƒâ–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–ƒâ–â–â–
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 1e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run rose-tree-114 at: https://wandb.ai/nreints/ThesisFinal/runs/94vt8t05
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_183018-94vt8t05/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_183932-jywjejw9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-totem-117
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/jywjejw9
Epoch 9/9
	 Logging train Loss: 6.7125e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.4827e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.8189e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.0869e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.1465e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.65597724914551
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  551.121310710907  seconds.
----- ITERATION 7/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.5515456199646 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.904189109802246 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 5.013667106628418 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 5.00467586517334 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.996248006820679 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0019874272 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002133267 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002642374 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.000308342 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 7.33595e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.95859932899475
Epoch 1/9
	 Logging train Loss: 0.0001583226 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001076968 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001308177 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001493781 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 5.1336e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.23157835006714
Epoch 2/9
	 Logging train Loss: 7.41994e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.54548e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.40649e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 6.15937e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.80807e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.02338528633118
Epoch 3/9
	 Logging train Loss: 2.85718e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.66379e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.01634e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.32303e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 8.767e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.78421974182129
Epoch 4/9
	 Logging train Loss: 1.07099e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.1724e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.04821e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.22889e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.6572e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.92527890205383
Epoch 5/9
	 Logging train Loss: 7.2897e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.1197e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.6155e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.3892e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.861e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.783448934555054
Epoch 6/9
	 Logging train Loss: 6.9144e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.47517e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.46838e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.16609e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.7723e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.93967294692993
Epoch 7/9
	 Logging train Loss: 7.4685e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.18826e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.55291e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.7644e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6909e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.96863055229187
Epoch 8/9
	 Logging train Loss: 6.4186e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.08194e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.42968e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.64304e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.497e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 54.26463603973389
Epoch 9/9
	 Logging train Loss: 6.4879e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.0453e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.4203e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 7.8326e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.086e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 52.58047556877136
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  553.7666909694672  seconds.
----- ITERATION 8/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.13990044593811 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.779392719268799 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.979465961456299 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.977588415145874 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.978314161300659 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0023419475 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001904463 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002369281 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002565038 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.85269e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.952858448028564
Epoch 1/9
	 Logging train Loss: 0.0001344743 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.15234e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001116905 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.000118374 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 4.29277e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.961724519729614
Epoch 2/9
	 Logging train Loss: 5.83106e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.6593e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 4.33083e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 4.47436e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–‚â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–‚â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–‚â–â–
wandb:                                   Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 2e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run swept-totem-117 at: https://wandb.ai/nreints/ThesisFinal/runs/jywjejw9
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_183932-jywjejw9/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_184832-pralodhj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sponge-119
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/pralodhj
	 Logging test loss: 2.13565e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.94056558609009
Epoch 3/9
	 Logging train Loss: 2.33306e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.57983e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.8409e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.94714e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 8.0899e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.69560742378235
Epoch 4/9
	 Logging train Loss: 1.10982e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.50947e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.90346e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.03583e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.6328e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.96209239959717
Epoch 5/9
	 Logging train Loss: 7.9089e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.4696e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.7749e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.7004e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.2667e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.807517528533936
Epoch 6/9
	 Logging train Loss: 7.0244e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.40057e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.78279e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.01162e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.1773e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.784908294677734
Epoch 7/9
	 Logging train Loss: 7.1623e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.08982e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.74138e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 3.01027e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.0477e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.0254807472229
Epoch 8/9
	 Logging train Loss: 7.2714e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.51797e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.00062e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.14691e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6898e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.949992179870605
Epoch 9/9
	 Logging train Loss: 7.439e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.11359e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.39702e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.58474e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.3377e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.90927791595459
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  540.148844242096  seconds.
----- ITERATION 9/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.193519115447998 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.848067998886108 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.976518869400024 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.981766700744629 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.955465793609619 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0013890041 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001935876 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002524052 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.000238754 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.52264e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.96651649475098
Epoch 1/9
	 Logging train Loss: 0.0001229814 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.0503e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001006306 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 9.56229e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.88565e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.68273901939392
Epoch 2/9
	 Logging train Loss: 4.61346e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.75731e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.31647e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 3.2438e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6712e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.12075114250183
Epoch 3/9
	 Logging train Loss: 1.72533e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.45042e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.78777e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.80796e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.4921e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.73010778427124
Epoch 4/9
	 Logging train Loss: 9.9758e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.13328e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.47118e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.44853e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.7078e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.19079613685608
Epoch 5/9
	 Logging train Loss: 7.8689e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.11691e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.89247e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.67493e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.244e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 51.10969400405884
Epoch 6/9
	 Logging train Loss: 7.4291e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.70055e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.39195e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.25407e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.8701e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 54.49451494216919
Epoch 7/9
	 Logging train Loss: 6.9578e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.6838e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.17485e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.11624e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.378e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.814424991607666
Epoch 8/9
	 Logging train Loss: 7.8597e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.1871e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.1581e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.0903e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–‚â–â–â–â–‚
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–‚â–â–â–â–‚
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–‚â–â–â–â–‚
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 3e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 4e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 4e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run colorful-sponge-119 at: https://wandb.ai/nreints/ThesisFinal/runs/pralodhj
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_184832-pralodhj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230616_185740-m60cgwu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-glade-120
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal/runs/m60cgwu5
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:                                   Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb: Test loss t(5,20)_r(5,20)_combiR_pNone_gNone 1e-05
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.0
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 1e-05
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 1e-05
wandb:                                   Train loss 1e-05
wandb: 
wandb: ðŸš€ View run electric-glade-120 at: https://wandb.ai/nreints/ThesisFinal/runs/m60cgwu5
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230616_185740-m60cgwu5/logs
	 Logging test loss: 1.1559e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.854867696762085
Epoch 9/9
	 Logging train Loss: 6.9985e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.60281e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.71967e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 3.5104e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.8201e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.88407230377197
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  547.9271397590637  seconds.
----- ITERATION 10/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  800
Number of test simulations:  200
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 19.066449880599976 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combiR_pNone_gNone took 4.750407695770264 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 4.9455626010894775 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 4.904246807098389 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 4.904538869857788 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0014992383 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002126139 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0002600518 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0002792933 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 6.1877e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.15619778633118
Epoch 1/9
	 Logging train Loss: 0.0001354594 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.01484e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 0.0001059915 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 0.0001120791 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 3.66846e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.09351682662964
Epoch 2/9
	 Logging train Loss: 5.04171e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 2.75553e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 3.16905e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 3.34236e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.29568e-05 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.732494592666626
Epoch 3/9
	 Logging train Loss: 1.40531e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.7242e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.18744e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.32747e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 2.6797e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 49.01667857170105
Epoch 4/9
	 Logging train Loss: 6.8772e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 9.1919e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.18065e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.27738e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.6296e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.86458683013916
Epoch 5/9
	 Logging train Loss: 6.0668e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.0326e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.03521e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.14725e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.3368e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.938496351242065
Epoch 6/9
	 Logging train Loss: 5.9024e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 6.2523e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.0212e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.9947e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.0961e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.84947943687439
Epoch 7/9
	 Logging train Loss: 6.36e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 5.6338e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 7.2997e-06 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 8.3139e-06 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 9.574e-07 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.605043172836304
Epoch 8/9
	 Logging train Loss: 6.2521e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.40256e-05 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.942e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 2.02828e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 1.2082e-06 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.773961544036865
Epoch 9/9
	 Logging train Loss: 6.0383e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 8.8323e-06 [MSELoss(): t(5,20)_r(5,20)_combiR_pNone_gNone]
	 Logging test loss: 1.14985e-05 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
	 Logging test loss: 1.29103e-05 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 8.915e-07 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
		--> Epoch time; 48.86178755760193
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combiR_pNone_gNone/'log_dualQ_1'_'None'.pth
It took  539.9978950023651  seconds.

JOB STATISTICS
==============
Job ID: 2928286
Array Job ID: 2928286_36
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 20:22:49
CPU Efficiency: 74.22% of 1-03:27:36 core-walltime
Job Wall-clock time: 01:31:32
Memory Utilized: 7.25 GB
Memory Efficiency: 0.00% of 0.00 MB
