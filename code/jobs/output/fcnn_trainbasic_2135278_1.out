wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_162535-hfg3uafg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-orchid-1129
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/hfg3uafg
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–ƒâ–‚â–ƒâ–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.34805
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.41964
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.11931
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22286
wandb:                         Train loss 1.92909
wandb: 
wandb: ðŸš€ View run flashing-orchid-1129 at: https://wandb.ai/nreints/thesis/runs/hfg3uafg
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_162535-hfg3uafg/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3763796091079712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9891487956047058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.25693678855896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.072531223297119
0 4.2711670087 	 4.0725312104 	 4.0725312104
epoch_time;  35.743406772613525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3031582534313202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7997249960899353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9115145206451416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.582381248474121
1 2.2237319895 	 3.5823812949 	 3.5823812949
epoch_time;  35.152806758880615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29252201318740845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6769725680351257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9699251651763916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5137948989868164
2 2.1439872644 	 3.5137949351 	 3.5137949351
epoch_time;  34.416608810424805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27334392070770264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6354714035987854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7656257152557373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2488112449645996
3 2.0987207494 	 3.2488112991 	 3.2488112991
epoch_time;  34.23739433288574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2572259306907654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.591691792011261
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7656867504119873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.199070692062378
4 2.0709057435 	 3.199070616 	 3.199070616
epoch_time;  34.24699354171753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26236140727996826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5924744009971619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7225821018218994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.145040988922119
5 2.0440254805 	 3.145040976 	 3.145040976
epoch_time;  34.091094732284546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24314159154891968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5143551230430603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.599468946456909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.936595916748047
6 2.0273497006 	 2.9365960198 	 2.9365960198
epoch_time;  33.8699996471405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24408452212810516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5182598233222961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.551208972930908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8790955543518066
7 2.0073359547 	 2.8790956239 	 2.8790956239
epoch_time;  30.978641510009766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24060291051864624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48048028349876404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.509136438369751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7984375953674316
8 1.9980632392 	 2.7984375 	 2.7984375
epoch_time;  29.828620672225952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24106381833553314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4880771040916443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.421403646469116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7027623653411865
9 1.9912813531 	 2.7027624182 	 2.7027624182
epoch_time;  30.224525451660156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22185982763767242
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4737952947616577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3886301517486572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6898670196533203
10 1.98022213 	 2.6898670093 	 2.6898670093
epoch_time;  31.297493934631348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22208938002586365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4591848850250244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.319132089614868
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.612091541290283
11 1.9731288282 	 2.6120915593 	 2.6120915593
epoch_time;  34.166687965393066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2534332573413849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5180665850639343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3066108226776123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.607578754425049
12 1.9690141413 	 2.6075787518 	 2.6075787518
epoch_time;  33.730090379714966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23011453449726105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44998931884765625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.309610366821289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5635058879852295
13 1.9581481479 	 2.5635059254 	 2.5635059254
epoch_time;  33.62854313850403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2399980127811432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5099749565124512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.201683521270752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5074656009674072
14 1.9487461718 	 2.5074655894 	 2.5074655894
epoch_time;  33.99087715148926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2599734961986542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5040811896324158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.219186544418335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4895853996276855
15 1.9431596987 	 2.4895854228 	 2.4895854228
epoch_time;  33.903772592544556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24263133108615875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47798243165016174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1526100635528564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.428394317626953
16 1.9396127999 	 2.4283943795 	 2.4283943795
epoch_time;  34.15207648277283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26935091614723206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4973604679107666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1903133392333984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4456655979156494
17 1.9384611901 	 2.4456655141 	 2.4456655141
epoch_time;  34.2851448059082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22732983529567719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4370619058609009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.118060350418091
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3554294109344482
18 1.9310130001 	 2.3554293246 	 2.3554293246
epoch_time;  34.51695537567139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22285717725753784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4197026491165161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.119366407394409
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.348550796508789
19 1.9290932647 	 2.3485508274 	 2.3485508274
epoch_time;  34.40324592590332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22286365926265717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41963958740234375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.119305372238159
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3480467796325684
It took 751.807694196701 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn14: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135278.0

JOB STATISTICS
==============
Job ID: 2135278
Array Job ID: 2135278_1
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:49:30 core-walltime
Job Wall-clock time: 00:12:45
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
