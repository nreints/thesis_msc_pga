wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195346-hap38l2p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-horse-1161
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/hap38l2p
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–â–ƒâ–„â–ƒâ–…â–…â–…â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–‚â–„â–„â–ƒâ–†â–â–â–‚â–‚â–†â–ƒâ–„â–„â–‚â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–‚â–â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 40.70098
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.24507
wandb:    Test loss t(0, 0)_r(-5, 5)_none 24.3602
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29345
wandb:                         Train loss 1.52335
wandb: 
wandb: ðŸš€ View run bright-horse-1161 at: https://wandb.ai/nreints/thesis/runs/hap38l2p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195346-hap38l2p/logs
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7993993759155273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.8091533184051514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.881298065185547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.96428298950195
0 4.4490329585 	 45.9642842061 	 45.9682643581
epoch_time;  38.452380418777466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6616542339324951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4025321006774902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.02589225769043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.05562400817871
1 2.75721843 	 24.0556244721 	 24.0556587838
epoch_time;  37.65056014060974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4499359130859375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.33712100982666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.036537170410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.986961364746094
2 2.2065752013 	 30.9869615709 	 30.9869774071
epoch_time;  36.819639921188354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39244940876960754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2786002159118652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.295936584472656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.714866638183594
3 1.9220953382 	 32.7148675042 	 32.7148886191
epoch_time;  36.83556604385376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4394585192203522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4698057174682617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.448625564575195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.911087036132812
4 1.7817452236 	 30.9110879434 	 30.9110879434
epoch_time;  36.32165741920471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4071759283542633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4249107837677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.099552154541016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.39516830444336
5 1.7021282715 	 35.3951699747 	 35.3952491554
epoch_time;  36.33107590675354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37747544050216675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3275954723358154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.515541076660156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.54620361328125
6 1.6575260769 	 36.546204603 	 36.5464764569
epoch_time;  36.71124505996704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4430464506149292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6424999237060547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.973831176757812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.99897766113281
7 1.6293198179 	 36.9989785684 	 36.9994246199
epoch_time;  36.34174585342407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2513556480407715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.166095733642578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.768239974975586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.74613952636719
8 1.6069334285 	 38.7461386191 	 38.7469013936
epoch_time;  36.312007427215576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2677757143974304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1782772541046143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.346914291381836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.40065002441406
9 1.5883979152 	 38.4006519215 	 38.401702386
epoch_time;  36.33006453514099
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35582810640335083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.27987003326416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.47604751586914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.32875061035156
10 1.580675005 	 38.3287505279 	 38.3302813556
epoch_time;  36.78882193565369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.273963063955307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2963714599609375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.113988876342773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.22315216064453
11 1.56735058 	 38.2231524493 	 38.225036951
epoch_time;  36.8069167137146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37912437319755554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.585963726043701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.001510620117188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.79199981689453
12 1.5606062159 	 37.7920001056 	 37.7942250845
epoch_time;  36.616111040115356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3226475417613983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3629114627838135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.071069717407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.59175491333008
13 1.5498803926 	 38.5917546453 	 38.5946130701
epoch_time;  36.40364384651184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36951711773872375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4436986446380615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.564218521118164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.26199722290039
14 1.5477079212 	 39.261998522 	 39.2652238176
epoch_time;  36.68086862564087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2832934558391571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3967089653015137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.70454216003418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.42440414428711
15 1.5409103321 	 39.4244035051 	 39.4281118032
epoch_time;  36.691001415252686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29319092631340027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.295168876647949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.467716217041016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.989200592041016
16 1.533597684 	 40.989202386 	 40.9931139147
epoch_time;  36.807071924209595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3219619691371918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3017683029174805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.765424728393555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.75531005859375
17 1.538426941 	 39.7553103885 	 39.7595808699
epoch_time;  36.68787455558777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3278045952320099
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.293142318725586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.439756393432617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.41135787963867
18 1.5304494766 	 39.4113571579 	 39.4161423142
epoch_time;  36.41201305389404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29354098439216614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2494547367095947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.385658264160156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.710453033447266
19 1.5233489019 	 40.7104518581 	 40.7157543285
epoch_time;  36.3979287147522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29344749450683594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2450671195983887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.360198974609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.70097732543945
It took 794.136148929596 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn38: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135937.0

JOB STATISTICS
==============
Job ID: 2135937
Array Job ID: 2135932_5
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 04:02:42 core-walltime
Job Wall-clock time: 00:13:29
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
