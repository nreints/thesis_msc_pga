wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_163537-8vuey81n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-lamp-1323
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/8vuey81n
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▃▂▂▃▄▂▃▂▂▂▂▂▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▄▁▂▃▄▁▄▂▂▁▁▂▂▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▃▁▂▂▄▂▃▁▂▁▁▂▁▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▅▁▂▄▅▂▄▂▂▂▁▃▃▂▃▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.46444
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39986
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.39832
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.33849
wandb:                         Train loss 3.89487
wandb: 
wandb: 🚀 View run brilliant-lamp-1323 at: https://wandb.ai/nreints/thesis/runs/8vuey81n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_163537-8vuey81n/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_164718-0xbwrkbz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-firecracker-1330
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/0xbwrkbz
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5231118202209473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7202944159507751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6434341669082642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8374584913253784
0 8.2535399564 	 0.8374585126 	 0.8374585126
epoch_time;  32.20676898956299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4027481973171234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5160504579544067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5091754198074341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6179835796356201
1 4.4370031234 	 0.6179835964 	 0.6179835964
epoch_time;  31.597201108932495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40501460433006287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5174164772033691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4902035593986511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6024525761604309
2 4.2886244684 	 0.6024525823 	 0.6024525823
epoch_time;  31.294822931289673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42904844880104065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5197609663009644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47152605652809143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5634379982948303
3 4.205672195 	 0.5634380031 	 0.5634380031
epoch_time;  31.256391525268555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3182205259799957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40235769748687744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4094215929508209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4956323504447937
4 4.1486623011 	 0.4956323572 	 0.4956323572
epoch_time;  31.26090717315674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3482002019882202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4310901463031769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43191078305244446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5149503350257874
5 4.0969494488 	 0.5149503553 	 0.5149503553
epoch_time;  32.78514289855957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3999212384223938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49329251050949097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4482966363430023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5411415100097656
6 4.0646289988 	 0.5411415306 	 0.5411415306
epoch_time;  31.53376317024231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4411058723926544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5501149892807007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5078371167182922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6136412024497986
7 4.0303901357 	 0.6136411925 	 0.6136411925
epoch_time;  31.469500303268433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3575742244720459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4218912124633789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42580175399780273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4904976785182953
8 4.0156028505 	 0.4904976922 	 0.4904976922
epoch_time;  31.389236211776733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4129624664783478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5203650593757629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4690099358558655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5801169872283936
9 3.9871645174 	 0.5801170143 	 0.5801170143
epoch_time;  31.252166509628296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3458736836910248
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4441356062889099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41396859288215637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5115646123886108
10 3.9725134352 	 0.5115646362 	 0.5115646362
epoch_time;  31.024043798446655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35326361656188965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4303436279296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41591066122055054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49428847432136536
11 3.9562375037 	 0.4942884703 	 0.4942884703
epoch_time;  31.23926591873169
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.342235803604126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4148642420768738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41067054867744446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4832437336444855
12 3.9469492505 	 0.4832437464 	 0.4832437464
epoch_time;  30.907320022583008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3309558629989624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4155287742614746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4092635214328766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48775383830070496
13 3.9394620509 	 0.487753832 	 0.487753832
epoch_time;  31.42860507965088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36465713381767273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.449920654296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4196367859840393
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.506949245929718
14 3.9241306147 	 0.5069492237 	 0.5069492237
epoch_time;  30.893763065338135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36592575907707214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4408526122570038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40855613350868225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48209965229034424
15 3.9243587218 	 0.4820996671 	 0.4820996671
epoch_time;  31.354640007019043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3604397773742676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4120859205722809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4058378338813782
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4554217457771301
16 3.9111278537 	 0.4554217364 	 0.4554217364
epoch_time;  31.04551672935486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3767443001270294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4518353343009949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4184976816177368
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48643991351127625
17 3.9087247958 	 0.4864399266 	 0.4864399266
epoch_time;  31.524009943008423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3359319269657135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40535736083984375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4085133373737335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4806338846683502
18 3.9044436569 	 0.4806338748 	 0.4806338748
epoch_time;  31.15134859085083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3385116755962372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39974644780158997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39835453033447266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46445682644844055
19 3.8948654764 	 0.4644568366 	 0.4644568366
epoch_time;  31.204407930374146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3384896218776703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3998590111732483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39831823110580444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4644436538219452
It took 701.6463437080383 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▅▃▃▃▃▂▃▄▂▃▂▃▂▁▂▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▄▃▃▄▃▃▃▄▂▂▂▃▃▂▃▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▄▅▃▂▃▃▂▂▄▂▃▂▃▂▁▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▅▅▃▄▅▃▃▂▄▂▂▃▃▃▂▃▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.45129
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35746
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.38034
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30302
wandb:                         Train loss 3.89308
wandb: 
wandb: 🚀 View run twinkling-firecracker-1330 at: https://wandb.ai/nreints/thesis/runs/0xbwrkbz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_164718-0xbwrkbz/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165840-p26i5sw6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-envelope-1337
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/p26i5sw6
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48352888226509094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6461250185966492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5547025799751282
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7563093304634094
0 8.2182283292 	 0.7563093031 	 0.7563093031
epoch_time;  30.99923801422119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4744388461112976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5904826521873474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6000521779060364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7344683408737183
1 4.4177822973 	 0.7344683673 	 0.7344683673
epoch_time;  31.160616159439087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41150254011154175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5305291414260864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47912028431892395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6154348254203796
2 4.285588331 	 0.6154348013 	 0.6154348013
epoch_time;  30.733712434768677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.398240864276886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49976846575737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49330323934555054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6130126714706421
3 4.198722204 	 0.6130126953 	 0.6130126953
epoch_time;  31.150883436203003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36486926674842834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43853995203971863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4323442280292511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5279003381729126
4 4.1390804841 	 0.5279003246 	 0.5279003246
epoch_time;  30.98596715927124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3707367777824402
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4512261748313904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4212374985218048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5216166973114014
5 4.0973385531 	 0.5216166728 	 0.5216166728
epoch_time;  31.10727286338806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39561888575553894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4803643822669983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44852563738822937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5547769069671631
6 4.0588562384 	 0.5547769083 	 0.5547769083
epoch_time;  30.738958597183228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3556448817253113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4211729168891907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4448750615119934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5339974761009216
7 4.031047679 	 0.5339974481 	 0.5339974481
epoch_time;  31.160893440246582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3495505154132843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42346298694610596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4105575680732727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5088114738464355
8 4.0030506834 	 0.508811497 	 0.508811497
epoch_time;  30.76243543624878
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3333616852760315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44702592492103577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4052584171295166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5377666354179382
9 3.9851251084 	 0.5377666576 	 0.5377666576
epoch_time;  30.939959049224854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3916228115558624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4705757796764374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47748562693595886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5696956515312195
10 3.9726023638 	 0.5696956325 	 0.5696956325
epoch_time;  30.76820468902588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33618226647377014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40250346064567566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4178716838359833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5028200149536133
11 3.9607143518 	 0.5028200304 	 0.5028200304
epoch_time;  30.76152539253235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.338090717792511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40370166301727295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4406512379646301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5249879360198975
12 3.947232566 	 0.5249879167 	 0.5249879167
epoch_time;  31.927398920059204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.344710111618042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39156466722488403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40711817145347595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47879382967948914
13 3.934248318 	 0.4787938298 	 0.4787938298
epoch_time;  31.562645196914673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3536444306373596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4250844717025757
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4316581189632416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5219869017601013
14 3.9265911952 	 0.521986884 	 0.521986884
epoch_time;  31.93526840209961
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3490300476551056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43424859642982483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41335374116897583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5163156986236572
15 3.9244360798 	 0.516315687 	 0.516315687
epoch_time;  30.828025579452515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32728004455566406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38747918605804443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38726291060447693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46272042393684387
16 3.9162103424 	 0.4627204276 	 0.4627204276
epoch_time;  30.940374851226807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3573415279388428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42510709166526794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41557610034942627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5063988566398621
17 3.91229153 	 0.5063988351 	 0.5063988351
epoch_time;  31.293837785720825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3518892526626587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4240451753139496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41407084465026855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5071252584457397
18 3.8985194863 	 0.5071252359 	 0.5071252359
epoch_time;  31.0675790309906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30297398567199707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35744237899780273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38021326065063477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4513564705848694
19 3.8930779769 	 0.4513564651 	 0.4513564651
epoch_time;  31.1869695186615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3030170500278473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3574589788913727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3803372085094452
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4512937664985657
It took 681.5592339038849 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▄▅▅▅▃▃▃▄▃▃▃▃▃▄▂▁▃▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▅▅▄▅▄▃▄▄▃▃▃▃▃▄▃▁▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▅▄▄▃▃▃▃▄▂▃▂▂▃▃▁▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▆▆▄▅▅▃▅▄▅▃▂▂▃▄▄▁▃▄▄
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49946
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.44748
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40864
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.37251
wandb:                         Train loss 3.90376
wandb: 
wandb: 🚀 View run flashing-envelope-1337 at: https://wandb.ai/nreints/thesis/runs/p26i5sw6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165840-p26i5sw6/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171009-idwikj06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-rabbit-1343
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/idwikj06
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.452483594417572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.566313624382019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5742398500442505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7031430006027222
0 8.2613577459 	 0.7031429806 	 0.7031429806
epoch_time;  31.129281044006348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4454934597015381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6091465950012207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5201066136360168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7068951725959778
1 4.4261670484 	 0.7068951581 	 0.7068951581
epoch_time;  30.952654361724854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4043950140476227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4960968494415283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4553384780883789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.562705397605896
2 4.275552614 	 0.5627054163 	 0.5627054163
epoch_time;  31.123221158981323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41599851846694946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5137526392936707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48383280634880066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5956435203552246
3 4.1972900887 	 0.5956435332 	 0.5956435332
epoch_time;  31.116694927215576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37369903922080994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48113444447517395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45974934101104736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5904821157455444
4 4.1384014525 	 0.5904821035 	 0.5904821035
epoch_time;  31.099172830581665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38662612438201904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4914092719554901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46023958921432495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5723429918289185
5 4.0966069577 	 0.5723429912 	 0.5723429912
epoch_time;  30.733442306518555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3983785808086395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47335654497146606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44025176763534546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5313077569007874
6 4.0602129792 	 0.531307736 	 0.531307736
epoch_time;  30.710104942321777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35885703563690186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4437497854232788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42872413992881775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5284308195114136
7 4.0417382945 	 0.5284307944 	 0.5284307944
epoch_time;  31.135408401489258
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39101117849349976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4680711030960083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43907326459884644
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5220127701759338
8 4.0128556486 	 0.5220127415 	 0.5220127415
epoch_time;  30.85371732711792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3639993667602539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44988569617271423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44079652428627014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.54197758436203
9 3.9992353456 	 0.5419775886 	 0.5419775886
epoch_time;  31.017423629760742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3896632492542267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43964383006095886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46543487906455994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5320491194725037
10 3.9793834234 	 0.5320491069 	 0.5320491069
epoch_time;  30.91192889213562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3532753884792328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42509835958480835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41156449913978577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4965803921222687
11 3.9684508162 	 0.4965803817 	 0.4965803817
epoch_time;  31.07875657081604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34116125106811523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42000535130500793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4300675690174103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.51861572265625
12 3.9536083224 	 0.5186157227 	 0.5186157227
epoch_time;  30.873244762420654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34025973081588745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4287485182285309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41554850339889526
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5133415460586548
13 3.9461829431 	 0.5133415428 	 0.5133415428
epoch_time;  31.034382343292236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3515068590641022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43057671189308167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41622352600097656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5023263692855835
14 3.9283988607 	 0.502326388 	 0.502326388
epoch_time;  30.741379976272583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3675670623779297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45713454484939575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43312278389930725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5369702577590942
15 3.9280388483 	 0.5369702313 	 0.5369702313
epoch_time;  30.85288643836975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36471742391586304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41989263892173767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4241856634616852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4913810193538666
16 3.9241180523 	 0.4913810111 	 0.4913810111
epoch_time;  30.829425573349
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31427109241485596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3605627119541168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38045772910118103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43610715866088867
17 3.9128277468 	 0.4361071612 	 0.4361071612
epoch_time;  31.033122301101685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36163243651390076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4423729181289673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4168083369731903
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.504587709903717
18 3.9121963201 	 0.5045876993 	 0.5045876993
epoch_time;  30.71473526954651
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37243837118148804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4476358890533447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4085618853569031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.499508261680603
19 3.9037590053 	 0.4995082546 	 0.4995082546
epoch_time;  31.483917474746704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3725144565105438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4474823474884033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4086386263370514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49946272373199463
It took 688.7584345340729 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▄▂▄▂▃▃▂▂▁▂▃▂▃▂▁▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▃▂▃▂▄▃▂▂▁▁▂▂▃▂▁▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▃▃▂▃▃▂▂▁▂▂▁▃▁▁▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▄▂▂▁▃▄▃▃▁▁▂▁▄▁▂▁▄▄
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.54279
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.4792
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.4573
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.3999
wandb:                         Train loss 3.91426
wandb: 
wandb: 🚀 View run luminous-rabbit-1343 at: https://wandb.ai/nreints/thesis/runs/idwikj06
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171009-idwikj06/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_172125-nc3dri64
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-lamp-1350
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nc3dri64
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48668113350868225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6248109340667725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.590442419052124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7369527220726013
0 8.2289229117 	 0.7369527456 	 0.7369527456
epoch_time;  31.129711866378784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44950491189956665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5643988251686096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5449028015136719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6749262809753418
1 4.4239563939 	 0.6749262629 	 0.6749262629
epoch_time;  30.954293727874756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42385581135749817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5519094467163086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5164889097213745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6542558073997498
2 4.2808000672 	 0.6542558 	 0.6542558
epoch_time;  31.0382661819458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39644497632980347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4827323853969574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47955989837646484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5739442706108093
3 4.1903305105 	 0.573944298 	 0.573944298
epoch_time;  31.16247582435608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36565515398979187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4382440149784088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4530692994594574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5207474827766418
4 4.137932676 	 0.5207474992 	 0.5207474992
epoch_time;  30.91369342803955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36795881390571594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46328166127204895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47045478224754333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5620148777961731
5 4.0904497212 	 0.5620148942 	 0.5620148942
epoch_time;  30.881678819656372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34923025965690613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43550312519073486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44116896390914917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5174741148948669
6 4.0555093464 	 0.5174741178 	 0.5174741178
epoch_time;  30.688719511032104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38661620020866394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48959413170814514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4490925073623657
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5464996099472046
7 4.0214622255 	 0.5464995925 	 0.5464995925
epoch_time;  30.83942747116089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3977920114994049
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4681762158870697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.464739590883255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5286458730697632
8 4.0161397284 	 0.5286458608 	 0.5286458608
epoch_time;  31.013593673706055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37985366582870483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45575377345085144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4397059977054596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5152261853218079
9 3.9930394301 	 0.5152262095 	 0.5152262095
epoch_time;  31.019758462905884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3865031599998474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4577663242816925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43786922097206116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5002977848052979
10 3.9854629713 	 0.5002977938 	 0.5002977938
epoch_time;  30.938814163208008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34242358803749084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4248446226119995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4061867296695709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48327958583831787
11 3.9691543114 	 0.4832795839 	 0.4832795839
epoch_time;  30.93411087989807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34286341071128845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41301587224006653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42597636580467224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48961141705513
12 3.9600697016 	 0.489611404 	 0.489611404
epoch_time;  30.777843952178955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36484238505363464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45418721437454224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43832194805145264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5242518782615662
13 3.9534760865 	 0.5242518657 	 0.5242518657
epoch_time;  30.563523292541504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3397562503814697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44068530201911926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4188205599784851
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5101692080497742
14 3.9411483899 	 0.5101691994 	 0.5101691994
epoch_time;  30.862598419189453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.394572913646698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4751366376876831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46984949707984924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5405394434928894
15 3.9299182517 	 0.5405394271 	 0.5405394271
epoch_time;  30.7809796333313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34969383478164673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43371251225471497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40998032689094543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4894750714302063
16 3.9295750994 	 0.4894750647 	 0.4894750647
epoch_time;  30.849068880081177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3561727702617645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41908982396125793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4068887233734131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4645659923553467
17 3.9191244922 	 0.4645659988 	 0.4645659988
epoch_time;  30.818200826644897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.347309947013855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4173484146595001
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43795785307884216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5076984167098999
18 3.9132697455 	 0.5076984302 	 0.5076984302
epoch_time;  30.900797128677368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3998737633228302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4791397154331207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4573494493961334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5428438186645508
19 3.9142552339 	 0.5428437929 	 0.5428437929
epoch_time;  31.11140275001526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3999016284942627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4791976511478424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4572966396808624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5427888631820679
It took 676.4317901134491 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.153 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▄▆▅▄▃▂▃▃▁▄▂▄▃▃▃▃▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▅▇▇▃▄▂▄▄▁▄▂▄▃▄▃▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▅▄▆▆▅▂▁▄▃▁▃▂▃▃▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▃▇█▃▄▁▄▄▁▄▃▃▄▃▃▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49758
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.41733
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40633
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.34109
wandb:                         Train loss 3.90248
wandb: 
wandb: 🚀 View run glowing-lamp-1350 at: https://wandb.ai/nreints/thesis/runs/nc3dri64
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_172125-nc3dri64/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173245-f5fyaqik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-ox-1356
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/f5fyaqik
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4315435588359833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5386365056037903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.546696126461029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6795490980148315
0 8.2204174953 	 0.6795490987 	 0.6795490987
epoch_time;  30.9099338054657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40025025606155396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.517589807510376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5183789134025574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.660904049873352
1 4.4230175186 	 0.6609040544 	 0.6609040544
epoch_time;  30.7052743434906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3864186406135559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4614979028701782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4845515191555023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.577193021774292
2 4.2723938278 	 0.5771930179 	 0.5771930179
epoch_time;  30.89257502555847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36476531624794006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4579777419567108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45877212285995483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5562337636947632
3 4.1881882881 	 0.5562337927 	 0.5562337927
epoch_time;  30.81496524810791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42016345262527466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5239437222480774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49753817915916443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6178503632545471
4 4.1364169214 	 0.6178503913 	 0.6178503913
epoch_time;  31.374579906463623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4336710274219513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5148739218711853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49843287467956543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5920025706291199
5 4.0973911169 	 0.59200258 	 0.59200258
epoch_time;  31.512313842773438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3589147925376892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43260589241981506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4783465266227722
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5624464154243469
6 4.0660426011 	 0.562446388 	 0.562446388
epoch_time;  31.465453624725342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3715142607688904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45479291677474976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.433662086725235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5279220938682556
7 4.0326318657 	 0.5279220993 	 0.5279220993
epoch_time;  30.840256452560425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33861321210861206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39370763301849365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41315075755119324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4706016480922699
8 4.008982127 	 0.4706016334 	 0.4706016334
epoch_time;  30.703038692474365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3776448965072632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43794283270835876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4640488922595978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5264536738395691
9 3.9985546611 	 0.5264536677 	 0.5264536677
epoch_time;  31.13548254966736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3761637508869171
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4412117600440979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44555234909057617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5228739380836487
10 3.9759525617 	 0.5228739558 	 0.5228739558
epoch_time;  30.682366371154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3367704153060913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3767354190349579
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40354862809181213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4479231536388397
11 3.9637445387 	 0.4479231551 	 0.4479231551
epoch_time;  30.89006495475769
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3702988028526306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45701178908348083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43845483660697937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5447009801864624
12 3.9555852499 	 0.5447009937 	 0.5447009937
epoch_time;  30.73604965209961
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35651925206184387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41121992468833923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4241263270378113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4905155599117279
13 3.9419507315 	 0.4905155491 	 0.4905155491
epoch_time;  30.985154390335083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35646021366119385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4421984553337097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4421718418598175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5437227487564087
14 3.9394544474 	 0.5437227404 	 0.5437227404
epoch_time;  30.537575006484985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3719783425331116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4295884072780609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4542219340801239
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.525443971157074
15 3.9276418098 	 0.5254439895 	 0.5254439895
epoch_time;  30.65129280090332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3624565005302429
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4374482035636902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42294827103614807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5109224915504456
16 3.9166358116 	 0.5109224887 	 0.5109224887
epoch_time;  31.010339498519897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35602378845214844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4266751706600189
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4205912947654724
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5014278292655945
17 3.9211453881 	 0.5014278103 	 0.5014278103
epoch_time;  30.901731967926025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33205485343933105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41967329382896423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4156479239463806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5142078399658203
18 3.9034813098 	 0.5142078297 	 0.5142078297
epoch_time;  30.711294412612915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3410709798336029
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41721224784851074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40631428360939026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4976281523704529
19 3.9024839201 	 0.4976281656 	 0.4976281656
epoch_time;  30.417861938476562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34108784794807434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4173312187194824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40632858872413635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4975759983062744
It took 679.6394028663635 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▃▃▃▃▃▁▂▃▂▁▂▂▂▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▄▄▃▃▂▂▁▂▂▁▂▃▂▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▄▄▃▃▃▄▂▂▃▂▂▂▂▃▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▃▄▅▂▃▃▄▂▂▂▁▃▂▂▃▁▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47167
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.40847
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40513
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.34128
wandb:                         Train loss 3.91648
wandb: 
wandb: 🚀 View run vibrant-ox-1356 at: https://wandb.ai/nreints/thesis/runs/f5fyaqik
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173245-f5fyaqik/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174358-fl8znlrk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-noodles-1362
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/fl8znlrk
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4721604287624359
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.627530574798584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5842068195343018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.743254542350769
0 8.3048792421 	 0.743254543 	 0.743254543
epoch_time;  30.723021268844604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3794136345386505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5187872648239136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48320886492729187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6231493353843689
1 4.4430882913 	 0.6231493563 	 0.6231493563
epoch_time;  30.623669862747192
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3793480098247528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48610877990722656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47168484330177307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5774874091148376
2 4.2925396443 	 0.5774873888 	 0.5774873888
epoch_time;  30.583081483840942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3943555951118469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49160248041152954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.483524352312088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5756528973579407
3 4.210115323 	 0.5756529112 	 0.5756529112
epoch_time;  30.947869300842285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4089633524417877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49160221219062805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47802284359931946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5502605438232422
4 4.1559313114 	 0.5502605541 	 0.5502605541
epoch_time;  30.91167116165161
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36488065123558044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4589484632015228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46503010392189026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5496017336845398
5 4.1030617251 	 0.5496017044 	 0.5496017044
epoch_time;  30.581372261047363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3765309453010559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47010090947151184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4446161687374115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5359951257705688
6 4.0684304518 	 0.5359951535 	 0.5359951535
epoch_time;  30.799918174743652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37908077239990234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45123741030693054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45283958315849304
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5310022830963135
7 4.0536230006 	 0.5310023127 	 0.5310023127
epoch_time;  30.75331211090088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3894513249397278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43907153606414795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4813331365585327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5333544611930847
8 4.0229343422 	 0.5333544757 	 0.5333544757
epoch_time;  30.98912525177002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3585626184940338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41769173741340637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4234583377838135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4864872694015503
9 4.0095838006 	 0.48648727 	 0.48648727
epoch_time;  30.711381196975708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36899876594543457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4491981565952301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4321465492248535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5145893692970276
10 3.996936093 	 0.5145893819 	 0.5145893819
epoch_time;  30.922178745269775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3559027314186096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43723878264427185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44750016927719116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5355408787727356
11 3.9791252327 	 0.535540854 	 0.535540854
epoch_time;  31.084676265716553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35000351071357727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41505059599876404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43075841665267944
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49876469373703003
12 3.9687228916 	 0.4987646979 	 0.4987646979
epoch_time;  31.63782286643982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36972174048423767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4345364272594452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42398974299430847
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48834842443466187
13 3.9553022649 	 0.4883484299 	 0.4883484299
epoch_time;  31.918601751327515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3635641038417816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45563045144081116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4305647909641266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5154463648796082
14 3.9439081542 	 0.5154463897 	 0.5154463897
epoch_time;  31.281251907348633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36396554112434387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43941164016723633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42624858021736145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49732068181037903
15 3.9433035053 	 0.4973206804 	 0.4973206804
epoch_time;  30.884244918823242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3789210915565491
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44293105602264404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46218568086624146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5288915634155273
16 3.9356685321 	 0.5288915686 	 0.5288915686
epoch_time;  30.673904180526733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3464180529117584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4385806620121002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4415857195854187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.533733606338501
17 3.9277522269 	 0.5337335947 	 0.5337335947
epoch_time;  31.093733310699463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34363853931427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4255642592906952
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43217307329177856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5169482827186584
18 3.9230813225 	 0.5169483082 	 0.5169483082
epoch_time;  30.683093309402466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3412616550922394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.408437579870224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4051932692527771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4717630445957184
19 3.9164767979 	 0.4717630335 	 0.4717630335
epoch_time;  30.626654863357544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34127727150917053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4084688425064087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40512770414352417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4716734290122986
It took 673.493935585022 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▆▃▄▃▂▃▃▂▅▃▃▃▂▂▂▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▆▃▆▅▂▃▄▃▇▂▃▄▂▁▃▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▅▃▃▃▂▃▃▂▅▃▂▂▁▂▃▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▆▄▆▅▂▄▄▄▇▂▃▃▂▁▄▁▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47604
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39586
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.403
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32604
wandb:                         Train loss 3.91197
wandb: 
wandb: 🚀 View run alight-noodles-1362 at: https://wandb.ai/nreints/thesis/runs/fl8znlrk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174358-fl8znlrk/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_175517-bnmghmeu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-orchid-1368
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/bnmghmeu
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4467572867870331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6020268201828003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5535820722579956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7177935838699341
0 8.2677348246 	 0.7177935626 	 0.7177935626
epoch_time;  30.746004581451416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.377729594707489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49926894903182983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4508858025074005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5740270018577576
1 4.4302587593 	 0.5740269841 	 0.5740269841
epoch_time;  30.622390508651733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4107307493686676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5358468294143677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4929257035255432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6260000467300415
2 4.2795025019 	 0.626000028 	 0.626000028
epoch_time;  30.42699432373047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36931559443473816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45868465304374695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43284037709236145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5223295092582703
3 4.1909659043 	 0.5223295057 	 0.5223295057
epoch_time;  30.637088298797607
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40510794520378113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5351503491401672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43873485922813416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5668823719024658
4 4.1323324835 	 0.5668823655 	 0.5668823655
epoch_time;  30.515663146972656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3972757160663605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5018483400344849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4481155276298523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5441910028457642
5 4.0915439737 	 0.5441910202 	 0.5441910202
epoch_time;  30.337520599365234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3401211202144623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43000125885009766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41718676686286926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5031850337982178
6 4.0562052381 	 0.5031850042 	 0.5031850042
epoch_time;  30.262623071670532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3813718259334564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45279642939567566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4516177475452423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5246111750602722
7 4.0370528441 	 0.5246111483 	 0.5246111483
epoch_time;  30.227442979812622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37177276611328125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4745085537433624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4354681372642517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5369698405265808
8 4.0069341227 	 0.5369698602 	 0.5369698602
epoch_time;  30.419442892074585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3702506422996521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4421469271183014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.425656259059906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4905325472354889
9 3.9944084028 	 0.49053254 	 0.49053254
epoch_time;  30.536670684814453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.433362752199173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5600748062133789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49083852767944336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6147669553756714
10 3.9765340693 	 0.6147669612 	 0.6147669612
epoch_time;  30.60906457901001
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34344324469566345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4251424968242645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43269646167755127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5177035331726074
11 3.9696300303 	 0.5177035358 	 0.5177035358
epoch_time;  30.47502040863037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36244016885757446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45521900057792664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4229820966720581
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5104941725730896
12 3.9597110116 	 0.5104941703 	 0.5104941703
epoch_time;  30.49688148498535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3646101653575897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4675723910331726
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42738214135169983
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5263347625732422
13 3.9446457002 	 0.5263347729 	 0.5263347729
epoch_time;  30.327429056167603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35124194622039795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41880425810813904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39974865317344666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4704655706882477
14 3.9408075517 	 0.4704655828 	 0.4704655828
epoch_time;  30.313050270080566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33080270886421204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3873085677623749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.417415052652359
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4725587069988251
15 3.9286269677 	 0.4725587175 	 0.4725587175
epoch_time;  30.341503381729126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3799249529838562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4562940001487732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4307830333709717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5059542059898376
16 3.9210634018 	 0.5059542269 	 0.5059542269
epoch_time;  30.320514917373657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33379507064819336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.393384724855423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41455185413360596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4743042290210724
17 3.912598648 	 0.4743042405 	 0.4743042405
epoch_time;  30.439101934432983
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33949339389801025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3978768289089203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3953430950641632
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45122846961021423
18 3.9088190139 	 0.4512284562 	 0.4512284562
epoch_time;  30.21550703048706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32603418827056885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3958601951599121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40314117074012756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47594138979911804
19 3.911973172 	 0.4759413848 	 0.4759413848
epoch_time;  31.208282232284546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3260391056537628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39586326479911804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4029981791973114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4760371744632721
It took 678.3075759410858 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▃▅▄▄▃▄▄▃▂▃▂▃▂▃▃▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▇▃▅▄▃▃▆▅▃▁▃▂▃▂▃▃▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▆▄▆▃▄▃▄▃▃▃▄▁▃▂▂▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▆▃▅▄▃▃▆▄▃▂▄▁▃▂▃▂▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47228
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.40244
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40993
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.33429
wandb:                         Train loss 3.89985
wandb: 
wandb: 🚀 View run radiant-orchid-1368 at: https://wandb.ai/nreints/thesis/runs/bnmghmeu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_175517-bnmghmeu/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180631-00ebrn7m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-ox-1375
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/00ebrn7m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4121541678905487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5647644996643066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5517397522926331
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7099335789680481
0 8.2082888139 	 0.7099335542 	 0.7099335542
epoch_time;  30.497041702270508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4632255733013153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5842850804328918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5521954894065857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6727051734924316
1 4.4269505464 	 0.6727052018 	 0.6727052018
epoch_time;  30.407075881958008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41996529698371887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.553020715713501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.516810417175293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6424164772033691
2 4.2753360777 	 0.6424164643 	 0.6424164643
epoch_time;  30.15290665626526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3685210645198822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4401519000530243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46075087785720825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5290647149085999
3 4.1874494197 	 0.529064694 	 0.529064694
epoch_time;  30.267387628555298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4132431745529175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5049716234207153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.506463348865509
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6009141206741333
4 4.1412617161 	 0.6009141252 	 0.6009141252
epoch_time;  30.31008744239807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.389105886220932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47756606340408325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4493502378463745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5397358536720276
5 4.0891918268 	 0.539735825 	 0.539735825
epoch_time;  30.57755994796753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3713100254535675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4517437219619751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4661063253879547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5409913659095764
6 4.0543777373 	 0.5409913759 	 0.5409913759
epoch_time;  30.71291995048523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37130865454673767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44276073575019836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4378722310066223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5013738870620728
7 4.031483727 	 0.5013739096 	 0.5013739096
epoch_time;  30.760405778884888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4261161983013153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5280671715736389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4620771110057831
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5595805048942566
8 4.0114546087 	 0.5595804988 	 0.5595804988
epoch_time;  30.554203987121582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3953576982021332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.508418619632721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4545658826828003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5620146989822388
9 3.9987073936 	 0.562014688 	 0.562014688
epoch_time;  30.621606588363647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3630310893058777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4537135660648346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44605469703674316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5352897047996521
10 3.9745495382 	 0.5352897025 	 0.5352897025
epoch_time;  30.300960063934326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3451293706893921
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3884470462799072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4362167418003082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.473591685295105
11 3.9682177503 	 0.4735916962 	 0.4735916962
epoch_time;  30.617879390716553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38185593485832214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44052132964134216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4560556411743164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5141777396202087
12 3.955947907 	 0.5141777245 	 0.5141777245
epoch_time;  30.424943685531616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3423730134963989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41924822330474854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40813788771629333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48347464203834534
13 3.9418687452 	 0.483474649 	 0.483474649
epoch_time;  30.197911739349365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36286023259162903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4346821904182434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45436373353004456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5299863219261169
14 3.9345200628 	 0.5299863248 	 0.5299863248
epoch_time;  30.386350393295288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35064345598220825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4233682453632355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42377346754074097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4914562404155731
15 3.9280041363 	 0.4914562328 	 0.4914562328
epoch_time;  30.60989809036255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3666955530643463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45431217551231384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.425447940826416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5040130019187927
16 3.9170916951 	 0.504012979 	 0.504012979
epoch_time;  30.28324031829834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3553329408168793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4360094964504242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42883846163749695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.509589672088623
17 3.9215982963 	 0.509589654 	 0.509589654
epoch_time;  30.33831238746643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3341856598854065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38563352823257446
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4019624888896942
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44210389256477356
18 3.9054367489 	 0.4421039066 	 0.4421039066
epoch_time;  30.07838535308838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33430996537208557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40251922607421875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41002002358436584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47234126925468445
19 3.8998458129 	 0.4723412591 	 0.4723412591
epoch_time;  30.43018364906311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33429399132728577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40244218707084656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4099332094192505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4722825884819031
It took 674.4869952201843 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▅▄▃▃▄▃▃▂▂▃▃▂▃▃▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▆▄▄▄▅▃▃▃▂▃▄▂▄▄▂▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▅▅▄▃▄▄▂▃▂▂▃▂▄▂▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▅▆▅▅▅▅▃▃▄▂▂▃▂▆▄▂▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.44948
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.33123
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.39978
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29512
wandb:                         Train loss 3.88501
wandb: 
wandb: 🚀 View run crimson-ox-1375 at: https://wandb.ai/nreints/thesis/runs/00ebrn7m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180631-00ebrn7m/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_181750-zj6lbjgm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-rooster-1381
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/zj6lbjgm
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45744869112968445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6179954409599304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5792749524116516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7800570130348206
0 8.1958414438 	 0.7800570101 	 0.7800570101
epoch_time;  30.538853406906128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4438789188861847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5857716202735901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5341377854347229
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7036960124969482
1 4.4045005184 	 0.7036960086 	 0.7036960086
epoch_time;  30.899791717529297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3892737030982971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5057028532028198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47013455629348755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5901951789855957
2 4.265408133 	 0.5901951558 	 0.5901951558
epoch_time;  30.42872905731201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4216074049472809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5347629189491272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49685293436050415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6132195591926575
3 4.1792035164 	 0.613219555 	 0.613219555
epoch_time;  31.190945863723755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.395372211933136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4667642116546631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.505611002445221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5800262689590454
4 4.1257097765 	 0.5800262451 	 0.5800262451
epoch_time;  31.42626118659973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3925733268260956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47029703855514526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45550110936164856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5412794351577759
5 4.0787792374 	 0.5412794371 	 0.5412794371
epoch_time;  31.01143455505371
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3793491721153259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46199682354927063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44658830761909485
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5427565574645996
6 4.0424007966 	 0.5427565291 	 0.5427565291
epoch_time;  31.146186351776123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3901090919971466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4953322410583496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4786621034145355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5906150341033936
7 4.0177249069 	 0.5906150612 	 0.5906150612
epoch_time;  30.38690757751465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3432381749153137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39953839778900146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4563291370868683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.523248016834259
8 3.9958560559 	 0.5232480023 	 0.5232480023
epoch_time;  30.5205135345459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.332343727350235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4149967133998871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4189271628856659
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5137856006622314
9 3.9735564462 	 0.5137856148 	 0.5137856148
epoch_time;  30.44810724258423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3547113239765167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4066343605518341
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4372650980949402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4956637918949127
10 3.9627825221 	 0.4956637821 	 0.4956637821
epoch_time;  30.419148206710815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3175552487373352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38136982917785645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4116224944591522
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49209335446357727
11 3.9442387441 	 0.4920933491 	 0.4920933491
epoch_time;  30.399826049804688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32670968770980835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4066113829612732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4237726032733917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5137542486190796
12 3.94815954 	 0.5137542312 	 0.5137542312
epoch_time;  30.486839532852173
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3499595820903778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4519160985946655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.434515118598938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5610236525535583
13 3.9214531673 	 0.5610236503 	 0.5610236503
epoch_time;  30.621224641799927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31223809719085693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36763879656791687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40382468700408936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4726817011833191
14 3.9119949909 	 0.4726816951 	 0.4726816951
epoch_time;  30.825512170791626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4152972996234894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4714590013027191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45713236927986145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5198025703430176
15 3.9070057285 	 0.5198025678 	 0.5198025678
epoch_time;  30.213926315307617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3571111261844635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4342319667339325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4252210557460785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5155741572380066
16 3.9036142069 	 0.5155741511 	 0.5155741511
epoch_time;  30.267846822738647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3117847144603729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35403159260749817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3849605321884155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4401314854621887
17 3.8958155917 	 0.4401314813 	 0.4401314813
epoch_time;  30.420196771621704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3409503996372223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3958752751350403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3977974057197571
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4713817834854126
18 3.8872400902 	 0.47138177 	 0.47138177
epoch_time;  30.295374155044556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2951590120792389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3311770260334015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39978477358818054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4494416117668152
19 3.8850102615 	 0.4494416108 	 0.4494416108
epoch_time;  30.264031648635864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29511767625808716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3312312662601471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39978206157684326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44947749376296997
It took 678.5098993778229 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▃▄▄▄▄▄▃▃▃▂▂▃▁▁▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▃▄▄▄▄▄▄▃▃▂▂▃▁▁▃▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▄▃▄▃▄▃▃▂▃▃▂▁▁▁▃▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▃▅▃▄▄▅▃▄▃▃▂▂▂▄▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47508
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.40297
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40439
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.34327
wandb:                         Train loss 3.88492
wandb: 
wandb: 🚀 View run auspicious-rooster-1381 at: https://wandb.ai/nreints/thesis/runs/zj6lbjgm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_181750-zj6lbjgm/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47947853803634644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6257832646369934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5885416865348816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7650347948074341
0 8.2404320677 	 0.7650347735 	 0.7650347735
epoch_time;  30.05901837348938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4038919508457184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5477514863014221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5029715299606323
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6682907938957214
1 4.4122852488 	 0.6682907929 	 0.6682907929
epoch_time;  29.896599769592285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38000062108039856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4581994414329529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45876604318618774
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5520309209823608
2 4.2726387949 	 0.5520309448 	 0.5520309448
epoch_time;  30.067362308502197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38623329997062683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4643564224243164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46775901317596436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5608596801757812
3 4.1928234888 	 0.5608596802 	 0.5608596802
epoch_time;  30.1950581073761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3665168881416321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48291105031967163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45013806223869324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5776596665382385
4 4.1388804772 	 0.5776596894 	 0.5776596894
epoch_time;  30.134224891662598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3986545503139496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48990362882614136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4749692380428314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5746662020683289
5 4.0981375889 	 0.5746662037 	 0.5746662037
epoch_time;  30.217447996139526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3685038387775421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49712371826171875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4432510733604431
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5783207416534424
6 4.0681431692 	 0.5783207661 	 0.5783207661
epoch_time;  30.201823949813843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39183247089385986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48698148131370544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4766866862773895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5836778283119202
7 4.0330695667 	 0.5836778383 	 0.5836778383
epoch_time;  30.06060218811035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3892476558685303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4998047351837158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4558267891407013
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5743130445480347
8 4.0108661587 	 0.5743130246 	 0.5743130246
epoch_time;  30.02381467819214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40423113107681274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4858570098876953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4599398672580719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5522083044052124
9 3.9910291131 	 0.5522082767 	 0.5522082767
epoch_time;  30.256979942321777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3549337089061737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44012948870658875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4303381145000458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5317078828811646
10 3.9720826766 	 0.5317078874 	 0.5317078874
epoch_time;  30.139698266983032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37589529156684875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4476187527179718
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4453025162220001
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5299087166786194
11 3.9594187701 	 0.5299087112 	 0.5299087112
epoch_time;  31.783623218536377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3629554212093353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.419254332780838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44466260075569153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5185540914535522
12 3.9462179857 	 0.5185541101 	 0.5185541101
epoch_time;  30.304124116897583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3622722029685974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4161730706691742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4254513680934906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49174997210502625
13 3.9361809464 	 0.4917499852 	 0.4917499852
epoch_time;  30.996161699295044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3369258642196655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44473180174827576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4103262722492218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5280277729034424
14 3.9269786465 	 0.5280277561 	 0.5280277561
epoch_time;  30.357321977615356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33358895778656006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38751769065856934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39729952812194824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4633023142814636
15 3.9169792945 	 0.4633023236 	 0.4633023236
epoch_time;  30.230794191360474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3407925069332123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39234763383865356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4039807617664337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47504723072052
16 3.9067399342 	 0.4750472198 	 0.4750472198
epoch_time;  29.982539415359497
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.383402556180954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45990967750549316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4417666494846344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5315486788749695
17 3.9028062452 	 0.5315486599 	 0.5315486599
epoch_time;  30.00167942047119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3173384666442871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38521865010261536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4082880914211273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4900308847427368
18 3.8920151304 	 0.490030897 	 0.490030897
epoch_time;  30.045616149902344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3431641161441803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40289995074272156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40451285243034363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47496020793914795
19 3.8849180269 	 0.4749602034 	 0.4749602034
epoch_time;  30.089441299438477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3432738482952118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4029666781425476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40439292788505554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47508367896080017
It took 661.555342912674 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139615
Array Job ID: 2137927_22
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 04:03:08
CPU Efficiency: 11.90% of 1-10:02:42 core-walltime
Job Wall-clock time: 01:53:29
Memory Utilized: 8.54 GB
Memory Efficiency: 27.33% of 31.25 GB
