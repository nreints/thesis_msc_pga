wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_124733-ngkwmnsl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-darkness-480
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/ngkwmnsl
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.00065
wandb:                                             Train loss 0.00086
wandb: 
wandb: ðŸš€ View run scarlet-darkness-480 at: https://wandb.ai/nreints/test/runs/ngkwmnsl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_124733-ngkwmnsl/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_125546-hon0tdgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-shape-495
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/hon0tdgo
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–‚â–‚â–‚â–â–â–â–â–„â–â–
wandb:                                             Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.00084
wandb:                                             Train loss 0.00104
wandb: 
wandb: ðŸš€ View run classic-shape-495 at: https://wandb.ai/nreints/test/runs/hon0tdgo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_125546-hon0tdgo/logs
Running for data type: quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 0.7454955403 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.015001997351646423 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 46.38529014587402
Epoch 1
	 Logging train Loss: 0.0056674802 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0024061345029622316 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 44.66267442703247
Epoch 2
	 Logging train Loss: 0.0024645364 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0023393062874674797 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.328065156936646
Epoch 3
	 Logging train Loss: 0.00219825 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0014819796197116375 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.45318150520325
Epoch 4
	 Logging train Loss: 0.0018768037 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.001223818864673376 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.6072998046875
Epoch 5
	 Logging train Loss: 0.0016245414 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0010249539045616984 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.4337432384491
Epoch 6
	 Logging train Loss: 0.0013004583 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0012436351971700788 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.8723304271698
Epoch 7
	 Logging train Loss: 0.0011586735 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0007986612617969513 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.38838791847229
Epoch 8
	 Logging train Loss: 0.0010123213 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0006429768400266767 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.93543267250061
Epoch 9
	 Logging train Loss: 0.0008600696 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0006547340890392661 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.54223823547363
	 Logging test loss: 0.0006545499200001359 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  494.17633509635925  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 0.1714789438 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.010287364944815636 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 42.28105688095093
Epoch 1
	 Logging train Loss: 0.0042587078 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.002658337587490678 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.99898982048035
Epoch 2
	 Logging train Loss: 0.0027364621 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.002043596701696515 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.809295654296875
Epoch 3
	 Logging train Loss: 0.0025900173 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.002117095747962594 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.66257619857788
Epoch 4
	 Logging train Loss: 0.0020686722 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.001407728181220591 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 42.070780992507935
Epoch 5
	 Logging train Loss: 0.0018429893 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0013557677157223225 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.85114598274231
Epoch 6
	 Logging train Loss: 0.0015851162 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0014606441836804152 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.66602349281311
Epoch 7
	 Logging train Loss: 0.001447535 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0008345955866388977 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.74596905708313
Epoch 8
	 Logging train Loss: 0.0010766881 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.004975802265107632 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 41.88374924659729
Epoch 9
	 Logging train Loss: 0.0010356503 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.0008415661286562681 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 45.138854026794434
	 Logging test loss: 0.000840551860164851 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  484.5259277820587  seconds.

JOB STATISTICS
==============
Job ID: 2514682
Array Job ID: 2514679_3
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:20:48
CPU Efficiency: 67.00% of 04:59:42 core-walltime
Job Wall-clock time: 00:16:39
Memory Utilized: 24.93 GB
Memory Efficiency: 79.78% of 31.25 GB
