wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_183209-nmpgnxri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-noodles-1389
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nmpgnxri
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▆▃▄▃▁▄▃▂▃▅▄▄▂▁▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆▆█▅▂▃▅▂▄▂▂▄▆▅▃▃▁▄▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▅▆▄▅▃▂▃▃▂▃▄▄▄▂▁▃▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅█▅▄▄▄▂▃▂▂▃▄▃▂▂▁▃▃▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.46937
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24747
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33855
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12361
wandb:                         Train loss 3.68058
wandb: 
wandb: 🚀 View run sweet-noodles-1389 at: https://wandb.ai/nreints/thesis/runs/nmpgnxri
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_183209-nmpgnxri/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_184427-p14bmrq7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-monkey-1392
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/p14bmrq7
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2684885263442993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3520701229572296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5088561177253723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6917123198509216
0 8.2031607037 	 0.6917123331 	 0.6917123331
epoch_time;  35.588735818862915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1998273730278015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34040355682373047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4143346846103668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.606403112411499
1 4.1901903404 	 0.6064030828 	 0.6064030828
epoch_time;  34.76099872589111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27434423565864563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3890314996242523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4388423264026642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6036972999572754
2 4.0684347495 	 0.6036973283 	 0.6036973283
epoch_time;  33.32560467720032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21150678396224976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3380553424358368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4646757245063782
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.62017822265625
3 3.9992965047 	 0.6201782227 	 0.6201782227
epoch_time;  33.52523851394653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17506124079227448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27189692854881287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40965819358825684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5439888834953308
4 3.9484772369 	 0.5439888619 	 0.5439888619
epoch_time;  33.458532094955444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19250376522541046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28026068210601807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42011263966560364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5523241758346558
5 3.9119897015 	 0.5523242023 	 0.5523242023
epoch_time;  33.40581393241882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18223345279693604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3244175314903259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36670300364494324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5360302329063416
6 3.8734905266 	 0.5360302487 	 0.5360302487
epoch_time;  33.260398387908936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1441977173089981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2636989653110504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3495965600013733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4824603199958801
7 3.8547130215 	 0.4824603107 	 0.4824603107
epoch_time;  33.17345380783081
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15151888132095337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3105359375476837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38255634903907776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5684558749198914
8 3.827843668 	 0.5684558765 	 0.5684558765
epoch_time;  33.2320990562439
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1382157951593399
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27228957414627075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36983704566955566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5307105779647827
9 3.8074753644 	 0.5307105812 	 0.5307105812
epoch_time;  33.11429834365845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13876481354236603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2737572193145752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34192487597465515
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49776265025138855
10 3.7836299165 	 0.4977626491 	 0.4977626491
epoch_time;  33.26154613494873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16025272011756897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2982374429702759
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3732108771800995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5226538777351379
11 3.7675883733 	 0.5226538581 	 0.5226538581
epoch_time;  33.0633819103241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.178620845079422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3557252585887909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4051239490509033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5837100148200989
12 3.7570197455 	 0.5837100055 	 0.5837100055
epoch_time;  33.59516787528992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16125620901584625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33458974957466125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3900896906852722
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5645858645439148
13 3.7456385277 	 0.5645858765 	 0.5645858765
epoch_time;  33.301082372665405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14360971748828888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28230851888656616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3947935402393341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5496019721031189
14 3.7294422684 	 0.5496019518 	 0.5496019518
epoch_time;  33.00673961639404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14596723020076752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28974393010139465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34514954686164856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4918873906135559
15 3.7116211599 	 0.4918873967 	 0.4918873967
epoch_time;  32.92768311500549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11282965540885925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25506165623664856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32398533821105957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4772050976753235
16 3.7092343009 	 0.4772051012 	 0.4772051012
epoch_time;  33.45984888076782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15408335626125336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30444541573524475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3716067373752594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5217825770378113
17 3.6992656938 	 0.5217825812 	 0.5217825812
epoch_time;  33.223491191864014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15424847602844238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3116690218448639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3463329076766968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.512890100479126
18 3.69021249 	 0.5128901301 	 0.5128901301
epoch_time;  33.31753945350647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12359944730997086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24760092794895172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33859583735466003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4694238305091858
19 3.6805751338 	 0.4694238199 	 0.4694238199
epoch_time;  33.378734827041626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12360750883817673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24746951460838318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33855077624320984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4693748354911804
It took 737.8123598098755 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▂▄▂▃▃▃▂▃▂▂▂▄▂▂▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▂▄▂▃▃▂▃▃▄▃▂▄▂▂▁▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▂▄▂▂▃▄▂▃▁▁▂▂▁▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▂▂▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.50197
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26409
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33855
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13772
wandb:                         Train loss 3.65669
wandb: 
wandb: 🚀 View run red-monkey-1392 at: https://wandb.ai/nreints/thesis/runs/p14bmrq7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_184427-p14bmrq7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_185634-lh6evm0j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-rat-1395
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/lh6evm0j
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3747609853744507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45580852031707764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5514311194419861
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7805211544036865
0 8.1850609079 	 0.7805211248 	 0.7805211248
epoch_time;  33.13288378715515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22175228595733643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.330240398645401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4185750484466553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6262169480323792
1 4.1773528743 	 0.6262169503 	 0.6262169503
epoch_time;  33.34260654449463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16304557025432587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3078761398792267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39869871735572815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6168115735054016
2 4.0589045045 	 0.6168115564 	 0.6168115564
epoch_time;  33.21969246864319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14638295769691467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2519749402999878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32887592911720276
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49267902970314026
3 3.98816356 	 0.4926790392 	 0.4926790392
epoch_time;  32.70428729057312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17728202044963837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32054218649864197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39858317375183105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5997574925422668
4 3.9378689383 	 0.599757509 	 0.599757509
epoch_time;  32.98756957054138
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1482173651456833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2526753544807434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3356796205043793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4981676936149597
5 3.8892295801 	 0.4981677081 	 0.4981677081
epoch_time;  35.36444163322449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14678986370563507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2731090188026428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35798904299736023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5434432029724121
6 3.8563679545 	 0.5434432159 	 0.5434432159
epoch_time;  33.267521142959595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15335749089717865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27000612020492554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3711184561252594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5265737175941467
7 3.8188499704 	 0.5265737173 	 0.5265737173
epoch_time;  33.91625165939331
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16753995418548584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24492214620113373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4009532332420349
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.524651288986206
8 3.7997058411 	 0.524651316 	 0.524651316
epoch_time;  32.83565402030945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1631474643945694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2838132679462433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35537195205688477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5068625807762146
9 3.7690446217 	 0.5068625785 	 0.5068625785
epoch_time;  32.91048812866211
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13470469415187836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2607242166996002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36833372712135315
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5586405396461487
10 3.7553585313 	 0.5586405161 	 0.5586405161
epoch_time;  32.79478192329407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14518572390079498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3016810119152069
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3222983181476593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5143000483512878
11 3.7408952242 	 0.5143000422 	 0.5143000422
epoch_time;  32.80367565155029
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13125655055046082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26408007740974426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32279491424560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48798513412475586
12 3.7259016508 	 0.487985147 	 0.487985147
epoch_time;  32.86604070663452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13197243213653564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2530357241630554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34457334876060486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5045344829559326
13 3.7094167866 	 0.5045344585 	 0.5045344585
epoch_time;  32.77271318435669
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14212127029895782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32162365317344666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3514748513698578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5829505324363708
14 3.6975277828 	 0.5829505302 	 0.5829505302
epoch_time;  33.148629665374756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11287060379981995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24355126917362213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32109785079956055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4988569915294647
15 3.6860539999 	 0.498856993 	 0.498856993
epoch_time;  32.75313329696655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13297608494758606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2510626018047333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32243070006370544
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4871833920478821
16 3.6780900897 	 0.4871834007 	 0.4871834007
epoch_time;  33.145814180374146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10557445138692856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20676514506340027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3133750259876251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45008721947669983
17 3.6722726996 	 0.4500872225 	 0.4500872225
epoch_time;  33.01267910003662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10938633978366852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25547632575035095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3101040720939636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4949903190135956
18 3.6598802075 	 0.4949903334 	 0.4949903334
epoch_time;  32.73014974594116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13768960535526276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26380831003189087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33850598335266113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.501962423324585
19 3.6566900151 	 0.501962404 	 0.501962404
epoch_time;  33.34231400489807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13772371411323547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26409491896629333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33855143189430237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5019691586494446
It took 726.6699628829956 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▄▃▃▂▂▁▂▃▃▂▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▅▃▂▃▃▂▃▃▃▂▃▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▄▃▂▂▂▂▂▂▃▂▁▁▁▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.46112
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22459
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.32741
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12251
wandb:                         Train loss 3.6601
wandb: 
wandb: 🚀 View run scintillating-rat-1395 at: https://wandb.ai/nreints/thesis/runs/lh6evm0j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_185634-lh6evm0j/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_190837-x6ykz9km
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-paper-1397
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/x6ykz9km
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35277312994003296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5077890753746033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5987839102745056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8342890739440918
0 8.1958427662 	 0.8342890559 	 0.8342890559
epoch_time;  33.01153302192688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2345508635044098
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3626890778541565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47784143686294556
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6630521416664124
1 4.1689482878 	 0.6630521207 	 0.6630521207
epoch_time;  32.544699907302856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19704324007034302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31547367572784424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4379958212375641
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6054105758666992
2 4.04568686 	 0.6054105604 	 0.6054105604
epoch_time;  33.13771915435791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17884494364261627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2782207727432251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38603949546813965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5396901965141296
3 3.9783863644 	 0.5396901724 	 0.5396901724
epoch_time;  32.75564622879028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18737679719924927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.367025762796402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42720749974250793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6374519467353821
4 3.9213936611 	 0.6374519554 	 0.6374519554
epoch_time;  33.1217999458313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15063966810703278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.289889395236969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3917422294616699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5630842447280884
5 3.8823516577 	 0.5630842467 	 0.5630842467
epoch_time;  33.29491448402405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13085618615150452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2673380970954895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36120134592056274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5365034937858582
6 3.848145523 	 0.5365034774 	 0.5365034774
epoch_time;  32.83379030227661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17485205829143524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31027740240097046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3567845821380615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.514712929725647
7 3.8182849923 	 0.5147129368 	 0.5147129368
epoch_time;  33.03011178970337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13305054605007172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2755810618400574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.346933513879776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5121777057647705
8 3.7942798695 	 0.5121777096 	 0.5121777096
epoch_time;  33.193079710006714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13104179501533508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23820677399635315
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3421611785888672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4783969819545746
9 3.7756375756 	 0.4783969776 	 0.4783969776
epoch_time;  32.684242963790894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13180379569530487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2902204692363739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34828129410743713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5266939997673035
10 3.7571519814 	 0.526693973 	 0.526693973
epoch_time;  33.129961252212524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13406512141227722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3000105917453766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3549189865589142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.541587769985199
11 3.741788478 	 0.5415877883 	 0.5415877883
epoch_time;  34.56125044822693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15244783461093903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29935163259506226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39234352111816406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5697702169418335
12 3.7245640842 	 0.5697702356 	 0.5697702356
epoch_time;  34.253865480422974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13093213737010956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24900661408901215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34098541736602783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48544442653656006
13 3.7145710134 	 0.4854444349 	 0.4854444349
epoch_time;  33.04752445220947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13984498381614685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27961215376853943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33160778880119324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4897485673427582
14 3.7036010494 	 0.4897485681 	 0.4897485681
epoch_time;  32.7871458530426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11664740741252899
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2164054811000824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33629298210144043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45358243584632874
15 3.6917500635 	 0.4535824338 	 0.4535824338
epoch_time;  33.050525426864624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10715339332818985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21126164495944977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.336578369140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46989214420318604
16 3.6828767006 	 0.469892141 	 0.469892141
epoch_time;  32.73430395126343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12332258373498917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2472083866596222
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31692972779273987
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4705835282802582
17 3.6731547134 	 0.4705835291 	 0.4705835291
epoch_time;  32.795372009277344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1202932745218277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23179620504379272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33883172273635864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4676663279533386
18 3.667564108 	 0.4676663373 	 0.4676663373
epoch_time;  32.74928402900696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12248235940933228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2245611697435379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32756343483924866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46095943450927734
19 3.6601040168 	 0.4609594397 	 0.4609594397
epoch_time;  32.88286852836609
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12251485884189606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22459477186203003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3274148404598236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46111854910850525
It took 723.4973077774048 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▅▅▂▅▃▂▃▃▃▂▃▂▁▁▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▇▆▅▂▆▂▂▂▃▃▃▄▃▁▁▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▆▆▃▄▄▂▄▂▃▂▂▂▁▁▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▅▄▃▃▃▃▃▃▃▂▂▂▂▁▃▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49504
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25564
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.34355
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13118
wandb:                         Train loss 3.67235
wandb: 
wandb: 🚀 View run dancing-paper-1397 at: https://wandb.ai/nreints/thesis/runs/x6ykz9km
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_190837-x6ykz9km/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_192038-sjox7agv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-festival-1400
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sjox7agv
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28362128138542175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4181569814682007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5208743810653687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7703805565834045
0 8.1919523845 	 0.770380546 	 0.770380546
epoch_time;  33.131450176239014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21680819988250732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4046502709388733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44724413752555847
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6683799028396606
1 4.1818677402 	 0.6683799125 	 0.6683799125
epoch_time;  32.936585903167725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19658027589321136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3793482482433319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45205986499786377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.671306312084198
2 4.0530451295 	 0.6713063008 	 0.6713063008
epoch_time;  33.19594407081604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22072075307369232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3763497769832611
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44842323660850525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6347677707672119
3 3.9744186246 	 0.6347677695 	 0.6347677695
epoch_time;  32.64792013168335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1874905824661255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33874765038490295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46067607402801514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6408215165138245
4 3.9331170394 	 0.6408215085 	 0.6408215085
epoch_time;  32.90230178833008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15087495744228363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2569791078567505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37108612060546875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5120090246200562
5 3.8840581627 	 0.5120090382 	 0.5120090382
epoch_time;  33.175708532333374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17348897457122803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3576487898826599
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3986089825630188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6180513501167297
6 3.8551936992 	 0.6180513537 	 0.6180513537
epoch_time;  32.550490617752075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1729549914598465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2675045430660248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4039492905139923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5272002220153809
7 3.8304143349 	 0.5272002349 	 0.5272002349
epoch_time;  32.85025334358215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15366442501544952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2683849632740021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35137587785720825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4972817897796631
8 3.8059318403 	 0.4972817911 	 0.4972817911
epoch_time;  32.865060806274414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15870587527751923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2725856602191925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4033048450946808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5417624115943909
9 3.790739913 	 0.5417624396 	 0.5417624396
epoch_time;  33.063472032547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1560952514410019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2971736490726471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34999650716781616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5221690535545349
10 3.7580584589 	 0.5221690822 	 0.5221690822
epoch_time;  32.895079612731934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15121176838874817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2883587181568146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3669895529747009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5269697308540344
11 3.7528781155 	 0.5269697447 	 0.5269697447
epoch_time;  32.72710132598877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14339181780815125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2935559153556824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3316166400909424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5033600926399231
12 3.734924771 	 0.5033600678 	 0.5033600678
epoch_time;  32.93549346923828
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13440507650375366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3044620156288147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33982646465301514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5372490286827087
13 3.7246652447 	 0.5372490135 	 0.5372490135
epoch_time;  32.77572321891785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1459532380104065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2762281596660614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3305398225784302
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4973987936973572
14 3.7161994991 	 0.4973987889 	 0.4973987889
epoch_time;  32.928041219711304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12739017605781555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23413784801959991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31844499707221985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.449968546628952
15 3.70099534 	 0.4499685339 	 0.4499685339
epoch_time;  32.81560683250427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11323319375514984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23851463198661804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3073512017726898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4560471475124359
16 3.7003073825 	 0.4560471406 	 0.4560471406
epoch_time;  34.53013849258423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15233944356441498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25821200013160706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3322063684463501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4728289246559143
17 3.6853710012 	 0.4728289218 	 0.4728289218
epoch_time;  33.59883093833923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1285293996334076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23453952372074127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3340355455875397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45932626724243164
18 3.6757105038 	 0.4593262544 	 0.4593262544
epoch_time;  32.82686734199524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1311761736869812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25564226508140564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34355080127716064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49495190382003784
19 3.6723517106 	 0.4949518977 	 0.4949518977
epoch_time;  32.885863065719604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13118359446525574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2556386888027191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3435524106025696
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4950394928455353
It took 720.710770368576 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▄▅▅▅▂▄▃▃▄▂▂▂▂▃▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▇▅▆▆▆▂▄▃▄▅▃▃▃▄▃▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▅▄▇▄▂▄▃▄▄▂▃▃▃▃▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▄▄▅▄▂▂▂▃▃▁▃▂▂▁▁▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.48029
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25325
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33087
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12646
wandb:                         Train loss 3.66301
wandb: 
wandb: 🚀 View run auspicious-festival-1400 at: https://wandb.ai/nreints/thesis/runs/sjox7agv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_192038-sjox7agv/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_193235-jiixr9p8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-horse-1403
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/jiixr9p8
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28308171033859253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4031410813331604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4907890260219574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7334302067756653
0 8.1775915602 	 0.7334301923 	 0.7334301923
epoch_time;  32.97060990333557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23650743067264557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.388604998588562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45975255966186523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6682765483856201
1 4.1776841253 	 0.6682765239 	 0.6682765239
epoch_time;  33.045103549957275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2084900289773941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3719947636127472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45333132147789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6687039136886597
2 4.0642332827 	 0.668703935 	 0.668703935
epoch_time;  32.89899039268494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19747407734394073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3156590461730957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4225497543811798
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5910322666168213
3 3.9840870562 	 0.5910322859 	 0.5910322859
epoch_time;  32.672532081604004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18150360882282257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35791659355163574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39796629548072815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6213295459747314
4 3.9311556496 	 0.6213295602 	 0.6213295602
epoch_time;  32.71161937713623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21231825649738312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35131096839904785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45907020568847656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6342755556106567
5 3.8914620536 	 0.6342755292 	 0.6342755292
epoch_time;  33.51974391937256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1941039264202118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3585226237773895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40882769227027893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6182854175567627
6 3.859798155 	 0.6182853905 	 0.6182853905
epoch_time;  32.786643266677856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14953437447547913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2615129053592682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35779088735580444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5071999430656433
7 3.8223548843 	 0.5071999627 	 0.5071999627
epoch_time;  32.89345455169678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1492791324853897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3004818260669708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40856754779815674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.594352662563324
8 3.8015987988 	 0.5943526397 	 0.5943526397
epoch_time;  33.3733811378479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15325912833213806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27930933237075806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37737980484962463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5346740484237671
9 3.7773321794 	 0.5346740723 	 0.5346740723
epoch_time;  32.91642475128174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16149736940860748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29545432329177856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39527273178100586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5622307062149048
10 3.7622552974 	 0.562230703 	 0.562230703
epoch_time;  32.768107652664185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1656845510005951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3241303861141205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4008675515651703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5900082588195801
11 3.7383807576 	 0.5900082562 	 0.5900082562
epoch_time;  32.85393047332764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13375337421894073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27364903688430786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3571844696998596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5248123407363892
12 3.7284352917 	 0.5248123169 	 0.5248123169
epoch_time;  32.865642786026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16346551477909088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2767270505428314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3744949400424957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5068744421005249
13 3.7122902738 	 0.5068744556 	 0.5068744556
epoch_time;  32.76118540763855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14888785779476166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2642918527126312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37088367342948914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.508081316947937
14 3.6964817964 	 0.5080813434 	 0.5080813434
epoch_time;  32.6299467086792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14115503430366516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29167643189430237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3728194236755371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5326502323150635
15 3.6894961017 	 0.5326502207 	 0.5326502207
epoch_time;  33.03935170173645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13131920993328094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26963287591934204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.380685955286026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.534522294998169
16 3.6879496022 	 0.5345223092 	 0.5345223092
epoch_time;  32.62973690032959
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12839990854263306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22488708794116974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3603442907333374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4843055009841919
17 3.6749491553 	 0.4843055106 	 0.4843055106
epoch_time;  32.96705627441406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12489742785692215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2374534159898758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36280444264411926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5047916769981384
18 3.665572304 	 0.5047916722 	 0.5047916722
epoch_time;  33.02410674095154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12642933428287506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2532322406768799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3309933543205261
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4802609384059906
19 3.6630115548 	 0.4802609418 	 0.4802609418
epoch_time;  32.573813676834106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12645862996578217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25324535369873047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3308683931827545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.480294793844223
It took 717.214638710022 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▅▂▂▄▂▂▃▃▂▃▃▃▁▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▇▃▃▅▃▃▃▃▂▃▃▃▁▃▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▄▄▁▂▃▁▂▂▂▂▂▂▂▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▄▄▂▂▃▂▂▂▁▁▂▂▁▁▁▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49572
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23923
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.35139
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14204
wandb:                         Train loss 3.65515
wandb: 
wandb: 🚀 View run golden-horse-1403 at: https://wandb.ai/nreints/thesis/runs/jiixr9p8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_193235-jiixr9p8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_194436-q6hyiw3t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-moon-1406
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/q6hyiw3t
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3209453821182251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4480718970298767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5164195895195007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7974855303764343
0 8.1265636901 	 0.7974855165 	 0.7974855165
epoch_time;  34.85236954689026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24917460978031158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3840935230255127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42625489830970764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.642102062702179
1 4.1633887578 	 0.642102092 	 0.642102092
epoch_time;  33.68467617034912
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22543303668498993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33907991647720337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.450798362493515
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.668025016784668
2 4.0413511737 	 0.6680250013 	 0.6680250013
epoch_time;  32.80915117263794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20173358917236328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3233957588672638
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40718114376068115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6054158210754395
3 3.9712214899 	 0.6054157979 	 0.6054157979
epoch_time;  32.88265538215637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21899119019508362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4293336570262909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4162115454673767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.669579803943634
4 3.9170060723 	 0.6695797894 	 0.6695797894
epoch_time;  32.63762712478638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15716685354709625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2855217158794403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3342052102088928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5208375453948975
5 3.8759894555 	 0.5208375261 	 0.5208375261
epoch_time;  32.682770013809204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1453133523464203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27008023858070374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3409309387207031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.525330662727356
6 3.8412147062 	 0.5253306621 	 0.5253306621
epoch_time;  32.98218083381653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18726487457752228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3439781963825226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3819645345211029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5899497866630554
7 3.8107465513 	 0.589949778 	 0.589949778
epoch_time;  32.869306802749634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14014799892902374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2698509693145752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.334917277097702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5108504295349121
8 3.7867090307 	 0.5108504012 	 0.5108504012
epoch_time;  33.25718665122986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14432507753372192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2584197521209717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34796959161758423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5079195499420166
9 3.7690495805 	 0.507919559 	 0.507919559
epoch_time;  32.78824067115784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15170028805732727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2852298617362976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.364535927772522
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5367624163627625
10 3.7524767794 	 0.5367624231 	 0.5367624231
epoch_time;  32.871286153793335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12323901057243347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26307520270347595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3508872091770172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5546898245811462
11 3.7280332944 	 0.5546898094 	 0.5546898094
epoch_time;  33.17791199684143
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13147130608558655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24186745285987854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3470696806907654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49698778986930847
12 3.7176068202 	 0.4969877913 	 0.4969877913
epoch_time;  32.882364988327026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15249380469322205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2832651436328888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3653789162635803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5405060052871704
13 3.706691073 	 0.5405059814 	 0.5405059814
epoch_time;  32.86731457710266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13306599855422974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.279421865940094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34180527925491333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5298963189125061
14 3.6905186162 	 0.5298963392 	 0.5298963392
epoch_time;  32.49690079689026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11825799942016602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2642836570739746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34575796127319336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5319394469261169
15 3.6914607973 	 0.5319394498 	 0.5319394498
epoch_time;  32.98240613937378
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12227874994277954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20101866126060486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3267878592014313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45333409309387207
16 3.6801754507 	 0.4533340867 	 0.4533340867
epoch_time;  33.377082109451294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11859120428562164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25560253858566284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33618006110191345
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5143925547599792
17 3.6689082864 	 0.5143925435 	 0.5143925435
epoch_time;  33.238022327423096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1408027857542038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2573762536048889
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3281939625740051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48244979977607727
18 3.6615890265 	 0.4824497945 	 0.4824497945
epoch_time;  32.796759366989136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1419968158006668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23924818634986877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3513422906398773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49575290083885193
19 3.6551488051 	 0.4957529016 	 0.4957529016
epoch_time;  32.72107744216919
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14204254746437073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23922578990459442
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3513891398906708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49572041630744934
It took 721.0054876804352 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▄▄▇▃▃▁▂▂▂▂▂▂▁▂▃▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▄▄▇▃▃▁▂▃▂▂▂▂▁▂▃▃▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▄▄▇▂▃▁▂▂▂▂▂▂▂▂▃▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇█▃▄█▂▄▂▃▃▂▂▂▂▁▁▂▂▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49675
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29209
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.32607
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11233
wandb:                         Train loss 3.68715
wandb: 
wandb: 🚀 View run lambent-moon-1406 at: https://wandb.ai/nreints/thesis/runs/q6hyiw3t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_194436-q6hyiw3t/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_195638-jdxjzusb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-firecracker-1409
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/jdxjzusb
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2543269693851471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41197308897972107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5068367123603821
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7300848960876465
0 8.2403328908 	 0.7300848884 	 0.7300848884
epoch_time;  33.05236887931824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2657223343849182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4345405399799347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5359198451042175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7208786606788635
1 4.2063006464 	 0.7208786423 	 0.7208786423
epoch_time;  32.675798416137695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16461946070194244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3072289824485779
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40367868542671204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5674453973770142
2 4.0829750841 	 0.5674454148 	 0.5674454148
epoch_time;  32.87615346908569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18033233284950256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29581207036972046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4170254170894623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5608433485031128
3 4.0020549467 	 0.5608433491 	 0.5608433491
epoch_time;  32.62271523475647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26200902462005615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41760820150375366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5042232871055603
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6753490567207336
4 3.9525110946 	 0.6753490551 	 0.6753490551
epoch_time;  32.837690353393555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13926813006401062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2891938388347626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3481939733028412
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5202029347419739
5 3.9030436088 	 0.5202029254 	 0.5202029254
epoch_time;  35.15362763404846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16710087656974792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2952587902545929
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37526607513427734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5190636515617371
6 3.8713982233 	 0.5190636712 	 0.5190636712
epoch_time;  33.410128116607666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13693074882030487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23347215354442596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3319188356399536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.456667423248291
7 3.8434091618 	 0.456667431 	 0.456667431
epoch_time;  33.51629567146301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14468462765216827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25931477546691895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3580471873283386
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4758613109588623
8 3.8241225483 	 0.4758612968 	 0.4758612968
epoch_time;  32.964460134506226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14791172742843628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2941807210445404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3498651385307312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49922049045562744
9 3.8017095464 	 0.4992204821 	 0.4992204821
epoch_time;  32.65916109085083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1288314163684845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24781781435012817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.361398845911026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49410486221313477
10 3.7787497488 	 0.4941048699 	 0.4941048699
epoch_time;  32.86196517944336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13920624554157257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2584103047847748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3506769835948944
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4796600043773651
11 3.7614463439 	 0.4796599929 	 0.4796599929
epoch_time;  32.80630588531494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12132857739925385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23665790259838104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3640217185020447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.494052916765213
12 3.7582783011 	 0.4940529076 	 0.4940529076
epoch_time;  32.9146203994751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12331283837556839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.263592928647995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3425767123699188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4902876913547516
13 3.7339184554 	 0.4902876983 	 0.4902876983
epoch_time;  33.307920932769775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11698778718709946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23071454465389252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34077563881874084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4570395052433014
14 3.7306866087 	 0.457039498 	 0.457039498
epoch_time;  32.82175374031067
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10985124111175537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24540318548679352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34503573179244995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4832066595554352
15 3.7162001603 	 0.4832066716 	 0.4832066716
epoch_time;  32.96198630332947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13986410200595856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26788270473480225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36602577567100525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5069791674613953
16 3.7053475558 	 0.5069791639 	 0.5069791639
epoch_time;  32.815882205963135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13515642285346985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27953171730041504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3454723358154297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4951191246509552
17 3.6985122794 	 0.4951191258 	 0.4951191258
epoch_time;  32.80434465408325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11502330750226974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2187313437461853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31822341680526733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44225525856018066
18 3.6864434347 	 0.4422552573 	 0.4422552573
epoch_time;  32.88520121574402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11231276392936707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29209187626838684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32618802785873413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4968840181827545
19 3.6871512277 	 0.4968840316 	 0.4968840316
epoch_time;  32.94036245346069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11233369261026382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29208725690841675
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.326071172952652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.496746301651001
It took 721.6377282142639 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇█▄▄▄▂▃▁▃▂▃▁▃▂▃▂▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅█▂▄▄▂▂▂▃▂▃▁▃▂▃▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▅▅▅▃▃▂▃▂▃▁▂▂▃▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▄▅▅▃▂▂▃▁▃▁▂▂▂▁▂▁▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.47886
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25029
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33689
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14583
wandb:                         Train loss 3.65011
wandb: 
wandb: 🚀 View run brilliant-firecracker-1409 at: https://wandb.ai/nreints/thesis/runs/jdxjzusb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_195638-jdxjzusb/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_200839-x5g62jkg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-bao-1412
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/x5g62jkg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3031071722507477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39232707023620605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5011957883834839
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7233160734176636
0 8.1643316002 	 0.7233160895 	 0.7233160895
epoch_time;  32.91969156265259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2926596701145172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4945189952850342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49470698833465576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7821534872055054
1 4.1683079352 	 0.7821534853 	 0.7821534853
epoch_time;  32.671098709106445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19718603789806366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29311227798461914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43123993277549744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6090381145477295
2 4.0483494309 	 0.6090381107 	 0.6090381107
epoch_time;  32.76113438606262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23231849074363708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3644839823246002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41917502880096436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5991611480712891
3 3.9823894772 	 0.5991611378 	 0.5991611378
epoch_time;  32.819042682647705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21819132566452026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3397970199584961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41289135813713074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5890583992004395
4 3.9236856408 	 0.589058376 	 0.589058376
epoch_time;  33.03794717788696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1757626086473465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2881178855895996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3579918444156647
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5399070978164673
5 3.8696143603 	 0.5399070946 	 0.5399070946
epoch_time;  33.04752731323242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.155256986618042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2759324610233307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36975976824760437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5476969480514526
6 3.847043667 	 0.5476969539 	 0.5476969539
epoch_time;  32.74127006530762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1470489352941513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2707778215408325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32527169585227966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49487560987472534
7 3.8072541998 	 0.4948756038 	 0.4948756038
epoch_time;  32.8158905506134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1688210815191269
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2994311451911926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3751152753829956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5710015296936035
8 3.7869523448 	 0.5710015374 	 0.5710015374
epoch_time;  33.096635580062866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13918480277061462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27523279190063477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3415415287017822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.516929566860199
9 3.765930465 	 0.516929544 	 0.516929544
epoch_time;  33.06503367424011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1664758175611496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3098691403865814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37559545040130615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5696316957473755
10 3.7474048695 	 0.5696317106 	 0.5696317106
epoch_time;  33.67004084587097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13130135834217072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2541899085044861
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3078565001487732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4797820746898651
11 3.7346341825 	 0.4797820633 	 0.4797820633
epoch_time;  33.93441438674927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15151983499526978
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3309306502342224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3490529954433441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5730065107345581
12 3.7182319657 	 0.5730065011 	 0.5730065011
epoch_time;  33.86719560623169
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14547470211982727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27553698420524597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33829453587532043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5119233131408691
13 3.7057862485 	 0.5119233415 	 0.5119233415
epoch_time;  32.776997804641724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15115809440612793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3007887899875641
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36922261118888855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5698667168617249
14 3.6986021999 	 0.5698666959 	 0.5698666959
epoch_time;  32.99895358085632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13468420505523682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25741633772850037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3370693325996399
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5018976330757141
15 3.6821239474 	 0.501897616 	 0.501897616
epoch_time;  33.14232516288757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1418055295944214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25263234972953796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3274042010307312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4853980839252472
16 3.6754271882 	 0.4853980812 	 0.4853980812
epoch_time;  32.9008948802948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1321309357881546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24313510954380035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3442716896533966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5045985579490662
17 3.6674487321 	 0.5045985454 	 0.5045985454
epoch_time;  33.05769395828247
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12821803987026215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25502362847328186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32763180136680603
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48980164527893066
18 3.6577472416 	 0.489801644 	 0.489801644
epoch_time;  33.159783601760864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14574430882930756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25026658177375793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.336927205324173
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47878608107566833
19 3.6501076401 	 0.4787860767 	 0.4787860767
epoch_time;  33.142427921295166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14582529664039612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2502855062484741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3368898332118988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47885677218437195
It took 721.4450252056122 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▄▃▄▃▂▂▂▂▂▂▂▂▃▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▄▄▂▂▂▂▂▂▂▂▂▁▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▅▃▄▃▁▃▂▃▂▂▃▃▂▂▃▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▃▂▃▃▁▃▂▂▂▂▂▂▂▁▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.44614
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20697
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.30387
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10896
wandb:                         Train loss 3.6567
wandb: 
wandb: 🚀 View run chromatic-bao-1412 at: https://wandb.ai/nreints/thesis/runs/x5g62jkg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_200839-x5g62jkg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_202044-dp3grwh3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-lamp-1415
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/dp3grwh3
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33672863245010376
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.527570903301239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5607509016990662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8787427544593811
0 8.1745117849 	 0.8787427748 	 0.8787427748
epoch_time;  33.383721113204956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2452644556760788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34856343269348145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4367729127407074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.626992404460907
1 4.155929989 	 0.6269924267 	 0.6269924267
epoch_time;  32.83330702781677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17749658226966858
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31526508927345276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43129655718803406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6314037442207336
2 4.0367258907 	 0.6314037426 	 0.6314037426
epoch_time;  32.82226824760437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16948242485523224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3027939200401306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43520429730415344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6245009899139404
3 3.9664738887 	 0.6245009964 	 0.6245009964
epoch_time;  33.0677125453949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15105020999908447
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2886451184749603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3593117296695709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5547342300415039
4 3.9129636192 	 0.5547342249 	 0.5547342249
epoch_time;  32.98165583610535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18444742262363434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33511215448379517
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40946894884109497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6089053750038147
5 3.8689763218 	 0.6089054005 	 0.6089054005
epoch_time;  32.88923740386963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17297297716140747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33585062623023987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3802495300769806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5837662816047668
6 3.8396321725 	 0.5837662568 	 0.5837662568
epoch_time;  32.960012912750244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11750940978527069
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2576684355735779
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.313554972410202
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49521154165267944
7 3.8100850409 	 0.4952115446 	 0.4952115446
epoch_time;  33.10244917869568
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15788985788822174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2601954936981201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3688446879386902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5207864046096802
8 3.7911111655 	 0.5207864297 	 0.5207864297
epoch_time;  33.17963433265686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13880935311317444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2541521191596985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35274797677993774
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5163251161575317
9 3.7665199068 	 0.5163250897 	 0.5163250897
epoch_time;  33.31782126426697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1513051986694336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2676026523113251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36601802706718445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5379290580749512
10 3.7504201797 	 0.5379290607 	 0.5379290607
epoch_time;  33.18487620353699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12686921656131744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2348596304655075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3435297906398773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4966376721858978
11 3.7320605403 	 0.496637664 	 0.496637664
epoch_time;  32.83199334144592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13051895797252655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24750970304012299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32467085123062134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4888545274734497
12 3.7182845295 	 0.4888545268 	 0.4888545268
epoch_time;  32.970881938934326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12913498282432556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2428831160068512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3591751158237457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5144271850585938
13 3.7134030393 	 0.5144271851 	 0.5144271851
epoch_time;  32.99431848526001
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1401110291481018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2586343586444855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3708409070968628
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5342505574226379
14 3.6984025236 	 0.534250579 	 0.534250579
epoch_time;  33.07654523849487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12535056471824646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26206719875335693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3560159206390381
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5419421195983887
15 3.690472003 	 0.5419421222 	 0.5419421222
epoch_time;  33.482635498046875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11219726502895355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2211708277463913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.345097154378891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5081697702407837
16 3.6783720829 	 0.5081697619 	 0.5081697619
epoch_time;  34.173983097076416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1342325657606125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24002692103385925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36422011256217957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5140975117683411
17 3.6711926625 	 0.5140975127 	 0.5140975127
epoch_time;  34.321009159088135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12836410105228424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23674514889717102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31526920199394226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46398240327835083
18 3.6631074258 	 0.463982412 	 0.463982412
epoch_time;  33.2004976272583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10901498794555664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20699132978916168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3039238452911377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4462823271751404
19 3.6566976187 	 0.4462823404 	 0.4462823404
epoch_time;  32.85488533973694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10896345973014832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2069682627916336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3038671612739563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44614461064338684
It took 725.1130425930023 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▃▃▄▃▂▂▂▂▂▂▂▃▂▃▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▄▅▃▂▂▁▂▂▂▁▄▂▃▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▃▃▃▅▁▂▂▂▁▂▂▂▂▁▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▃▃▄▆▂▂▂▂▁▂▂▁▃▁▃▁▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49258
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26023
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.35154
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1545
wandb:                         Train loss 3.65706
wandb: 
wandb: 🚀 View run fortuitous-lamp-1415 at: https://wandb.ai/nreints/thesis/runs/dp3grwh3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_202044-dp3grwh3/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2587960958480835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4621499180793762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5278806686401367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.825552225112915
0 8.2323233592 	 0.8255522032 	 0.8255522032
epoch_time;  33.33470892906189
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19867002964019775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33246496319770813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46080315113067627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6574133634567261
1 4.1636978594 	 0.6574133383 	 0.6574133383
epoch_time;  32.568512201309204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.189005047082901
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3025638163089752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4273577630519867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6030029654502869
2 4.0389540665 	 0.6030029709 	 0.6030029709
epoch_time;  32.97050595283508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1514115035533905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28981587290763855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36334624886512756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.544719934463501
3 3.9717596903 	 0.5447199641 	 0.5447199641
epoch_time;  32.80727934837341
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1585058718919754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30075639486312866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37551984190940857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5631431937217712
4 3.9140760542 	 0.5631431786 	 0.5631431786
epoch_time;  33.05732703208923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18083436787128448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31640058755874634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37222862243652344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5575196743011475
5 3.8735883812 	 0.557519655 	 0.557519655
epoch_time;  32.917439699172974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21023602783679962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.363323450088501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4225523769855499
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6234819293022156
6 3.8374899501 	 0.6234819567 	 0.6234819567
epoch_time;  32.86441993713379
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12973541021347046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2799810767173767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3290342688560486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5314897894859314
7 3.8149010743 	 0.5314898104 	 0.5314898104
epoch_time;  33.181233644485474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14220592379570007
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26832959055900574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3437781035900116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5125912427902222
8 3.7914668802 	 0.5125912641 	 0.5125912641
epoch_time;  33.04684233665466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12932735681533813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2456543743610382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3463994264602661
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5118102431297302
9 3.7778730244 	 0.5118102203 	 0.5118102203
epoch_time;  32.71746015548706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1277790516614914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2379605621099472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3496161699295044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.503960132598877
10 3.7541452665 	 0.5039601506 	 0.5039601506
epoch_time;  32.76650309562683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11450576037168503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23901259899139404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32885804772377014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49465933442115784
11 3.7361591936 	 0.4946593413 	 0.4946593413
epoch_time;  33.165268659591675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12679368257522583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2429962158203125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3397212624549866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5146894454956055
12 3.7218172129 	 0.5146894713 	 0.5146894713
epoch_time;  32.853057622909546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13741596043109894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2581816017627716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3526307940483093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.514157772064209
13 3.7120565467 	 0.5141577643 	 0.5141577643
epoch_time;  33.183879137039185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12016991525888443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22271066904067993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35989680886268616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5106231570243835
14 3.700329532 	 0.5106231689 	 0.5106231689
epoch_time;  33.124226093292236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14748497307300568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3143019676208496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34405794739723206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5434043407440186
15 3.6861994594 	 0.5434043678 	 0.5434043678
epoch_time;  33.081702709198
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1245492473244667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25509029626846313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3254528343677521
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49086856842041016
16 3.6760358042 	 0.4908685633 	 0.4908685633
epoch_time;  32.91601753234863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15172918140888214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3031116724014282
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3595459461212158
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5393680334091187
17 3.6696858338 	 0.5393680469 	 0.5393680469
epoch_time;  32.92365741729736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11543996632099152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22158223390579224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3171517252922058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4511067867279053
18 3.6632704067 	 0.4511067983 	 0.4511067983
epoch_time;  32.778706312179565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15445078909397125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26020345091819763
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3515213131904602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49248459935188293
19 3.6570625899 	 0.4924845927 	 0.4924845927
epoch_time;  32.56658482551575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1545039564371109
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2602309286594391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35154467821121216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4925825297832489
It took 720.4138987064362 seconds to train & eval the model.
