wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_171736-et8jcqi2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-springroll-1157
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/et8jcqi2
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ˆâ–‡â–„â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–‡â–ˆâ–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–ƒâ–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–ˆâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.44948
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.21743
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28879
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10876
wandb:                         Train loss 3.64054
wandb: 
wandb: ðŸš€ View run scintillating-springroll-1157 at: https://wandb.ai/nreints/thesis/runs/et8jcqi2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_171736-et8jcqi2/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33094435930252075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45890012383461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5608270764350891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8366151452064514
0 8.1785238236 	 0.8366151552 	 0.8366151552
epoch_time;  31.405913591384888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3200489282608032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48992887139320374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5473178625106812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8208171129226685
1 4.1545408503 	 0.8208171123 	 0.8208171123
epoch_time;  30.551411390304565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33926910161972046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5398063063621521
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5117012858390808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7883910536766052
2 4.033221638 	 0.7883910308 	 0.7883910308
epoch_time;  30.19150948524475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1752537488937378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3165799677371979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38707882165908813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5896347761154175
3 3.9688005803 	 0.5896347871 	 0.5896347871
epoch_time;  30.888877153396606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.201551154255867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2776643931865692
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3703157603740692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5277740359306335
4 3.9132168511 	 0.5277740479 	 0.5277740479
epoch_time;  30.824657201766968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16084422171115875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29531165957450867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3730257749557495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5606815814971924
5 3.8717863358 	 0.5606815647 	 0.5606815647
epoch_time;  30.406649112701416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16143709421157837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30717933177948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35739824175834656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5457054376602173
6 3.8421704416 	 0.5457054344 	 0.5457054344
epoch_time;  30.83843159675598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20739783346652985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3629530072212219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4300048351287842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6368857622146606
7 3.8095220464 	 0.6368857719 	 0.6368857719
epoch_time;  30.614171028137207
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16095995903015137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2866212725639343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4046464264392853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5872364640235901
8 3.7827574898 	 0.5872364353 	 0.5872364353
epoch_time;  30.79644536972046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16649645566940308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2704046964645386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35628026723861694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5187262892723083
9 3.7618698296 	 0.518726287 	 0.518726287
epoch_time;  31.09475564956665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15226754546165466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27028197050094604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3658849000930786
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5319332480430603
10 3.7415960747 	 0.5319332226 	 0.5319332226
epoch_time;  31.696528673171997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13406118750572205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24764493107795715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34903132915496826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5141435265541077
11 3.7271730995 	 0.5141435365 	 0.5141435365
epoch_time;  30.532235860824585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15589337050914764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2856387794017792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36013883352279663
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5365517139434814
12 3.7170153949 	 0.5365517281 	 0.5365517281
epoch_time;  30.85217046737671
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14643050730228424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28555360436439514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3599596321582794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.546882152557373
13 3.6989853535 	 0.5468821345 	 0.5468821345
epoch_time;  30.485229969024658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11692716181278229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23152896761894226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.321410208940506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48586252331733704
14 3.6836013536 	 0.4858625257 	 0.4858625257
epoch_time;  30.558385848999023
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1773218959569931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2954593002796173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36457526683807373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5292523503303528
15 3.674273099 	 0.5292523771 	 0.5292523771
epoch_time;  30.526939153671265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12049312144517899
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24247251451015472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3278663754463196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5044461488723755
16 3.6664053905 	 0.5044461225 	 0.5044461225
epoch_time;  31.082111835479736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12348107248544693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2358779013156891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32448261976242065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48670271039009094
17 3.6516352959 	 0.4867027077 	 0.4867027077
epoch_time;  30.616001844406128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15293152630329132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2923266291618347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3442704379558563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5370126962661743
18 3.6494659651 	 0.5370126673 	 0.5370126673
epoch_time;  30.839948654174805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10878801345825195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21734555065631866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2887968420982361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44953402876853943
19 3.6405397078 	 0.4495340296 	 0.4495340296
epoch_time;  30.630056381225586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1087585985660553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21742914617061615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28878921270370483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44947683811187744
It took 671.1872007846832 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn30: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135328.0
