wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121537-z1l026ke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-springroll-1180
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/z1l026ke
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–…â–„â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–‡â–†â–…â–…â–…â–„â–„â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ˆâ–…â–…â–‚â–ƒâ–‚â–ƒâ–â–‡â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.44013
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.44967
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.21065
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.23608
wandb:                         Train loss 1.93039
wandb: 
wandb: ðŸš€ View run vivid-springroll-1180 at: https://wandb.ai/nreints/thesis/runs/z1l026ke
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121537-z1l026ke/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122740-yhhzg69h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-snake-1183
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/yhhzg69h
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35662660002708435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.999275267124176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9999709129333496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8241493701934814
0 4.2400991637 	 3.8241494668 	 3.8241494668
epoch_time;  32.76605248451233
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35778290033340454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.887457013130188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8741257190704346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5185084342956543
1 2.2095334848 	 3.5185084987 	 3.5185084987
epoch_time;  31.90080428123474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30216655135154724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7096601128578186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8798460960388184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3840227127075195
2 2.1399254718 	 3.3840226457 	 3.3840226457
epoch_time;  31.495752096176147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3003230392932892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6265580654144287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7763803005218506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1765689849853516
3 2.0977641876 	 3.1765690984 	 3.1765690984
epoch_time;  31.86012864112854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25799739360809326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5898017883300781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6660854816436768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.054858446121216
4 2.063029605 	 3.0548583984 	 3.0548583984
epoch_time;  31.510139226913452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.266256719827652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5694422721862793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7173917293548584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0465309619903564
5 2.0441600306 	 3.0465308937 	 3.0465308937
epoch_time;  31.88605499267578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2529276907444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5629815459251404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.653094530105591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.00183367729187
6 2.0246532443 	 3.001833694 	 3.001833694
epoch_time;  31.4621844291687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26614925265312195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6128839254379272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5868325233459473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9775500297546387
7 2.0080356482 	 2.9775499499 	 2.9775499499
epoch_time;  31.441789388656616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24076621234416962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5047377943992615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5295794010162354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.822573661804199
8 1.9995446124 	 2.8225737701 	 2.8225737701
epoch_time;  31.705275774002075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3407835364341736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6360061764717102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7220401763916016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0174756050109863
9 1.9890487144 	 3.01747552 	 3.01747552
epoch_time;  31.590583324432373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24560706317424774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5061758756637573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4466302394866943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7260844707489014
10 1.9748100431 	 2.7260844463 	 2.7260844463
epoch_time;  31.764801025390625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24630619585514069
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48888731002807617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4856832027435303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7572154998779297
11 1.9686337977 	 2.7572155102 	 2.7572155102
epoch_time;  31.305830717086792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24783898890018463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5062185525894165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4295990467071533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7049648761749268
12 1.9619977017 	 2.7049649625 	 2.7049649625
epoch_time;  31.987187385559082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2383594959974289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46794986724853516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.345123529434204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.608539581298828
13 1.9598505205 	 2.6085394782 	 2.6085394782
epoch_time;  31.673038005828857
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2334369719028473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.457645982503891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3192145824432373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.564295530319214
14 1.9507533482 	 2.5642955883 	 2.5642955883
epoch_time;  31.832534074783325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23870453238487244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4733821153640747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2711825370788574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.522657632827759
15 1.9500640683 	 2.5226577346 	 2.5226577346
epoch_time;  31.66450595855713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24634841084480286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4599236845970154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3071956634521484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.524712324142456
16 1.9415528666 	 2.52471231 	 2.52471231
epoch_time;  31.765623092651367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2448081523180008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5124319195747375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3049728870391846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5981736183166504
17 1.9361265644 	 2.5981735642 	 2.5981735642
epoch_time;  31.599037170410156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24083678424358368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43527257442474365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2148172855377197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.41666316986084
18 1.9340841801 	 2.4166630925 	 2.4166630925
epoch_time;  31.812353134155273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23612985014915466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44954952597618103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.210150718688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.43975567817688
19 1.9303926482 	 2.4397556614 	 2.4397556614
epoch_time;  31.856614589691162
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2360755354166031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44967135787010193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2106502056121826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4401323795318604
It took 722.8386225700378 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–†â–…â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–‚â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.37147
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.46439
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.12375
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.24604
wandb:                         Train loss 1.92132
wandb: 
wandb: ðŸš€ View run sparkling-snake-1183 at: https://wandb.ai/nreints/thesis/runs/yhhzg69h
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122740-yhhzg69h/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_123933-6e9kjgwy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run virtuous-noodles-1190
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6e9kjgwy
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41593945026397705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9047709107398987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.060431718826294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.771010637283325
0 4.2564570815 	 3.7710106102 	 3.7710106102
epoch_time;  31.21708083152771
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3223790228366852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8126760721206665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8834290504455566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5878121852874756
1 2.2050877121 	 3.5878121041 	 3.5878121041
epoch_time;  31.704504013061523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2993549406528473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7673483490943909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.808901309967041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4468789100646973
2 2.126909983 	 3.446878959 	 3.446878959
epoch_time;  31.492305517196655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24952054023742676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6127060055732727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6599080562591553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1258115768432617
3 2.0822697706 	 3.1258116026 	 3.1258116026
epoch_time;  33.879077434539795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2905794084072113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6263709664344788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.721299171447754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.164693593978882
4 2.051438628 	 3.1646936365 	 3.1646936365
epoch_time;  32.478389501571655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2569071054458618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5510614514350891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.592452049255371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9840567111968994
5 2.0268460469 	 2.9840566274 	 2.9840566274
epoch_time;  32.807568073272705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.238942950963974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5101212859153748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.557490587234497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8935837745666504
6 2.0125362988 	 2.8935837204 	 2.8935837204
epoch_time;  31.443989753723145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25534313917160034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5294392704963684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4718997478485107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.827669382095337
7 1.9962385484 	 2.8276693808 	 2.8276693808
epoch_time;  31.82654094696045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24250885844230652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5332052111625671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.456387519836426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8138937950134277
8 1.9847766667 	 2.813893911 	 2.813893911
epoch_time;  31.658238649368286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2360619455575943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49047574400901794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.391019582748413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6898860931396484
9 1.9724142582 	 2.6898859797 	 2.6898859797
epoch_time;  32.03612303733826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22659963369369507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4585290551185608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.376957893371582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6672446727752686
10 1.968326845 	 2.6672447411 	 2.6672447411
epoch_time;  31.658215522766113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25117871165275574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49993303418159485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3817849159240723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.686389207839966
11 1.9565920613 	 2.6863893251 	 2.6863893251
epoch_time;  31.207523822784424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27709969878196716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.530869722366333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.296732187271118
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.595061779022217
12 1.9528841653 	 2.595061761 	 2.595061761
epoch_time;  31.61569046974182
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2448250651359558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5223164558410645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.23291277885437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5639162063598633
13 1.9496865347 	 2.5639161806 	 2.5639161806
epoch_time;  31.54335069656372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21431821584701538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4316362142562866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2239890098571777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.487901449203491
14 1.9436005403 	 2.4879015124 	 2.4879015124
epoch_time;  31.809621810913086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2680834233760834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5369870662689209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2236509323120117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.532968521118164
15 1.9306544754 	 2.532968552 	 2.532968552
epoch_time;  31.608367681503296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20578046143054962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3889050781726837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.140650749206543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3558473587036133
16 1.9324778439 	 2.3558473329 	 2.3558473329
epoch_time;  31.646398782730103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21436041593551636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42260921001434326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1272084712982178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3623125553131104
17 1.9276859436 	 2.3623126056 	 2.3623126056
epoch_time;  31.604390859603882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21550387144088745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.403609037399292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.090669870376587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.317254066467285
18 1.9310764734 	 2.3172541438 	 2.3172541438
epoch_time;  31.420879364013672
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2460852563381195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4644571542739868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.124197006225586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3720502853393555
19 1.9213250637 	 2.3720501874 	 2.3720501874
epoch_time;  31.411781549453735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24604497849941254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4643874168395996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1237542629241943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.371474266052246
It took 713.0301439762115 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–‚â–‚â–…â–‚â–â–‚â–‚â–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.42077
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39449
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.21864
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22167
wandb:                         Train loss 1.93032
wandb: 
wandb: ðŸš€ View run virtuous-noodles-1190 at: https://wandb.ai/nreints/thesis/runs/6e9kjgwy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_123933-6e9kjgwy/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125117-8xfxzl9d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-dog-1197
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/8xfxzl9d
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40405169129371643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0124152898788452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1276612281799316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8882651329040527
0 4.2296482259 	 3.8882650839 	 3.8882650839
epoch_time;  31.000730276107788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3135454058647156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.791372537612915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8322057723999023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.436443328857422
1 2.2139774392 	 3.436443267 	 3.436443267
epoch_time;  31.56914234161377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2676810324192047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6659136414527893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7688159942626953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2805771827697754
2 2.141054271 	 3.2805772936 	 3.2805772936
epoch_time;  31.346611261367798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29493436217308044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.706018328666687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.706476926803589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.21179461479187
3 2.098218914 	 3.2117946315 	 3.2117946315
epoch_time;  31.564863204956055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27626046538352966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6300342082977295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7199912071228027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1431634426116943
4 2.0658779674 	 3.1431634027 	 3.1431634027
epoch_time;  30.996689796447754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28370094299316406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5981033444404602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.677637815475464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.073594570159912
5 2.0450800623 	 3.0735945418 	 3.0735945418
epoch_time;  31.536364316940308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2742979824542999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5852746367454529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6612422466278076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.021845579147339
6 2.0233495631 	 3.0218456371 	 3.0218456371
epoch_time;  31.475062608718872
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31294864416122437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6057213544845581
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.620487689971924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.96014404296875
7 2.0103069659 	 2.960144043 	 2.960144043
epoch_time;  31.655002117156982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2582743167877197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5548033714294434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5508530139923096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.908958911895752
8 1.996595916 	 2.9089589712 	 2.9089589712
epoch_time;  31.147048950195312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24621663987636566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5191822648048401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.449976921081543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.767298936843872
9 1.9866962368 	 2.7672988479 	 2.7672988479
epoch_time;  33.583876609802246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32239454984664917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5944652557373047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.577787399291992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8917906284332275
10 1.9746987996 	 2.8917906065 	 2.8917906065
epoch_time;  32.61406207084656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25899168848991394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5379769206047058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3973922729492188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.726105213165283
11 1.9734410704 	 2.7261052312 	 2.7261052312
epoch_time;  32.05341339111328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22968092560768127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.451070100069046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3385188579559326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5954010486602783
12 1.9652665745 	 2.5954010835 	 2.5954010835
epoch_time;  31.437803745269775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23666641116142273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4482889473438263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.300560474395752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5316367149353027
13 1.9599295315 	 2.531636666 	 2.531636666
epoch_time;  31.608497381210327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24669189751148224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4813602864742279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2950448989868164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5484461784362793
14 1.9545571154 	 2.5484460779 	 2.5484460779
epoch_time;  31.54375982284546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24351972341537476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47980383038520813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2936413288116455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5643694400787354
15 1.945407214 	 2.5643694903 	 2.5643694903
epoch_time;  31.5491943359375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2345293015241623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45543769001960754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.302248954772949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5471036434173584
16 1.9419163502 	 2.5471036344 	 2.5471036344
epoch_time;  31.261521100997925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2331141084432602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4382343888282776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.246267080307007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.478179693222046
17 1.9346937878 	 2.4781796017 	 2.4781796017
epoch_time;  31.48780083656311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22225366532802582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41253402829170227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.241328239440918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.461320161819458
18 1.9304024006 	 2.4613202069 	 2.4613202069
epoch_time;  31.347942352294922
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2217114418745041
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3946708142757416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2190747261047363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.420968532562256
19 1.9303220673 	 2.4209685454 	 2.4209685454
epoch_time;  31.46611475944519
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22167056798934937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39448902010917664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.218636989593506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.420766592025757
It took 704.2045686244965 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–‡â–†â–†â–…â–…â–„â–…â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–„â–„â–„â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–„â–ƒâ–‚â–â–ƒâ–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.61092
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.47776
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.34177
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26309
wandb:                         Train loss 1.91169
wandb: 
wandb: ðŸš€ View run twinkling-dog-1197 at: https://wandb.ai/nreints/thesis/runs/8xfxzl9d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125117-8xfxzl9d/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_130300-vvb1ufp9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-lamp-1204
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/vvb1ufp9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3833496868610382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0847803354263306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.059532642364502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.99711012840271
0 4.2386849002 	 3.9971102328 	 3.9971102328
epoch_time;  31.568496465682983
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31013989448547363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8710340261459351
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.821208953857422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.619626522064209
1 2.2122509336 	 3.6196265968 	 3.6196265968
epoch_time;  31.405104875564575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28595516085624695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7024133801460266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9075138568878174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5188751220703125
2 2.1388095657 	 3.5188750396 	 3.5188750396
epoch_time;  31.68286681175232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.287983238697052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6904494762420654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8780019283294678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4679229259490967
3 2.090406414 	 3.4679228912 	 3.4679228912
epoch_time;  31.29879856109619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2930353581905365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6718793511390686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.800823211669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.331918478012085
4 2.0641010468 	 3.3319184174 	 3.3319184174
epoch_time;  31.3583664894104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3038259446620941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6107468008995056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.760349750518799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.190676689147949
5 2.0378912201 	 3.1906767974 	 3.1906767974
epoch_time;  31.268856525421143
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23249343037605286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5335106253623962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6772024631500244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.097980260848999
6 2.0222928325 	 3.0979802312 	 3.0979802312
epoch_time;  31.44085383415222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25033843517303467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5303637385368347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.649845600128174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0399091243743896
7 2.0088576599 	 3.0399090741 	 3.0399090741
epoch_time;  31.11111807823181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25921645760536194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5642333030700684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7013654708862305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.106329917907715
8 1.9995305624 	 3.1063298406 	 3.1063298406
epoch_time;  31.39097285270691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2441295087337494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5082871913909912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6281256675720215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9806597232818604
9 1.990449589 	 2.9806597735 	 2.9806597735
epoch_time;  31.365705251693726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28154176473617554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5614733099937439
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6660351753234863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0438060760498047
10 1.9763788573 	 3.0438060864 	 3.0438060864
epoch_time;  31.789300441741943
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2512609660625458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5566442012786865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6149849891662598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.015824317932129
11 1.968863227 	 3.0158242715 	 3.0158242715
epoch_time;  31.443068265914917
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23129183053970337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4909435510635376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.500828742980957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8289132118225098
12 1.9642326546 	 2.8289131783 	 2.8289131783
epoch_time;  31.270610094070435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25284504890441895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5214560627937317
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4906842708587646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8443493843078613
13 1.9549212601 	 2.8443494642 	 2.8443494642
epoch_time;  31.234779596328735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29866522550582886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5524851083755493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5823240280151367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9061410427093506
14 1.9506695437 	 2.9061411265 	 2.9061411265
epoch_time;  31.489957809448242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27130645513534546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5097010731697083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4445226192474365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.757248640060425
15 1.9372234616 	 2.7572486671 	 2.7572486671
epoch_time;  32.97493600845337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2603507339954376
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4686714708805084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4056308269500732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6791749000549316
16 1.930954651 	 2.6791748047 	 2.6791748047
epoch_time;  32.82341146469116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23259636759757996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44585344195365906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.275683641433716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.542583465576172
17 1.9258721622 	 2.5425835687 	 2.5425835687
epoch_time;  32.87285375595093
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2834916412830353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5180438756942749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3775289058685303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.681737184524536
18 1.9196089717 	 2.6817371265 	 2.6817371265
epoch_time;  31.691927909851074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26314789056777954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4775927662849426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3416190147399902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6114964485168457
19 1.9116857241 	 2.6114963841 	 2.6114963841
epoch_time;  31.66261577606201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.263085275888443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4777618944644928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3417656421661377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6109182834625244
It took 703.3773248195648 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–†â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–…â–„â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–„â–„â–ƒâ–‚â–ƒâ–‚â–â–â–ƒâ–‚â–â–‚â–„â–ˆâ–â–‚â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.38567
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.41079
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.22421
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22785
wandb:                         Train loss 1.92358
wandb: 
wandb: ðŸš€ View run red-lamp-1204 at: https://wandb.ai/nreints/thesis/runs/vvb1ufp9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_130300-vvb1ufp9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131438-8ggegrcq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-mandu-1211
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/8ggegrcq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3766665458679199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9496605396270752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.107558488845825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.750269651412964
0 4.2109348553 	 3.7502695444 	 3.7502695444
epoch_time;  31.2457914352417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32346487045288086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.801767885684967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9483118057250977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.484398126602173
1 2.2094719951 	 3.4843980944 	 3.4843980944
epoch_time;  31.425014972686768
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2956179678440094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7365105748176575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8742454051971436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.34023380279541
2 2.1354279618 	 3.3402337152 	 3.3402337152
epoch_time;  30.94962477684021
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2872580587863922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6594151854515076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.753702163696289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.143702268600464
3 2.0925732653 	 3.1437021616 	 3.1437021616
epoch_time;  31.965000867843628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2608977258205414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5864433646202087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7306385040283203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0481817722320557
4 2.0587560696 	 3.0481818122 	 3.0481818122
epoch_time;  31.4391348361969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23269352316856384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4919607937335968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6096439361572266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8697988986968994
5 2.036441914 	 2.8697988149 	 2.8697988149
epoch_time;  31.348904848098755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2712996304035187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5975901484489441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.627439498901367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9557390213012695
6 2.0173994412 	 2.9557389543 	 2.9557389543
epoch_time;  31.389936923980713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23260650038719177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49443742632865906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5060160160064697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.760748863220215
7 1.9976266953 	 2.7607489509 	 2.7607489509
epoch_time;  31.547309398651123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22594967484474182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47275030612945557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4677634239196777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7105371952056885
8 1.9849108862 	 2.7105373073 	 2.7105373073
epoch_time;  31.107706785202026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22227521240711212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4346022307872772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.419189453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6000261306762695
9 1.9810970361 	 2.6000262286 	 2.6000262286
epoch_time;  31.73751926422119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2679465115070343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4815526306629181
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4478914737701416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6465442180633545
10 1.9675583888 	 2.6465442554 	 2.6465442554
epoch_time;  31.649296522140503
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.241570845246315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49054664373397827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4095466136932373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6449851989746094
11 1.9575904428 	 2.6449852196 	 2.6449852196
epoch_time;  31.278443574905396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22272121906280518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44660329818725586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3701839447021484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5722897052764893
12 1.9549797745 	 2.5722897091 	 2.5722897091
epoch_time;  31.471394062042236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23031830787658691
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4468851089477539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3170952796936035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.510925769805908
13 1.9475464611 	 2.5109257878 	 2.5109257878
epoch_time;  31.212915658950806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27855169773101807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5092290639877319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.466273069381714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6778061389923096
14 1.9428451424 	 2.6778061325 	 2.6778061325
epoch_time;  31.600451231002808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36783042550086975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5987848043441772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.486119031906128
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.687713861465454
15 1.9428362165 	 2.687713788 	 2.687713788
epoch_time;  31.159741401672363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2185015082359314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40707507729530334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2265169620513916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.394850730895996
16 1.9373370192 	 2.3948506123 	 2.3948506123
epoch_time;  31.211087942123413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23071569204330444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44568154215812683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.208786964416504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.396043300628662
17 1.9318695585 	 2.3960432723 	 2.3960432723
epoch_time;  31.287174463272095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2350306510925293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41407981514930725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2020249366760254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3615169525146484
18 1.9260878721 	 2.3615168391 	 2.3615168391
epoch_time;  31.503443479537964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22780413925647736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4107166528701782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2239394187927246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3858861923217773
19 1.9235768767 	 2.3858861975 	 2.3858861975
epoch_time;  31.15230369567871
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2278463989496231
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41078776121139526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.224210500717163
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.385671377182007
It took 697.7453405857086 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–…â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–†â–‡â–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–„â–‡â–…â–„â–ƒâ–„â–‚â–ƒâ–†â–…â–ƒâ–ƒâ–â–‚â–â–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.32625
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.45812
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.06886
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2299
wandb:                         Train loss 1.92913
wandb: 
wandb: ðŸš€ View run fortuitous-mandu-1211 at: https://wandb.ai/nreints/thesis/runs/8ggegrcq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131438-8ggegrcq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132628-20gfj69v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-orchid-1218
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/20gfj69v
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36189937591552734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.937712550163269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.044938325881958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.784254550933838
0 4.2780131945 	 3.7842545793 	 3.7842545793
epoch_time;  33.37018299102783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32276415824890137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.794683575630188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8452348709106445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4314675331115723
1 2.2103902085 	 3.4314674171 	 3.4314674171
epoch_time;  33.03026723861694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27990907430648804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7099085450172424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.786879062652588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.325474500656128
2 2.1368433824 	 3.3254744246 	 3.3254744246
epoch_time;  31.91430974006653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3466350734233856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.734231173992157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8798511028289795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3605639934539795
3 2.0917334017 	 3.3605640308 	 3.3605640308
epoch_time;  31.570926666259766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3017520606517792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6748904585838318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.632533311843872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0622479915618896
4 2.0607663867 	 3.0622479413 	 3.0622479413
epoch_time;  31.40620255470276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28162989020347595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5605742335319519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6282622814178467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9436283111572266
5 2.0361271925 	 2.9436282596 	 2.9436282596
epoch_time;  31.51558208465576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24809394776821136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.564978301525116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5110647678375244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.86802077293396
6 2.0175840756 	 2.8680208773 	 2.8680208773
epoch_time;  31.605342388153076
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2643723487854004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.577355682849884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.420884847640991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7664830684661865
7 2.0067886631 	 2.7664829563 	 2.7664829563
epoch_time;  32.03527069091797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2333577275276184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48353320360183716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3558738231658936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6519041061401367
8 1.9911315959 	 2.6519041319 	 2.6519041319
epoch_time;  31.69800329208374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2465924471616745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5240212678909302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4242680072784424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.75360369682312
9 1.9812820011 	 2.7536037136 	 2.7536037136
epoch_time;  31.986247062683105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31386885046958923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5863122344017029
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3955156803131104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7092225551605225
10 1.9745369758 	 2.7092225771 	 2.7092225771
epoch_time;  31.345922708511353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28559407591819763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5331926345825195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3008065223693848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5872063636779785
11 1.9618626558 	 2.5872063714 	 2.5872063714
epoch_time;  32.0205340385437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25131329894065857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5401394367218018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.277946710586548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6082327365875244
12 1.9616654589 	 2.6082326528 	 2.6082326528
epoch_time;  31.69780397415161
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2591651976108551
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5295711755752563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2174525260925293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5021088123321533
13 1.9541294973 	 2.5021088471 	 2.5021088471
epoch_time;  31.847689628601074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.215714693069458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.441419780254364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.162936210632324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4114816188812256
14 1.9463601393 	 2.4114815377 	 2.4114815377
epoch_time;  31.70082712173462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22407546639442444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47015273571014404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.168480396270752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.452584743499756
15 1.9418546952 	 2.4525847564 	 2.4525847564
epoch_time;  31.884360313415527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20770145952701569
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41380560398101807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.118095874786377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3555710315704346
16 1.9377800096 	 2.3555710251 	 2.3555710251
epoch_time;  31.66179633140564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23275364935398102
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4753977954387665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1192047595977783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.385585308074951
17 1.9338377253 	 2.3855853107 	 2.3855853107
epoch_time;  31.57874846458435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21982601284980774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4574277400970459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.074000597000122
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3436286449432373
18 1.9348853647 	 2.3436287545 	 2.3436287545
epoch_time;  31.683119535446167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22990567982196808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45821651816368103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.069362163543701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3264033794403076
19 1.9291304561 	 2.3264033137 	 2.3264033137
epoch_time;  32.003690242767334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22990202903747559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45811906456947327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.068863868713379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.326249837875366
It took 709.421550989151 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–…â–„â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–…â–…â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–…â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–„â–„â–‚â–„â–‚â–‚â–„â–‚â–‚â–â–„â–‚â–‚â–‚â–â–ƒâ–â–…â–…
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.49213
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.51931
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.27404
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30659
wandb:                         Train loss 1.9245
wandb: 
wandb: ðŸš€ View run radiant-orchid-1218 at: https://wandb.ai/nreints/thesis/runs/20gfj69v
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132628-20gfj69v/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_133811-vvsw3hw0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-pig-1224
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/vvsw3hw0
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36185702681541443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9739148020744324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1063568592071533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.852234125137329
0 4.2338305185 	 3.8522342166 	 3.8522342166
epoch_time;  31.866490840911865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32471132278442383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8311426639556885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8903067111968994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.458693742752075
1 2.2119537333 	 3.4586937157 	 3.4586937157
epoch_time;  31.364163637161255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26668310165405273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6903284192085266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.846020221710205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3451027870178223
2 2.137331333 	 3.345102671 	 3.345102671
epoch_time;  31.369264841079712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.269267201423645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6921566724777222
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7393741607666016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2255918979644775
3 2.0902315319 	 3.2255918761 	 3.2255918761
epoch_time;  31.627004861831665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23655158281326294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5671616196632385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6257851123809814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9943673610687256
4 2.0578600058 	 2.9943672799 	 2.9943672799
epoch_time;  31.555081129074097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2705777883529663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6194972395896912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.704110622406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0824925899505615
5 2.0348126018 	 3.0824924778 	 3.0824924778
epoch_time;  32.98379039764404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24061048030853271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5372468829154968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5138895511627197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8528289794921875
6 2.0161665062 	 2.852829062 	 2.852829062
epoch_time;  32.586060523986816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22941279411315918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.487393856048584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4539504051208496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7483365535736084
7 2.0024521504 	 2.7483365446 	 2.7483365446
epoch_time;  32.64105582237244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2826346755027771
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5433982610702515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.406418561935425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7144579887390137
8 1.9911291165 	 2.7144579088 	 2.7144579088
epoch_time;  31.477430820465088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2211163491010666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43530136346817017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.316525459289551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.565638780593872
9 1.9774879864 	 2.5656388566 	 2.5656388566
epoch_time;  31.32749032974243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22682376205921173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4690089821815491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2722835540771484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.528109550476074
10 1.9661151987 	 2.5281094938 	 2.5281094938
epoch_time;  31.434170484542847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22032853960990906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4549223482608795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.233272075653076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4735941886901855
11 1.9580990553 	 2.4735942119 	 2.4735942119
epoch_time;  31.58595061302185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2787889838218689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5088294148445129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3628523349761963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.596442222595215
12 1.9507867378 	 2.5964423102 	 2.5964423102
epoch_time;  31.066808223724365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23029854893684387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44216448068618774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.278179168701172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5016582012176514
13 1.9486456724 	 2.5016581767 	 2.5016581767
epoch_time;  31.399410724639893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22828809916973114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4404789209365845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.151474952697754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3771145343780518
14 1.9384896208 	 2.3771144558 	 2.3771144558
epoch_time;  31.168253183364868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22277112305164337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45270463824272156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2090327739715576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4627814292907715
15 1.9359814354 	 2.4627814216 	 2.4627814216
epoch_time;  31.877874612808228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.210374653339386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42659857869148254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.15655779838562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.390192985534668
16 1.9348931335 	 2.3901929701 	 2.3901929701
epoch_time;  31.38303017616272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24445569515228271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5040210485458374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1683151721954346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4346373081207275
17 1.9289547475 	 2.4346372862 	 2.4346372862
epoch_time;  31.206547260284424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21006442606449127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3904491066932678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1160991191864014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3026022911071777
18 1.9249994049 	 2.3026024071 	 2.3026024071
epoch_time;  31.329303741455078
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3065146803855896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5192030668258667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2745635509490967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4915430545806885
19 1.9245000489 	 2.4915430017 	 2.4915430017
epoch_time;  31.668731927871704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30659401416778564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5193069577217102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.274038553237915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4921345710754395
It took 703.5036022663116 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–…â–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–‡â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–†â–†â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.39494
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.451
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.18816
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2327
wandb:                         Train loss 1.92281
wandb: 
wandb: ðŸš€ View run bright-pig-1224 at: https://wandb.ai/nreints/thesis/runs/vvsw3hw0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_133811-vvsw3hw0/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134955-nkcozmd4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-festival-1230
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/nkcozmd4
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41013145446777344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.015856146812439
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1684675216674805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.904021739959717
0 4.2700826739 	 3.9040217219 	 3.9040217219
epoch_time;  31.807044744491577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27420297265052795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7207775115966797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8614578247070312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4030985832214355
1 2.2220309392 	 3.4030986064 	 3.4030986064
epoch_time;  31.703075885772705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35041049122810364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7926926016807556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.008967161178589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5300772190093994
2 2.1430320249 	 3.5300771352 	 3.5300771352
epoch_time;  31.569100379943848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35594305396080017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7203981280326843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9249627590179443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3531394004821777
3 2.0946599486 	 3.3531395165 	 3.3531395165
epoch_time;  31.720621824264526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2786148190498352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.535241961479187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8289389610290527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.121122121810913
4 2.0636878094 	 3.1211221231 	 3.1211221231
epoch_time;  31.867651224136353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26862138509750366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5852850675582886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7226645946502686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0683536529541016
5 2.0392149019 	 3.0683537664 	 3.0683537664
epoch_time;  31.530274391174316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24800866842269897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4732204079627991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5797860622406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.810594081878662
6 2.0163364294 	 2.8105940535 	 2.8105940535
epoch_time;  31.290875673294067
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24877355992794037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.485067218542099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5495574474334717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8064489364624023
7 2.0008106064 	 2.8064489416 	 2.8064489416
epoch_time;  31.615219593048096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26178616285324097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5349583625793457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.556748628616333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8519797325134277
8 1.9890759881 	 2.8519798485 	 2.8519798485
epoch_time;  31.46781873703003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2480381727218628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4996536672115326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4622066020965576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.731994152069092
9 1.9777587395 	 2.731994134 	 2.731994134
epoch_time;  31.33055853843689
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2781107425689697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5155051350593567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.464899778366089
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7052502632141113
10 1.9671720945 	 2.7052501782 	 2.7052501782
epoch_time;  31.09294319152832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26748213171958923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46522223949432373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4274327754974365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.62325119972229
11 1.9577264805 	 2.6232512603 	 2.6232512603
epoch_time;  31.568414211273193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23224854469299316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4431866705417633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3287429809570312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5479416847229004
12 1.9551553177 	 2.5479416306 	 2.5479416306
epoch_time;  33.62660360336304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2641136050224304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4750956892967224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.31191086769104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.528510093688965
13 1.9465459308 	 2.5285100164 	 2.5285100164
epoch_time;  32.660202980041504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21689988672733307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4006608724594116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2639684677124023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4408817291259766
14 1.9422419812 	 2.4408818425 	 2.4408818425
epoch_time;  31.388434886932373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23145027458667755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4535561203956604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.263185739517212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4841079711914062
15 1.9381638245 	 2.48410793 	 2.48410793
epoch_time;  31.327259302139282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25284233689308167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45313361287117004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2328972816467285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4153645038604736
16 1.9361354903 	 2.4153645283 	 2.4153645283
epoch_time;  31.816492795944214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22486190497875214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41812559962272644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1526894569396973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3449180126190186
17 1.9313430941 	 2.344917916 	 2.344917916
epoch_time;  31.61421799659729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23831774294376373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4294624924659729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1662802696228027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3425824642181396
18 1.9283077831 	 2.3425825789 	 2.3425825789
epoch_time;  31.22704529762268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2326289266347885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45123496651649475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1876773834228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3941969871520996
19 1.922806437 	 2.3941968763 	 2.3941968763
epoch_time;  31.813780546188354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23269863426685333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4510037899017334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.188159227371216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3949437141418457
It took 703.3122506141663 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–…â–…â–„â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–†â–…â–…â–…â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ƒâ–ƒâ–„â–ˆâ–„â–â–‚â–„â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–ƒâ–…â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.44161
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.49421
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.10721
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26275
wandb:                         Train loss 1.90403
wandb: 
wandb: ðŸš€ View run lambent-festival-1230 at: https://wandb.ai/nreints/thesis/runs/nkcozmd4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134955-nkcozmd4/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140142-zs45vvkk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-festival-1237
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/zs45vvkk
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39179477095603943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1217085123062134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1520650386810303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.092957496643066
0 4.2745568773 	 4.0929575327 	 4.0929575327
epoch_time;  31.50540328025818
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.344422310590744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8802331686019897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.032222270965576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.770540475845337
1 2.2130630276 	 3.7705404746 	 3.7705404746
epoch_time;  31.60107398033142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.277798593044281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6791210770606995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.790665864944458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.365849256515503
2 2.1360737691 	 3.3658493454 	 3.3658493454
epoch_time;  31.611759901046753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2778351604938507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6207877993583679
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7642977237701416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.280041456222534
3 2.0926241762 	 3.2800415039 	 3.2800415039
epoch_time;  31.417871236801147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29424893856048584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5830709338188171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7515463829040527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1968441009521484
4 2.0564553294 	 3.1968439875 	 3.1968439875
epoch_time;  31.288577795028687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38879093527793884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7298610210418701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7748501300811768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2621254920959473
5 2.0318529959 	 3.2621255411 	 3.2621255411
epoch_time;  31.82797360420227
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28052058815956116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5583317875862122
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.672590494155884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0767486095428467
6 2.0185994824 	 3.0767485747 	 3.0767485747
epoch_time;  31.798954725265503
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22422263026237488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4949568510055542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.461026668548584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8557069301605225
7 2.0032629221 	 2.8557069521 	 2.8557069521
epoch_time;  31.571662187576294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24336107075214386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5016605854034424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4007983207702637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.774388551712036
8 1.9907702612 	 2.7743886587 	 2.7743886587
epoch_time;  31.766659021377563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2856653034687042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5443727374076843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4373409748077393
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8111798763275146
9 1.9813760539 	 2.811179991 	 2.811179991
epoch_time;  31.418370962142944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25298285484313965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49645403027534485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3752524852752686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7376904487609863
10 1.9667729072 	 2.7376903637 	 2.7376903637
epoch_time;  31.334277153015137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24096807837486267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.489057332277298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.280416250228882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.649883270263672
11 1.9533025268 	 2.6498833734 	 2.6498833734
epoch_time;  31.633321046829224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24952806532382965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4519452750682831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.328613519668579
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.647590160369873
12 1.9369358484 	 2.647590266 	 2.647590266
epoch_time;  31.491121530532837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24446627497673035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4819250702857971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2673075199127197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.635376214981079
13 1.932190396 	 2.6353761415 	 2.6353761415
epoch_time;  31.685915231704712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21528765559196472
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42493388056755066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1895017623901367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.515660285949707
14 1.923885317 	 2.5156603014 	 2.5156603014
epoch_time;  31.581907749176025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2392640858888626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4794209599494934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1832079887390137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5262277126312256
15 1.919855757 	 2.5262277964 	 2.5262277964
epoch_time;  31.589409351348877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27080830931663513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.510232150554657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.19684100151062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5414621829986572
16 1.9140428299 	 2.5414621714 	 2.5414621714
epoch_time;  31.803914785385132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27449434995651245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.52470862865448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1508781909942627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.508692502975464
17 1.9114792707 	 2.508692396 	 2.508692396
epoch_time;  31.577049732208252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30973589420318604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5408527851104736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.18363094329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.519016981124878
18 1.9075968232 	 2.5190169051 	 2.5190169051
epoch_time;  33.75334429740906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2627388536930084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4941277503967285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1073007583618164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4414150714874268
19 1.9040324031 	 2.4414151578 	 2.4414151578
epoch_time;  33.106826066970825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2627490162849426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4942052960395813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.107205629348755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4416098594665527
It took 707.5880038738251 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.44995
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.48976
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.19463
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.24853
wandb:                         Train loss 1.92041
wandb: 
wandb: ðŸš€ View run sweet-festival-1237 at: https://wandb.ai/nreints/thesis/runs/zs45vvkk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140142-zs45vvkk/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3783663511276245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.055121898651123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0529069900512695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9229047298431396
0 4.2284247128 	 3.9229046796 	 3.9229046796
epoch_time;  31.49948239326477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29863065481185913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8274255990982056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8069260120391846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4802355766296387
1 2.2201097162 	 3.4802354967 	 3.4802354967
epoch_time;  31.358876705169678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28572073578834534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.748437225818634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.778640031814575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.341914415359497
2 2.1456752563 	 3.3419143264 	 3.3419143264
epoch_time;  31.52102756500244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.273001104593277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6592311263084412
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6897807121276855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1828653812408447
3 2.1078088305 	 3.1828652872 	 3.1828652872
epoch_time;  31.030211448669434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2792000472545624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.678053617477417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7078073024749756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1714136600494385
4 2.0748538132 	 3.1714137722 	 3.1714137722
epoch_time;  31.284733057022095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2339039295911789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.553908109664917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5375277996063232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.918088912963867
5 2.0516464037 	 2.9180888408 	 2.9180888408
epoch_time;  31.75219202041626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23922765254974365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5587241649627686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.478673219680786
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8407790660858154
6 2.0321776352 	 2.8407790725 	 2.8407790725
epoch_time;  31.619874715805054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23012326657772064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5219442248344421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.428082227706909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7750957012176514
7 2.0178963178 	 2.7750956767 	 2.7750956767
epoch_time;  31.60983443260193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23991258442401886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5049862265586853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.428389549255371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7354798316955566
8 2.0066646919 	 2.7354799013 	 2.7354799013
epoch_time;  33.24905180931091
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2366229146718979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5040404200553894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.386054039001465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.695291757583618
9 1.9910825033 	 2.6952917151 	 2.6952917151
epoch_time;  31.77815556526184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23354201018810272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5119541883468628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.387375831604004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7019143104553223
10 1.9827630438 	 2.7019141945 	 2.7019141945
epoch_time;  31.27772808074951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2591482102870941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5227815508842468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.513521671295166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.802563428878784
11 1.9759090891 	 2.8025634766 	 2.8025634766
epoch_time;  31.54757595062256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21329306066036224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45599567890167236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.307762384414673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.582793951034546
12 1.9677970747 	 2.5827940245 	 2.5827940245
epoch_time;  31.112268686294556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2274162918329239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49750998616218567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.270372152328491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.58064603805542
13 1.9635784172 	 2.5806460819 	 2.5806460819
epoch_time;  31.674695014953613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23257312178611755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48684173822402954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2837250232696533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5713701248168945
14 1.9592509957 	 2.5713700578 	 2.5713700578
epoch_time;  31.170114994049072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23779113590717316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48253166675567627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.341966152191162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.611013412475586
15 1.954392151 	 2.6110133815 	 2.6110133815
epoch_time;  31.24870753288269
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23640024662017822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4620042145252228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.24324893951416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4893929958343506
16 1.9468965214 	 2.4893930796 	 2.4893930796
epoch_time;  31.163188695907593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23440438508987427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4931306540966034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2076356410980225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4879705905914307
17 1.9409900373 	 2.4879706305 	 2.4879706305
epoch_time;  31.928390502929688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25041744112968445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47883346676826477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2430622577667236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4822123050689697
18 1.9270603022 	 2.482212211 	 2.482212211
epoch_time;  31.39153265953064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2485402673482895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4899071753025055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1943249702453613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4496419429779053
19 1.9204149498 	 2.4496418721 	 2.4496418721
epoch_time;  31.62376594543457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24852724373340607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48976102471351624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.194634437561035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4499547481536865
It took 711.4057564735413 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137931
Array Job ID: 2137927_1
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 04:29:59
CPU Efficiency: 12.60% of 1-11:42:36 core-walltime
Job Wall-clock time: 01:59:02
Memory Utilized: 9.01 GB
Memory Efficiency: 28.83% of 31.25 GB
