wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195345-70ymbvf3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-monkey-1159
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/70ymbvf3
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–‚â–„â–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–â–‚â–â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.49961
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29516
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.16958
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13775
wandb:                         Train loss 1.32639
wandb: 
wandb: ðŸš€ View run vibrant-monkey-1159 at: https://wandb.ai/nreints/thesis/runs/70ymbvf3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195345-70ymbvf3/logs
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2502442002296448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.747501015663147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.614791870117188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.48134994506836
0 4.4422763626 	 15.4813502956 	 15.6016957876
epoch_time;  35.27469730377197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17613261938095093
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5563950538635254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.319631576538086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.033221244812012
1 1.7372367513 	 9.0332216005 	 9.038458087
epoch_time;  34.19989776611328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1283092200756073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38839101791381836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.406956672668457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.941322326660156
2 1.5719902436 	 6.9413224504 	 6.9434768264
epoch_time;  34.62823963165283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1644180417060852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47222474217414856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.726245880126953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2161335945129395
3 1.5000809945 	 5.2161334064 	 5.2179842562
epoch_time;  35.09756565093994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12402050197124481
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.330171674489975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5854597091674805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.981290817260742
4 1.4569018241 	 4.9812905801 	 4.9829510663
epoch_time;  33.84407138824463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14413432776927948
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3794974684715271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.333437919616699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.719394683837891
5 1.4316959988 	 4.7193946632 	 4.7208611566
epoch_time;  34.08860802650452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11256802827119827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30944809317588806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.838557004928589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.164699077606201
6 1.4186168715 	 4.1646992451 	 4.1660301415
epoch_time;  33.89194631576538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12477127462625504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.329975962638855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9985129833221436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3065056800842285
7 1.3995072888 	 4.3065056878 	 4.3079078468
epoch_time;  34.14159798622131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15840449929237366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3324859142303467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.857240676879883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.122151851654053
8 1.3873895169 	 4.1221518027 	 4.1230580923
epoch_time;  33.80248308181763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10763498395681381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.274417519569397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3325459957122803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.660069227218628
9 1.3818509793 	 3.6600691512 	 3.6609530062
epoch_time;  34.18727350234985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14874738454818726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31496503949165344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.642784357070923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9250073432922363
10 1.3673268436 	 3.9250072582 	 3.926011534
epoch_time;  33.782623529434204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12481033056974411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30702653527259827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.35548996925354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6444625854492188
11 1.3645274912 	 3.6444626267 	 3.645522329
epoch_time;  34.05110239982605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10413739085197449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24226708710193634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.958674669265747
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1949994564056396
12 1.355157169 	 3.1949994061 	 3.1958258552
epoch_time;  33.786197662353516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13271774351596832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3010919392108917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3038930892944336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6300764083862305
13 1.3496736747 	 3.6300764754 	 3.6305274757
epoch_time;  33.83291554450989
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1268310248851776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29171955585479736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.698730707168579
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.992295980453491
14 1.3459345379 	 2.9922960436 	 2.9927252692
epoch_time;  34.00802564620972
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13950100541114807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27268221974372864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.854588747024536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1244239807128906
15 1.3409010591 	 3.1244239601 	 3.1249656883
epoch_time;  33.66700458526611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09871327877044678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24293647706508636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.753399610519409
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.971747636795044
16 1.3336415533 	 2.971747651 	 2.9725351694
epoch_time;  34.27834224700928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.122488833963871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2766176164150238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.525480031967163
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.877197027206421
17 1.3360752733 	 3.8771969357 	 3.8779230891
epoch_time;  34.26720428466797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0961604043841362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21923953294754028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9357798099517822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.186983346939087
18 1.3288594881 	 3.1869833457 	 3.1876610008
epoch_time;  34.19272208213806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13774418830871582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29526010155677795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.169130325317383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.499804973602295
19 1.3263935355 	 3.4998050174 	 3.5003434465
epoch_time;  33.95365762710571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.137754887342453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29516303539276123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.169583320617676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4996087551116943
It took 749.2630128860474 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn53: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135935.0

JOB STATISTICS
==============
Job ID: 2135935
Array Job ID: 2135932_3
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:48:54 core-walltime
Job Wall-clock time: 00:12:43
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
