wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170615-sg1kscb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-firecracker-1152
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sg1kscb4
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▇▆▃▆▂▃▅▅▄▃▂▃▁▃▂▄▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▇▂▅▄▄▆▅▅▃▁▂▂▃▃▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▄█▆▄▇▃▃▆▅▅▄▂▂▁▃▃▄▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▄▆▇▃▆▆▄█▆▆▃▁▁▂▃▄▂▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36173
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27161
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.29249
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22066
wandb:                         Train loss 3.52485
wandb: 
wandb: 🚀 View run sparkling-firecracker-1152 at: https://wandb.ai/nreints/thesis/runs/sg1kscb4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170615-sg1kscb4/logs
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2983357608318329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4408247172832489
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39279526472091675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5510144233703613
0 6.7013305581 	 0.5510144208 	 0.5510144208
epoch_time;  33.4356210231781
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2520380914211273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.390735924243927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3408910036087036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48178306221961975
1 4.0258478308 	 0.4817830679 	 0.4817830679
epoch_time;  32.23425030708313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29214707016944885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38482385873794556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40961921215057373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5228627920150757
2 3.8805535794 	 0.522862821 	 0.522862821
epoch_time;  32.251694679260254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.306257963180542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4095405340194702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3772377073764801
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48545265197753906
3 3.7989527574 	 0.4854526417 	 0.4854526417
epoch_time;  32.28650236129761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2334250807762146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29099181294441223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3347391188144684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4055350124835968
4 3.7444196429 	 0.4055350226 	 0.4055350226
epoch_time;  32.084887981414795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2905553877353668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.374154657125473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39303097128868103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4983457326889038
5 3.7085678318 	 0.4983457411 	 0.4983457411
epoch_time;  32.35302734375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2852107584476471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34291931986808777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32292306423187256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39005571603775024
6 3.6693489627 	 0.3900557234 	 0.3900557234
epoch_time;  32.260340213775635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2595072388648987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3304447531700134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3239947259426117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41190657019615173
7 3.6491499212 	 0.4119065568 	 0.4119065568
epoch_time;  33.32061290740967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3196006417274475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3887479901313782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36765536665916443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4576093852519989
8 3.6337976578 	 0.4576093931 	 0.4576093931
epoch_time;  32.62493395805359
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28032445907592773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3511573076248169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3583051860332489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45278358459472656
9 3.6151490828 	 0.4527835743 	 0.4527835743
epoch_time;  32.5985152721405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28229987621307373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3535217344760895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34365326166152954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4350473880767822
10 3.6043397856 	 0.4350473765 	 0.4350473765
epoch_time;  32.611640214920044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24547834694385529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3119405508041382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3375449478626251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4192644953727722
11 3.5898738337 	 0.4192645099 	 0.4192645099
epoch_time;  32.37930750846863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21028895676136017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2608976662158966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29981762170791626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3718712031841278
12 3.5779213566 	 0.3718712059 	 0.3718712059
epoch_time;  32.15892672538757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20757132768630981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2817535996437073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29943180084228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39524275064468384
13 3.5625796722 	 0.3952427632 	 0.3952427632
epoch_time;  32.182835817337036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21698857843875885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2766016721725464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2772359549999237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3467918336391449
14 3.5569682396 	 0.346791819 	 0.346791819
epoch_time;  32.554020404815674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23279684782028198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31279584765434265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3072963058948517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4021020531654358
15 3.5582102657 	 0.4021020425 	 0.4021020425
epoch_time;  32.508620738983154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25446826219558716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3055352568626404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31346067786216736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3842358887195587
16 3.5372756617 	 0.3842358976 	 0.3842358976
epoch_time;  32.1977481842041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22562268376350403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29871806502342224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3316035568714142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4200488030910492
17 3.5364634024 	 0.4200488116 	 0.4200488116
epoch_time;  32.1331672668457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21142858266830444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29497030377388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.295352041721344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3991055190563202
18 3.5391699417 	 0.399105505 	 0.399105505
epoch_time;  32.18464398384094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22068093717098236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2715829014778137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29245704412460327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36170539259910583
19 3.5248477964 	 0.3617053882 	 0.3617053882
epoch_time;  32.3500497341156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22066375613212585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27161160111427307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2924934923648834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3617282211780548
It took 713.521154165268 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn20: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135421.0

JOB STATISTICS
==============
Job ID: 2135421
Array Job ID: 2135328_23
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:38:24 core-walltime
Job Wall-clock time: 00:12:08
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
