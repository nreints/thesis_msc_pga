wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203636-7xmoscgd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-moon-397
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7xmoscgd
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▂▃▂▂▂▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▁▁▂▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▂▄▂▂▂▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▂▄▂▂▂▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run olive-moon-397 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7xmoscgd
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203636-7xmoscgd/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204356-1qxtndkj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-plasma-414
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/1qxtndkj
Training on dataset: data_t(0,0)_r(5,20)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(0,0)_r(5,20)_tennis_pNone_gTrue', 'data_t(0,0)_r(5,20)_semi_pNone_gTrue', 'data_t(0,0)_r(5,20)_combi_pNone_gTrue', 'data_t(0,0)_r(5,20)_full_pNone_gTrue']
Focussing on identity: True
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 52.2388334274292 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.070392608642578 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.530261039733887 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.520104885101318 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.679885149002075 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0005533799 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.9808e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.66934e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.14786e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.5013e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 33.485159158706665
Epoch 1/9
	 Logging train Loss: 9.1152e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.2607e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.9496e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.7281e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.215e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.060755252838135
Epoch 2/9
	 Logging train Loss: 6.1597e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.5961e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.3822e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3955e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.532e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.29365658760071
Epoch 3/9
	 Logging train Loss: 6.306e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.72358e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.62229e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 8.8894e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.105e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.299686431884766
Epoch 4/9
	 Logging train Loss: 5.8704e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.4667e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.2167e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3401e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.589e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.08249354362488
Epoch 5/9
	 Logging train Loss: 5.5079e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.2049e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.7477e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.6644e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.03e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.06823134422302
Epoch 6/9
	 Logging train Loss: 5.1756e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.23e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.7565e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.656e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.723e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.1837420463562
Epoch 7/9
	 Logging train Loss: 4.0796e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.5143e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.3129e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.2562e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.91e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.06361985206604
Epoch 8/9
	 Logging train Loss: 3.6929e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.7281e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.3698e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.915e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.477e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 31.836299419403076
Epoch 9/9
	 Logging train Loss: 3.1585e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.7294e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.5641e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.0206e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.617e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.16723442077637
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  440.99786257743835  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 46.22186040878296 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 11.702558994293213 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 11.725083351135254 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 11.55713963508606 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 11.700559854507446 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0003620336 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.57386e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.55563e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.37514e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.0896e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.365398645401
Epoch 1/9
	 Logging train Loss: 9.3471e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.1163e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 9.9298e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.9956e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.195e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.60071539878845
Epoch 2/9
	 Logging train Loss: 7.8751e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.59154e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.61712e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 8.5021e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.304e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.438573598861694
Epoch 3/9
	 Logging train Loss: 6.8983e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.2839e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.8895e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.9245e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.498e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.87392330169678
Epoch 4/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▃▅▂▃▂▄▂▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▃▃▂▂▃▆▂▂▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▃▅▃▃▂▄▂▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▃▅▂▃▂▄▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run decent-plasma-414 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/1qxtndkj
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204356-1qxtndkj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205107-q13zxgh0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-thunder-425
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/q13zxgh0
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▃▄▂▂▂▁▂▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▂▁▁▂▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▃▅▂▂▂▁▂▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▃▅▂▂▂▁▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run still-thunder-425 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/q13zxgh0
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205107-q13zxgh0/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205816-oudxtgrw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-lion-437
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/oudxtgrw
	 Logging train Loss: 5.7936e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.1858e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.6041e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3888e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.229e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.92383003234863
Epoch 5/9
	 Logging train Loss: 4.8474e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.6117e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.0474e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.111e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.514e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.58548903465271
Epoch 6/9
	 Logging train Loss: 4.4572e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.20192e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.20339e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.6539e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.025e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.234095335006714
Epoch 7/9
	 Logging train Loss: 3.4702e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.2053e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.2685e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.343e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.579e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.378990173339844
Epoch 8/9
	 Logging train Loss: 2.8966e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.8914e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.9412e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.1069e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.799e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.17591071128845
Epoch 9/9
	 Logging train Loss: 2.4589e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.612e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.6584e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.371e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.43e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.49485397338867
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  430.96383929252625  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 46.01272487640381 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 11.668574571609497 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 11.621186971664429 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 11.538706302642822 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 11.67838740348816 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0007270603 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.98697e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.68342e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.64408e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.2556e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.312506437301636
Epoch 1/9
	 Logging train Loss: 1.18938e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.87005e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.76951e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.236e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.0375e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.34366178512573
Epoch 2/9
	 Logging train Loss: 6.7345e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.81313e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.57496e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.33897e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.125e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.350582122802734
Epoch 3/9
	 Logging train Loss: 6.1555e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.7358e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.6521e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.2937e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.708e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.28575897216797
Epoch 4/9
	 Logging train Loss: 6.314e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.29339e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.22975e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.2892e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.41e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.131975412368774
Epoch 5/9
	 Logging train Loss: 5.6681e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.9783e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.897e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.0999e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.853e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.569724798202515
Epoch 6/9
	 Logging train Loss: 5.2093e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.0206e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.0713e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.4284e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.325e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.43751883506775
Epoch 7/9
	 Logging train Loss: 4.5722e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.4814e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 9.0602e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.7311e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.333006858825684
Epoch 8/9
	 Logging train Loss: 3.8691e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.8609e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.8771e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.8927e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.335e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.326908111572266
Epoch 9/9
	 Logging train Loss: 3.2967e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.3816e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.2886e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.6834e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.982e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 32.573381185531616
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  429.0663456916809  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▂▂▂▂▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▂▂▂▂▁▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▂▂▂▂▁▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 1e-05
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 1e-05
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run fast-lion-437 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/oudxtgrw
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205816-oudxtgrw/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210528-i40ljotu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-star-449
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/i40ljotu
slurmstepd: error: *** JOB 3085846 ON gcn25 CANCELLED AT 2023-07-16T21:09:10 ***
slurmstepd: error: *** STEP 3085846.0 ON gcn25 CANCELLED AT 2023-07-16T21:09:10 ***
