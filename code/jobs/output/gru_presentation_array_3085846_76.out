wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203638-6x6x2ulj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-cloud-402
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/6x6x2ulj
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▂▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run fanciful-cloud-402 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/6x6x2ulj
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203638-6x6x2ulj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204432-lwpvbqo2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-dawn-415
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/lwpvbqo2
Training on dataset: data_t(0,0)_r(5,20)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(0,0)_r(5,20)_full_pNone_gTrue', 'data_t(0,0)_r(5,20)_semi_pNone_gTrue', 'data_t(0,0)_r(5,20)_tennis_pNone_gTrue', 'data_t(0,0)_r(5,20)_combi_pNone_gTrue']
Focussing on identity: False
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.22429347038269 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.932931661605835 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.43558406829834 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.228662729263306 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.633441686630249 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0008172828 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.3718e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.75801e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.66896e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.78991e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 36.65421962738037
Epoch 1/9
	 Logging train Loss: 1.6101e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.559e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.12704e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.26042e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.0927e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.41686010360718
Epoch 2/9
	 Logging train Loss: 4.5988e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.528e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.1857e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.6138e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.9554e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.41428589820862
Epoch 3/9
	 Logging train Loss: 4.039e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.24e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.2359e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.499e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.3331e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.25327467918396
Epoch 4/9
	 Logging train Loss: 4.2212e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.161e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.888e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.1755e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.7687e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.31531286239624
Epoch 5/9
	 Logging train Loss: 4.1551e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.123e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.1191e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.4545e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.8606e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.53998923301697
Epoch 6/9
	 Logging train Loss: 4.3862e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.22e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.1875e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.5642e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.8928e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.62162470817566
Epoch 7/9
	 Logging train Loss: 3.7359e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.984e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.11897e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.2426e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.2791e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.39485692977905
Epoch 8/9
	 Logging train Loss: 3.6216e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.407e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.8361e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.1846e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.1212e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.22738432884216
Epoch 9/9
	 Logging train Loss: 3.3034e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.91e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.1681e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3624e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.1904e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.773876667022705
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'log_dualQ_1'_'False'.pth
It took  474.05776286125183  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 47.83882212638855 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.122145891189575 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.170924663543701 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.142146587371826 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.080307483673096 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0005895962 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.9998e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.89237e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.81834e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.72995e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.75443243980408
Epoch 1/9
	 Logging train Loss: 1.04351e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.585e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 9.6811e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.3327e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.1329e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.63324451446533
Epoch 2/9
	 Logging train Loss: 4.6551e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.957e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 8.0506e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.5722e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.0929e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.86193037033081
Epoch 3/9
	 Logging train Loss: 4.7405e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.215e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.0041e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.4939e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.0022e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.554638147354126
Epoch 4/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▂▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▂▁▂▁▁▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▂▁▂▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run vibrant-dawn-415 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/lwpvbqo2
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204432-lwpvbqo2/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205217-jlqwqope
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-totem-428
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/jlqwqope
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▃▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▂▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▃▂▁▁▁▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▃▂▁▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 1e-05
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 1e-05
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run stoic-totem-428 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/jlqwqope
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205217-jlqwqope/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210004-7rqb6c6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-pond-440
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7rqb6c6h
	 Logging train Loss: 4.7005e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.778e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.705e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.3105e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.9737e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.84828758239746
Epoch 5/9
	 Logging train Loss: 4.6269e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.879e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.8972e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.543e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.5159e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.81719899177551
Epoch 6/9
	 Logging train Loss: 4.2246e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.66e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.0587e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.6335e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.5798e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 36.0225887298584
Epoch 7/9
	 Logging train Loss: 3.7386e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.058e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.8641e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.5993e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.459e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.98978066444397
Epoch 8/9
	 Logging train Loss: 3.4649e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.322e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.0102e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.7748e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.5406e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.83719205856323
Epoch 9/9
	 Logging train Loss: 2.9819e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.754e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.2306e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.0598e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.2243e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.49030590057373
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'log_dualQ_1'_'False'.pth
It took  465.96810126304626  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 47.31730628013611 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.050402402877808 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.059382200241089 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.147924184799194 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.000746011734009 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0007428885 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.5893e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.13489e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.51757e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.43896e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.56996488571167
Epoch 1/9
	 Logging train Loss: 1.52653e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.7149e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.09331e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.09282e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.0787e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.90039587020874
Epoch 2/9
	 Logging train Loss: 5.2987e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.785e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.0688e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.26212e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.08725e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.47228980064392
Epoch 3/9
	 Logging train Loss: 4.5222e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.294e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 8.6498e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 8.85e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.3448e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.424641132354736
Epoch 4/9
	 Logging train Loss: 4.6995e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.152e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 8.0553e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 8.2419e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.045e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.32132840156555
Epoch 5/9
	 Logging train Loss: 4.6611e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.85e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.8429e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.8203e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.838e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.725502490997314
Epoch 6/9
	 Logging train Loss: 4.1714e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.406e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.9004e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.8604e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.3581e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.629085540771484
Epoch 7/9
	 Logging train Loss: 4.0258e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.776e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.9293e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 6.186e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.9776e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.51631450653076
Epoch 8/9
	 Logging train Loss: 3.5411e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.23e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.2851e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.262e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.0812e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 35.87776207923889
Epoch 9/9
	 Logging train Loss: 3.2481e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.274e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.0483e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.25e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.581e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 37.14989423751831
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'log_dualQ_1'_'False'.pth
It took  466.16564989089966  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run balmy-pond-440 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7rqb6c6h
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_210004-7rqb6c6h/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210753-wd9c5fjc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-shadow-452
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/wd9c5fjc
slurmstepd: error: *** JOB 3085906 ON gcn21 CANCELLED AT 2023-07-16T21:09:10 ***
slurmstepd: error: *** STEP 3085906.0 ON gcn21 CANCELLED AT 2023-07-16T21:09:10 ***

JOB STATISTICS
==============
Job ID: 3085906
Array Job ID: 3085846_76
Cluster: snellius
User/Group: nreints/nreints
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 09:49:30 core-walltime
Job Wall-clock time: 00:32:45
Memory Utilized: 6.69 MB
Memory Efficiency: 0.00% of 0.00 MB
