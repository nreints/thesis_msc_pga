wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121450-2oru0dd3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-wonton-1174
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2oru0dd3
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▁▂▄▅██▆██▆█▇█▆▇█▇▆▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▅▆▂▇▄▁▄▃▄▃▄▆▄█▇▄▄▅▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▁▁▄▄█▇▅▇▇▅▇▆▇▆▇█▇▆▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▂▁▅▂▁▂▁▃▁▂▂▂▃▂▁▂▂▂▂
wandb:                         Train loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.49977
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.41498
wandb:    Test loss t(0, 0)_r(-5, 5)_none 21.22136
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30876
wandb:                         Train loss 1.53204
wandb: 
wandb: 🚀 View run red-wonton-1174 at: https://wandb.ai/nreints/thesis/runs/2oru0dd3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121450-2oru0dd3/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122913-amjjk9y8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-ox-1187
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/amjjk9y8
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7992280125617981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3105363845825195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.155567169189453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.453773498535156
0 7.1706042125 	 35.4537742821 	 35.4569916596
epoch_time;  40.92057728767395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5843287706375122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.278865337371826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.394383430480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.85442543029785
1 2.7739234671 	 25.8544262035 	 25.8544341216
epoch_time;  39.4924955368042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3330255150794983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3795571327209473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.865743637084961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.038074493408203
2 2.1883920968 	 27.0380753801 	 27.038141364
epoch_time;  39.50431561470032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28103822469711304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0016181468963623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.076921463012695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.780338287353516
3 1.9166235798 	 32.7803367821 	 32.7803473395
epoch_time;  39.562551975250244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5509775876998901
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5068602561950684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.276756286621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.27033996582031
4 1.7825854178 	 33.2703415329 	 33.2703520904
epoch_time;  39.716755867004395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3635464608669281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2581863403320312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.518644332885742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.552860260009766
5 1.7223155771 	 40.5528584248 	 40.5530405405
epoch_time;  39.566065311431885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25702354311943054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.945248007774353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.357698440551758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.872894287109375
6 1.6693873112 	 39.8728937922 	 39.8733741554
epoch_time;  39.40255641937256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3316543698310852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2308337688446045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.468685150146484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.35039520263672
7 1.6359955846 	 36.3503959037 	 36.3512009079
epoch_time;  39.53591012954712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28021153807640076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.177753210067749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.45669937133789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.72208786010742
8 1.6148945286 	 39.7220861486 	 39.7234348606
epoch_time;  39.57996892929077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3877844214439392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2268173694610596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.659481048583984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.01179122924805
9 1.5969463082 	 40.011792652 	 40.0138275971
epoch_time;  39.78599786758423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2798975706100464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.11338210105896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.18340301513672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.331600189208984
10 1.5942463807 	 36.3315983953 	 36.3343881968
epoch_time;  39.72520852088928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29926276206970215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.208662271499634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.414676666259766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.78106689453125
11 1.5778613546 	 39.7810652449 	 39.784514886
epoch_time;  39.14325952529907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30122286081314087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.42130184173584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.355257034301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.817665100097656
12 1.568874434 	 37.8176652238 	 37.8209697002
epoch_time;  39.0795419216156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3063015639781952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2509982585906982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.75240707397461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.160804748535156
13 1.5608083716 	 40.1608028927 	 40.1648833404
epoch_time;  39.590660572052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41162773966789246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5968358516693115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.073257446289062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.05649948120117
14 1.5542972386 	 37.056500739 	 37.0606102196
epoch_time;  39.923004150390625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31181639432907104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4707846641540527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.05630874633789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.596012115478516
15 1.5467705336 	 38.5960119299 	 38.6006545608
epoch_time;  39.94003248214722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25114160776138306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.198727607727051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.962486267089844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.458778381347656
16 1.548281164 	 40.4587785051 	 40.4637721706
epoch_time;  39.3519344329834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29180169105529785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.265192747116089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.21910858154297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.24117660522461
17 1.5408096675 	 39.2411766258 	 39.2469700169
epoch_time;  39.61920523643494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2959784269332886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2850046157836914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.79964828491211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.35316848754883
18 1.5353152637 	 37.3531672297 	 37.3587019637
epoch_time;  39.635451555252075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30871716141700745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.412541389465332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.22311782836914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.5101318359375
19 1.5320424239 	 37.5101324958 	 37.5156645904
epoch_time;  39.334362268447876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3087556064128876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.414980173110962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.221355438232422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.49977111816406
It took 863.0703799724579 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.148 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.148 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▂▁▁▃▃▃▄▅▆▅▆▆▆█▆▇▆█▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▂▁▁▂▅▅▄▃▆▄▅▅▄▆▅▅▅▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▃▁▁▄▃▃▄▅▅▅▅▅▆▇▅▇▅█▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▂▃▂▁▂▂▁▁▄▁▁▂▁▂▂▁▁▁▁
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 39.65285
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.30974
wandb:    Test loss t(0, 0)_r(-5, 5)_none 23.86182
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31841
wandb:                         Train loss 1.53762
wandb: 
wandb: 🚀 View run beaming-ox-1187 at: https://wandb.ai/nreints/thesis/runs/amjjk9y8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122913-amjjk9y8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124311-vka2rx1r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-rabbit-1195
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/vka2rx1r
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7938729524612427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.786696672439575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.536113739013672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.61677932739258
0 5.9623696154 	 36.6167783995 	 36.6198163007
epoch_time;  38.635676860809326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4606955349445343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.295198917388916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.95267677307129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.500831604003906
1 2.6559017236 	 30.5008313978 	 30.5008366765
epoch_time;  39.12435030937195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3442549705505371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1641688346862793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.373233795166016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.90095329284668
2 2.1481994753 	 28.9009528083 	 28.9009686444
epoch_time;  39.570608615875244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3948881924152374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0667734146118164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.533245086669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.08194351196289
3 1.9064169479 	 29.0819441512 	 29.0819494299
epoch_time;  39.59335398674011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38397926092147827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1053249835968018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.095842361450195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.960750579833984
4 1.8004435524 	 32.9607501056 	 32.9608240076
epoch_time;  38.97246336936951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.297467440366745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1194679737091064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.88911247253418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.71787452697754
5 1.7348444376 	 31.7178737331 	 31.7180136191
epoch_time;  39.56911087036133
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36062386631965637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.511404275894165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.01542854309082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.8598575592041
6 1.6940159265 	 31.8598580025 	 31.8601113809
epoch_time;  38.75346803665161
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3233228027820587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4627034664154053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.883852005004883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.4520263671875
7 1.6642682922 	 34.452027027 	 34.4525258657
epoch_time;  39.209794998168945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31022271513938904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.395066022872925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.883800506591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.348026275634766
8 1.638945934 	 36.3480257601 	 36.3486169764
epoch_time;  39.84930205345154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2915791869163513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.263498067855835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.1731014251709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.69261932373047
9 1.6180017429 	 36.6926203547 	 36.6935203758
epoch_time;  39.573803424835205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4934333860874176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5862927436828613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.740861892700195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.35506820678711
10 1.6099221262 	 36.3550675676 	 36.3559860642
epoch_time;  38.69276237487793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31956830620765686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.414210319519043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.01409339904785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.351139068603516
11 1.5973263213 	 37.3511375633 	 37.3524070946
epoch_time;  39.120609998703
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28588372468948364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4581093788146973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.312917709350586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.68665313720703
12 1.5795260399 	 37.686652766 	 37.6881070524
epoch_time;  39.07631325721741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3589004874229431
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.468108892440796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.617944717407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.268775939941406
13 1.5708545021 	 38.2687763936 	 38.2702940245
epoch_time;  38.95902490615845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.306498646736145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.325343132019043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.050994873046875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.21454620361328
14 1.5613645891 	 40.2145455025 	 40.2163613809
epoch_time;  38.43739175796509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35499107837677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5380771160125732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.288246154785156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.450965881347656
15 1.5597070114 	 37.4509660051 	 37.4531223606
epoch_time;  39.143487215042114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33919018507003784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.50582218170166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.62742805480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.37228775024414
16 1.5468991331 	 39.3722867399 	 39.3746568834
epoch_time;  38.88245749473572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3070867657661438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.469630002975464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.213563919067383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.9250373840332
17 1.550586202 	 36.925036951 	 36.9275892103
epoch_time;  39.3326461315155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3100104033946991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.474102020263672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.878578186035156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.97905731201172
18 1.5445468208 	 40.9790566934 	 40.9820365287
epoch_time;  38.93028998374939
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3182888329029083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3100640773773193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.87358856201172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.66072463989258
19 1.5376207975 	 39.660723712 	 39.6639622044
epoch_time;  38.71988105773926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3184075653553009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.309739351272583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.8618221282959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.65285110473633
It took 838.0292980670929 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▁▃▆▇▇▇██▇▇██▇▇██▇▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▅█▂▃▂▄▄▂▄▃▂▁▁▂▃▂▃▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▁▄▇▇▇▇██▇▇█▇▆▆▇▇▆▆▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▅▂▁▂▂▂▁▂▂▁▁▁▁▂▁▁▂▂▂
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 40.73354
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.1306
wandb:    Test loss t(0, 0)_r(-5, 5)_none 22.74757
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32897
wandb:                         Train loss 1.52645
wandb: 
wandb: 🚀 View run lucky-rabbit-1195 at: https://wandb.ai/nreints/thesis/runs/vka2rx1r
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124311-vka2rx1r/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125834-nl2qbjd9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-fireworks-1202
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nl2qbjd9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9759082794189453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.048676013946533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.984861373901367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.378633499145508
0 5.3520223505 	 30.3786343961 	 30.3793813345
epoch_time;  39.131407022476196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5014671683311462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.701984405517578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.983621597290039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.550865173339844
1 2.5907008043 	 22.5508657095 	 22.5509633657
epoch_time;  38.73998260498047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6648956537246704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.175812005996704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.69392967224121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.851520538330078
2 2.1022388869 	 28.8515202703 	 28.8516205659
epoch_time;  38.97324252128601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34880203008651733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2386271953582764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.256616592407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.90662384033203
3 1.8863332822 	 38.9066221495 	 38.9066221495
epoch_time;  44.554996490478516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3038069009780884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4404282569885254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.020381927490234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.10350036621094
4 1.768169385 	 41.1034997889 	 41.1036106419
epoch_time;  52.97697067260742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3364967405796051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1834189891815186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.336681365966797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.62068557739258
5 1.7105383127 	 40.6206846495 	 40.6211254223
epoch_time;  53.427852392196655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41695529222488403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.470860719680786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.059661865234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.31232833862305
6 1.665074105 	 40.3123284417 	 40.3131228885
epoch_time;  53.11799931526184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40938180685043335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5518693923950195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.915542602539062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.55203628540039
7 1.6352082848 	 43.5520349451 	 43.5534021326
epoch_time;  53.55121850967407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2804677486419678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.185997247695923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.606515884399414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.52003479003906
8 1.6125018844 	 43.5200353674 	 43.5219277872
epoch_time;  50.738319396972656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4011392891407013
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5007903575897217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.178600311279297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.313636779785156
9 1.5952708787 	 41.3136375633 	 41.3159945101
epoch_time;  38.891581535339355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3376612365245819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.403078317642212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.023073196411133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.388160705566406
10 1.5844876981 	 41.3881624789 	 41.3907780828
epoch_time;  39.97863841056824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2837804853916168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2122130393981934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.299907684326172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.87455368041992
11 1.5736521189 	 43.8745539485 	 43.8779930321
epoch_time;  39.51363253593445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30541032552719116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.046203374862671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.58604621887207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.54859924316406
12 1.5648128068 	 43.5486011402 	 43.5527898015
epoch_time;  40.813045501708984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27753761410713196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.099277973175049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.851808547973633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.367637634277344
13 1.5571793212 	 40.3676361909 	 40.371948902
epoch_time;  38.807063579559326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3244343101978302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.224489212036133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.353300094604492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.53947830200195
14 1.5510338207 	 40.5394795186 	 40.5443148226
epoch_time;  38.52912998199463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42704516649246216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4047234058380127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.6779842376709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.66646957397461
15 1.5465220953 	 42.6664695946 	 42.6716480152
epoch_time;  41.06381964683533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30729833245277405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2460618019104004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.262441635131836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.603607177734375
16 1.5418402815 	 42.6036080025 	 42.6094700169
epoch_time;  39.73992943763733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31293049454689026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.318500518798828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.851778030395508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.36787033081055
17 1.5326604617 	 40.3678684544 	 40.3735615498
epoch_time;  39.2300009727478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39062491059303284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3067994117736816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.154705047607422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.259151458740234
18 1.5303119512 	 41.2591532939 	 41.2652079814
epoch_time;  40.18460941314697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3290737271308899
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1374130249023438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.73861312866211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.75645065307617
19 1.5264513226 	 40.7564505912 	 40.7632918074
epoch_time;  40.48230695724487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3289684057235718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1305992603302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.74757194519043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.73353576660156
It took 922.8851437568665 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▁▁▁▂▃▃▄▅▄▃▄▄▄▅▄▄▄▄▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▅▂▁▅▃▁▂▂▂▂▄▄▂▂▃▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▁▂▁▃▂▃▄▅▄▂▄▄▄▅▃▃▄▃▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▃▄▂▁▃▂▁▁▂▁▂▂▂▂▁▂▁▁▁
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 35.79916
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.24411
wandb:    Test loss t(0, 0)_r(-5, 5)_none 20.95848
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.24812
wandb:                         Train loss 1.54703
wandb: 
wandb: 🚀 View run twinkling-fireworks-1202 at: https://wandb.ai/nreints/thesis/runs/nl2qbjd9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125834-nl2qbjd9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131230-e6xbtlfd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-wish-1210
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/e6xbtlfd
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7532217502593994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0085761547088623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.537145614624023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.409244537353516
0 6.5617455939 	 44.4092430321 	 44.4156408361
epoch_time;  38.90817999839783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6582362651824951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.656095266342163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.421249389648438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.60000991821289
1 2.8193600706 	 29.6000105574 	 29.6002956081
epoch_time;  38.83853006362915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3652544617652893
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5456154346466064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.842885971069336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.777524948120117
2 2.2753628555 	 28.7775258657 	 28.7775364231
epoch_time;  38.9352388381958
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4404798746109009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6997416019439697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.257862091064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.530258178710938
3 1.9695935001 	 28.5302576014 	 28.5302681588
epoch_time;  38.567546129226685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31213027238845825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3554465770721436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.86705207824707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.818986892700195
4 1.8170580079 	 31.8189875422 	 31.8189980997
epoch_time;  38.647443532943726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25620630383491516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2348132133483887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.731660842895508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.31217956542969
5 1.7314710985 	 32.3121806377 	 32.3124445735
epoch_time;  39.07758617401123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3953448235988617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6809260845184326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.065526962280273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.40951919555664
6 1.6928550601 	 33.4095175253 	 33.4099319046
epoch_time;  38.996328830718994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34102663397789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5048787593841553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.349279403686523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.77408218383789
7 1.6603886547 	 35.7740815034 	 35.7750422297
epoch_time;  39.15622854232788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24147087335586548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2784669399261475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.28999900817871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.48036575317383
8 1.636235097 	 37.480365815 	 37.4818887247
epoch_time;  38.7100031375885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26770228147506714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3419201374053955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.912334442138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.36373519897461
9 1.6259679672 	 35.3637352196 	 35.3658150338
epoch_time;  39.01706314086914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2903316617012024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.349660873413086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.494138717651367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.01691436767578
10 1.6048374886 	 33.0169156461 	 33.0192567568
epoch_time;  39.02418661117554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23684929311275482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.375758171081543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.336854934692383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.85783386230469
11 1.6027670042 	 35.8578336149 	 35.8610061233
epoch_time;  39.26300024986267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30571603775024414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.346424102783203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.615907669067383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.270301818847656
12 1.5919805177 	 36.2703019426 	 36.2737014358
epoch_time;  39.22833180427551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3363558053970337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5927822589874268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.121618270874023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.84096145629883
13 1.581579003 	 35.8409628378 	 35.8448743666
epoch_time;  39.29097270965576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2974192500114441
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.565103769302368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.706205368041992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.97711944580078
14 1.5748233989 	 37.9771194046 	 37.981677576
epoch_time;  39.732895374298096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3139476776123047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3705246448516846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.292499542236328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.71743392944336
15 1.5697172729 	 34.7174355997 	 34.7223263302
epoch_time;  39.037787199020386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24820907413959503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.29738187789917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.457794189453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.95341873168945
16 1.5658434207 	 34.9534206081 	 34.9585726351
epoch_time;  38.68872857093811
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2898397743701935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.479090452194214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.041305541992188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.8570671081543
17 1.5593944387 	 35.8570655617 	 35.8629328547
epoch_time;  38.669217109680176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26050469279289246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4066128730773926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.258962631225586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.786338806152344
18 1.5517589696 	 34.7863386824 	 34.7922640414
epoch_time;  38.70779347419739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24816513061523438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.243788242340088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.955753326416016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.79642868041992
19 1.5470305425 	 35.7964289485 	 35.8030854096
epoch_time;  38.828070878982544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24811924993991852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2441093921661377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.958484649658203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.79916000366211
It took 836.1002473831177 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▃▁▂▂▃▃▃▂▂▂▂▂▂▂▂▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▄▄▃▂▄▃▃▂▂▂▂▁▂▂▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▂▁▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▃▃▂▃▃▂▂▂▂▁▁▂▁▂▁▁▁▁
wandb:                         Train loss █▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.42603
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.9437
wandb:    Test loss t(0, 0)_r(-5, 5)_none 21.29568
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.71043
wandb:                         Train loss 1.50597
wandb: 
wandb: 🚀 View run radiant-wish-1210 at: https://wandb.ai/nreints/thesis/runs/e6xbtlfd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131230-e6xbtlfd/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132620-5jly7p81
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-rabbit-1217
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5jly7p81
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.569873094558716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.083174705505371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 45.783660888671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 78.02900695800781
0 8.5429892466 	 78.0290065456 	 78.0634607264
epoch_time;  38.94642376899719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1027259826660156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.546504974365234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.56161117553711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 73.85535430908203
1 4.7492902235 	 73.8553526182 	 73.8826963682
epoch_time;  38.65290069580078
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5643091201782227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.240147113800049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 36.124168395996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 62.79133224487305
2 3.8023971733 	 62.791332348 	 62.8093961149
epoch_time;  38.959869384765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2221415042877197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.002172946929932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.707042694091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.207183837890625
3 3.0170452141 	 46.2071843328 	 46.2104571368
epoch_time;  38.940784215927124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0525057315826416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5842626094818115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.1063175201416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.55791473388672
4 2.3423745808 	 34.557915435 	 34.5582796664
epoch_time;  38.584919691085815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8844068646430969
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.363640308380127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.725317001342773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.131622314453125
5 2.0473540247 	 42.1316221495 	 42.1316670186
epoch_time;  38.850892782211304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1285420656204224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.415196895599365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.595834732055664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.67314147949219
6 1.8668322812 	 43.6731418919 	 43.6731656461
epoch_time;  38.57874655723572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0676180124282837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.96665096282959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.638582229614258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.896854400634766
7 1.7537923618 	 46.8968538851 	 46.8969647382
epoch_time;  38.42213535308838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9972158670425415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.863255739212036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.546175003051758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.63915252685547
8 1.6809526675 	 44.6391522382 	 44.6394847973
epoch_time;  38.306262731552124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7906659245491028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.172100782394409
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.923641204833984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.08138656616211
9 1.6405284678 	 44.0813872466 	 44.0821289062
epoch_time;  38.614442348480225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8386563062667847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3368048667907715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.55962371826172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.775489807128906
10 1.6170382387 	 42.7754882812 	 42.7764490076
epoch_time;  38.281057357788086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8434445858001709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2279303073883057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.645004272460938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.21940612792969
11 1.5881422039 	 39.2194045608 	 39.2206899282
epoch_time;  38.62381315231323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7652676105499268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.289642810821533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.783723831176758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.82075500488281
12 1.5767742098 	 39.8207559122 	 39.8221864443
epoch_time;  38.65245008468628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6321105360984802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.895639181137085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.714073181152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.8770866394043
13 1.5617153449 	 39.8770877323 	 39.8791305954
epoch_time;  38.794432163238525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8448567986488342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.284640312194824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.644405364990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.19170379638672
14 1.5505326464 	 39.1917044975 	 39.1940851985
epoch_time;  38.627177000045776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7430467009544373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.112152576446533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.675697326660156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.311767578125
15 1.5350939338 	 39.3117688978 	 39.3143449113
epoch_time;  38.36728024482727
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7784656286239624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1859261989593506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.942535400390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.97394561767578
16 1.5296091171 	 37.9739468961 	 37.9766680743
epoch_time;  38.48305916786194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6765750646591187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.840162754058838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.16448974609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.66755294799805
17 1.521535947 	 38.6675543708 	 38.6709274704
epoch_time;  38.825913429260254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7321717739105225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.006732940673828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.88375473022461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.01767349243164
18 1.5150444842 	 38.0176731419 	 38.0211227829
epoch_time;  38.4726026058197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7104775309562683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.944594383239746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.293027877807617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.398345947265625
19 1.5059668167 	 37.3983477618 	 37.4021537162
epoch_time;  38.76790189743042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7104347944259644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.943704843521118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.29567527770996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.426029205322266
It took 829.8427364826202 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▁▁▂▃▃▅▅▇▆▆▇▇▆▇▇█▇█▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▇▇▂█▃▃▅▆▂▁▄▅▃▄▅▂▁▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▂▁▁▃▂▅▆█▆▇█▇▅▆▇█▆█▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▃▅▂▂▃▃▂▂▂▂▂▂▂▂▁▁▂▂
wandb:                         Train loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 38.01572
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.29395
wandb:    Test loss t(0, 0)_r(-5, 5)_none 20.44026
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29231
wandb:                         Train loss 1.53242
wandb: 
wandb: 🚀 View run luminous-rabbit-1217 at: https://wandb.ai/nreints/thesis/runs/5jly7p81
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132620-5jly7p81/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134141-g1twp2ha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-rat-1225
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g1twp2ha
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.780916690826416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.379453659057617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.93898582458496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.688907623291016
0 4.4811170368 	 36.6889067779 	 36.6915487753
epoch_time;  39.256646156311035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6536936163902283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.605942487716675
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.41472053527832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.34703254699707
1 2.7169996919 	 27.3470333615 	 27.3470676731
epoch_time;  38.91489386558533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4753541648387909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5692179203033447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.908784866333008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.952959060668945
2 2.1898858672 	 26.9529587204 	 26.9529587204
epoch_time;  51.18066477775574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4232633709907532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.283250093460083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.83875846862793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.339540481567383
3 1.9280595101 	 28.3395402238 	 28.3395481419
epoch_time;  52.71325373649597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5503499507904053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6437065601348877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.161344528198242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.96288299560547
4 1.7989344096 	 30.9628827069 	 30.9629117399
epoch_time;  54.370320320129395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3382699191570282
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3272182941436768
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.807571411132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.21544647216797
5 1.7205272512 	 31.2154455236 	 31.2156434755
epoch_time;  54.8830201625824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30434659123420715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3388922214508057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.82829475402832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.50060272216797
6 1.6785897763 	 34.500604413 	 34.5011797931
epoch_time;  52.196239948272705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4272700548171997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.443160057067871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.140575408935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.78771209716797
7 1.6386318736 	 34.7877111486 	 34.7885372677
epoch_time;  44.673630475997925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36518892645835876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.482182502746582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.761404037475586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.84799575805664
8 1.621961879 	 37.8479940878 	 37.8491765203
epoch_time;  39.996901988983154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28161290287971497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2433056831359863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.206327438354492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.70967483520508
9 1.5988862096 	 35.7096758868 	 35.7112726985
epoch_time;  39.78631091117859
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2874336540699005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2109639644622803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.032190322875977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.23802185058594
10 1.5832679867 	 37.2380199535 	 37.2401525549
epoch_time;  39.394859790802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2932701110839844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.385453939437866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.89845085144043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.89160919189453
11 1.575313829 	 38.8916094806 	 38.894251478
epoch_time;  41.332109689712524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.323646605014801
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.417790174484253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.01333999633789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.39707565307617
12 1.5674646335 	 37.3970755912 	 37.3999023438
epoch_time;  38.6524338722229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31190457940101624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3458144664764404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.088031768798828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.30643081665039
13 1.5607817591 	 36.3064294764 	 36.3095993454
epoch_time;  40.04464840888977
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3217485547065735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3622612953186035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.72414207458496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.46436309814453
14 1.55101101 	 37.4643633868 	 37.4678156672
epoch_time;  40.634326457977295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3149246573448181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4353065490722656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.16083526611328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.848087310791016
15 1.5467844184 	 38.8480864654 	 38.8521695524
epoch_time;  39.97189211845398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2762724757194519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.239295721054077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.74275779724121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.9151725769043
16 1.5438221678 	 39.9151736698 	 39.9199245144
epoch_time;  41.580421686172485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23439642786979675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1886346340179443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.73911476135254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.554378509521484
17 1.5407630543 	 38.5543786951 	 38.5593802787
epoch_time;  39.4211790561676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2712884247303009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.303514242172241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.798864364624023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.14437484741211
18 1.536663244 	 40.1443755279 	 40.149535473
epoch_time;  39.33101463317871
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29216864705085754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.29404354095459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.452653884887695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.99824523925781
19 1.5324189658 	 37.9982448269 	 38.0039247255
epoch_time;  38.93740272521973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29230570793151855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2939531803131104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.440263748168945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.015716552734375
It took 920.8217101097107 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▁▁▄▅▆█▇▇▇▇▇██▇██▇▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▁█▅▃▆▂▂▃▃▄▃▃▂▃▄▂▅▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▁▁▄▆▆█▇▇▇▇▇██▇██▇▇▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▆▅▂▄▂▁▂▂▂▂▁▁▂▂▁▂▂▂▂
wandb:                         Train loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 47.32657
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.27543
wandb:    Test loss t(0, 0)_r(-5, 5)_none 26.56757
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30843
wandb:                         Train loss 1.54092
wandb: 
wandb: 🚀 View run flashing-rat-1225 at: https://wandb.ai/nreints/thesis/runs/g1twp2ha
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134141-g1twp2ha/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135540-x4hkb524
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-fuse-1234
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/x4hkb524
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7205978035926819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2801365852355957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.901025772094727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.035587310791016
0 4.5150914279 	 34.0355891047 	 34.0369167019
epoch_time;  39.26553153991699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44953981041908264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0152952671051025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.625288009643555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.569677352905273
1 2.7234169373 	 25.5696764147 	 25.5697001689
epoch_time;  39.1665620803833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5986296534538269
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.765634059906006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.681112289428711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.422197341918945
2 2.1794145981 	 25.4221970017 	 25.422228674
epoch_time;  38.474403381347656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4944441616535187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.474736213684082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.837617874145508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.77839660644531
3 1.9118230842 	 34.7783968539 	 34.7784074113
epoch_time;  38.43010330200195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3089289963245392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.210970163345337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.22669792175293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.248497009277344
4 1.7869057316 	 41.2484982052 	 41.2486697635
epoch_time;  38.97831082344055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4494018852710724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.501939535140991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.46952247619629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.51450729370117
5 1.7185136282 	 42.5145085515 	 42.5150469806
epoch_time;  38.996171712875366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2756218910217285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.071107864379883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.04786491394043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.66909408569336
6 1.6760920045 	 49.6690931166 	 49.6704814189
epoch_time;  39.42693638801575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26294973492622375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.106544017791748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.967844009399414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.92634963989258
7 1.6394219834 	 47.9263513514 	 47.9283519848
epoch_time;  39.22670817375183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28882813453674316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.237067699432373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.725614547729492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.02326202392578
8 1.6271129652 	 47.0232633024 	 47.0259501689
epoch_time;  38.7590651512146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29553326964378357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2543022632598877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.0712833404541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.25663375854492
9 1.6106715735 	 48.2566353463 	 48.259802576
epoch_time;  38.94910979270935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2973799705505371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.329868793487549
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.40333366394043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.07098388671875
10 1.5951358328 	 49.070982897 	 49.0747571791
epoch_time;  38.80097436904907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3033480942249298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2822659015655518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.007732391357422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.28820037841797
11 1.5869064589 	 48.2882020693 	 48.2922086149
epoch_time;  39.41197752952576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23329344391822815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.228908061981201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.53874397277832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.900089263916016
12 1.5755706973 	 50.9000897382 	 50.9049619932
epoch_time;  39.32761883735657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24625013768672943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1632442474365234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.02256965637207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.91044616699219
13 1.5724976991 	 49.9104465794 	 49.9156302787
epoch_time;  39.57409596443176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3170105814933777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.235072374343872
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.76021385192871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.071083068847656
14 1.5641656771 	 48.0710831926 	 48.0760135135
epoch_time;  40.22410297393799
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33243441581726074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3670639991760254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.829740524291992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.1795539855957
15 1.5578256244 	 49.1795555321 	 49.1850242821
epoch_time;  39.74565935134888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2565089166164398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.166351079940796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.348419189453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 50.753135681152344
16 1.5539038367 	 50.7531355574 	 50.7596758868
epoch_time;  39.78427171707153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2746516168117523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3914501667022705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.210323333740234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.65077209472656
17 1.5510189441 	 48.6507706926 	 48.6568253801
epoch_time;  39.27866530418396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2938533127307892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2879695892333984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.94767951965332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.07496643066406
18 1.547133356 	 48.0749683277 	 48.0810335726
epoch_time;  39.856569051742554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3082353472709656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2745583057403564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.572025299072266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.34499740600586
19 1.5409200845 	 47.344995777 	 47.3516627956
epoch_time;  39.372657775878906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3084271550178528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.275428295135498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.567567825317383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.326568603515625
It took 838.8433725833893 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▁▄▅▆█▆▇▇▆▆▆▅▅▆▅▅▆▇▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▁▄▄▄▇▄▅▄▇▅▆█▅▅▆▄▄▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▁▅▅▆█▆▆▆▆▅▅▄▄▄▄▄▄▅▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▄▃▂▄▃▂▂▂▂▂▅▁▂▂▁▂▂▁▁
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 42.06622
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.48276
wandb:    Test loss t(0, 0)_r(-5, 5)_none 24.18816
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.21731
wandb:                         Train loss 1.56463
wandb: 
wandb: 🚀 View run fortuitous-fuse-1234 at: https://wandb.ai/nreints/thesis/runs/x4hkb524
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135540-x4hkb524/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140943-do6x4s0m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-fireworks-1242
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/do6x4s0m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4954940676689148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.291567802429199
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.992536544799805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.112403869628906
0 5.5092981709 	 38.1124023437 	 38.1147487331
epoch_time;  39.164228439331055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46944424510002136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2415881156921387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.803388595581055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.137989044189453
1 2.5853938449 	 30.1379882812 	 30.1380173142
epoch_time;  39.67948389053345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33659929037094116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5728867053985596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.64823341369629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.29465103149414
2 2.0874258156 	 38.2946526605 	 38.2946684966
epoch_time;  38.58464789390564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3130235970020294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.508814811706543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.507400512695312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.39295959472656
3 1.8812850095 	 39.3929608319 	 39.3930294552
epoch_time;  38.58876848220825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2718050181865692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5865352153778076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.108572006225586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.93787384033203
4 1.7698851465 	 42.9378747889 	 42.938207348
epoch_time;  38.87135672569275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34248730540275574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.8202009201049805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.51226806640625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.467613220214844
5 1.7185098265 	 46.467615076 	 46.4682590794
epoch_time;  39.28341341018677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27880793809890747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.567415475845337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.559358596801758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.366249084472656
6 1.6836415202 	 42.3662478885 	 42.3670001056
epoch_time;  38.994497776031494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2551402449607849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.650784492492676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.551008224487305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.775020599365234
7 1.6594233322 	 43.7750211149 	 43.7762114654
epoch_time;  38.239720821380615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24968008697032928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5750412940979004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.151809692382812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.39582824707031
8 1.6441203268 	 43.3958271748 	 43.3972999367
epoch_time;  38.71792268753052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2731935977935791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.870922803878784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.204788208007812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.882747650146484
9 1.6272476806 	 42.8827491554 	 42.8845782306
epoch_time;  39.04340672492981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24294044077396393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.608915328979492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.197307586669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.02925491333008
10 1.616987989 	 42.0292546453 	 42.031463788
epoch_time;  38.64640474319458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25555649399757385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7338783740997314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.24711799621582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.33913803100586
11 1.6028646935 	 42.3391390414 	 42.3421980574
epoch_time;  38.795026779174805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37494412064552307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9525015354156494
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.175119400024414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.82707214355469
12 1.5975927767 	 38.8270718961 	 38.8296743032
epoch_time;  38.53686332702637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2277223765850067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.683692216873169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.09269905090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.49801254272461
13 1.5930395624 	 40.4980125633 	 40.5014754012
epoch_time;  39.614444971084595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23818416893482208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6059348583221436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.13170623779297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.10785675048828
14 1.5898085422 	 41.1078573691 	 41.1120882601
epoch_time;  39.103336572647095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25170058012008667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7226133346557617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.798664093017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.31232833862305
15 1.5860565124 	 40.3123284417 	 40.3166279561
epoch_time;  39.04419779777527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21998897194862366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5741093158721924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.541133880615234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.415897369384766
16 1.5755078852 	 40.4158968539 	 40.4204840583
epoch_time;  38.91381239891052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.258985310792923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5492937564849854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.657169342041016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.942909240722656
17 1.5744559482 	 40.9429080448 	 40.9478383657
epoch_time;  38.80286478996277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24279190599918365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5684993267059326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.21816062927246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.25456237792969
18 1.5695896652 	 43.2545608108 	 43.2605257601
epoch_time;  38.514822483062744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2172013223171234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4829323291778564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.19684600830078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.09270095825195
19 1.5646323047 	 42.0926995355 	 42.0989732897
epoch_time;  38.91262626647949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2173064798116684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4827630519866943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.188159942626953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.06621551513672
It took 843.2460360527039 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▁▂▄▅▆▆▇▇▇▇█▇▇▇▇▇▇▇██
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▆▅▁▂▅▅▄█▆▅▅█▆▆▅▆▄▆▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▁▃▄▅▆▆▇▇▇▇█▇▇▇▇▇▇▇██
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▂▁▃▂▂▃▂▂▂▂▂▂▁▂▁▂▁▁
wandb:                         Train loss █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 45.43996
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.23086
wandb:    Test loss t(0, 0)_r(-5, 5)_none 27.70271
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.21924
wandb:                         Train loss 1.54102
wandb: 
wandb: 🚀 View run lambent-fireworks-1242 at: https://wandb.ai/nreints/thesis/runs/do6x4s0m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140943-do6x4s0m/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142505-5nfl5i7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-tiger-1250
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5nfl5i7r
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8261953592300415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.58200740814209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.781932830810547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.25030517578125
0 4.5200023406 	 36.2503061655 	 36.2520850929
epoch_time;  38.99339294433594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6131206750869751
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.460787057876587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.185481071472168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.493013381958008
1 2.6430356614 	 21.4930136191 	 21.4930254962
epoch_time;  53.33328557014465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4522419571876526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.366114377975464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.172550201416016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.09046745300293
2 2.1377551493 	 26.0904666385 	 26.0904824747
epoch_time;  53.46424651145935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26973679661750793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0017600059509277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.857999801635742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.526527404785156
3 1.8953896266 	 31.5265281883 	 31.5266443201
epoch_time;  53.38228440284729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23462611436843872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0834085941314697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.510623931884766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.411495208740234
4 1.7824192964 	 34.4114944046 	 34.4118269637
epoch_time;  54.08614253997803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41113024950027466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.347283363342285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.357688903808594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.81700134277344
5 1.7147857315 	 38.8170027449 	 38.8177655194
epoch_time;  53.11549711227417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32074597477912903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.409790515899658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.707204818725586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.0538330078125
6 1.6689172124 	 40.053832348 	 40.0551863387
epoch_time;  41.32134747505188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2715894877910614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3323886394500732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.507909774780273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.469390869140625
7 1.6401874643 	 42.469391364 	 42.4712521115
epoch_time;  40.63678312301636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39763665199279785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6580722332000732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.599609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.660587310791016
8 1.6229052172 	 42.6605864654 	 42.6628747889
epoch_time;  39.614121437072754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30141422152519226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5014960765838623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.871692657470703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.80392837524414
9 1.6118484735 	 41.8039300042 	 41.8071209882
epoch_time;  39.12039637565613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30161744356155396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3811116218566895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.6905517578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.953369140625
10 1.5958884206 	 42.9533678209 	 42.9569599873
epoch_time;  40.94168257713318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3155890107154846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4190609455108643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.92680549621582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.9919548034668
11 1.5824484544 	 44.9919552365 	 44.9960673564
epoch_time;  39.02701282501221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.331119567155838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.676086187362671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.022069931030273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.995567321777344
12 1.5753053989 	 41.9955658784 	 42.0000976562
epoch_time;  41.7635281085968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.289434552192688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.437729835510254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.84868049621582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.07384490966797
13 1.5653948103 	 42.0738466005 	 42.078972234
epoch_time;  39.928459882736206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2986099421977997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4579873085021973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.545209884643555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.067115783691406
14 1.5625523985 	 43.0671162373 	 43.0726351351
epoch_time;  38.920385122299194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2598511874675751
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3866820335388184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.027050018310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.46857452392578
15 1.5574971834 	 42.4685758024 	 42.4743771115
epoch_time;  40.31471562385559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29037225246429443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.478996992111206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.311063766479492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.880435943603516
16 1.5499003933 	 42.8804370777 	 42.886607897
epoch_time;  39.81063985824585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23636029660701752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.311178207397461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.84088706970215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.24626922607422
17 1.5466287105 	 42.2462679476 	 42.2523173564
epoch_time;  39.943965911865234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28596851229667664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.441176414489746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.894668579101562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.467716217041016
18 1.5478104041 	 42.467718011 	 42.4746252111
epoch_time;  39.16948127746582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2192453145980835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.223346710205078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.684083938598633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.41242218017578
19 1.5410220715 	 45.4124208193 	 45.4200696791
epoch_time;  38.98131585121155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2192389965057373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2308578491210938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.702707290649414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.439964294433594
It took 921.8094806671143 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▁▂▄▄▆▅▇▆▇█▇▇▇█▇██▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄█▆▂▃▁▂▃▃▃▂▃▃▃▂▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▁▂▃▄▆▅▇▇▇██▇▇█▇██▇▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▅▂▃▁▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 35.93348
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.23293
wandb:    Test loss t(0, 0)_r(-5, 5)_none 21.21972
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.28916
wandb:                         Train loss 1.54764
wandb: 
wandb: 🚀 View run sweet-tiger-1250 at: https://wandb.ai/nreints/thesis/runs/5nfl5i7r
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142505-5nfl5i7r/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8503423929214478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5202574729919434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.857221603393555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.102148056030273
0 4.8979260775 	 31.1021484375 	 31.1035393792
epoch_time;  38.91029453277588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7341601252555847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.120175838470459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.860575675964355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.356487274169922
1 2.6357097897 	 19.3564875422 	 19.3565838788
epoch_time;  38.96659708023071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.626763641834259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.874906301498413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.84736442565918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.54878044128418
2 2.1388393188 	 21.5487806166 	 21.5488967483
epoch_time;  38.77457594871521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32885652780532837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1451504230499268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.984206199645996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.666433334350586
3 1.899073059 	 26.6664326436 	 26.6664563978
epoch_time;  38.717097759246826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3792552947998047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.265871047973633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.261707305908203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.174150466918945
4 1.7753127711 	 27.1741501267 	 27.1741633235
epoch_time;  38.73737645149231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2779425382614136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.004234790802002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.122194290161133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.217979431152344
5 1.7139680174 	 34.2179793074 	 34.2182062922
epoch_time;  39.16414499282837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32988640666007996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1153171062469482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.698894500732422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.821104049682617
6 1.6702980863 	 30.8211043074 	 30.8215371622
epoch_time;  38.76495909690857
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36326488852500916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.398432970046997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.899744033813477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.7354621887207
7 1.6446069551 	 36.7354624155 	 36.7361987965
epoch_time;  38.80935502052307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31345435976982117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3351025581359863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.5640811920166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.76759338378906
8 1.6226800855 	 34.7675939611 	 34.7686127534
epoch_time;  38.75851511955261
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3612678647041321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3221678733825684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.875593185424805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.11748504638672
9 1.6106659535 	 37.1174857475 	 37.1188872466
epoch_time;  38.80816054344177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28592434525489807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1458544731140137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.91083526611328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.44695281982422
10 1.5968120887 	 38.4469541807 	 38.4487753378
epoch_time;  39.04332447052002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31621643900871277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.259063243865967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.21484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.28132247924805
11 1.5859528724 	 37.281323902 	 37.2831345017
epoch_time;  39.39495277404785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3364211618900299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.374927282333374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.760591506958008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.09907531738281
12 1.5835086562 	 37.0990762247 	 37.1010715794
epoch_time;  39.3039767742157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30263790488243103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2945756912231445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.410478591918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.733455657958984
13 1.5690098106 	 36.733453864 	 36.7358108108
epoch_time;  39.20541048049927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2564364969730377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1450862884521484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.59960174560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.406978607177734
14 1.5654943178 	 38.4069784628 	 38.4096204603
epoch_time;  39.199151039123535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2537018954753876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1711926460266113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.722070693969727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.9822883605957
15 1.5548547785 	 35.9822872677 	 35.9850269215
epoch_time;  39.239129304885864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2525375783443451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.100743532180786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.385644912719727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.001708984375
16 1.5534717557 	 39.0017076647 	 39.004938239
epoch_time;  39.093666076660156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24716372787952423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1644835472106934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.890254974365234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.34601593017578
17 1.5469920288 	 39.3460145693 	 39.349432538
epoch_time;  39.43394899368286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26980894804000854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.144012212753296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.438072204589844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.557979583740234
18 1.548914409 	 37.5579814189 	 37.5615683066
epoch_time;  39.06409692764282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2891024947166443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.234569549560547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.224882125854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.94157409667969
19 1.5476406462 	 35.9415751689 	 35.9451435811
epoch_time;  39.21586012840271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2891642451286316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.232933521270752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.219715118408203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.933475494384766
It took 838.8409290313721 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137935
Array Job ID: 2137927_5
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-19:21:54 core-walltime
Job Wall-clock time: 02:24:33
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
