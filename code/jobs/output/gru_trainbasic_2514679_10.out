wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_124733-hhetcdqe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-donkey-482
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/hhetcdqe
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(5, 20)_r(0, 0)_full_pNone_gNone, MSELoss() █▂▁▁▁▁▁▁▁▁▁
wandb:                                             Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(5, 20)_r(0, 0)_full_pNone_gNone, MSELoss() 0.00043
wandb:                                             Train loss 0.00051
wandb: 
wandb: 🚀 View run robust-donkey-482 at: https://wandb.ai/nreints/test/runs/hhetcdqe
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_124733-hhetcdqe/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_125542-11bspcb0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-dream-494
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/11bspcb0
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(5, 20)_r(0, 0)_full_pNone_gNone, MSELoss() █▂▁▁▁▁▁▁▁▁▁
wandb:                                             Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(5, 20)_r(0, 0)_full_pNone_gNone, MSELoss() 0.00054
wandb:                                             Train loss 0.00067
wandb: 
wandb: 🚀 View run ancient-dream-494 at: https://wandb.ai/nreints/test/runs/11bspcb0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_125542-11bspcb0/logs
Running for data type: quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.7175980735 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.23550257086753845 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 45.86383557319641
Epoch 1
	 Logging train Loss: 0.096300856 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.03275306150317192 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 43.69901371002197
Epoch 2
	 Logging train Loss: 0.0191536434 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.010661321692168713 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 43.86741065979004
Epoch 3
	 Logging train Loss: 0.0074110477 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.005052976310253143 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.280794620513916
Epoch 4
	 Logging train Loss: 0.0037785585 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0028272680938243866 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 40.88776755332947
Epoch 5
	 Logging train Loss: 0.0021901198 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.00168679712805897 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.164875507354736
Epoch 6
	 Logging train Loss: 0.0013894938 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0011623607715591788 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.541555404663086
Epoch 7
	 Logging train Loss: 0.0009466172 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0011131091741845012 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.320353746414185
Epoch 8
	 Logging train Loss: 0.0006917846 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0004929823917336762 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.50369143486023
Epoch 9
	 Logging train Loss: 0.000511283 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.00043429710785858333 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.21431756019592
	 Logging test loss: 0.0004345406196080148 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
It took  490.22352480888367  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.7338831332 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.26651424169540405 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 42.191155195236206
Epoch 1
	 Logging train Loss: 0.1132765236 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0477728396654129 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.87010979652405
Epoch 2
	 Logging train Loss: 0.0248932476 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.01601414941251278 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.51981544494629
Epoch 3
	 Logging train Loss: 0.0100450096 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.008050570264458656 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.7795090675354
Epoch 4
	 Logging train Loss: 0.0052866961 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0043946439400315285 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.59512281417847
Epoch 5
	 Logging train Loss: 0.0030321133 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0030050906352698803 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.652058362960815
Epoch 6
	 Logging train Loss: 0.001901804 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0015755672939121723 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 42.037890911102295
Epoch 7
	 Logging train Loss: 0.0012555445 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0010267409961670637 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.93719792366028
Epoch 8
	 Logging train Loss: 0.0009177259 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0008459532400593162 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 41.71414375305176
Epoch 9
	 Logging train Loss: 0.0006688267 (MSELoss(): data_t(5, 20)_r(0, 0)_full_pNone_gNone)
	 Logging test loss: 0.0005404145340435207 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
     --> Epoch time; 44.55503797531128
	 Logging test loss: 0.0005385460099205375 (MSELoss(): t(5, 20)_r(0, 0)_full_pNone_gNone)
It took  483.38842129707336  seconds.

JOB STATISTICS
==============
Job ID: 2514689
Array Job ID: 2514679_10
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:19:46
CPU Efficiency: 66.99% of 04:58:12 core-walltime
Job Wall-clock time: 00:16:34
Memory Utilized: 24.95 GB
Memory Efficiency: 79.85% of 31.25 GB
