/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170117-ggkatt0y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-rooster-1146
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/ggkatt0y
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.2386
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.197
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.12993
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10869
wandb:                         Train loss 2.23051
wandb: 
wandb: ðŸš€ View run glowing-rooster-1146 at: https://wandb.ai/nreints/thesis/runs/ggkatt0y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170117-ggkatt0y/logs
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35385337471961975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.802833080291748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3931150734424591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8737602233886719
0 5.7472792453 	 0.873760244 	 0.8924812111
epoch_time;  33.16230773925781
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20381669700145721
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5293761491775513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22548867762088776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5636095404624939
1 2.7590361784 	 0.5636095614 	 0.5779481218
epoch_time;  33.10294461250305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1500263661146164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33440250158309937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17465870082378387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3755790889263153
2 2.5412979488 	 0.3755790917 	 0.3908141265
epoch_time;  32.31987404823303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1633913218975067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35623204708099365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19405414164066315
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40514102578163147
3 2.456775704 	 0.405141016 	 0.4203554308
epoch_time;  32.92593479156494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15599088370800018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29037365317344666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18931548297405243
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34366798400878906
4 2.4100898014 	 0.3436679943 	 0.3561577049
epoch_time;  32.82023572921753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14690040051937103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2869769036769867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17353594303131104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32372698187828064
5 2.365986467 	 0.323726984 	 0.3354235056
epoch_time;  32.30111122131348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12804003059864044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23443110287189484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16361841559410095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29250892996788025
6 2.345057483 	 0.2925089243 	 0.3052452087
epoch_time;  32.447699546813965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21142037212848663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33208245038986206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21373683214187622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35343682765960693
7 2.3247369496 	 0.3534368154 	 0.3639611322
epoch_time;  32.26439666748047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15713119506835938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28228411078453064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19298207759857178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3428686261177063
8 2.303771766 	 0.34286864 	 0.353858123
epoch_time;  32.434598445892334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14526548981666565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2695237994194031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17527863383293152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3256404399871826
9 2.2912999323 	 0.3256404361 	 0.3365810291
epoch_time;  32.57484436035156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16864444315433502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2769142687320709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20338402688503265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32909733057022095
10 2.2735241145 	 0.3290973354 	 0.3393156722
epoch_time;  32.643030881881714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16263291239738464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32840946316719055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1998308300971985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38256222009658813
11 2.2696968755 	 0.3825622146 	 0.3939208984
epoch_time;  32.550424337387085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1304868906736374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2540571689605713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1695624440908432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3178451359272003
12 2.2648251378 	 0.3178451332 	 0.3293585906
epoch_time;  32.19377660751343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15168210864067078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2943348288536072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18540187180042267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3396608233451843
13 2.2577394396 	 0.3396608301 	 0.3499503471
epoch_time;  33.089330434799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12371724098920822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2564792335033417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14203307032585144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28827667236328125
14 2.2517582423 	 0.2882766724 	 0.3004660117
epoch_time;  33.32466387748718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15792600810527802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2699495851993561
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17765368521213531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31819066405296326
15 2.2463816938 	 0.3181906623 	 0.3284966752
epoch_time;  33.808974504470825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13105782866477966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24903596937656403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16312117874622345
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2983566224575043
16 2.2455537314 	 0.2983566284 	 0.3088739962
epoch_time;  32.784937620162964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1677049845457077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28334084153175354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20119638741016388
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34418633580207825
17 2.23746023 	 0.3441863395 	 0.3532490498
epoch_time;  32.22749638557434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1394442319869995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28560346364974976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1763952076435089
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33695077896118164
18 2.2381559564 	 0.3369507867 	 0.3477882591
epoch_time;  32.36563515663147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10867121815681458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19694072008132935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12995025515556335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23865708708763123
19 2.2305066025 	 0.2386570905 	 0.2490580172
epoch_time;  32.132885217666626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10868930071592331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19699877500534058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12993107736110687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23859819769859314
It took 714.1961030960083 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn41: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135404.0

JOB STATISTICS
==============
Job ID: 2135404
Array Job ID: 2135328_17
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:38:42 core-walltime
Job Wall-clock time: 00:12:09
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
