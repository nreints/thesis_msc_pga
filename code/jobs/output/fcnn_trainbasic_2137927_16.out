wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144101-id0m2yod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-bao-1259
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/id0m2yod
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▃▃▃▂▂▃▂▂▁▁▂▂▂▂▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▄▂▂▂▃▂▂▁▂▁▂▂▃▂▁▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▄▄▃▂▃▃▃▂▁▁▂▃▃▃▂▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▆▆▂▃▃▄▄▃▂▂▂▄▃▅▂▁▄▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.37811
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32525
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25072
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20819
wandb:                         Train loss 2.46348
wandb: 
wandb: 🚀 View run dazzling-bao-1259 at: https://wandb.ai/nreints/thesis/runs/id0m2yod
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144101-id0m2yod/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_145249-hn3frl6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-mandu-1265
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/hn3frl6q
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32322028279304504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.75802081823349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3896832764148712
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9052867889404297
0 4.5472012922 	 0.9052867993 	 0.9052867993
epoch_time;  32.09932231903076
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3162596523761749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6507281064987183
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4403481185436249
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8308634161949158
1 2.8233559104 	 0.8308634165 	 0.8308634165
epoch_time;  30.553639888763428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28515303134918213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48794856667518616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3198561668395996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5611600279808044
2 2.6991600703 	 0.5611600309 	 0.5611600309
epoch_time;  30.321619272232056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2784900367259979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4722067415714264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3328545093536377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5593100786209106
3 2.6323205823 	 0.5593100883 	 0.5593100883
epoch_time;  30.389380931854248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20225384831428528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3728458285331726
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2928144037723541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5070230960845947
4 2.5940949705 	 0.5070230845 	 0.5070230845
epoch_time;  30.318025588989258
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2193184494972229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3608587682247162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26541855931282043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43204253911972046
5 2.5699713312 	 0.4320425498 	 0.4320425498
epoch_time;  30.305039644241333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2236730009317398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36925315856933594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2905280292034149
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4621680676937103
6 2.556336317 	 0.4621680595 	 0.4621680595
epoch_time;  30.524011850357056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23488198220729828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43664106726646423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2938373386859894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5305033922195435
7 2.5400899733 	 0.5305033916 	 0.5305033916
epoch_time;  30.603783130645752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23848140239715576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37755414843559265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29393041133880615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46275651454925537
8 2.5365367933 	 0.4627565126 	 0.4627565126
epoch_time;  30.716442108154297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2240568846464157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35705435276031494
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28041020035743713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43820226192474365
9 2.5209789031 	 0.4382022755 	 0.4382022755
epoch_time;  31.44966745376587
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19343313574790955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3144218325614929
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2555996775627136
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40450820326805115
10 2.5115359338 	 0.4045081886 	 0.4045081886
epoch_time;  31.915802001953125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20916281640529633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34159961342811584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25084689259529114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40432310104370117
11 2.5073761214 	 0.4043231036 	 0.4043231036
epoch_time;  30.71992254257202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1986357420682907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32271236181259155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27074679732322693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4206785559654236
12 2.4959866389 	 0.420678546 	 0.420678546
epoch_time;  30.845176696777344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23190130293369293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38876909017562866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2863980531692505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45634710788726807
13 2.4932531565 	 0.4563471201 	 0.4563471201
epoch_time;  30.54585886001587
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22810567915439606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3732614517211914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29953649640083313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4752533733844757
14 2.4866256559 	 0.4752533784 	 0.4752533784
epoch_time;  30.53859257698059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26466724276542664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.400598406791687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30009323358535767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45143139362335205
15 2.4827213894 	 0.4514313981 	 0.4514313981
epoch_time;  30.453911066055298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20856764912605286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33691784739494324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26941728591918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42411017417907715
16 2.4770238382 	 0.4241101652 	 0.4241101652
epoch_time;  30.607227087020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1785462200641632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2918514907360077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24287386238574982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37730973958969116
17 2.4711955386 	 0.377309727 	 0.377309727
epoch_time;  30.401971578598022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24762247502803802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3729720711708069
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27065661549568176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41088709235191345
18 2.4682822153 	 0.4108871048 	 0.4108871048
epoch_time;  30.718656539916992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2081855982542038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.325361043214798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2506452798843384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.378131240606308
19 2.4634820503 	 0.3781312272 	 0.3781312272
epoch_time;  30.840767860412598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.208185076713562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3252483606338501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2507232427597046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3781057298183441
It took 708.0460424423218 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▃▃▄▃▂▂▂▂▁▁▂▂▁▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▄▄▅▃▂▃▃▂▁▁▂▃▂▃▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▃▅█▄▂▃▂▂▁▁▂▃▂▄▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▃▇▅▅█▃▃▅▄▃▁▂▃▄▂▅▃▁▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.35661
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.31892
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25106
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2045
wandb:                         Train loss 2.43813
wandb: 
wandb: 🚀 View run lambent-mandu-1265 at: https://wandb.ai/nreints/thesis/runs/hn3frl6q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_145249-hn3frl6q/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150411-9m0834fe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-fireworks-1272
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9m0834fe
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28393691778182983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7252944707870483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3821081221103668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8233613967895508
0 4.4687929767 	 0.823361371 	 0.823361371
epoch_time;  30.1125807762146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21174563467502594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5075322985649109
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33018237352371216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6172807216644287
1 2.8221072724 	 0.6172807436 	 0.6172807436
epoch_time;  29.925752639770508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2769607901573181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5491054654121399
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33437782526016235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5989887714385986
2 2.6958268979 	 0.5989887959 	 0.5989887959
epoch_time;  30.316477060317993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24281558394432068
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45163798332214355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29054880142211914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4901306629180908
3 2.6260537552 	 0.4901306565 	 0.4901306565
epoch_time;  30.29930877685547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24859844148159027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.433505117893219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31824150681495667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49986159801483154
4 2.5830466568 	 0.4998615987 	 0.4998615987
epoch_time;  30.302263975143433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2999955117702484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5080016255378723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37456727027893066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5781469941139221
5 2.5590907919 	 0.5781469809 	 0.5781469809
epoch_time;  30.511235237121582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2146623581647873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3765028715133667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2974036633968353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45363757014274597
6 2.536707543 	 0.4536375716 	 0.4536375716
epoch_time;  30.50536060333252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20236678421497345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3529840111732483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27378779649734497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41672149300575256
7 2.5245929116 	 0.4167214883 	 0.4167214883
epoch_time;  30.375499725341797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23584622144699097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37257322669029236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2777744233608246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4053502380847931
8 2.510846158 	 0.4053502263 	 0.4053502263
epoch_time;  30.54522943496704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22606799006462097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3662499189376831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2690832316875458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40428581833839417
9 2.5010044973 	 0.4042858227 	 0.4042858227
epoch_time;  30.257702589035034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2122352570295334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3410365581512451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26147812604904175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3915170133113861
10 2.487071291 	 0.3915170206 	 0.3915170206
epoch_time;  30.247243642807007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17254361510276794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2646763026714325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25380635261535645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3442358672618866
11 2.4868699618 	 0.3442358687 	 0.3442358687
epoch_time;  30.02968716621399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18740731477737427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2899925708770752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25497812032699585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35764989256858826
12 2.4752379917 	 0.3576498908 	 0.3576498908
epoch_time;  30.19331431388855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20084261894226074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3451530337333679
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25663790106773376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39375174045562744
13 2.4699221064 	 0.3937517321 	 0.3937517321
epoch_time;  30.26115131378174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2328716218471527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3659224212169647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2880038619041443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4155041575431824
14 2.4620005118 	 0.4155041669 	 0.4155041669
epoch_time;  30.284088134765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19338662922382355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30410996079444885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25917327404022217
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3678036332130432
15 2.4585703112 	 0.3678036252 	 0.3678036252
epoch_time;  30.395365953445435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2513357102870941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3926624059677124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2993066608905792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43903475999832153
16 2.4479234989 	 0.4390347455 	 0.4390347455
epoch_time;  30.961973905563354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20375728607177734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3219963312149048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2651965916156769
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3784424364566803
17 2.4398924789 	 0.3784424241 	 0.3784424241
epoch_time;  31.600216150283813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1683795154094696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2966741621494293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24643351137638092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37354254722595215
18 2.4357705191 	 0.3735425382 	 0.3735425382
epoch_time;  30.460714101791382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2045283168554306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3189433217048645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2510845959186554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.356619268655777
19 2.4381301043 	 0.3566192627 	 0.3566192627
epoch_time;  31.30490231513977
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20450355112552643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3189201056957245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.251059353351593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35661327838897705
It took 681.9156587123871 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▃▃▂▂▂▃▂▁▂▂▁▂▁▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▃▃▂▂▂▃▃▁▂▂▁▂▁▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▄▅▄▃▃▄▅▃▁▂▃▂▂▂▃▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▆▆▄▅▃▂▄▄▄▁▂▃▁▃▂▃▄▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.43971
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34506
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26361
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18324
wandb:                         Train loss 2.4553
wandb: 
wandb: 🚀 View run chromatic-fireworks-1272 at: https://wandb.ai/nreints/thesis/runs/9m0834fe
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150411-9m0834fe/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151558-g805vbtl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-ox-1278
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g805vbtl
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3263336718082428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7306167483329773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3940757215023041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8306284546852112
0 4.4276646205 	 0.8306284312 	 0.8306284312
epoch_time;  32.938095569610596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24275358021259308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5987415909767151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31832534074783325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6949318647384644
1 2.8240594057 	 0.6949318551 	 0.6949318551
epoch_time;  32.94745492935181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.273527055978775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5649316906929016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32935279607772827
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6452211737632751
2 2.7054191285 	 0.6452211947 	 0.6452211947
epoch_time;  32.73592948913574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27079829573631287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4974295198917389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3008325397968292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5495002269744873
3 2.6393899162 	 0.5495002128 	 0.5495002128
epoch_time;  32.74070453643799
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24077633023262024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4124647080898285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31760016083717346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5060876607894897
4 2.5952875735 	 0.5060876795 	 0.5060876795
epoch_time;  33.04094195365906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2534865438938141
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4349882900714874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29334530234336853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4997451901435852
5 2.5686349217 	 0.4997451782 	 0.4997451782
epoch_time;  33.239991188049316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20624858140945435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3424409329891205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2736417353153229
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42986881732940674
6 2.5497865051 	 0.4298688322 	 0.4298688322
epoch_time;  32.47896456718445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2007816582918167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.339257150888443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26692402362823486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42419636249542236
7 2.5324019404 	 0.4241963567 	 0.4241963567
epoch_time;  32.987929582595825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22493071854114532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3522910177707672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2961045801639557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43138018250465393
8 2.5270839064 	 0.4313801946 	 0.4313801946
epoch_time;  32.993977546691895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24201498925685883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3863334357738495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3289588987827301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48923662304878235
9 2.5096581833 	 0.4892366152 	 0.4892366152
epoch_time;  33.16992449760437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24324184656143188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40701624751091003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27639466524124146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4417184293270111
10 2.5023197492 	 0.4417184366 	 0.4417184366
epoch_time;  32.49722719192505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17271870374679565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2910008430480957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22473333775997162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36014407873153687
11 2.4972281692 	 0.3601440842 	 0.3601440842
epoch_time;  32.87682604789734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19330155849456787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3300435543060303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26008448004722595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41361677646636963
12 2.4912304423 	 0.4136167784 	 0.4136167784
epoch_time;  30.948625326156616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21257606148719788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3696698546409607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27685853838920593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4493752121925354
13 2.4811292686 	 0.4493752144 	 0.4493752144
epoch_time;  30.30808973312378
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16798827052116394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28434497117996216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2371804118156433
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36597830057144165
14 2.4788867121 	 0.3659783028 	 0.3659783028
epoch_time;  29.949119091033936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20459811389446259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35149675607681274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25310373306274414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40629661083221436
15 2.4716611745 	 0.4062966012 	 0.4062966012
epoch_time;  30.388978004455566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1838424652814865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2988505959510803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24503499269485474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37287968397140503
16 2.4628780626 	 0.3728796882 	 0.3728796882
epoch_time;  30.241790771484375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21348199248313904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3469642400741577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2740607261657715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4178693890571594
17 2.461355035 	 0.4178694029 	 0.4178694029
epoch_time;  30.25989294052124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22517186403274536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34995096921920776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2744906544685364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41742441058158875
18 2.4612747017 	 0.4174244236 	 0.4174244236
epoch_time;  30.235108852386475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18323838710784912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3451019823551178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2635878324508667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4397699236869812
19 2.4553027608 	 0.4397699305 	 0.4397699305
epoch_time;  30.3540940284729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18323886394500732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3450555205345154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2636091411113739
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43971186876296997
It took 707.0332450866699 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▄▃▂▃▃▃▂▁▂▂▁▂▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▅▄▃▂▄▂▃▁▂▃▁▂▁▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▄█▇▆▃▄▆▄▅▁▂▅▂▃▂▃▃▃▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.40199
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.326
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26704
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20541
wandb:                         Train loss 2.44836
wandb: 
wandb: 🚀 View run bright-ox-1278 at: https://wandb.ai/nreints/thesis/runs/g805vbtl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151558-g805vbtl/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_152712-8m3zjr10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-wonton-1284
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/8m3zjr10
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29814860224723816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.774616539478302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4007537066936493
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9215555787086487
0 4.5159833594 	 0.9215555552 	 0.9215555552
epoch_time;  30.881062507629395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23128926753997803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5460513830184937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3308367133140564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6736987829208374
1 2.8178208441 	 0.6736987965 	 0.6736987965
epoch_time;  31.298353910446167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31720224022865295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5788454413414001
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3500910997390747
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6322736740112305
2 2.7060014625 	 0.6322736998 	 0.6322736998
epoch_time;  29.932039260864258
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2923392653465271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48574957251548767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3430386781692505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5553898811340332
3 2.6466453723 	 0.5553898992 	 0.5553898992
epoch_time;  30.025663137435913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28056520223617554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4552469551563263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.322033166885376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5118920207023621
4 2.6055509016 	 0.5118919991 	 0.5118919991
epoch_time;  30.0841166973114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19120250642299652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3573666214942932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28650328516960144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47088566422462463
5 2.5757887213 	 0.4708856531 	 0.4708856531
epoch_time;  30.3152277469635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21692976355552673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39458799362182617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27561137080192566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4666898846626282
6 2.5608794484 	 0.4666898985 	 0.4666898985
epoch_time;  30.01432991027832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2713613510131836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4499787986278534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3074897825717926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4921965003013611
7 2.5369873873 	 0.4921964903 	 0.4921964903
epoch_time;  30.326643466949463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21644581854343414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38966119289398193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27248090505599976
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45317596197128296
8 2.5221721672 	 0.4531759726 	 0.4531759726
epoch_time;  30.15724015235901
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24915608763694763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.380893737077713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2873349189758301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42968136072158813
9 2.5129239154 	 0.4296813552 	 0.4296813552
epoch_time;  29.95896339416504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15672795474529266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28113165497779846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2532554566860199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3917441666126251
10 2.4999095837 	 0.3917441703 	 0.3917441703
epoch_time;  30.22870111465454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18711566925048828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35520291328430176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2785399854183197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46926212310791016
11 2.4935103554 	 0.469262118 	 0.469262118
epoch_time;  30.033021211624146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24589674174785614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36558327078819275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2829926013946533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4249342083930969
12 2.4868812019 	 0.4249342222 	 0.4249342222
epoch_time;  30.0719792842865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1745937019586563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30792173743247986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2454605996608734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38963383436203003
13 2.4781607368 	 0.3896338386 	 0.3896338386
epoch_time;  29.810425758361816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19490782916545868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.339122474193573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2666637599468231
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4187369644641876
14 2.4758935514 	 0.4187369682 	 0.4187369682
epoch_time;  30.036633729934692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1870022565126419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32545205950737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25055697560310364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.409839004278183
15 2.4693015892 	 0.4098389909 	 0.4098389909
epoch_time;  30.217905521392822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20223157107830048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34756091237068176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.253251314163208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40192532539367676
16 2.4666401754 	 0.4019253293 	 0.4019253293
epoch_time;  30.420358180999756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1965508610010147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.329004168510437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2532666325569153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39754074811935425
17 2.4546192662 	 0.3975407368 	 0.3975407368
epoch_time;  30.17520761489868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20480287075042725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34671202301979065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2531505823135376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40692636370658875
18 2.4570429859 	 0.4069263768 	 0.4069263768
epoch_time;  30.068909168243408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20538920164108276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3260294198989868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2670017182826996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4020218849182129
19 2.4483583898 	 0.402021872 	 0.402021872
epoch_time;  30.086628675460815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20540818572044373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3260016441345215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26704221963882446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4019855856895447
It took 673.9168019294739 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.152 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▄▄▃▄▂▂▂▂▃▂▃▃▁▂▁▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ██▄▄▄▅▂▂▂▃▃▂▃▂▁▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▇▅▃▅█▃▂▃▂▃▂▄▅▁▂▁▁▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▆▄▃▆█▂▃▂▃▃▂▅▃▁▃▂▂▃▂▂
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.40367
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32079
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26586
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19212
wandb:                         Train loss 2.45138
wandb: 
wandb: 🚀 View run prosperous-wonton-1284 at: https://wandb.ai/nreints/thesis/runs/8m3zjr10
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_152712-8m3zjr10/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_153825-12hy79em
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-firecracker-1291
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/12hy79em
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24389351904392242
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.662233293056488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3657168447971344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7680484056472778
0 4.4526787037 	 0.7680484256 	 0.7680484256
epoch_time;  30.043113231658936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2960507273674011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6685930490493774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3665214478969574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7081825137138367
1 2.821872223 	 0.708182505 	 0.708182505
epoch_time;  29.60392928123474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23066985607147217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46853381395339966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3319896459579468
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5656519532203674
2 2.6983884735 	 0.5656519709 	 0.5656519709
epoch_time;  29.89899492263794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21698255836963654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44918006658554077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2948775887489319
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.522892951965332
3 2.6381707007 	 0.5228929262 	 0.5228929262
epoch_time;  29.617560863494873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28821152448654175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4598502218723297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3320758640766144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4937531352043152
4 2.5987752967 	 0.4937531342 	 0.4937531342
epoch_time;  29.874481439590454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34379976987838745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5221160650253296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39112573862075806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5602255463600159
5 2.5741899887 	 0.5602255744 	 0.5602255744
epoch_time;  29.94683313369751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19260592758655548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3514288067817688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2900727689266205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44648900628089905
6 2.5556546407 	 0.4464889939 	 0.4464889939
epoch_time;  29.860058069229126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20992472767829895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36567771434783936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2687391936779022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42109763622283936
7 2.5427041128 	 0.4210976266 	 0.4210976266
epoch_time;  29.821686029434204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19123731553554535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.350890576839447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28852367401123047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4456978738307953
8 2.5262263562 	 0.4456978875 	 0.4456978875
epoch_time;  31.279319763183594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22131460905075073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37388694286346436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.272428959608078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42469045519828796
9 2.5188854427 	 0.4246904528 	 0.4246904528
epoch_time;  31.130340814590454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21855701506137848
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3925347924232483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2848023474216461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4547402560710907
10 2.50828607 	 0.4547402459 	 0.4547402459
epoch_time;  30.157185316085815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19759880006313324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35950395464897156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27722465991973877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44220107793807983
11 2.4959463069 	 0.442201068 	 0.442201068
epoch_time;  30.3420729637146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2574426531791687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4170304536819458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3009965717792511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46266183257102966
12 2.4921354321 	 0.4626618256 	 0.4626618256
epoch_time;  30.22171115875244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22006621956825256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3668650984764099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32540392875671387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48410969972610474
13 2.4806999976 	 0.4841097033 	 0.4841097033
epoch_time;  30.16312289237976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16755974292755127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28941425681114197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24639251828193665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36711764335632324
14 2.4766934136 	 0.3671176395 	 0.3671176395
epoch_time;  29.84605884552002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20648472011089325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3576829135417938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26900115609169006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42901748418807983
15 2.4703854281 	 0.4290174742 	 0.4290174742
epoch_time;  30.148155212402344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18828105926513672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32225140929222107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2502336800098419
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3826948404312134
16 2.4672664779 	 0.3826948424 	 0.3826948424
epoch_time;  29.917085647583008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18842457234859467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33083033561706543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25311169028282166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3938353955745697
17 2.4618175303 	 0.393835408 	 0.393835408
epoch_time;  30.137226581573486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2107783704996109
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3618861138820648
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2750537395477295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42629390954971313
18 2.4569586855 	 0.4262939041 	 0.4262939041
epoch_time;  29.934691905975342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1921028196811676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32087627053260803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2658609449863434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4036755859851837
19 2.4513839484 	 0.4036755948 	 0.4036755948
epoch_time;  30.146276235580444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19212163984775543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32078590989112854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26586341857910156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4036729633808136
It took 673.0498969554901 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▅▃▂▂▃▂▂▁▂▂▂▁▂▁▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▆▅▅▁▃▃▁▂▂▂▁▂▂▂▁▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆█▆▂▅▅▃▄▂▂▂▃▂▂▁▂▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▆███▂▆▅▂▃▂▂▁▂▄▄▁▃▃▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.43146
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32416
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26895
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18366
wandb:                         Train loss 2.4716
wandb: 
wandb: 🚀 View run chromatic-firecracker-1291 at: https://wandb.ai/nreints/thesis/runs/12hy79em
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_153825-12hy79em/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154935-ffbp1tla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-laughter-1298
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ffbp1tla
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27495211362838745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6774950623512268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36827266216278076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8450307250022888
0 4.498002576 	 0.8450307485 	 0.8450307485
epoch_time;  30.15449333190918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26738065481185913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6045167446136475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3536110520362854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7356904149055481
1 2.8383322931 	 0.7356903901 	 0.7356903901
epoch_time;  30.140592575073242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30441203713417053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5591965913772583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3233460485935211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5872299671173096
2 2.7143359639 	 0.5872299607 	 0.5872299607
epoch_time;  30.14858627319336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30165788531303406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5433980822563171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36783212423324585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.631613552570343
3 2.6540056253 	 0.6316135303 	 0.6316135303
epoch_time;  29.959041595458984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3021511733531952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5033344626426697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3292825520038605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5418590307235718
4 2.6106203321 	 0.5418590236 	 0.5418590236
epoch_time;  30.36685347557068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19894489645957947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3394693434238434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2610526382923126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42546510696411133
5 2.5849857317 	 0.4254651044 	 0.4254651044
epoch_time;  30.184931755065918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2637518644332886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40959984064102173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30284383893013
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47012728452682495
6 2.5658301971 	 0.4701272913 	 0.4701272913
epoch_time;  30.271502017974854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2601277828216553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42750346660614014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.313498854637146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49370262026786804
7 2.5479852861 	 0.4937026153 	 0.4937026153
epoch_time;  30.01143479347229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19851721823215485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33137860894203186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2669845521450043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4215414524078369
8 2.5333031284 	 0.4215414511 	 0.4215414511
epoch_time;  30.336896181106567
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21590612828731537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36556676030158997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28410056233406067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45488104224205017
9 2.5228060733 	 0.4548810392 	 0.4548810392
epoch_time;  30.1229305267334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1922990381717682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3478042483329773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2505141496658325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4142322540283203
10 2.5131454106 	 0.4142322437 	 0.4142322437
epoch_time;  30.267170429229736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19958291947841644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3542035222053528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2622225880622864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4430457055568695
11 2.5126534929 	 0.4430457038 	 0.4430457038
epoch_time;  29.942240953445435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18226635456085205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3285132646560669
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25226545333862305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41691192984580994
12 2.5001864527 	 0.4169119345 	 0.4169119345
epoch_time;  29.991432666778564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20092514157295227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35219550132751465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2693649232387543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4404340982437134
13 2.4952104138 	 0.4404341002 	 0.4404341002
epoch_time;  30.209174871444702
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2284003645181656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37028607726097107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.256646990776062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41244742274284363
14 2.4912653195 	 0.412447419 	 0.412447419
epoch_time;  30.01708436012268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22699327766895294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3849303424358368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2604789435863495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4467029571533203
15 2.4873732849 	 0.4467029468 	 0.4467029468
epoch_time;  30.78787350654602
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1885419338941574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32001811265945435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23630082607269287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38214200735092163
16 2.482880899 	 0.3821420206 	 0.3821420206
epoch_time;  31.64777112007141
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2236645519733429
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35895979404449463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24860478937625885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3969731032848358
17 2.4774635227 	 0.3969731099 	 0.3969731099
epoch_time;  30.37582492828369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21413463354110718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3690025210380554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26957041025161743
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43793433904647827
18 2.4726466629 	 0.4379343394 	 0.4379343394
epoch_time;  29.985087871551514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18368472158908844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32424822449684143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26890304684638977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4313696324825287
19 2.4715953871 	 0.4313696372 	 0.4313696372
epoch_time;  29.990166187286377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.183663472533226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32416409254074097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26894891262054443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43145784735679626
It took 669.7671976089478 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▄▃▂▂▂▂▁▂▂▂▂▃▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▄▄▃▃▂▃▂▂▂▂▂▃▁▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▄▅▃▂▂▂▂▂▂▂▂▂▃▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▆▄▅▄▃▄▃▃▃▃▂▃▄▄▁▃▃▁▁
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.37066
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26929
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25019
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16237
wandb:                         Train loss 2.43896
wandb: 
wandb: 🚀 View run festive-laughter-1298 at: https://wandb.ai/nreints/thesis/runs/ffbp1tla
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154935-ffbp1tla/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_160043-qmy9bq01
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-wonton-1305
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/qmy9bq01
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35615238547325134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8208625316619873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43000632524490356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9113350510597229
0 4.4599473833 	 0.9113350533 	 0.9113350533
epoch_time;  30.032883882522583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29727494716644287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6266199350357056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36653825640678406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7020641565322876
1 2.8256773125 	 0.702064143 	 0.702064143
epoch_time;  29.91804575920105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2958574891090393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5830998420715332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3281785845756531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6174293756484985
2 2.7125858211 	 0.6174293724 	 0.6174293724
epoch_time;  29.926567554473877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23379197716712952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4753860533237457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3325882852077484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5883288383483887
3 2.6474148202 	 0.5883288409 	 0.5883288409
epoch_time;  29.850980520248413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27623745799064636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49422281980514526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.357981413602829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5823410153388977
4 2.6079217269 	 0.5823410447 	 0.5823410447
epoch_time;  29.81622576713562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2516310513019562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4499817192554474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29765942692756653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4930371940135956
5 2.571116164 	 0.4930372084 	 0.4930372084
epoch_time;  29.95609188079834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20692533254623413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3758213222026825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2756715416908264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45195361971855164
6 2.5524137028 	 0.4519536199 	 0.4519536199
epoch_time;  29.982988357543945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23620137572288513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4006919264793396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2720746695995331
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4320560395717621
7 2.5382626378 	 0.4320560352 	 0.4320560352
epoch_time;  30.248851537704468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20638719201087952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35166704654693604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2694348692893982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42398619651794434
8 2.5211423798 	 0.4239861978 	 0.4239861978
epoch_time;  29.732796907424927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22592398524284363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36815300583839417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28055325150489807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4317820072174072
9 2.5054309305 	 0.4317819956 	 0.4317819956
epoch_time;  30.065603971481323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2056075483560562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3061038553714752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2626732587814331
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3679974675178528
10 2.4978695135 	 0.367997453 	 0.367997453
epoch_time;  29.988195180892944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21012206375598907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34705084562301636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2841430902481079
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4326021373271942
11 2.4877926382 	 0.4326021349 	 0.4326021349
epoch_time;  30.01877450942993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18574604392051697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3060992956161499
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2698291838169098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39338672161102295
12 2.4798670764 	 0.3933867171 	 0.3933867171
epoch_time;  29.71793007850647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21637479960918427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35863420367240906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2616158127784729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41786450147628784
13 2.4675811995 	 0.4178644954 	 0.4178644954
epoch_time;  30.097496032714844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23244047164916992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3434017598628998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27538660168647766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.389173686504364
14 2.4667443112 	 0.389173683 	 0.389173683
epoch_time;  29.824692010879517
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23478510975837708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39755237102508545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29884853959083557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48048973083496094
15 2.4569282713 	 0.4804897411 	 0.4804897411
epoch_time;  30.274691343307495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1570092886686325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24001425504684448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24632395803928375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3453565537929535
16 2.4544581037 	 0.3453565649 	 0.3453565649
epoch_time;  30.123571634292603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22014345228672028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35620254278182983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28049367666244507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4224044382572174
17 2.4507530176 	 0.4224044387 	 0.4224044387
epoch_time;  30.10671830177307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21208299696445465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3182082176208496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2502577602863312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36382919549942017
18 2.4433904504 	 0.3638292055 	 0.3638292055
epoch_time;  30.24652624130249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16233204305171967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2692968547344208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25024694204330444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3706267476081848
19 2.4389580667 	 0.3706267486 	 0.3706267486
epoch_time;  30.30859065055847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16236615180969238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2692856788635254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25019073486328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3706553280353546
It took 667.721312046051 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▄▃▂▁▂▁▂▂▂▁▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▄▂▃▂▂▂▃▂▂▁▂▁▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▆▃▃▂▂▂▃▃▂▂▄▁▁▂▂▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▅▂▄▂▃▃▄▃▂▂▂▂▁▂▃▃▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.39143
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35063
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25775
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22039
wandb:                         Train loss 2.45108
wandb: 
wandb: 🚀 View run beaming-wonton-1305 at: https://wandb.ai/nreints/thesis/runs/qmy9bq01
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_160043-qmy9bq01/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_161204-aert775j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-lantern-1311
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/aert775j
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3663247227668762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8546490669250488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4000471830368042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9545387625694275
0 4.4369141947 	 0.9545387887 	 0.9545387887
epoch_time;  30.808648586273193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.252975195646286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5781312584877014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3228428363800049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6735081672668457
1 2.8266664375 	 0.6735081441 	 0.6735081441
epoch_time;  30.874406337738037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23218798637390137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49867740273475647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2975579798221588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5752705931663513
2 2.7097329958 	 0.5752705754 	 0.5752705754
epoch_time;  30.966115951538086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2765561640262604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5457353591918945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3506934940814972
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6329212784767151
3 2.6458719573 	 0.6329212498 	 0.6329212498
epoch_time;  29.971909284591675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2063564956188202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4238220155239105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2913387715816498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5320554375648499
4 2.6011879417 	 0.5320554166 	 0.5320554166
epoch_time;  29.978235483169556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24825699627399445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4344800114631653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2924385368824005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4820999503135681
5 2.5731117698 	 0.4820999558 	 0.4820999558
epoch_time;  29.962072610855103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21361073851585388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35420793294906616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2611212134361267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4236065745353699
6 2.5526770177 	 0.4236065839 	 0.4236065839
epoch_time;  29.760706663131714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22502388060092926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40372925996780396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2706623375415802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46731317043304443
7 2.5372498757 	 0.4673131582 	 0.4673131582
epoch_time;  30.126880407333374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2467144876718521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3962784707546234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2715162932872772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42311516404151917
8 2.5230971577 	 0.4231151684 	 0.4231151684
epoch_time;  30.13434100151062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25742924213409424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43487077951431274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29382234811782837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4808335602283478
9 2.5117746197 	 0.4808335588 	 0.4808335588
epoch_time;  29.9578115940094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22652199864387512
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4034827947616577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2803022563457489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47316911816596985
10 2.4991725335 	 0.4731691103 	 0.4731691103
epoch_time;  30.20868229866028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20594020187854767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38761457800865173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26595765352249146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4560525715351105
11 2.4937363136 	 0.4560525843 	 0.4560525843
epoch_time;  30.017683506011963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2020535171031952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34746092557907104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2670604884624481
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41990336775779724
12 2.4829051974 	 0.4199033583 	 0.4199033583
epoch_time;  30.045852184295654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2056281864643097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3735184967517853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30222123861312866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49321410059928894
13 2.4799123672 	 0.4932140866 	 0.4932140866
epoch_time;  30.184006690979004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19881556928157806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33464348316192627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2520780861377716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.399313360452652
14 2.4729139448 	 0.3993133545 	 0.3993133545
epoch_time;  29.780773162841797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18028688430786133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3143954575061798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24245302379131317
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3909328579902649
15 2.4690281914 	 0.3909328564 	 0.3909328564
epoch_time;  29.841429233551025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2145565301179886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36068981885910034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2655599117279053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4236610233783722
16 2.4609793196 	 0.4236610206 	 0.4236610206
epoch_time;  29.975608110427856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2281290590763092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36947372555732727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27052071690559387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4288504123687744
17 2.4566894201 	 0.4288504111 	 0.4288504111
epoch_time;  31.015246868133545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22920146584510803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3709527552127838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28273093700408936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42971813678741455
18 2.4545220728 	 0.4297181413 	 0.4297181413
epoch_time;  33.11484146118164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22039465606212616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3506523072719574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2578009068965912
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3914438486099243
19 2.4510796404 	 0.3914438609 	 0.3914438609
epoch_time;  32.79811382293701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22039401531219482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35063233971595764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2577478289604187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3914308249950409
It took 680.9305827617645 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▂▁▃▃▃▂▁▂▁▁▁▂▁▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▂▁▃▄▂▂▁▂▁▁▂▃▁▁▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▃▃▁▂▄▄▃▁▂▁▁▂▄▂▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▆▃▄▃▁▅█▄▃▁▃▂▃▅▇▃▂▆▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.4021
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.36253
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.26077
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2271
wandb:                         Train loss 2.44599
wandb: 
wandb: 🚀 View run enchanting-lantern-1311 at: https://wandb.ai/nreints/thesis/runs/aert775j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_161204-aert775j/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162422-wx5amiad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-wonton-1319
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/wx5amiad
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2947695851325989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7389852404594421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38703781366348267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8389042019844055
0 4.4947072561 	 0.838904221 	 0.838904221
epoch_time;  33.25448226928711
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26500311493873596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5861168503761292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3327420949935913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6550778150558472
1 2.8214084054 	 0.6550778363 	 0.6550778363
epoch_time;  32.969844818115234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2307663857936859
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5122804641723633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3016466498374939
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5929228067398071
2 2.7009138496 	 0.5929228087 	 0.5929228087
epoch_time;  33.56994700431824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2353912740945816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4757804274559021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2967316210269928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.528897225856781
3 2.6340912216 	 0.5288972184 	 0.5288972184
epoch_time;  33.006054162979126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22303470969200134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4067500829696655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28742516040802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48012134432792664
4 2.5938944678 	 0.4801213445 	 0.4801213445
epoch_time;  33.237797021865845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19612787663936615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34566035866737366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2589154541492462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40924665331840515
5 2.5732091285 	 0.4092466612 	 0.4092466612
epoch_time;  33.34619092941284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2544685900211334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4591842293739319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2810955047607422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4887251853942871
6 2.551912198 	 0.4887251983 	 0.4887251983
epoch_time;  33.31446886062622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30553874373435974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48463600873947144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3076300024986267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4913111627101898
7 2.5341910929 	 0.4913111506 	 0.4913111506
epoch_time;  34.89585208892822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23899689316749573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40672358870506287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3071717917919159
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4850020110607147
8 2.5176098616 	 0.4850020125 	 0.4850020125
epoch_time;  34.441951751708984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2284950315952301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40131592750549316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28470346331596375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4539867043495178
9 2.5077701845 	 0.4539867092 	 0.4539867092
epoch_time;  33.78390860557556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18954023718833923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32538899779319763
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26343536376953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4018179476261139
10 2.5009795378 	 0.4018179404 	 0.4018179404
epoch_time;  33.30409622192383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2273624688386917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3782118856906891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28376567363739014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43921104073524475
11 2.4899128763 	 0.4392110464 	 0.4392110464
epoch_time;  33.04217743873596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20994536578655243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3336900770664215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2558567523956299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3878466486930847
12 2.4873311347 	 0.3878466632 	 0.3878466632
epoch_time;  33.066144704818726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2190556675195694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3415859043598175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2620280981063843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39196306467056274
13 2.4779223815 	 0.3919630721 	 0.3919630721
epoch_time;  33.08659362792969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26135772466659546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3735232651233673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27903100848197937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3938375413417816
14 2.4667681137 	 0.3938375525 	 0.3938375525
epoch_time;  33.103086948394775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2947629690170288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4143730700016022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3076360821723938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43437105417251587
15 2.465939986 	 0.434371041 	 0.434371041
epoch_time;  33.15307140350342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2207317054271698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34461671113967896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26631081104278564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3981533646583557
16 2.4575405237 	 0.3981533566 	 0.3981533566
epoch_time;  33.14847230911255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2094741016626358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32372114062309265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26507681608200073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3917488753795624
17 2.4542247072 	 0.3917488717 	 0.3917488717
epoch_time;  33.08365488052368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27924951910972595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4235776364803314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28376269340515137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4331492781639099
18 2.450689875 	 0.4331492656 	 0.4331492656
epoch_time;  32.92245030403137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22713173925876617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3624897003173828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2608017921447754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4020749032497406
19 2.4459921928 	 0.4020749066 	 0.4020749066
epoch_time;  33.28813457489014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22709710896015167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36252665519714355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2607738971710205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4020984470844269
It took 738.3454258441925 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▃▃▂▂▂▂▂▃▃▂▂▁▁▂▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▄▃▂▂▂▃▂▃▃▂▂▁▁▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▄▃▂▂▂▂▂▄▅▃▂▁▁▃▁▁▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▇▆▅▁▂▃▄▄▅▆▃▃▃▂▄▂▁▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.4315
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35395
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28136
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19917
wandb:                         Train loss 2.44537
wandb: 
wandb: 🚀 View run dancing-wonton-1319 at: https://wandb.ai/nreints/thesis/runs/wx5amiad
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162422-wx5amiad/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2854096591472626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7512950301170349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3831843137741089
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8392515182495117
0 4.5617290644 	 0.839251544 	 0.839251544
epoch_time;  32.97829842567444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23257961869239807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5975426435470581
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3482156991958618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.690542459487915
1 2.8234752534 	 0.6905424376 	 0.6905424376
epoch_time;  33.15421152114868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2755036950111389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5933986306190491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33652007579803467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6110436320304871
2 2.6963804706 	 0.6110436104 	 0.6110436104
epoch_time;  33.17992043495178
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24514594674110413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48247644305229187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30606183409690857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5204030871391296
3 2.6316308065 	 0.5204031042 	 0.5204031042
epoch_time;  33.10070776939392
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2343875914812088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.444307804107666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2894429862499237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.470251202583313
4 2.5879803802 	 0.4702512174 	 0.4702512174
epoch_time;  32.56372261047363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1566161960363388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3323413133621216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2674952447414398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43221762776374817
5 2.5612700404 	 0.4322176134 	 0.4322176134
epoch_time;  33.3480806350708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17794521152973175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3271765112876892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25475388765335083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3975558280944824
6 2.5389011721 	 0.3975558307 	 0.3975558307
epoch_time;  32.817007303237915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18787050247192383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3612470030784607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2622748017311096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4277600646018982
7 2.5284717227 	 0.4277600675 	 0.4277600675
epoch_time;  32.97846436500549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21549755334854126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3925539255142212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26927244663238525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43807733058929443
8 2.5108187191 	 0.4380773183 	 0.4380773183
epoch_time;  33.013150453567505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20349356532096863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33633190393447876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2559509575366974
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38490211963653564
9 2.4981199354 	 0.3849021293 	 0.3849021293
epoch_time;  33.22655701637268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2344556450843811
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39947637915611267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3086094558238983
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4652285873889923
10 2.4952079344 	 0.4652286014 	 0.4652286014
epoch_time;  33.20658850669861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24626293778419495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4129070043563843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31346002221107483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47944673895835876
11 2.4812831582 	 0.4794467411 	 0.4794467411
epoch_time;  33.02391576766968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18984930217266083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3597847521305084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2752923369407654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44641008973121643
12 2.4762471173 	 0.4464101018 	 0.4464101018
epoch_time;  34.865070104599
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2026069462299347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35819682478904724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2627934515476227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4118589758872986
13 2.4712193411 	 0.4118589659 	 0.4118589659
epoch_time;  34.18563747406006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19097305834293365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30692628026008606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23930782079696655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3461902439594269
14 2.4642260428 	 0.3461902515 	 0.3461902515
epoch_time;  33.04130411148071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18217332661151886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2998112142086029
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2465498447418213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36354464292526245
15 2.461242965 	 0.3635446497 	 0.3635446497
epoch_time;  32.92932200431824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21757806837558746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35820919275283813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2705232501029968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3948519825935364
16 2.4548217525 	 0.3948519733 	 0.3948519733
epoch_time;  33.39192318916321
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18165180087089539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2944892346858978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24179373681545258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3510435223579407
17 2.4532888073 	 0.3510435362 	 0.3510435362
epoch_time;  32.7574987411499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16373023390769958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.281649112701416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24685867130756378
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36729246377944946
18 2.4433901199 	 0.3672924557 	 0.3672924557
epoch_time;  32.904468297958374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19919602572917938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3539700508117676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28127962350845337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4315800368785858
19 2.4453665514 	 0.4315800435 	 0.4315800435
epoch_time;  33.2287392616272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1991710513830185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3539522886276245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2813580334186554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4314951002597809
It took 738.5816729068756 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138705
Array Job ID: 2137927_16
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-10:49:30 core-walltime
Job Wall-clock time: 01:56:05
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
