/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_112143-ra8bewok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-fuse-1665
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ra8bewok
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(-10,', '10)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533cb90e8f0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdc4f0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdc670>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdc610>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02057560533285141
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057796962559223175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04763379320502281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08667812496423721
0 2.8515240585 	 0.0866781275
epoch_time;  34.8404655456543
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015283525921404362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05010470747947693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04476954787969589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07818139344453812
1 0.1818446492 	 0.0781813907
epoch_time;  33.872883796691895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00573221268132329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023083282634615898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02057448960840702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.037084225565195084
2 0.0526583005 	 0.0370842251
epoch_time;  34.21098327636719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05439039692282677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13122624158859253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08547534793615341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1535835713148117
3 0.1311196754 	 0.1535835727
epoch_time;  34.32106971740723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010855437256395817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02830936387181282
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025909703224897385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04335606098175049
4 0.0620652054 	 0.0433560599
epoch_time;  34.43261361122131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009936588816344738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03783920407295227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032622043043375015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.057792115956544876
5 0.0752698218 	 0.0577921161
epoch_time;  34.30683732032776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007113208994269371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01627000980079174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016472190618515015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.026204971596598625
6 0.0348828711 	 0.0262049707
epoch_time;  33.957897424697876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024298913776874542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009516214951872826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009359917603433132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016631901264190674
7 0.0297393221 	 0.0166319014
epoch_time;  34.16710591316223
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002942800521850586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00764900166541338
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007856571115553379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013324650935828686
8 0.0181659392 	 0.0133246511
epoch_time;  34.44176530838013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002252587117254734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00602271594107151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006336742080748081
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010355004109442234
9 0.019134894 	 0.0103550037
epoch_time;  34.43720364570618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005846885032951832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009706193581223488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010676185600459576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014574101194739342
10 0.0170766006 	 0.0145741013
epoch_time;  34.1667754650116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021731657907366753
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006904824636876583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006182405631989241
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01150893047451973
11 0.0308479436 	 0.0115089308
epoch_time;  34.36344814300537
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029812438879162073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.013349562883377075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008630810305476189
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.017849739640951157
12 0.035665886 	 0.0178497392
epoch_time;  34.188798904418945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003907020203769207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009192571975290775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0077708386816084385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012861181050539017
13 0.0141316283 	 0.0128611812
epoch_time;  34.013070821762085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001721204025670886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0052589839324355125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004383570048958063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008085155859589577
14 0.0143128683 	 0.0080851558
epoch_time;  34.270745277404785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016744529129937291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004295212682336569
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004389741457998753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0071009197272360325
15 0.013962807 	 0.0071009199
epoch_time;  33.881436586380005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004020966123789549
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00916779413819313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008522909134626389
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013752582482993603
16 0.0255004109 	 0.0137525829
epoch_time;  33.89163064956665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014963814057409763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03712547570466995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03312060609459877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.053786177188158035
17 0.0546601647 	 0.0537861769
epoch_time;  33.800565004348755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032035396434366703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009320206940174103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00829742569476366
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014478085562586784
18 0.0237525215 	 0.0144780856
epoch_time;  34.140114545822144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018422366119921207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005549360066652298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005067945923656225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009312344714999199
19 0.0142629192 	 0.0093123445
epoch_time;  34.38283848762512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003973525483161211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007018206641077995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00731286546215415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010462173260748386
20 0.0119912612 	 0.0104621732
epoch_time;  34.36718940734863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004176580812782049
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00786422286182642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007414449937641621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01149036455899477
21 0.0234525061 	 0.0114903644
epoch_time;  34.18919849395752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003045917022973299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006332901772111654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005797377787530422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008946509100496769
22 0.0132773296 	 0.0089465088
epoch_time;  33.82535266876221
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–„â–‚â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–ƒâ–â–â–â–â–â–â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–„â–‚â–ˆâ–‚â–ƒâ–‚â–â–â–â–â–â–‚â–â–â–â–â–ƒâ–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–…â–…â–‚â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–‚â–â–â–â–â–â–â–„â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–ƒâ–‚â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–‚â–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.0059
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00439
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00385
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00203
wandb:                         Train loss 0.0085
wandb: 
wandb: ğŸš€ View run vibrant-fuse-1665 at: https://wandb.ai/nreints/thesis/runs/ra8bewok
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_112143-ra8bewok/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_114009-7nt8qtgp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-lamp-1671
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/7nt8qtgp
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030826113652437925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005909079685807228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005848896689713001
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00837012194097042
23 0.0103819325 	 0.0083701222
epoch_time;  34.33119821548462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007887461222708225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.021752212196588516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01579366996884346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02745814248919487
24 0.0596232562 	 0.0274581419
epoch_time;  34.31586980819702
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00639178603887558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012310454621911049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01104048453271389
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016718801110982895
25 0.0185200607 	 0.0167188011
epoch_time;  34.06145119667053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002510539721697569
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005897033028304577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005358911585062742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008532803505659103
26 0.0125497461 	 0.0085328036
epoch_time;  33.86519408226013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002640438498929143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006150354631245136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005340250674635172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008568298071622849
27 0.010378995 	 0.0085682984
epoch_time;  33.886590003967285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019066238310188055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004195159301161766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003919920418411493
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006183392368257046
28 0.0093914988 	 0.0061833923
epoch_time;  34.207154989242554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020301041658967733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004385143052786589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0038451733998954296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005904997233301401
29 0.0085029118 	 0.0059049972
epoch_time;  34.36740684509277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002030892064794898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004388032481074333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003845280734822154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005903579760342836
It took  1106.9724833965302  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c52f9a80>, <torch.utils.data.dataloader.DataLoader object at 0x15339f5b6e90>, <torch.utils.data.dataloader.DataLoader object at 0x1533c52fbcd0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c53bfd60>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015241114422678947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04915827885270119
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.041904646903276443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09129439294338226
0 2.8393295946 	 0.0912943895
epoch_time;  34.03875970840454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010384058579802513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.035074155777692795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03366966173052788
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06972388178110123
1 0.1896400192 	 0.0697238784
epoch_time;  34.2592716217041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005581527482718229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0179492998868227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016945650801062584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.035148985683918
2 0.0488990651 	 0.0351489865
epoch_time;  34.2772490978241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006473194807767868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019468965008854866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0200217105448246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03942110016942024
3 0.1228270607 	 0.0394210988
epoch_time;  34.42228317260742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016227776184678078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04020367190241814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03884924203157425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07126608490943909
4 0.0787136474 	 0.0712660879
epoch_time;  34.339242458343506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034285192377865314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011238500475883484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011589497327804565
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02352260984480381
5 0.03656977 	 0.0235226104
epoch_time;  33.67796063423157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023927971720695496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052371419966220856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.042639970779418945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0765504539012909
6 0.0616418577 	 0.076550452
epoch_time;  34.27122735977173
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008226373232901096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01589708775281906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016025196760892868
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.026895735412836075
7 0.0346685492 	 0.0268957363
epoch_time;  33.90511870384216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033961704466491938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010031784884631634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008360403589904308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01734878122806549
8 0.0222277919 	 0.0173487807
epoch_time;  34.15663695335388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020451939199119806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006749176885932684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006808810401707888
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015386449173092842
9 0.0344060021 	 0.0153864489
epoch_time;  34.301252365112305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022096694447100163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006327054928988218
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006904173642396927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013683218508958817
10 0.021221646 	 0.0136832183
epoch_time;  34.16259503364563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021540229208767414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00483487406745553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004949606489390135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009361389093101025
11 0.0118016677 	 0.0093613888
epoch_time;  34.33727145195007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001605307450518012
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004231642931699753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004137569107115269
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008412857539951801
12 0.0168541411 	 0.0084128574
epoch_time;  33.7517614364624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034441882744431496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005799585487693548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0065801870077848434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010136271826922894
13 0.0191346312 	 0.0101362723
epoch_time;  33.823304891586304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005012029781937599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011036871001124382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009668739512562752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01935541443526745
14 0.0250358697 	 0.0193554138
epoch_time;  34.02625918388367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002360958606004715
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–ƒâ–„â–†â–‚â–‡â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–ƒâ–ƒâ–†â–‚â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–‚â–â–‚â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–ƒâ–„â–‡â–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–„â–‚â–ƒâ–†â–‚â–ˆâ–ƒâ–‚â–â–â–â–â–‚â–‚â–â–ƒâ–â–ƒâ–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.01288
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00725
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00658
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00295
wandb:                         Train loss 0.02188
wandb: 
wandb: ğŸš€ View run resplendent-lamp-1671 at: https://wandb.ai/nreints/thesis/runs/7nt8qtgp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_114009-7nt8qtgp/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_115816-n38x4jh7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-fuse-1676
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/n38x4jh7
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004647726193070412
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005362440831959248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009265773929655552
15 0.0126703928 	 0.0092657736
epoch_time;  34.18659591674805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00605398416519165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008570713922381401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009588129818439484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013098853640258312
16 0.0118856055 	 0.0130988536
epoch_time;  34.26870608329773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018880011048167944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0039408341981470585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0042994264513254166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007674836553633213
17 0.0187178488 	 0.0076748367
epoch_time;  34.32211756706238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0067746140994131565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00878466758877039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010972609743475914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013628103770315647
18 0.009354922 	 0.0136281039
epoch_time;  34.1464102268219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020062027033418417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004041033796966076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004084731452167034
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0069517758674919605
19 0.0105118199 	 0.0069517758
epoch_time;  33.74793863296509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038500051014125347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007966720499098301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007789374329149723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013666484504938126
20 0.0370877141 	 0.0136664843
epoch_time;  33.84187459945679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014391862787306309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003607220249250531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036107853520661592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0068378024734556675
21 0.0119515902 	 0.0068378024
epoch_time;  33.8050422668457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001102422014810145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0030480055138468742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003110977355390787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006041009910404682
22 0.0106079187 	 0.00604101
epoch_time;  33.779067277908325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031923498027026653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00699885468930006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006310167722404003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01220533438026905
23 0.0397701769 	 0.0122053342
epoch_time;  33.8952317237854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031846847850829363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0060959807597100735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00647442601621151
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011466207914054394
24 0.0196613872 	 0.0114662078
epoch_time;  34.207884311676025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004032109398394823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006825389340519905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006671489682048559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010400376282632351
25 0.0102649266 	 0.0104003759
epoch_time;  34.110920429229736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002425768878310919
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0050872270949184895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004661802668124437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008257884532213211
26 0.0100922978 	 0.0082578846
epoch_time;  33.91259431838989
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002451155334711075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0038580605760216713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0043137515895068645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006571677513420582
27 0.0109465712 	 0.0065716775
epoch_time;  34.10888075828552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029405199456959963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004723225254565477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005188641604036093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007643745746463537
28 0.0096859762 	 0.0076437457
epoch_time;  33.71565532684326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029454908799380064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0072510819882154465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006574970670044422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01291743479669094
29 0.021880178 	 0.0129174351
epoch_time;  33.914912939071655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002945865970104933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00725120073184371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0065765087492764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012883245013654232
It took  1087.0748302936554  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c4c137f0>, <torch.utils.data.dataloader.DataLoader object at 0x1533cce794b0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdd360>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdd2a0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028970640152692795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06638443470001221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05771271511912346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09937357902526855
0 2.8633767994 	 0.0993735783
epoch_time;  33.97734045982361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01892905868589878
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06558666378259659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05486252158880234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10158015787601471
1 0.2079729749 	 0.1015801559
epoch_time;  34.284931659698486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009602357633411884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029164757579565048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02710775099694729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.046487823128700256
2 0.0654936277 	 0.0464878226
epoch_time;  33.969205141067505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03955485671758652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07223972678184509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07111171633005142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10116322338581085
3 0.1225076888 	 0.1011632245
epoch_time;  34.1869900226593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008914683945477009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.020709730684757233
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01918736658990383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03263619542121887
4 0.0437573614 	 0.0326361959
epoch_time;  33.8864483833313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009733423590660095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03136198967695236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026641899719834328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05083305388689041
5 0.0841490452 	 0.0508330538
epoch_time;  34.045652866363525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006538397632539272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.015030447393655777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01653691940009594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.026545030996203423
6 0.0341714026 	 0.0265450319
epoch_time;  34.30515694618225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03930281102657318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0897103101015091
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‡â–‡â–„â–‡â–ƒâ–„â–‚â–ˆâ–‚â–‚â–â–ƒâ–â–â–â–â–‚â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–ƒâ–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–†â–ƒâ–‡â–‚â–ƒâ–‚â–ˆâ–‚â–‚â–â–ƒâ–â–â–â–â–‚â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–ƒâ–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–†â–ƒâ–ˆâ–ƒâ–ƒâ–‚â–ˆâ–‚â–‚â–â–ƒâ–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–ƒâ–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–†â–„â–ƒâ–ˆâ–‚â–ƒâ–‚â–ˆâ–â–‚â–â–„â–‚â–â–â–â–ƒâ–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–ƒâ–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.01102
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00717
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00562
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00233
wandb:                         Train loss 0.01919
wandb: 
wandb: ğŸš€ View run brilliant-fuse-1676 at: https://wandb.ai/nreints/thesis/runs/n38x4jh7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_115816-n38x4jh7/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_121622-w3ran430
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-fireworks-1683
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/w3ran430
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06823938339948654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11359361559152603
7 0.0585077244 	 0.1135936172
epoch_time;  34.34827756881714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003877210896462202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012676550075411797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012005791999399662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.022325685247778893
8 0.0416987475 	 0.0223256857
epoch_time;  33.89339089393616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005238682497292757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011734683066606522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012036439962685108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.019141562283039093
9 0.0232272469 	 0.0191415631
epoch_time;  34.377962589263916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029173330403864384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00683510722592473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006942321080714464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01186302863061428
10 0.0186207849 	 0.0118630285
epoch_time;  33.98646354675293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015218207612633705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023681141436100006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023815883323550224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03252492472529411
11 0.0167207841 	 0.0325249243
epoch_time;  33.84883451461792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005484779365360737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009113512933254242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009091413579881191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013310031965374947
12 0.0143687691 	 0.0133100319
epoch_time;  33.79663705825806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013936086324974895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003612459171563387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0037461507599800825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006694542244076729
13 0.0152475659 	 0.0066945423
epoch_time;  33.95925331115723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00406666100025177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006527680438011885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007904439233243465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010651744902133942
14 0.0121709764 	 0.0106517447
epoch_time;  34.14323449134827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024526643101125956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004549623001366854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005171230528503656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0077163465321063995
15 0.010837804 	 0.0077163467
epoch_time;  33.73981952667236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00973694957792759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.015184413641691208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01349978893995285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.020534075796604156
16 0.0621755168 	 0.020534076
epoch_time;  34.09586834907532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003449747571721673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007267825771123171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0062354812398552895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010655026882886887
17 0.0158566165 	 0.0106550264
epoch_time;  34.140669107437134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002645270200446248
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005565156694501638
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005305781029164791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008620942942798138
18 0.0133958278 	 0.0086209428
epoch_time;  34.177345752716064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019466524245217443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003986166790127754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00415154080837965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00647808937355876
19 0.0124735706 	 0.0064780892
epoch_time;  34.47062277793884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008373068645596504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011237476952373981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013596724718809128
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01611536554992199
20 0.0102456986 	 0.0161153658
epoch_time;  34.25471806526184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002548650838434696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007609408348798752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006740448996424675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012026512995362282
21 0.0504620629 	 0.0120265131
epoch_time;  33.93286919593811
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0054686241783201694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01009473204612732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009828486479818821
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014443379826843739
22 0.0128115799 	 0.0144433802
epoch_time;  33.606926679611206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0063874381594359875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009950296021997929
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011197580955922604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015108609572052956
23 0.0125645212 	 0.0151086098
epoch_time;  33.68394136428833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002869695657864213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005077835638076067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005299681331962347
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007654199376702309
24 0.0094783727 	 0.0076541994
epoch_time;  33.671013593673706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005313654895871878
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012765210121870041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009291796945035458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.017181167379021645
25 0.0298862385 	 0.0171811674
epoch_time;  33.83726739883423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022939008194953203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005224823020398617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004676650743931532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007625256199389696
26 0.0118747259 	 0.0076252564
epoch_time;  33.936792612075806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001661679707467556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003735028672963381
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036179861053824425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005690064746886492
27 0.0098092507 	 0.0056900647
epoch_time;  34.1385715007782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01201562862843275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02526126243174076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020412813872098923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03324932977557182
28 0.0671426685 	 0.0332493307
epoch_time;  34.123491287231445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023257664870470762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007159475237131119
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0056240977719426155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011017078533768654
29 0.0191945257 	 0.0110170784
epoch_time;  34.23616933822632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002325181383639574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00716820452362299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005623636767268181
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011020166799426079
It took  1085.9461915493011  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533cb885060>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdc700>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdfac0>, <torch.utils.data.dataloader.DataLoader object at 0x15339f51ec20>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018555007874965668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05297614261507988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04583054035902023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09259045124053955
0 2.8519454367 	 0.092590453
epoch_time;  34.17712378501892
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06955736875534058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1521780639886856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1193145141005516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23195040225982666
1 0.2341912246 	 0.2319503969
epoch_time;  34.59861445426941
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009780347347259521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.031244441866874695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029968958348035812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.057815711945295334
2 0.0960401987 	 0.0578157131
epoch_time;  34.30058526992798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03886374831199646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10230543464422226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08439762145280838
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14865857362747192
3 0.1278492121 	 0.1486585796
epoch_time;  34.23665428161621
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006232928019016981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018872588872909546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0189446322619915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03672599792480469
4 0.0624709552 	 0.0367259979
epoch_time;  34.18196439743042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027726393193006516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06834769248962402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05430092662572861
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10181669145822525
5 0.1280734158 	 0.1018166902
epoch_time;  34.06181478500366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00650615431368351
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018022626638412476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01649434305727482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03082686848938465
6 0.0482438901 	 0.0308268682
epoch_time;  33.929683208465576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003210346447303891
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009714430198073387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009560962207615376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.018580177798867226
7 0.0333914659 	 0.0185801774
epoch_time;  33.85000443458557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004956506658345461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009539278224110603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01043157372623682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016812575981020927
8 0.0201893936 	 0.0168125752
epoch_time;  33.98049211502075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009628787636756897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.016112258657813072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015468502417206764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.022656485438346863
9 0.0195539158 	 0.0226564854
epoch_time;  34.35022258758545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003978261724114418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011299860663712025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011190044693648815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021443696692585945
10 0.0757267019 	 0.0214436969
epoch_time;  34.015286684036255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003951834049075842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009157851338386536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008690685965120792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015803012996912003
11 0.0208002938 	 0.015803013
epoch_time;  34.15759325027466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024998739827424288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005864584352821112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005708599928766489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010658485814929008
12 0.015570133 	 0.010658486
epoch_time;  34.47062826156616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004453544970601797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007919159717857838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008800661191344261
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013453506864607334
13 0.0137557955 	 0.0134535066
epoch_time;  34.637197971343994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006324873771518469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01205612625926733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011531834490597248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.019279399886727333
14 0.0345155483 	 0.0192793993
epoch_time;  34.53432559967041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005673576146364212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008576610125601292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010449123568832874
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014182855375111103
15 0.0141820337 	 0.0141828557
epoch_time;  34.18249154090881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0045139179565012455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010882426053285599
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009833664633333683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01849728636443615
16 0.0437395759 	 0.0184972855
epoch_time;  33.93812298774719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028420030139386654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00619041733443737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00599562656134367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011024530977010727
17 0.0154346867 	 0.0110245313
epoch_time;  34.283196687698364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024821236729621887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00555386021733284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005664675496518612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009675990790128708
18 0.0129011517 	 0.0096759904
epoch_time;  34.0626277923584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018394204089418054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004430070519447327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004645539913326502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008286970667541027
19 0.0283220625 	 0.008286971
epoch_time;  34.269614934921265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001999892760068178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003981071524322033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004289874341338873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007050846237689257
20 0.0108390219 	 0.0070508463
epoch_time;  34.43225407600403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002455672714859247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005218212027102709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004531234037131071
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007979271002113819
21 0.0101336256 	 0.0079792713
epoch_time;  34.106497049331665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022572302259504795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004324453417211771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004081611055880785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006773059722036123
22 0.0096996764 	 0.0067730598
epoch_time;  34.29712700843811
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004760530311614275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012995240278542042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011369407176971436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021533731371164322
23 0.0946882365 	 0.0215337312
epoch_time;  34.28407597541809
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: | 0.143 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: / 0.143 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ˆâ–ƒâ–…â–‚â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–ˆâ–‚â–†â–‚â–„â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–„â–ˆâ–ƒâ–†â–‚â–„â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ƒâ–ˆâ–‚â–…â–â–„â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.00733
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00428
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00371
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00166
wandb:                         Train loss 0.01445
wandb: 
wandb: ğŸš€ View run radiant-fireworks-1683 at: https://wandb.ai/nreints/thesis/runs/w3ran430
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_121622-w3ran430/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_123430-2wr9n33h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-rabbit-1688
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/2wr9n33h
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002978178206831217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007102627772837877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00647363206371665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012252937071025372
24 0.0185887599 	 0.0122529373
epoch_time;  34.037529706954956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025855323765426874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004706129897385836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0051551260985434055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008668748661875725
25 0.0133083482 	 0.0086687483
epoch_time;  33.847617387771606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020838165655732155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00419606314972043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004404318518936634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007347968406975269
26 0.011359923 	 0.0073479684
epoch_time;  33.92719841003418
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022087625693529844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004444518592208624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004312812816351652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007284386083483696
27 0.0102555674 	 0.0072843863
epoch_time;  34.030789613723755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009255827404558659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017825499176979065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015629766508936882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02727409452199936
28 0.0251900805 	 0.0272740943
epoch_time;  34.140116453170776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016588554717600346
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004288997035473585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0037143719382584095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007331650238484144
29 0.0144488561 	 0.0073316501
epoch_time;  34.27323031425476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016586463898420334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004278258420526981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003714531660079956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007327568717300892
It took  1088.0219507217407  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533cb885900>, <torch.utils.data.dataloader.DataLoader object at 0x15339f51ed70>, <torch.utils.data.dataloader.DataLoader object at 0x1533c53a2c80>, <torch.utils.data.dataloader.DataLoader object at 0x1533c53a2fb0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014542961493134499
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0488719642162323
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04127438738942146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09001243114471436
0 2.9156602257 	 0.0900124322
epoch_time;  34.151692628860474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022147558629512787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06914820522069931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05985867232084274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11750128120183945
1 0.272559929 	 0.1175012848
epoch_time;  34.12456202507019
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009768071584403515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02863328531384468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.027370447292923927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.052945248782634735
2 0.069427317 	 0.0529452494
epoch_time;  33.855369329452515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011146635748445988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023440249264240265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02224014885723591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.037647731602191925
3 0.0406174064 	 0.0376477314
epoch_time;  33.69530701637268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011279112659394741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02790447324514389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02603418193757534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.046503696590662
4 0.1483789945 	 0.0465036951
epoch_time;  34.210206747055054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0061182365752756596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0149947265163064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014960101805627346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.026428833603858948
5 0.0327219562 	 0.0264288335
epoch_time;  33.82152247428894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009706685319542885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018052512779831886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02136458456516266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03179818019270897
6 0.0439900175 	 0.0317981783
epoch_time;  34.19023871421814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006070616655051708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011731527745723724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012294057756662369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01972799003124237
7 0.0208139239 	 0.0197279893
epoch_time;  34.36419081687927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002869977615773678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008138636127114296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008848550729453564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016278784722089767
8 0.0352383079 	 0.0162787855
epoch_time;  34.05837821960449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022308211773633957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005794524680823088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006295446306467056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011330999433994293
9 0.0188554566 	 0.0113309993
epoch_time;  34.30847144126892
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010121640749275684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.013961728662252426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01702192611992359
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021557332947850227
10 0.014685594 	 0.0215573325
epoch_time;  33.93183135986328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031310338526964188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005370329599827528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0070523666217923164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010121321305632591
11 0.0146206257 	 0.010121321
epoch_time;  34.014363527297974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005102274473756552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007939356379210949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008829277008771896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012499264441430569
12 0.0188349176 	 0.0124992647
epoch_time;  33.87906789779663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006048919167369604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010844610631465912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011090336367487907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016737433150410652
13 0.0182501615 	 0.0167374323
epoch_time;  34.57346034049988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003715474857017398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006388407200574875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007084217853844166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010690522380173206
14 0.0135285899 	 0.0106905227
epoch_time;  34.42544221878052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004057439509779215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0061193606816232204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006875316146761179
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009705799631774426
15 0.0111375407 	 0.0097057992
epoch_time;  33.90376830101013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002075429307296872
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–†â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–ƒâ–‚â–â–â–â–ˆâ–‚â–â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–…â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–ˆâ–‚â–â–â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–„â–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–ƒâ–‚â–â–â–‚â–ˆâ–‚â–â–â–ƒâ–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–‚â–ˆâ–‚â–â–â–ƒâ–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.01056
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00679
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00634
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00335
wandb:                         Train loss 0.01071
wandb: 
wandb: ğŸš€ View run alight-rabbit-1688 at: https://wandb.ai/nreints/thesis/runs/2wr9n33h
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_123430-2wr9n33h/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_125237-quarn7ed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-rat-1693
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/quarn7ed
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004503420554101467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004334898665547371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0071860929019749165
16 0.0097208196 	 0.007186093
epoch_time;  34.002354860305786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01245911791920662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02944694459438324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02594141662120819
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.045643702149391174
17 0.1451084619 	 0.0456437027
epoch_time;  34.350889921188354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005025314632803202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01349320076406002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0112250205129385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021342745050787926
18 0.028872404 	 0.0213427457
epoch_time;  34.02144932746887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002876661252230406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007577940821647644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007110993377864361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012440094724297523
19 0.0188734703 	 0.0124400951
epoch_time;  34.028526306152344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004197956994175911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006755064707249403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008059424348175526
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011540492996573448
20 0.0142677301 	 0.0115404929
epoch_time;  33.873114824295044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0074490648694336414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01014435850083828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012724059633910656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015787027776241302
21 0.0120204388 	 0.0157870281
epoch_time;  34.453224658966064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06292533874511719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12648099660873413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09488102048635483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16491538286209106
22 0.0451368004 	 0.1649153902
epoch_time;  34.22443962097168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008613365702331066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01331106387078762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015319684520363808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0210593119263649
23 0.0285060805 	 0.0210593128
epoch_time;  34.0907084941864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003348276950418949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006621073931455612
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007819884456694126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012011229060590267
24 0.012609759 	 0.0120112291
epoch_time;  34.2934410572052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013483462389558554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0032988814637064934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0034209073055535555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006035264115780592
25 0.0101424599 	 0.0060352642
epoch_time;  34.31449365615845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017988812178373337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019803917035460472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026224473491311073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.028097884729504585
26 0.009197856 	 0.0280978845
epoch_time;  34.04878354072571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0051655154675245285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0079957889392972
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008724507875740528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01213164534419775
27 0.0087210932 	 0.0121316449
epoch_time;  34.463796854019165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025780005380511284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0066247014328837395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005713117308914661
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010684024542570114
28 0.0439022391 	 0.0106840249
epoch_time;  34.36233615875244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033427628222852945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006793234497308731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006345528643578291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010566417127847672
29 0.0107062836 	 0.0105664175
epoch_time;  34.163734912872314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003345645032823086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00679228687658906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006344124674797058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010561433620750904
It took  1087.0376727581024  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c53bf3d0>, <torch.utils.data.dataloader.DataLoader object at 0x15339f5907f0>, <torch.utils.data.dataloader.DataLoader object at 0x15339f591930>, <torch.utils.data.dataloader.DataLoader object at 0x15339f591d50>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02579491026699543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.056444693356752396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05077814310789108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09312275797128677
0 2.8748896654 	 0.0931227589
epoch_time;  34.22566223144531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0256834514439106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06885924190282822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06573443859815598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11476421356201172
1 0.1822403266 	 0.1147642107
epoch_time;  34.30304837226868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009958755224943161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023927776142954826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02337675169110298
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04141930863261223
2 0.0655949872 	 0.0414193087
epoch_time;  34.03911828994751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007565278094261885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017168594524264336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018578197807073593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.031376976519823074
3 0.0629699615 	 0.031376977
epoch_time;  34.75829005241394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014970880001783371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03876270353794098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0386238768696785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06766165047883987
4 0.118042512 	 0.0676616484
epoch_time;  34.522167444229126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00460182037204504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012996934354305267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014102303422987461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02600201964378357
5 0.0412841208 	 0.0260020196
epoch_time;  34.495832681655884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004780352581292391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011588072404265404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012593600898981094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021998656913638115
6 0.0401443493 	 0.0219986576
epoch_time;  34.57040238380432
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004985336679965258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0106985317543149
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010872247628867626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01827729120850563
7 0.0245607825 	 0.0182772907
epoch_time;  34.2533221244812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00511530414223671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01125510223209858
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.04327
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.02757
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.02517
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01224
wandb:                         Train loss 0.11019
wandb: 
wandb: ğŸš€ View run radiant-rat-1693 at: https://wandb.ai/nreints/thesis/runs/quarn7ed
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_125237-quarn7ed/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_131049-ngj95t94
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-festival-1697
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ngj95t94
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012036023661494255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01985115185379982
8 0.0472282623 	 0.0198511515
epoch_time;  34.250004053115845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002126681851223111
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0065059661865234375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0067231906577944756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012960230931639671
9 0.0238394495 	 0.0129602308
epoch_time;  34.52834105491638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002527551492676139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005367402918636799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0061818319372832775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010212025605142117
10 0.0176227145 	 0.010212026
epoch_time;  33.77385973930359
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04907416179776192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05604514479637146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07469969242811203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08306042104959488
11 0.0228336285 	 0.0830604231
epoch_time;  34.211214780807495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013597757555544376
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01904795505106449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023431144654750824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.029345497488975525
12 0.0167182515 	 0.029345498
epoch_time;  33.9972710609436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008750866167247295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017010705545544624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017000261694192886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02919950895011425
13 0.0408754717 	 0.029199508
epoch_time;  34.29813551902771
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023952568881213665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006558272056281567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006676001008599997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012645958922803402
14 0.0186330664 	 0.0126459591
epoch_time;  34.063867807388306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021703816018998623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004935087636113167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005913469009101391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010261486284434795
15 0.01714168 	 0.0102614859
epoch_time;  33.9663462638855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00270170159637928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005129288882017136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005812005139887333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009362878277897835
16 0.0117727976 	 0.0093628784
epoch_time;  33.98521113395691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00510608684271574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00708311703056097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009060287848114967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011890352703630924
17 0.0126845961 	 0.011890353
epoch_time;  34.471351146698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016046140808612108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0031045805662870407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003622522111982107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00598462438210845
18 0.0116657384 	 0.0059846243
epoch_time;  34.285550594329834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3094894886016846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.818595051765442
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.4667959213256836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0784225463867188
19 0.0216085591 	 2.0784226155
epoch_time;  34.13265538215637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034206085838377476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008235904388129711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007900449447333813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014104710891842842
20 0.05863151 	 0.0141047106
epoch_time;  33.977171659469604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004693859256803989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007512773387134075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00911902915686369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012766861356794834
21 0.0147234839 	 0.0127668611
epoch_time;  34.035303831100464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001576047856360674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.003510295180603862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004298083484172821
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007037056144326925
22 0.0150553539 	 0.0070370562
epoch_time;  34.15585207939148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007708336226642132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010469488799571991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011274605058133602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014499153941869736
23 0.0125553599 	 0.0144991543
epoch_time;  34.159082651138306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008492729626595974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010551799088716507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01258077658712864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015176467597484589
24 0.0093425611 	 0.0151764677
epoch_time;  34.152960538864136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038245785981416702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008267130702733994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007325342390686274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012901208363473415
25 0.0532044404 	 0.0129012082
epoch_time;  34.10347056388855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038819201290607452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006184019148349762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006629719864577055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009684909135103226
26 0.0120008607 	 0.0096849091
epoch_time;  34.00581169128418
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033886893652379513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005864047911018133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006378455553203821
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009390423074364662
27 0.0102919868 	 0.0093904233
epoch_time;  34.1960813999176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002215960994362831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004190819337964058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004443945363163948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006629995070397854
28 0.0091470434 	 0.006629995
epoch_time;  34.30897665023804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012243478558957577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027613559737801552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025169208645820618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.043283309787511826
29 0.1101855986 	 0.0432833098
epoch_time;  34.266350984573364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012243918143212795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027574686333537102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025166291743516922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04326561093330383
It took  1091.5932083129883  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c52fabc0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c52f9750>, <torch.utils.data.dataloader.DataLoader object at 0x1533c52fb2e0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c52f98a0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018366092815995216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06202210113406181
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0454709567129612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08581081032752991
0 2.8316947019 	 0.085810814
epoch_time;  34.532799243927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011726784519851208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042717188596725464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.036030419170856476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06790796667337418
1 0.1892078062 	 0.0679079672
epoch_time;  34.422558069229126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007084850687533617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023254480212926865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019615301862359047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03598393127322197
2 0.0512779981 	 0.0359839327
epoch_time;  34.091123819351196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00641183415427804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.022654958069324493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020422665402293205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03819385915994644
3 0.1200438737 	 0.0381938583
epoch_time;  34.44803071022034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09241499751806259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19899772107601166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12573957443237305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22306954860687256
4 0.1108989961 	 0.2230695511
epoch_time;  34.059420585632324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006765983533114195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02114773727953434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017784152179956436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03429349884390831
5 0.0649649817 	 0.0342935003
epoch_time;  34.07462954521179
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08299737423658371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.133077472448349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11770153790712357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16309267282485962
6 0.032368795 	 0.163092668
epoch_time;  33.946651220321655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003578088479116559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00938971433788538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009848550893366337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.017131716012954712
7 0.031351345 	 0.0171317161
epoch_time;  34.280197858810425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004357027821242809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01240936852991581
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011801467277109623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021204788237810135
8 0.0503215804 	 0.0212047885
epoch_time;  34.4221351146698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002696528099477291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007113058585673571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007346436381340027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012959402054548264
9 0.0205369396 	 0.0129594025
epoch_time;  34.11363101005554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002434117952361703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007201887667179108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006142833735793829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011864889413118362
10 0.0164341807 	 0.0118648898
epoch_time;  34.50649929046631
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035404502414166927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008023635484278202
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008165230974555016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013253306038677692
11 0.0338276669 	 0.0132533056
epoch_time;  34.23014044761658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002906054025515914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008355134166777134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00722108781337738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013433190993964672
12 0.0440178697 	 0.0134331914
epoch_time;  34.07445788383484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005539222154766321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009596754796802998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010045543313026428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01473675761371851
13 0.0159940265 	 0.0147367575
epoch_time;  34.22796559333801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004309843294322491
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008956951089203358
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008302120491862297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01358229573816061
14 0.0215620623 	 0.0135822959
epoch_time;  33.935874938964844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032991645857691765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006331269629299641
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006869766395539045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010407070629298687
15 0.0125438941 	 0.0104070704
epoch_time;  33.85439372062683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005315542686730623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.015398897230625153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011735206469893456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02147497609257698
16 0.0406611527 	 0.0214749766
epoch_time;  34.3115816116333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001598321832716465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006118476390838623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0049134171567857265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009572796523571014
17 0.0158519629 	 0.0095727962
epoch_time;  34.50015735626221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025668207090348005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006561036687344313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005488119553774595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009596575982868671
18 0.0137960779 	 0.0095965761
epoch_time;  33.89077353477478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002541171619668603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005950005259364843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005787041503936052
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00937151163816452
19 0.0156159701 	 0.0093715119
epoch_time;  33.821157932281494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018284943653270602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004340259823948145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004103818908333778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006873872596770525
20 0.0107443569 	 0.0068738727
epoch_time;  34.09162640571594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013431578408926725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0033046496100723743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003153966274112463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005202076863497496
21 0.0098263266 	 0.0052020769
epoch_time;  34.352630376815796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003444923320785165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009193180128932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007892117835581303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014197571203112602
22 0.063834399 	 0.0141975714
epoch_time;  34.73224997520447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029055618215352297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006857260596007109
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006019051652401686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01040109246969223
23 0.0143331243 	 0.0104010926
epoch_time;  34.36009430885315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003655834589153528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006147992797195911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006046175491064787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008832667954266071
24 0.011619038 	 0.0088326679
epoch_time;  34.34196329116821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019498767796903849
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–ˆâ–‚â–†â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–…â–…
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–„
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ƒâ–ƒâ–‚â–‚â–ˆâ–‚â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–†
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‚â–â–â–ˆâ–â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–„
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.13732
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.09202
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.08601
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.04533
wandb:                         Train loss 0.20898
wandb: 
wandb: ğŸš€ View run chromatic-festival-1697 at: https://wandb.ai/nreints/thesis/runs/ngj95t94
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_131049-ngj95t94/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_132901-tgp7ibst
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-dog-1701
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/tgp7ibst
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0040555610321462154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003805241547524929
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0062067522667348385
25 0.009998676 	 0.0062067523
epoch_time;  34.75148391723633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003537515876814723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007204926572740078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005986040458083153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009617120958864689
26 0.0087686845 	 0.0096171212
epoch_time;  34.566062450408936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018359489040449262
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0037512616254389286
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0037673665210604668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005818014964461327
27 0.0083574473 	 0.0058180151
epoch_time;  33.92659044265747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029100258834660053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005021343939006329
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004883855581283569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007072357460856438
28 0.0079613021 	 0.0070723573
epoch_time;  34.4287896156311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04532819613814354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09204576909542084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08602654933929443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13732829689979553
29 0.208981418 	 0.1373282948
epoch_time;  34.38917779922485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04532669857144356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0920238345861435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08600785583257675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1373225748538971
It took  1091.938234090805  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c4bde020>, <torch.utils.data.dataloader.DataLoader object at 0x1533cb886860>, <torch.utils.data.dataloader.DataLoader object at 0x1533cb884190>, <torch.utils.data.dataloader.DataLoader object at 0x1533cb886fb0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.352733314037323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42571142315864563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34974175691604614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43912258744239807
0 2.8722529097 	 0.4391225947
epoch_time;  34.553162574768066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01205095462501049
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.040694404393434525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03244662657380104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.060951538383960724
1 0.2348235691 	 0.0609515383
epoch_time;  34.197834491729736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7923499941825867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0798801183700562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.831538736820221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1974488496780396
2 0.0917240521 	 1.1974488227
epoch_time;  34.82228946685791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006054817698895931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02250152826309204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01902852952480316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.038141053169965744
3 0.0875528467 	 0.0381410547
epoch_time;  34.315056800842285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009346979670226574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018643056973814964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01911652460694313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.029403991997241974
4 0.0327376857 	 0.0294039919
epoch_time;  34.61615610122681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008964311331510544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01641547679901123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016431445255875587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.025341350585222244
5 0.0353182346 	 0.0253413509
epoch_time;  34.60270929336548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011609992012381554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044752415269613266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032801304012537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06375331431627274
6 0.1435008024 	 0.0637533124
epoch_time;  34.56538391113281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007289812434464693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019347352907061577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017476392909884453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.030603034421801567
7 0.0409174853 	 0.0306030343
epoch_time;  34.58943057060242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07095708698034286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09136750549077988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11082462966442108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1328890472650528
8 0.0271709422 	 0.1328890475
epoch_time;  34.5476496219635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028244657441973686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00807560607790947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008406533859670162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015138652175664902
9 0.024813852 	 0.015138652
epoch_time;  34.219401597976685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012438352219760418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02106436900794506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023366829380393028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03196647763252258
10 0.0162038377 	 0.0319664774
epoch_time;  34.34409713745117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0324273519217968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03897532820701599
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.046493321657180786
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.051910318434238434
11 0.0148807456 	 0.0519103197
epoch_time;  34.36470937728882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002503451192751527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005975822452455759
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005760618019849062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009707614779472351
12 0.0157796093 	 0.0097076151
epoch_time;  34.67922306060791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014415552141144872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004185003228485584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0038500837981700897
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007193875964730978
13 0.0116497043 	 0.0071938758
epoch_time;  34.590513944625854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001631733844988048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004298907704651356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0038880242500454187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007439298555254936
14 0.0159987436 	 0.0074392986
epoch_time;  34.11665868759155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004649573937058449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008306832052767277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008705955930054188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012544096447527409
15 0.0152417644 	 0.0125440961
epoch_time;  34.11849570274353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35403385758399963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5802338719367981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4001716077327728
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5881255865097046
16 0.0706274017 	 0.5881255919
epoch_time;  34.71578288078308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006504220888018608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.016088638454675674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013431059196591377
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–„â–â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.00748
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00469
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00389
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00178
wandb:                         Train loss 0.01025
wandb: 
wandb: ğŸš€ View run cheerful-dog-1701 at: https://wandb.ai/nreints/thesis/runs/tgp7ibst
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_132901-tgp7ibst/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_134719-7jv6yelc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-orchid-1705
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/7jv6yelc
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.023978229612112045
17 0.0684896523 	 0.0239782305
epoch_time;  34.31081199645996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02070453204214573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02638115920126438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03292294219136238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03864004835486412
18 0.0198081115 	 0.0386400482
epoch_time;  34.691046476364136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038874743040651083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.008461128920316696
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007147197611629963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.012067601084709167
19 0.0154500813 	 0.012067601
epoch_time;  34.87225675582886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035335153806954622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009105067700147629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008590680547058582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015491817146539688
20 0.0332852897 	 0.0154918169
epoch_time;  34.233081340789795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018055504187941551
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005028764251619577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00458990503102541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008799252100288868
21 0.0140662933 	 0.0087992526
epoch_time;  34.99167060852051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018543117912486196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005213190335780382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004071630537509918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008042238652706146
22 0.0118999214 	 0.008042239
epoch_time;  34.10598707199097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017867614515125751
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00452693086117506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003362043993547559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006640407722443342
23 0.0099590649 	 0.0066404076
epoch_time;  34.33090400695801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017659686272963881
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0038828805554658175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003590670879930258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006239863578230143
24 0.0136253464 	 0.0062398637
epoch_time;  34.18514323234558
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016581257805228233
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00408895593136549
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003766073612496257
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00767067214474082
25 0.0146303372 	 0.0076706719
epoch_time;  34.24264407157898
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001175391604192555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.002909174421802163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002869094954803586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.004944765940308571
26 0.0080883891 	 0.004944766
epoch_time;  34.242100954055786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003927118610590696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01287360955029726
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008964628912508488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01899307779967785
27 0.0386265936 	 0.018993078
epoch_time;  34.23007678985596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002249844605103135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005762076936662197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0047945440746843815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009412196464836597
28 0.0129211304 	 0.0094121965
epoch_time;  34.17037868499756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017782422946766019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004686638247221708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0038923947140574455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0074777076952159405
29 0.010246287 	 0.0074777077
epoch_time;  34.31469488143921
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017788070254027843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00468679703772068
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0038931304588913918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007482340093702078
It took  1097.9591488838196  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533cb8841f0>, <torch.utils.data.dataloader.DataLoader object at 0x15339f5b77f0>, <torch.utils.data.dataloader.DataLoader object at 0x1533cb8853f0>, <torch.utils.data.dataloader.DataLoader object at 0x1533c53bfa00>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4344210624694824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6479344367980957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9645252227783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1872735023498535
0 2.8714801982 	 3.1872735614
epoch_time;  34.16255736351013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010214855894446373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030958864837884903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026606043800711632
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04801227152347565
1 0.1356263316 	 0.0480122725
epoch_time;  34.0614013671875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006379081401973963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019479187205433846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01727423071861267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03180579096078873
2 0.0574395283 	 0.0318057904
epoch_time;  34.578386306762695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007864648476243019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030158627778291702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02568342536687851
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.049401454627513885
3 0.1440066988 	 0.0494014533
epoch_time;  34.43606352806091
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04055839031934738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05524589121341705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0592871718108654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07477828860282898
4 0.0365310831 	 0.0747782889
epoch_time;  34.449928283691406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0052764867432415485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.014071538113057613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012569572776556015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02173525281250477
5 0.0328545812 	 0.0217352533
epoch_time;  34.06132936477661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006479857489466667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017912624403834343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014052274636924267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02675984799861908
6 0.0897778808 	 0.0267598478
epoch_time;  33.94090962409973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035513981711119413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.013418065384030342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010867497883737087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02128051035106182
7 0.0306963809 	 0.0212805105
epoch_time;  33.5362811088562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005390174686908722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012101300060749054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00942113995552063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.016589459031820297
8 0.0169670298 	 0.0165894586
epoch_time;  33.53994059562683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016978264320641756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005855985451489687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005240086931735277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009840443730354309
9 0.0224594445 	 0.0098404438
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.01076
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00731
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.0059
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00234
wandb:                         Train loss 0.04489
wandb: 
wandb: ğŸš€ View run beaming-orchid-1705 at: https://wandb.ai/nreints/thesis/runs/7jv6yelc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_134719-7jv6yelc/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_140518-9fktkxnd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-rat-1708
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/9fktkxnd
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  33.60343050956726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017306670546531677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04246433451771736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03271840512752533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06269507855176926
10 0.088030657 	 0.0626950797
epoch_time;  33.70203709602356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00333844474516809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.010870184749364853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0093058031052351
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.018780643120408058
11 0.0325306583 	 0.0187806435
epoch_time;  33.97683930397034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002834624145179987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007768845651298761
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007342164404690266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.013750323094427586
12 0.0185496138 	 0.0137503226
epoch_time;  33.6385555267334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001435260521247983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005312489811331034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004274414386600256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009136682376265526
13 0.015849416 	 0.0091366826
epoch_time;  34.36110329627991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14111094176769257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2158907949924469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1365216225385666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20209148526191711
14 0.0826417575 	 0.2020914821
epoch_time;  34.03672003746033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005166810471564531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01708805002272129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013057121075689793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02323598600924015
15 0.0561412202 	 0.0232359852
epoch_time;  33.95215630531311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011647136881947517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019791364669799805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018615934997797012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.026372887194156647
16 0.0210812011 	 0.0263728865
epoch_time;  33.74061989784241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003826477099210024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0073181092739105225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007241756189614534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011090347543358803
17 0.0150880258 	 0.0110903478
epoch_time;  33.59467363357544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016279674600809813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004836016800254583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0040408410131931305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0076170191168785095
18 0.0126190575 	 0.0076170191
epoch_time;  33.48271656036377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017630313523113728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0041782003827393055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003965830430388451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0065980590879917145
19 0.0111830226 	 0.0065980591
epoch_time;  33.67820334434509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004973867442458868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.015160010196268559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01198774203658104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021918339654803276
20 0.0882037825 	 0.02191834
epoch_time;  33.85299301147461
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042069848626852036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04611359164118767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.055164217948913574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05809434875845909
21 0.0170521935 	 0.0580943474
epoch_time;  33.883023738861084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012380455154925585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004615303594619036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003597585018724203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007130471523851156
22 0.0118416397 	 0.0071304716
epoch_time;  33.859676122665405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003070482751354575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0055676912888884544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00604737363755703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008465494960546494
23 0.0115045747 	 0.0084654954
epoch_time;  33.522268295288086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015513221733272076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004132958594709635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003519889898598194
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.005901230964809656
24 0.0097466266 	 0.0059012311
epoch_time;  33.364577770233154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07254578918218613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19498902559280396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1070370152592659
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2168467789888382
25 0.0225070821 	 0.2168467772
epoch_time;  33.60773468017578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031897835433483124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006129732355475426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00587341096252203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008570123463869095
26 0.0135151345 	 0.0085701237
epoch_time;  33.87872123718262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022812294773757458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004808120429515839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004225480370223522
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006597921717911959
27 0.0090572854 	 0.0065979216
epoch_time;  33.85420632362366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027400010731071234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004849291872233152
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0046338606625795364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006839933805167675
28 0.0085265573 	 0.0068399337
epoch_time;  33.90966057777405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002343344734981656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007283504121005535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005899484269320965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010753937996923923
29 0.0448881376 	 0.0107539384
epoch_time;  33.82437324523926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002342888619750738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007309467066079378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005901355296373367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010759894736111164
It took  1079.0300290584564  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1533c53bf430>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bdcc70>, <torch.utils.data.dataloader.DataLoader object at 0x1533c4bde950>, <torch.utils.data.dataloader.DataLoader object at 0x15339f5b6d10>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02327043190598488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0525546632707119
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.046559371054172516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0861714780330658
0 2.84837555 	 0.0861714815
epoch_time;  33.67664623260498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019987378269433975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04479122906923294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04344363138079643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07164834439754486
1 0.1628906804 	 0.0716483471
epoch_time;  33.6290545463562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010068907402455807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.025560801848769188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025804484263062477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04626964405179024
2 0.0588060216 	 0.0462696444
epoch_time;  33.55497622489929
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5157536864280701
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9505837559700012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7183133959770203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1166530847549438
3 0.0927561573 	 1.1166530275
epoch_time;  33.70074796676636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004914551042020321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.016497690230607986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017021551728248596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03158748522400856
4 0.0794070906 	 0.0315874855
epoch_time;  33.67246747016907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005380937363952398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01237479131668806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012684506364166737
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.021607374772429466
5 0.0296921856 	 0.0216073745
epoch_time;  33.62779450416565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004564607050269842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.012148022651672363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013336708769202232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.023320257663726807
6 0.0574099637 	 0.0233202586
epoch_time;  34.18839454650879
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026985632721334696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007764111272990704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008205357939004898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.014843245036900043
7 0.0230759008 	 0.0148432449
epoch_time;  34.043133020401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004090944305062294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.011087463237345219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01153036393225193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.020737219601869583
8 0.0515381368 	 0.0207372187
epoch_time;  34.37139272689819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004316006321460009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.009555589407682419
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010024814866483212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01664707250893116
9 0.019316521 	 0.0166470724
epoch_time;  33.71864461898804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003794923657551408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01014070026576519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010736212134361267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01904894784092903
10 0.0252530187 	 0.0190489472
epoch_time;  33.63477396965027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001817470183596015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004850191064178944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005143587943166494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009137498214840889
11 0.0142312523 	 0.0091374986
epoch_time;  33.47781157493591
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002589454175904393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00534875737503171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005716629326343536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.009235871955752373
12 0.0147420232 	 0.0092358719
epoch_time;  33.234325647354126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014744665240868926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0039015342481434345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004034234210848808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.007397863548249006
13 0.0171611029 	 0.0073978634
epoch_time;  33.43962121009827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09109162539243698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23476573824882507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11914991587400436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26201772689819336
14 0.0149703722 	 0.2620177255
epoch_time;  33.61122441291809
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002295203972607851
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006923231761902571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006088960450142622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.01189306378364563
15 0.021113678 	 0.0118930642
epoch_time;  33.00200414657593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031758216209709644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006077250465750694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005940896924585104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.00946942251175642
16 0.0099478201 	 0.0094694227
epoch_time;  33.55909299850464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002904519671574235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005150921177119017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0053268554620444775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008303544484078884
17 0.0108091006 	 0.0083035443
epoch_time;  33.78724408149719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009792471304535866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019084645435214043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01716403104364872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.027762623503804207
18 0.0957049674 	 0.0277626234
epoch_time;  33.77675795555115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004303502384573221
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00996081531047821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00886685959994793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.015532161109149456
19 0.0182785132 	 0.0155321608
epoch_time;  33.81561231613159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041300649754703045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.007191685494035482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007242647930979729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011277212761342525
20 0.0140364038 	 0.0112772132
epoch_time;  33.76473665237427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012217577546834946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018262632191181183
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02049168013036251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02667742781341076
21 0.012909285 	 0.0266774284
epoch_time;  33.75990438461304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002663566032424569
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005579134449362755
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005075532011687756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008423618040978909
22 0.0104699947 	 0.008423618
epoch_time;  33.23235845565796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009691230952739716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01332311425358057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013810352422297001
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.017777539789676666
23 0.0333083858 	 0.0177775395
epoch_time;  33.58805871009827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002348418580368161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.005439601838588715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004865915514528751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.008661696687340736
24 0.0104315979 	 0.0086616966
epoch_time;  34.0182147026062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003017452312633395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00736439973115921
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006689166650176048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.011764541268348694
25 0.0374468876 	 0.0117645415
epoch_time;  33.49871778488159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00209596985951066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.004224487114697695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004388061352074146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.006865133997052908
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‚â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.01099
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.00671
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00696
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00337
wandb:                         Train loss 0.0155
wandb: 
wandb: ğŸš€ View run fortuitous-rat-1708 at: https://wandb.ai/nreints/thesis/runs/9fktkxnd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_140518-9fktkxnd/logs
26 0.0108539789 	 0.0068651341
epoch_time;  33.74763607978821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13940565288066864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2598482370376587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16904202103614807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2745289206504822
27 0.066691795 	 0.2745289183
epoch_time;  33.87330889701843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006740995217114687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.013306244276463985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013318224810063839
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.02069232612848282
28 0.045101339 	 0.0206923254
epoch_time;  33.69489884376526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003370175138115883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.00671762228012085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006961590610444546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010995307005941868
29 0.0154962186 	 0.0109953074
epoch_time;  33.909279584884644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033690151758491993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.006714513525366783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006955433636903763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.010987721383571625
It took  1071.845021724701  seconds.
