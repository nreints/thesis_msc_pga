wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163842-the6bsd8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-monkey-1135
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/the6bsd8
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–ƒâ–„â–„â–ƒâ–‚â–â–‚â–ƒâ–â–‚â–‚â–â–‚â–ƒâ–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–„â–„â–„â–‡â–…â–„â–‚â–â–ƒâ–â–ƒâ–…â–‚â–‚â–†â–‚â–â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.57956
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26632
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.55527
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11593
wandb:                         Train loss 1.33077
wandb: 
wandb: ðŸš€ View run brilliant-monkey-1135 at: https://wandb.ai/nreints/thesis/runs/the6bsd8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163842-the6bsd8/logs
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1905413419008255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6043577194213867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.311984062194824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.606090545654297
0 3.3091868282 	 8.6060903188 	 8.6081846495
epoch_time;  36.753883600234985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12336985766887665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3911963701248169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.469688892364502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6548027992248535
1 1.5969335805 	 6.6548029719 	 6.6559134818
epoch_time;  34.58815407752991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14120498299598694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3783503472805023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.5873847007751465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.770577907562256
2 1.5068544505 	 5.7705777555 	 5.7721983214
epoch_time;  34.356741189956665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13307344913482666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3602437973022461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.796057224273682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0775017738342285
3 1.4630172409 	 5.0775017816 	 5.0794400602
epoch_time;  33.77782106399536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13302050530910492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4076756238937378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.746522426605225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.152740478515625
4 1.4306996009 	 5.1527406435 	 5.1555703653
epoch_time;  33.82092547416687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17799393832683563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40783071517944336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.383980751037598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.681057929992676
5 1.4148410392 	 4.6810579867 	 4.6821193386
epoch_time;  34.36001992225647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1426285356283188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37340545654296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.052358627319336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.28016996383667
6 1.3970597667 	 4.2801698427 	 4.2816270983
epoch_time;  34.39401698112488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12906020879745483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3205932080745697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.16248893737793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.35502290725708
7 1.3908193868 	 4.3550226985 	 4.3562839817
epoch_time;  34.426411151885986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10190007090568542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2695425748825073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.917987108230591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.031687259674072
8 1.3790012119 	 4.0316874736 	 4.0328332849
epoch_time;  33.776591777801514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09969831258058548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28266122937202454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.718313217163086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8392367362976074
9 1.3697067602 	 3.8392366976 	 3.8404808251
epoch_time;  34.05658316612244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1229819506406784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33216747641563416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.910599946975708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.013153553009033
10 1.3640437556 	 4.0131535711 	 4.0143452412
epoch_time;  33.754499435424805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09342703223228455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.257755845785141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.990070104598999
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.060470104217529
11 1.3569407014 	 4.0604703336 	 4.0614049963
epoch_time;  34.37197160720825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12020275741815567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31811660528182983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8605527877807617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.046786785125732
12 1.3506541217 	 4.0467869114 	 4.0475767393
epoch_time;  34.191336154937744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14611056447029114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32348111271858215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.134825229644775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1997785568237305
13 1.3500932759 	 4.1997786238 	 4.2006017736
epoch_time;  34.69132995605469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10186977684497833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25185731053352356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.614461660385132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6614890098571777
14 1.3423639193 	 3.6614891258 	 3.6622053817
epoch_time;  34.13068723678589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11040212213993073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28253528475761414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5575997829437256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6319098472595215
15 1.3400907833 	 3.6319098395 	 3.6327075855
epoch_time;  34.217557191848755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15784917771816254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3516402840614319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.966974973678589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.04490852355957
16 1.3363201578 	 4.0449086782 	 4.0454457876
epoch_time;  33.97245693206787
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10551559925079346
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2668026387691498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.614459991455078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6359407901763916
17 1.3356131087 	 3.6359407992 	 3.6365867821
epoch_time;  34.34994196891785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0984141081571579
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26626530289649963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.557771921157837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.605588912963867
18 1.3358896471 	 3.6055888408 	 3.606328191
epoch_time;  33.98309111595154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11590828001499176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2663033604621887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.555309295654297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5798122882843018
19 1.3307742646 	 3.5798122097 	 3.5805601378
epoch_time;  34.71539306640625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11592517793178558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2663220763206482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5552730560302734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.579564332962036
It took 745.1769852638245 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn54: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135331.0

JOB STATISTICS
==============
Job ID: 2135331
Array Job ID: 2135328_3
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:48:36 core-walltime
Job Wall-clock time: 00:12:42
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
