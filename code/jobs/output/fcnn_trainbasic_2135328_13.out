wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165126-tpxsu2jo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-fish-1141
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/tpxsu2jo
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂█▁▁▃▅▁▃▁▁▂▁▂▃▁▅▁▁▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆█▄▄▃▃▄▂▁▂▂▄▂▂▅▂▁▁▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▆▂▂█▃▃▁▂▂▂▂▂▂▁▆▂▃▁▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▅▆▄▃▃▅▂▂▂▃▆▂▃█▂▁▁▃▁▁
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 56458.63281
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.64757
wandb:    Test loss t(0, 0)_r(-5, 5)_none 140.13043
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.28901
wandb:                         Train loss 3.80299
wandb: 
wandb: 🚀 View run cheerful-fish-1141 at: https://wandb.ai/nreints/thesis/runs/tpxsu2jo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165126-tpxsu2jo/logs
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6397271752357483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3597371578216553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.98768615722656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63192.015625
0 8.1560972675 	 63192.0162162162 	 63192.0162162162
epoch_time;  35.687233209609985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6969693303108215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2744879722595215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 288.4892883300781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 407678.28125
1 5.3153810908 	 407678.2702702703 	 407678.2702702703
epoch_time;  34.15822625160217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8132466077804565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8937548398971558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.05111694335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6147.0654296875
2 5.1096880686 	 6147.0655405405 	 6147.0655405405
epoch_time;  34.43929409980774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5622901320457458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9013932943344116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.57213592529297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13009.365234375
3 4.7575880295 	 13009.3648648649 	 13009.3648648649
epoch_time;  34.18268847465515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45792022347450256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4797312021255493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 384.1797790527344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 115368.359375
4 4.584697788 	 115368.3567567568 	 115368.3567567568
epoch_time;  34.162540912628174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4504638612270355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4887669086456299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 145.77096557617188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 229688.46875
5 4.4660672631 	 229688.4756756757 	 229688.4756756757
epoch_time;  33.99585771560669
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7556489109992981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.770774245262146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 145.00430297851562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14812.1220703125
6 4.3183659868 	 14812.1216216216 	 14812.1216216216
epoch_time;  34.90512132644653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.365268349647522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0143083333969116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.79487991333008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 104185.0703125
7 4.2719984423 	 104185.0702702703 	 104185.0702702703
epoch_time;  33.90615224838257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3647734820842743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7621784806251526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 109.11157989501953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26454.5859375
8 4.1307965629 	 26454.5864864865 	 26454.5864864865
epoch_time;  34.334716796875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3733326196670532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8651015758514404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.06441497802734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14587.5810546875
9 4.1307962323 	 14587.5810810811 	 14587.5810810811
epoch_time;  34.42236042022705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45075592398643494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0138875246047974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 114.28378295898438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 64181.96875
10 4.0907922124 	 64181.9675675676 	 64181.9675675676
epoch_time;  34.25072360038757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8022881746292114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6986664533615112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.97771453857422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7648.25146484375
11 4.0323769809 	 7648.2513513514 	 7648.2513513514
epoch_time;  34.22666573524475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3679971992969513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8084461688995361
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 114.60177612304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 54984.62109375
12 3.9504254031 	 54984.6216216216 	 54984.6216216216
epoch_time;  33.881200075149536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5087814331054688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1172899007797241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 116.11979675292969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 115103.4296875
13 3.9078920399 	 115103.427027027 	 115103.427027027
epoch_time;  34.52497053146362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0546669960021973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.08390474319458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 55.282745361328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1651.889404296875
14 3.8958724531 	 1651.8893581081 	 1651.8893581081
epoch_time;  35.11750054359436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4016682505607605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9096636176109314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 291.30517578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 214472.734375
15 3.8760644994 	 214472.7351351351 	 214472.7351351351
epoch_time;  34.37660002708435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26889318227767944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5939795970916748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 81.03298950195312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16918.21875
16 3.8758234994 	 16918.2189189189 	 16918.2189189189
epoch_time;  33.8460431098938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3006317615509033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6862746477127075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.85020446777344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15659.271484375
17 3.8654003311 	 15659.2716216216 	 15659.2716216216
epoch_time;  34.183329582214355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4708593785762787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2616373300552368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.329193115234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100449.8046875
18 3.7797233359 	 100449.8054054054 	 100449.8054054054
epoch_time;  35.48694849014282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2887953817844391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6473629474639893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.12936401367188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 56458.5390625
19 3.8029932269 	 56458.5405405405 	 56458.5405405405
epoch_time;  34.24225664138794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28901153802871704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6475685238838196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.13043212890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 56458.6328125
It took 741.8424043655396 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn57: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135377.0

JOB STATISTICS
==============
Job ID: 2135377
Array Job ID: 2135328_13
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:57:48
CPU Efficiency: 25.55% of 03:46:12 core-walltime
Job Wall-clock time: 00:12:34
Memory Utilized: 3.88 GB
Memory Efficiency: 12.42% of 31.25 GB
