wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_164016-ehedu4y9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-ox-1327
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ehedu4y9
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▅▇▂▄▄▄▄▅▃▅▁▃▆▂▅▄▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇█▅█▃▄▃▄▃█▂▆▁▂▅▂▆▄▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▆▅▄█▃▅▄▃▅▅▄▄▁▃▇▃▅▃▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▆▄▃█▃▄▂▃▄▇▃▅▁▂▅▂▅▃▆▆
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.29285
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.30607
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.18737
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.21187
wandb:                         Train loss 3.3636
wandb: 
wandb: 🚀 View run thriving-ox-1327 at: https://wandb.ai/nreints/thesis/runs/ehedu4y9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_164016-ehedu4y9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165305-plxejnhg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-dragon-1334
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/plxejnhg
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18076840043067932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32016071677207947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2349783033132553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3756389617919922
0 8.7578706838 	 0.3756389721 	 0.3943259162
epoch_time;  37.16383647918701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20297202467918396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31034913659095764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21955254673957825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32005417346954346
1 4.0589382247 	 0.3200541728 	 0.3339139165
epoch_time;  35.71757507324219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18634521961212158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32503512501716614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1943751722574234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3304821252822876
2 3.7768435675 	 0.3304821324 	 0.3466897913
epoch_time;  35.2391037940979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1763785034418106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26870521903038025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19271007180213928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30673742294311523
3 3.6461551075 	 0.3067374358 	 0.3226540643
epoch_time;  34.14379358291626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23090729117393494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31748446822166443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24237792193889618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3478555977344513
4 3.5863745398 	 0.347855604 	 0.3603851731
epoch_time;  34.575239419937134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17339947819709778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23184612393379211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17374886572360992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2338305562734604
5 3.5317386118 	 0.2338305499 	 0.2488019995
epoch_time;  34.52092170715332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18897822499275208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25929954648017883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.196309432387352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2880760729312897
6 3.4979533181 	 0.2880760811 	 0.300717658
epoch_time;  35.17405295372009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16564568877220154
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24010984599590302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18136687576770782
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2788309156894684
7 3.4796283905 	 0.2788309046 	 0.2927425075
epoch_time;  34.60105299949646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16963014006614685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2444567084312439
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17289353907108307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2663615047931671
8 3.4563578384 	 0.2663615046 	 0.2794525146
epoch_time;  34.736090898513794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1901017278432846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24221323430538177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20415852963924408
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2728540599346161
9 3.4409536725 	 0.2728540575 	 0.2869033607
epoch_time;  34.84437012672424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2150709480047226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31791922450065613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1946566253900528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3100574016571045
10 3.4337838392 	 0.3100573978 	 0.3236334311
epoch_time;  34.64789319038391
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16885270178318024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22348012030124664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18154950439929962
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2589435279369354
11 3.4191129216 	 0.2589435268 	 0.2713257661
epoch_time;  35.14369583129883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19619901478290558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29578498005867004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.184687539935112
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29307398200035095
12 3.4073323513 	 0.2930739738 	 0.3050574019
epoch_time;  35.128058433532715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15029843151569366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19834323227405548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14607682824134827
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20550687611103058
13 3.3958985697 	 0.2055068764 	 0.2182878237
epoch_time;  35.114757776260376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16284611821174622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21432237327098846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17744843661785126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24664874374866486
14 3.3897383579 	 0.2466487369 	 0.2592137105
epoch_time;  34.918174505233765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2005055844783783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26420095562934875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23359282314777374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32624900341033936
15 3.3828220871 	 0.3262489937 	 0.3382972305
epoch_time;  34.63824415206909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16351507604122162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21058036386966705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1723959594964981
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23776593804359436
16 3.374639657 	 0.2377659359 	 0.2490954425
epoch_time;  34.80990552902222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19667071104049683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2817261219024658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19566300511360168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3018861711025238
17 3.3652505739 	 0.3018861719 	 0.3149306426
epoch_time;  35.04350447654724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17754150927066803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25728103518486023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17539221048355103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27506333589553833
18 3.3674919733 	 0.275063324 	 0.2874449653
epoch_time;  34.871098279953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21187689900398254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30610063672065735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18739162385463715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2928718626499176
19 3.3636045669 	 0.2928718773 	 0.3060071791
epoch_time;  34.4403715133667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21186521649360657
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3060734272003174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18736648559570312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29285138845443726
It took 768.5326476097107 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▅▁▂▄▄▂▂▂▂▁▁▃▃▂▁▂▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▄▅▁▂▃▄▂▂▃▁▃▂▃▃▂▁▁▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▅▅▅▂▃▅▅▂▃▂▃▁▃▃▄▂▁▂██
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▅▆▃▄▅▅▃▃▄▃▄▄▄▃▃▂▁██
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.31098
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24363
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25361
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19673
wandb:                         Train loss 3.34336
wandb: 
wandb: 🚀 View run fortuitous-dragon-1334 at: https://wandb.ai/nreints/thesis/runs/plxejnhg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165305-plxejnhg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_170531-xhizozjx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-lantern-1341
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/xhizozjx
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1964544951915741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36061933636665344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24286861717700958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40944600105285645
0 9.4086176847 	 0.4094460152 	 0.4292365822
epoch_time;  35.077946186065674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18437205255031586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32002905011177063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20631764829158783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3233824074268341
1 4.1011489321 	 0.3233824034 	 0.3369575088
epoch_time;  34.373642683029175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15631824731826782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23581667244434357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20910388231277466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30227652192115784
2 3.796470358 	 0.3022765082 	 0.3165689004
epoch_time;  34.4286630153656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17074330151081085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27912139892578125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2147178202867508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32717278599739075
3 3.6419635584 	 0.3271727897 	 0.3418917888
epoch_time;  34.53923535346985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13896116614341736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17346234619617462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1713586002588272
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21148009598255157
4 3.5630249767 	 0.2114800943 	 0.2247223313
epoch_time;  34.25960111618042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14224949479103088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20251701772212982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18531861901283264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2454713135957718
5 3.5233287359 	 0.2454713151 	 0.2581646481
epoch_time;  34.316386699676514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15795263648033142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22395414113998413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20548416674137115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29247620701789856
6 3.5049502528 	 0.292476221 	 0.30653004
epoch_time;  34.30275225639343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15805573761463165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23516200482845306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2061346173286438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2972905933856964
7 3.4762524067 	 0.2972905958 	 0.3098779421
epoch_time;  34.135998487472534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1385297030210495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20156735181808472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1661325842142105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23174409568309784
8 3.4588475108 	 0.2317440961 	 0.2455651567
epoch_time;  34.501930713653564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1385524719953537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1841832995414734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1778806746006012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23586204648017883
9 3.4435888044 	 0.2358620515 	 0.249019664
epoch_time;  34.604809284210205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1459697186946869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2163693755865097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16804032027721405
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24024435877799988
10 3.4238714323 	 0.2402443551 	 0.2542119
epoch_time;  34.478363275527954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13262629508972168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17587651312351227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18446354568004608
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23462778329849243
11 3.4122885547 	 0.2346277804 	 0.2469705324
epoch_time;  34.54866671562195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1422964185476303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21106299757957458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.152365043759346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21476052701473236
12 3.3967415739 	 0.2147605277 	 0.2266400724
epoch_time;  34.45888423919678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15006431937217712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1933390498161316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17796196043491364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22080065310001373
13 3.3914590783 	 0.2208006575 	 0.232956077
epoch_time;  34.93933844566345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14610324800014496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22369840741157532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17263197898864746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25451695919036865
14 3.3703049626 	 0.2545169727 	 0.265479526
epoch_time;  34.13035321235657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14018498361110687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20816850662231445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1866919994354248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26604533195495605
15 3.3619522787 	 0.2660453178 	 0.2780955134
epoch_time;  34.37121224403381
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13246764242649078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19521880149841309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16977114975452423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24804913997650146
16 3.3552727102 	 0.2480491432 	 0.2592662914
epoch_time;  34.623687744140625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12522561848163605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17561709880828857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1489861011505127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21377718448638916
17 3.3517010172 	 0.2137771813 	 0.2255710086
epoch_time;  34.21196246147156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11014890670776367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.163442000746727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16940775513648987
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23056428134441376
18 3.3456158493 	 0.2305642824 	 0.2433419717
epoch_time;  34.73812508583069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19674082100391388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24369466304779053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25363075733184814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.311066210269928
19 3.3433552757 	 0.3110662099 	 0.3225918744
epoch_time;  34.6888701915741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19672784209251404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24362978339195251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2536114752292633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.310976505279541
It took 745.8741056919098 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▄▄▄▃▄▄▄█▂▅▅▃▂▂▁▄▄▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▆▅▂▁▆▄▃█▁▇▄▃▁▂▁▅▃▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▂▃▃▄▃▄▄█▂▄▄▃▂▂▁▄▄▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▄▂▃▁▂▄▄▄█▂▆▄▄▂▃▂▅▄▄▄
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.25236
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26231
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.16846
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18979
wandb:                         Train loss 3.3889
wandb: 
wandb: 🚀 View run cheerful-lantern-1341 at: https://wandb.ai/nreints/thesis/runs/xhizozjx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_170531-xhizozjx/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171802-t22jn44p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-lamp-1347
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/t22jn44p
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19887806475162506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3107285499572754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26146793365478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3600657284259796
0 8.8048951104 	 0.3600657283 	 0.3794707015
epoch_time;  34.969913721084595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19171899557113647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30373334884643555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2446179986000061
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3548504710197449
1 4.1223529668 	 0.3548504804 	 0.3689800778
epoch_time;  34.932106256484985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.170734241604805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2834526598453522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1692294478416443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27871325612068176
2 3.8296176133 	 0.2787132676 	 0.2934435458
epoch_time;  34.68731093406677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17529171705245972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2606395483016968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1893746703863144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27283814549446106
3 3.6955121763 	 0.2728381389 	 0.286565461
epoch_time;  34.37541055679321
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15339143574237823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22096535563468933
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19088810682296753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26357728242874146
4 3.6149497371 	 0.2635772705 	 0.2778835812
epoch_time;  34.300727128982544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15997077524662018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20609150826931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1979835033416748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25389522314071655
5 3.5621423018 	 0.2538952183 	 0.2659059061
epoch_time;  34.70950508117676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19581793248653412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28530293703079224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18721099197864532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2745512127876282
6 3.5319366352 	 0.274551206 	 0.2865058693
epoch_time;  34.67878317832947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18648836016654968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24794210493564606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19556066393852234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26212531328201294
7 3.5024387615 	 0.2621253142 	 0.2746296032
epoch_time;  35.004722118377686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1878414899110794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2415820211172104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20676808059215546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27264973521232605
8 3.4900059374 	 0.2726497341 	 0.2863480439
epoch_time;  34.77237319946289
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24484528601169586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3129146695137024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2645268142223358
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3543163537979126
9 3.4664391768 	 0.3543163403 	 0.3651518538
epoch_time;  35.14996314048767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17022773623466492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20898883044719696
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1719435602426529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21937231719493866
10 3.45016027 	 0.2193723112 	 0.2326267552
epoch_time;  34.58342480659485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22255200147628784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29404500126838684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2075260877609253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29616960883140564
11 3.4447564478 	 0.2961696109 	 0.3073517671
epoch_time;  34.5904905796051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19362324476242065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2536573112010956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20356117188930511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28698593378067017
12 3.4307986126 	 0.2869859438 	 0.2987172926
epoch_time;  34.97988843917847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19068539142608643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23898088932037354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18955698609352112
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2548375427722931
13 3.4269502158 	 0.2548375516 	 0.2668048962
epoch_time;  34.76730036735535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17005687952041626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21287503838539124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1678740382194519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2241448312997818
14 3.4121622694 	 0.2241448274 	 0.2362318915
epoch_time;  34.55217146873474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1761723756790161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22358033061027527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16995559632778168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23023252189159393
15 3.4102200538 	 0.2302325274 	 0.2424008137
epoch_time;  34.6050808429718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16145636141300201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20947888493537903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15498560667037964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20586630702018738
16 3.4034727144 	 0.2058663033 	 0.2170646667
epoch_time;  34.81502342224121
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20625633001327515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2633077800273895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20631705224514008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27263548970222473
17 3.3927093692 	 0.2726354857 	 0.2830631359
epoch_time;  34.90744233131409
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19828678667545319
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24402563273906708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20263057947158813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2615017294883728
18 3.3846628115 	 0.2615017247 	 0.2725312929
epoch_time;  34.98965573310852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18980273604393005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2622498571872711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16844385862350464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2523456811904907
19 3.3889039491 	 0.2523456677 	 0.264449166
epoch_time;  35.152580976486206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1897912323474884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2623143494129181
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16845537722110748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2523644268512726
It took 750.9440152645111 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▂▄▆▄▃▄▂▃▄▁▃▄▃▂▄▄▆▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▂▆▇▅▄▅▅▄▇▁▅▇▄▃▅▅█▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▃▄▅▅▃▅▁▃▇▁▂▄▅▃▅▅▆▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▂▂▅▅▅▄▅▄▄█▁▃▆▆▄▄▅▆▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.24625
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22967
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17195
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16304
wandb:                         Train loss 3.33163
wandb: 
wandb: 🚀 View run sweet-lamp-1347 at: https://wandb.ai/nreints/thesis/runs/t22jn44p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171802-t22jn44p/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173027-834kzlt6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-envelope-1354
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/834kzlt6
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19038453698158264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28041040897369385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23055720329284668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33943307399749756
0 8.5077569609 	 0.3394330618 	 0.3571088224
epoch_time;  34.67328143119812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.150711327791214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24973975121974945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21809853613376617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3198087513446808
1 4.0758303757 	 0.3198087538 	 0.3331136136
epoch_time;  34.28433799743652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15107451379299164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19886897504329681
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17226998507976532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2305237054824829
2 3.7843757273 	 0.2305237023 	 0.246273247
epoch_time;  34.11930799484253
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17831748723983765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2581850290298462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19145998358726501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2771008610725403
3 3.6459279922 	 0.2771008672 	 0.2918655808
epoch_time;  34.473917961120605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17799687385559082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26877307891845703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19448009133338928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2977282702922821
4 3.5714705563 	 0.2977282756 	 0.3109767811
epoch_time;  34.44582962989807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17982062697410583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2439279705286026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19490286707878113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2782166302204132
5 3.5205755834 	 0.2782166352 	 0.2922405758
epoch_time;  34.47975015640259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16994638741016388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2355276197195053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1809338480234146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25674572587013245
6 3.4883093501 	 0.2567457251 	 0.2702994269
epoch_time;  34.56140327453613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18278193473815918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23772616684436798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19882875680923462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.271816611289978
7 3.4567251238 	 0.2718166248 	 0.2843437195
epoch_time;  34.718019247055054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1675117462873459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23730957508087158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15841591358184814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23126517236232758
8 3.4451435686 	 0.2312651763 	 0.2449030283
epoch_time;  34.67088174819946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16706116497516632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22959227859973907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1752183586359024
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2454034388065338
9 3.4224588217 	 0.2454034341 	 0.2572906907
epoch_time;  34.93249249458313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2073444426059723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2649327218532562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21919117867946625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2763727307319641
10 3.4021348172 	 0.2763727343 	 0.2887494474
epoch_time;  34.74095058441162
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1415911763906479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1911104917526245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15369752049446106
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2197539210319519
11 3.3927338328 	 0.2197539252 	 0.2323148985
epoch_time;  34.55709624290466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16386455297470093
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23617608845233917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16806142032146454
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24920080602169037
12 3.3814544368 	 0.2492008106 	 0.2614689183
epoch_time;  34.69508981704712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19221656024456024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26429998874664307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19190435111522675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.267511785030365
13 3.3734224251 	 0.2675117905 	 0.2787929019
epoch_time;  35.00763559341431
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18975888192653656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2333785444498062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20076502859592438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24770870804786682
14 3.3617697931 	 0.2477087072 	 0.2578291403
epoch_time;  34.620203256607056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1682291477918625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21774694323539734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17426477372646332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23701439797878265
15 3.3562519174 	 0.2370143993 	 0.2478329839
epoch_time;  34.60568046569824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17236852645874023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24190790951251984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19250065088272095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2707686424255371
16 3.3518491214 	 0.2707686553 	 0.2822780918
epoch_time;  34.64797759056091
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1753813922405243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24271853268146515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20053905248641968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2761503756046295
17 3.3446918506 	 0.2761503684 	 0.2866968928
epoch_time;  34.75003981590271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1923944652080536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.279113233089447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21372820436954498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.310137003660202
18 3.3404682343 	 0.3101369909 	 0.3209851445
epoch_time;  34.69624042510986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16303634643554688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22966209053993225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17197072505950928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2462560534477234
19 3.3316348727 	 0.2462560499 	 0.2580043896
epoch_time;  34.55368709564209
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16304229199886322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2296655774116516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17195381224155426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2462468147277832
It took 745.3721253871918 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▃▃▃▃▅▇▅▃▆▄▅▃▆▁▅▅▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▃▃▁▃▄▆▆▃▆▄▅▃▅▁▅▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▄▃▂▄▃▃▅▇▄▄▇▃▄▅█▁▄▆▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▄▄▃▄▆▇▇▅▇▃▅▅▇▁▅▄▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.22085
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.19329
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.16248
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12819
wandb:                         Train loss 3.32012
wandb: 
wandb: 🚀 View run radiant-envelope-1354 at: https://wandb.ai/nreints/thesis/runs/834kzlt6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173027-834kzlt6/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174254-94z61l93
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-ox-1361
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/94z61l93
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19726881384849548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30315595865249634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2221512645483017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33727630972862244
0 8.1335503766 	 0.3372763144 	 0.3544510712
epoch_time;  35.10002160072327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16223816573619843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2845095992088318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1816243827342987
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30462148785591125
1 4.0455830548 	 0.3046214748 	 0.3196273185
epoch_time;  34.990129709243774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1398744285106659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24255822598934174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17259782552719116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2774336040019989
2 3.7773668913 	 0.2774336119 	 0.293205055
epoch_time;  34.86755609512329
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14558500051498413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21007715165615082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1549256294965744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22800251841545105
3 3.6608118097 	 0.2280025173 	 0.2425757537
epoch_time;  34.598193883895874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15303660929203033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2091769278049469
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17776772379875183
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23679621517658234
4 3.5836567604 	 0.2367962193 	 0.2509276931
epoch_time;  34.50086545944214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13880734145641327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18452651798725128
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16895052790641785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2268732190132141
5 3.53704491 	 0.2268732226 	 0.2402265188
epoch_time;  34.839160680770874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14943073689937592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22064080834388733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16176116466522217
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23434937000274658
6 3.5016879919 	 0.2343493694 	 0.2485876341
epoch_time;  34.76381993293762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17598789930343628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23363777995109558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19734588265419006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2671668529510498
7 3.4677248408 	 0.2671668388 	 0.278929798
epoch_time;  34.60529279708862
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18700507283210754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26826056838035583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22185789048671722
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3221069872379303
8 3.4533739342 	 0.3221069748 	 0.3346184395
epoch_time;  34.51931691169739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18191969394683838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2703341841697693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18303035199642181
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27886393666267395
9 3.4326621477 	 0.2788639378 	 0.2916545352
epoch_time;  34.981873750686646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1612921804189682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22264201939105988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17790980637073517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23932233452796936
10 3.4108683406 	 0.2393223324 	 0.2512505402
epoch_time;  35.03841733932495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18636806309223175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2670040428638458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2156488299369812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2984312176704407
11 3.3904312743 	 0.2984312315 	 0.3111506488
epoch_time;  34.415515184402466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1407528817653656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23113657534122467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17323125898838043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2635478675365448
12 3.3775121525 	 0.2635478664 	 0.2773010666
epoch_time;  34.341975927352905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15451689064502716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24366648495197296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1860629916191101
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2763742506504059
13 3.371812783 	 0.2763742602 	 0.2888134931
epoch_time;  34.67817544937134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15844108164310455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21509553492069244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18922537565231323
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2396049052476883
14 3.3561854689 	 0.2396049087 	 0.2520448015
epoch_time;  35.169562339782715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1857762336730957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24883964657783508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2346193641424179
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29876449704170227
15 3.3496774765 	 0.2987644917 	 0.3097040537
epoch_time;  35.04871129989624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11068959534168243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17799372971057892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13923333585262299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1920197308063507
16 3.337609293 	 0.1920197358 	 0.2050304928
epoch_time;  35.0174503326416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15482524037361145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2478204220533371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1809675693511963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27326613664627075
17 3.3315254475 	 0.2732661273 	 0.2847841211
epoch_time;  35.21184945106506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14636997878551483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21658065915107727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20398248732089996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.278952956199646
18 3.319810334 	 0.2789529543 	 0.2901803919
epoch_time;  34.77003884315491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12820743024349213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19329161942005157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16244913637638092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22077839076519012
19 3.3201224108 	 0.2207783879 	 0.231696732
epoch_time;  34.83651828765869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12819133698940277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1932854801416397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16247914731502533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22085051238536835
It took 746.9292886257172 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▃▅▄▄▂▂▂▂▂▂▁▃▂▃▂▅▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▃▃▅▃▃▂▂▁▁▁▁▁▂▁▃▁▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▂▅▄▄▂▂▂▂▂▂▁▃▁▃▂▅▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▂▂▅▂▂▂▂▁▁▁▁▁▂▁▂▁▄▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.23556
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.19077
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17268
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14028
wandb:                         Train loss 3.36785
wandb: 
wandb: 🚀 View run flashing-ox-1361 at: https://wandb.ai/nreints/thesis/runs/94z61l93
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174254-94z61l93/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_175520-4ocucecm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-wonton-1369
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/4ocucecm
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3312046229839325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4763500690460205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35123246908187866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5027593970298767
0 7.5074257098 	 0.5027594077 	 0.5206997433
epoch_time;  34.81496834754944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1941169798374176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2796623110771179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2260538935661316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3212304711341858
1 4.0211937865 	 0.3212304605 	 0.3373755171
epoch_time;  35.13515520095825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16339650750160217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2656285762786865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1998327523469925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32500678300857544
2 3.749968594 	 0.325006784 	 0.3418394553
epoch_time;  34.917402029037476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17121165990829468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24804532527923584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1850539892911911
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2675851881504059
3 3.63020084 	 0.2675851977 	 0.2827788482
epoch_time;  34.96952939033508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25621601939201355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35754165053367615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26267895102500916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37368786334991455
4 3.574781414 	 0.3736878679 	 0.3870094918
epoch_time;  34.59027671813965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17486536502838135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27129220962524414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22486421465873718
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34283581376075745
5 3.5354931211 	 0.342835813 	 0.3576640773
epoch_time;  34.720385789871216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17821423709392548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2652469575405121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23045532405376434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3315102159976959
6 3.4989573195 	 0.3315102036 	 0.3474714537
epoch_time;  34.687405586242676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17477214336395264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23883271217346191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16611403226852417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25080713629722595
7 3.4709520592 	 0.2508071281 	 0.2639120978
epoch_time;  34.6897988319397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15830771625041962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20465007424354553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1653018444776535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22295942902565002
8 3.453463524 	 0.2229594256 	 0.2365577079
epoch_time;  34.49582099914551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14204315841197968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19016964733600616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17314504086971283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23200248181819916
9 3.4421745409 	 0.2320024851 	 0.2449129465
epoch_time;  34.93725109100342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14635810256004333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19482029974460602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18091724812984467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24730348587036133
10 3.4263647411 	 0.2473034833 	 0.2598437129
epoch_time;  34.545422315597534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1429135650396347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18807992339134216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1627722829580307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2173575758934021
11 3.4104749386 	 0.2173575736 	 0.2307546873
epoch_time;  34.446292877197266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14133892953395844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19723181426525116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1630978286266327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2302190661430359
12 3.4083905695 	 0.2302190626 	 0.2425830532
epoch_time;  35.04092597961426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13803163170814514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18218182027339935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14159958064556122
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19035853445529938
13 3.3943500867 	 0.1903585279 	 0.204306319
epoch_time;  34.611836433410645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17480920255184174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2280179262161255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19447070360183716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2617517411708832
14 3.3925087011 	 0.261751742 	 0.2733269356
epoch_time;  34.90790772438049
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13780122995376587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19679513573646545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14946652948856354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2188352793455124
15 3.3843067662 	 0.2188352843 	 0.2312750945
epoch_time;  35.19304370880127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1689286231994629
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24544934928417206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18954984843730927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27801913022994995
16 3.3822326454 	 0.2780191164 	 0.290490702
epoch_time;  35.16486859321594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13749980926513672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19627153873443604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16667205095291138
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22752566635608673
17 3.3759937532 	 0.2275256595 	 0.2402558816
epoch_time;  35.00409984588623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23333439230918884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32233312726020813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2532079219818115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3599960207939148
18 3.362067324 	 0.3599960327 	 0.3705510732
epoch_time;  34.50826668739319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14031390845775604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19070683419704437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1726229339838028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23561640083789825
19 3.3678473574 	 0.2356164056 	 0.2469114561
epoch_time;  34.72862219810486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1402830183506012
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1907736361026764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1726757287979126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23555928468704224
It took 746.5195722579956 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▅▄▁▃▅▅▂▂▄▃▄▄▃▂▃▃▄▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▄▃▁▂▄▃▂▃▃▃▄▃▂▃▃▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▅▅▁▃█▅▂▂▅▃▅▄▃▁▄▃▄▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▃▄▁▂▅▄▂▃▅▄▅▄▂▂▃▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.20367
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.17481
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.1477
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13099
wandb:                         Train loss 3.31005
wandb: 
wandb: 🚀 View run dazzling-wonton-1369 at: https://wandb.ai/nreints/thesis/runs/4ocucecm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_175520-4ocucecm/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180741-2fzg4wp5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-dragon-1376
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2fzg4wp5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.262636661529541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4396757483482361
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2749495804309845
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45782366394996643
0 8.1816789734 	 0.457823676 	 0.4790670446
epoch_time;  34.69404411315918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15150880813598633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2652756869792938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18832837045192719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2994927763938904
1 3.9945479117 	 0.299492769 	 0.3147986541
epoch_time;  34.18982815742493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16256946325302124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28291699290275574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2152988761663437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3472396731376648
2 3.7308598378 	 0.3472396851 	 0.3642736899
epoch_time;  34.61331129074097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18703831732273102
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26113399863243103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22755067050457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3142715394496918
3 3.6200037952 	 0.3142715248 	 0.3293484868
epoch_time;  34.82745957374573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13065874576568604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17208680510520935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15490181744098663
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2206386923789978
4 3.5489682951 	 0.2206386875 	 0.2350556245
epoch_time;  34.659913063049316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14259834587574005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20820344984531403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1799612045288086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2696327269077301
5 3.502979276 	 0.2696327209 	 0.2833859005
epoch_time;  34.725279331207275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21514098346233368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26956722140312195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26922208070755005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36460310220718384
6 3.4737723214 	 0.3646031148 	 0.3773622667
epoch_time;  34.70258140563965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18181206285953522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2626439034938812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22870922088623047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3387848138809204
7 3.4485469914 	 0.3387848107 	 0.3529880833
epoch_time;  34.39522457122803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1572035402059555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2058354765176773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17014655470848083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25146588683128357
8 3.4196511219 	 0.2514658747 	 0.2657545347
epoch_time;  34.5276038646698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17178376019001007
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2371828854084015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16000808775424957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23818877339363098
9 3.4095486258 	 0.2381887694 	 0.2529689892
epoch_time;  34.70003342628479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20356959104537964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2642187476158142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2178044319152832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3076309561729431
10 3.3961706452 	 0.3076309617 	 0.3209113456
epoch_time;  34.53868579864502
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18584370613098145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2542749345302582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1871328353881836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27205172181129456
11 3.3763273184 	 0.2720517133 	 0.2861372046
epoch_time;  34.432292222976685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20491179823875427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26898816227912903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2280404418706894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3184005916118622
12 3.3671620446 	 0.3184005944 	 0.3309122034
epoch_time;  34.29934620857239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18250538408756256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25651633739471436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20541326701641083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3017135560512543
13 3.3546740119 	 0.3017135414 	 0.3148836909
epoch_time;  34.759485721588135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15422745048999786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20660287141799927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19168296456336975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26231199502944946
14 3.3480635367 	 0.262311987 	 0.2731140137
epoch_time;  34.39429759979248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15241387486457825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23110409080982208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15586139261722565
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2514897882938385
15 3.339707547 	 0.2514897939 	 0.264865834
epoch_time;  34.55631422996521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1751500368118286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23911429941654205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20587992668151855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2918332815170288
16 3.3233413645 	 0.2918332899 	 0.3029718141
epoch_time;  34.52307152748108
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14875784516334534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2017153799533844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19045697152614594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2686510384082794
17 3.3274942345 	 0.2686510241 	 0.2801464803
epoch_time;  34.66113829612732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1551801860332489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22477559745311737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19768260419368744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2956431806087494
18 3.3217419706 	 0.2956431827 	 0.3077529701
epoch_time;  34.64564847946167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13103148341178894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17495626211166382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14765684306621552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20378834009170532
19 3.3100536349 	 0.2037883449 	 0.2168081335
epoch_time;  34.19045972824097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13099415600299835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17481279373168945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14770175516605377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2036702185869217
It took 740.9370386600494 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▂▃▃▃▂▂▄▄▄▄▃▃▄▂▁▂▃▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▂▂▃▃▂▃▄▄▆▄▃▂▃▂▁▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▁▄▃▂▃▂▃▃▄▃▃▃▄▁▁▂▅▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▃▄▃▃▄▅▃▆▄▃▂▃▂▁▂▃▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.24255
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.19887
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.1926
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14466
wandb:                         Train loss 3.3405
wandb: 
wandb: 🚀 View run twinkling-dragon-1376 at: https://wandb.ai/nreints/thesis/runs/2fzg4wp5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180741-2fzg4wp5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_182006-2vy4d835
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-cake-1383
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2vy4d835
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25914549827575684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41811808943748474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2816142141819
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4322485029697418
0 8.6409066626 	 0.4322485022 	 0.449216688
epoch_time;  34.36697220802307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14853535592556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19836194813251495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1573895514011383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21499334275722504
1 4.0540726028 	 0.214993348 	 0.2279172743
epoch_time;  34.74589204788208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1610579937696457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21267271041870117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19875238835811615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.251760333776474
2 3.7795510986 	 0.2517603281 	 0.2656502801
epoch_time;  34.55188059806824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17583067715168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24442064762115479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.186048224568367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2645191550254822
3 3.6726611427 	 0.2645191502 	 0.2785928056
epoch_time;  34.78196096420288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17234401404857635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2581438720226288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17498210072517395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25462818145751953
4 3.5994368071 	 0.2546281763 	 0.2683666745
epoch_time;  34.93129014968872
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15797355771064758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20904013514518738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1814049333333969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24108855426311493
5 3.548234055 	 0.2410885579 	 0.2550322868
epoch_time;  34.45270872116089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17447371780872345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23058953881263733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16691318154335022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23090487718582153
6 3.5189011457 	 0.2309048833 	 0.2436033713
epoch_time;  34.58757710456848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19481778144836426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2868790626525879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19069068133831024
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29116690158843994
7 3.487012446 	 0.2911668932 	 0.3039180034
epoch_time;  34.42120814323425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17266884446144104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2891530692577362
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.188320592045784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30778390169143677
8 3.4600759828 	 0.3077839001 	 0.3202824154
epoch_time;  34.345802545547485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22495415806770325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3327291011810303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20772185921669006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.302470326423645
9 3.4448179375 	 0.3024703361 	 0.3157447609
epoch_time;  34.67358684539795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1767830103635788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2765240967273712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19169150292873383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2937626838684082
10 3.4321335345 	 0.2937626813 	 0.3073989043
epoch_time;  34.213786125183105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16179732978343964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2454552948474884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18052083253860474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25911223888397217
11 3.4073660714 	 0.2591122395 	 0.2715946094
epoch_time;  34.45232939720154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15125469863414764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2227465808391571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18848536908626556
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.263675719499588
12 3.3987875947 	 0.2636757103 	 0.2759726859
epoch_time;  34.260538816452026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17208947241306305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24140140414237976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20860576629638672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2833351790904999
13 3.3712001999 	 0.2833351754 	 0.2948129602
epoch_time;  34.38244414329529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13759028911590576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21154803037643433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15694944560527802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22873909771442413
14 3.3724418955 	 0.2287391044 	 0.2410500191
epoch_time;  34.429532289505005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12695294618606567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1776607781648636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14918078482151031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19794543087482452
15 3.3643229387 	 0.1979454247 	 0.2079649023
epoch_time;  34.32740616798401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14936020970344543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21624498069286346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1608532816171646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22569099068641663
16 3.3522835165 	 0.2256909963 	 0.2375432401
epoch_time;  34.40055871009827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1605442315340042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21862846612930298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2166922688484192
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28003931045532227
17 3.3471038343 	 0.2800393182 	 0.2912201134
epoch_time;  34.464235067367554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1390897035598755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20271258056163788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1922924518585205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.266306608915329
18 3.3474506231 	 0.2663066142 	 0.2775726318
epoch_time;  34.33984017372131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14471685886383057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19881892204284668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19255773723125458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24255070090293884
19 3.3404989791 	 0.2425507004 	 0.2535450703
epoch_time;  34.760469913482666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14465616643428802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19886913895606995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1926046907901764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24255119264125824
It took 744.9667615890503 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▂▄▃▃▄▄▄▃▂▂▂▃▁▆▄▁▄▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▃▄▄▄▄▅▆▃▃▃▁▃▂█▄▂▅▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▂▄▂▂▃▄▃▄▃▃▂▂▂▇▄▂▄▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▄▁▄▃▃▄▆▅▃▂▄▁▄▃█▄▄▄▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.19994
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.18823
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.14532
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13479
wandb:                         Train loss 3.3633
wandb: 
wandb: 🚀 View run brilliant-cake-1383 at: https://wandb.ai/nreints/thesis/runs/2vy4d835
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_182006-2vy4d835/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_183233-5f0vwl1h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-rocket-1390
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5f0vwl1h
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17469649016857147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30211883783340454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23049384355545044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3560623228549957
0 8.0831453379 	 0.3560623169 	 0.37375113
epoch_time;  35.02984666824341
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16171175241470337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27682560682296753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2141423225402832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36318475008010864
1 4.0621032921 	 0.3631847485 	 0.3783653053
epoch_time;  34.78000283241272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13083569705486298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20868732035160065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1520359069108963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22913439571857452
2 3.7538100483 	 0.2291343895 	 0.2447283564
epoch_time;  34.71984815597534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15622849762439728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22346019744873047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18553058803081512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2669127881526947
3 3.6445924091 	 0.2669128006 	 0.2801865449
epoch_time;  34.49803400039673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14999809861183167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2245357185602188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1569831669330597
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24792250990867615
4 3.5716794891 	 0.2479225159 	 0.263784357
epoch_time;  34.71544098854065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.151991605758667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21990430355072021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16116061806678772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24196141958236694
5 3.5306595665 	 0.2419614225 	 0.2573117437
epoch_time;  34.73309874534607
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1537184715270996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22382262349128723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17433680593967438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2589064836502075
6 3.5108496292 	 0.2589064727 	 0.2740853799
epoch_time;  34.71994209289551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17861196398735046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23633690178394318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18038855493068695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26528623700141907
7 3.4752646041 	 0.2652862343 	 0.2794431738
epoch_time;  34.66621208190918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16727763414382935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2606019675731659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17284312844276428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2716129720211029
8 3.4537570878 	 0.2716129818 	 0.2862911534
epoch_time;  35.38060712814331
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15339581668376923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21088624000549316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17985659837722778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2576815187931061
9 3.4381968835 	 0.2576815219 	 0.2698809856
epoch_time;  34.67528986930847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14077706634998322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2004372626543045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1648739129304886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23201961815357208
10 3.4228347024 	 0.2320196203 	 0.2431185336
epoch_time;  34.20772910118103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1566661149263382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20365330576896667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1730635166168213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22905531525611877
11 3.4109916506 	 0.2290553119 	 0.2410306363
epoch_time;  34.486223459243774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12646204233169556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1701134443283081
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15460272133350372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21844658255577087
12 3.3994372038 	 0.2184465769 	 0.231094814
epoch_time;  34.34551763534546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15513959527015686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2152259349822998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16348962485790253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.239403635263443
13 3.3951451554 	 0.239403637 	 0.2514256864
epoch_time;  34.339072942733765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14836984872817993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19171035289764404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15291497111320496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20730909705162048
14 3.3870271904 	 0.2073091043 	 0.2185955151
epoch_time;  34.76192378997803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20249442756175995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29917269945144653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21823324263095856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3232193887233734
15 3.3704517445 	 0.3232194024 	 0.3344527167
epoch_time;  34.69411373138428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1630941480398178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22928860783576965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18022696673870087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2628014087677002
16 3.3762195461 	 0.2628014229 	 0.2749176438
epoch_time;  35.10181760787964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15668518841266632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19637486338615417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15265071392059326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20168215036392212
17 3.3670357592 	 0.2016821578 	 0.2128534265
epoch_time;  34.71398854255676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1588073968887329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2362942397594452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17803658545017242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2700965404510498
18 3.3618722759 	 0.2700965469 	 0.2825450588
epoch_time;  34.93273711204529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13479799032211304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18823952972888947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.145307719707489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1999780237674713
19 3.363295796 	 0.1999780191 	 0.2124375008
epoch_time;  34.85147190093994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13478751480579376
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18823347985744476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14531727135181427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19994114339351654
It took 746.8133037090302 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇█▁▂▄▂▂▆▃▃▂▅▃▄▅▂▅▃▁▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆█▁▂▅▂▂▇▄▄▃▅▃▄▅▄▆▄▃▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▁▂▅▂▁▆▄▂▂▆▃▄▆▂▃▂▁▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▆▁▂▆▃▃█▅▆▄▆▅▅▇▄▅▄▅▇▇
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.26741
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25711
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.20649
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20149
wandb:                         Train loss 3.34642
wandb: 
wandb: 🚀 View run cheerful-rocket-1390 at: https://wandb.ai/nreints/thesis/runs/5f0vwl1h
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_183233-5f0vwl1h/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15091535449028015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27055588364601135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23179812729358673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.350342333316803
0 8.6140144983 	 0.350342333 	 0.3695378072
epoch_time;  34.65955471992493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18081693351268768
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3060332238674164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2516719400882721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.370150089263916
1 4.0351744324 	 0.370150097 	 0.3868962468
epoch_time;  34.82004237174988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11926568299531937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1763189136981964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14500358700752258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20081134140491486
2 3.7735085768 	 0.2008113346 	 0.2154899597
epoch_time;  34.78165555000305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1324787735939026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20035779476165771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15518447756767273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2272745668888092
3 3.6539609957 	 0.2272745699 	 0.2424810049
epoch_time;  34.552215814590454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18893884122371674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2523024380207062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20369724929332733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2766359746456146
4 3.5863993341 	 0.276635969 	 0.2912790453
epoch_time;  34.52707576751709
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14411817491054535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2030147910118103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16240337491035461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23092354834079742
5 3.545308004 	 0.2309235444 	 0.2441170564
epoch_time;  34.30655288696289
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14473778009414673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19941197335720062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14889627695083618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21513281762599945
6 3.5012112813 	 0.2151328216 	 0.2298984837
epoch_time;  34.260950803756714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20804698765277863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28299054503440857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22840392589569092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3121270537376404
7 3.4765142339 	 0.3121270463 	 0.3244875315
epoch_time;  34.44195508956909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17623287439346313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2244683802127838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19010312855243683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2534884214401245
8 3.4519262811 	 0.2534884272 	 0.2661848326
epoch_time;  34.878984451293945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17740043997764587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23987728357315063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1635400801897049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2411072552204132
9 3.4360821 	 0.2411072602 	 0.2537099271
epoch_time;  34.520955324172974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1589600145816803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21229256689548492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16109216213226318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22505511343479156
10 3.4146552477 	 0.2250551172 	 0.2371561617
epoch_time;  34.66507601737976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1811092495918274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25456854701042175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21708695590496063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2999010980129242
11 3.4090362115 	 0.2999010859 	 0.3124782253
epoch_time;  34.75932550430298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1643439531326294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2140781730413437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17920680344104767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2499384880065918
12 3.394656213 	 0.2499384906 	 0.2628417763
epoch_time;  34.85355496406555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17556622624397278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2379126250743866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18568582832813263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26154986023902893
13 3.3880321836 	 0.2615498723 	 0.2734406549
epoch_time;  34.854758739471436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1984005719423294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2575806975364685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2161102145910263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28631412982940674
14 3.3718640244 	 0.286314124 	 0.2969567995
epoch_time;  34.96045637130737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15691250562667847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22824637591838837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15395306050777435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2286779284477234
15 3.3770390784 	 0.2286779249 	 0.2409299696
epoch_time;  35.06066870689392
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1717597395181656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26283174753189087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17851506173610687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28708550333976746
16 3.3639966465 	 0.2870854971 	 0.299197862
epoch_time;  35.06239700317383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15373556315898895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22296439111232758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16465513408184052
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23813144862651825
17 3.3519648279 	 0.2381314458 	 0.2489942396
epoch_time;  34.64999723434448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1657775193452835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2137749195098877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14895591139793396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20280252397060394
18 3.3545589667 	 0.2028025241 	 0.2130842879
epoch_time;  34.58158040046692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20152200758457184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2571548521518707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2064836472272873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26742976903915405
19 3.3464185215 	 0.2674297642 	 0.2774815327
epoch_time;  34.561373233795166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.201485276222229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25711241364479065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20649075508117676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26741188764572144
It took 745.7088782787323 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139745
Array Job ID: 2137927_24
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-13:30:54 core-walltime
Job Wall-clock time: 02:05:03
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
