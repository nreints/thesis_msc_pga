wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121535-wzso9n3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-firecracker-1178
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/wzso9n3h
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–ƒâ–â–‚â–‚â–â–â–‚â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–‚â–â–‚â–â–â–‚â–â–â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–„â–‚â–â–â–‚â–ƒâ–â–‚â–‚â–â–â–‚â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–…â–…â–…â–ƒâ–‡â–‚â–„â–â–ƒâ–â–‚â–…â–â–ƒâ–ƒâ–†â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.90235
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35802
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.62226
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1766
wandb:                         Train loss 1.52926
wandb: 
wandb: ðŸš€ View run floating-firecracker-1178 at: https://wandb.ai/nreints/thesis/runs/wzso9n3h
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121535-wzso9n3h/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122702-wtkt99tg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-goat-1182
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/wtkt99tg
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2786642611026764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8810244798660278
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.927972793579102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.815725803375244
0 3.2196306914 	 6.8157259554 	 6.8157259554
epoch_time;  32.545034885406494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20435731112957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5349986553192139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.217131614685059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.594555854797363
1 1.8057245604 	 5.5945556641 	 5.5945556641
epoch_time;  31.075177669525146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23023885488510132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5170891284942627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.929821968078613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.271238803863525
2 1.701672884 	 5.2712385848 	 5.2712385848
epoch_time;  31.161988496780396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22195950150489807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49201667308807373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.229126930236816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.532025337219238
3 1.6543127433 	 5.5320253114 	 5.5320253114
epoch_time;  31.729674577713013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22540976107120514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47602832317352295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.714524269104004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.021633148193359
4 1.6268479974 	 5.0216331688 	 5.0216331688
epoch_time;  31.44040083885193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1820099949836731
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4251859486103058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.594429016113281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.867674350738525
5 1.6070676479 	 4.8676744616 	 4.8676744616
epoch_time;  31.55348539352417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2562841773033142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5090093016624451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.585431098937988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.895862579345703
6 1.588219066 	 4.8958624762 	 4.8958624762
epoch_time;  31.63193440437317
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1599840223789215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33142557740211487
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6943159103393555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.933359622955322
7 1.5795933149 	 4.933359507 	 4.933359507
epoch_time;  31.363612174987793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20418640971183777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4366673529148102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.933306694030762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.28163480758667
8 1.5666036122 	 5.2816346864 	 5.2816346864
epoch_time;  31.87657642364502
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1508462131023407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32072019577026367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.666654109954834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.92561674118042
9 1.561972048 	 4.92561695 	 4.92561695
epoch_time;  31.402364492416382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18892423808574677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4231036901473999
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7359619140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.046330451965332
10 1.5589935985 	 5.0463306324 	 5.0463306324
epoch_time;  31.22808027267456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1598021537065506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34694820642471313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8240838050842285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.11529016494751
11 1.5506700065 	 5.1152901314 	 5.1152901314
epoch_time;  31.005468130111694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16895797848701477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3549577593803406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.57084321975708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.839856147766113
12 1.5440958962 	 4.839856287 	 4.839856287
epoch_time;  31.249441385269165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2299933284521103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41895294189453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.62284517288208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.892261981964111
13 1.5417230873 	 4.8922620619 	 4.8922620619
epoch_time;  31.271825551986694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1545974463224411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3282974362373352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.774326801300049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.066219806671143
14 1.5391332462 	 5.0662198453 	 5.0662198453
epoch_time;  31.34120464324951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19133645296096802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3459588587284088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.648015975952148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.920528411865234
15 1.5368592838 	 4.9205285974 	 4.9205285974
epoch_time;  31.435696125030518
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17985732853412628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38935908675193787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6304521560668945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.950156211853027
16 1.5339145544 	 4.950156382 	 4.950156382
epoch_time;  31.36183261871338
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24994172155857086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.439673513174057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.645593166351318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.935266494750977
17 1.5316625762 	 4.9352664432 	 4.9352664432
epoch_time;  31.449695587158203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15675310790538788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3364737033843994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.584953784942627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.87183952331543
18 1.5282915512 	 4.8718396986 	 4.8718396986
epoch_time;  31.71607995033264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17665235698223114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35818418860435486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.619784355163574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.90492057800293
19 1.5292623284 	 4.9049204234 	 4.9049204234
epoch_time;  31.683207988739014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17659932374954224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3580242991447449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.622258186340332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.902348518371582
It took 686.8977696895599 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–„â–„â–ƒâ–…â–„â–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–‚â–‚â–‚â–â–‚â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–†â–ˆâ–†â–…â–…â–ˆâ–…â–„â–„â–â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–â–‚â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–†â–„â–†â–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–…â–‚â–â–â–ƒâ–‚â–â–„â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.87462
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3485
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.60059
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15971
wandb:                         Train loss 1.52457
wandb: 
wandb: ðŸš€ View run legendary-goat-1182 at: https://wandb.ai/nreints/thesis/runs/wtkt99tg
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122702-wtkt99tg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_123821-tmkiniy9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-rabbit-1189
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/tmkiniy9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2632389962673187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8222765326499939
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.90279483795166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.630836486816406
0 3.2146318419 	 5.6308362806 	 5.6308362806
epoch_time;  31.102975845336914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21553769707679749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6427944302558899
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.825837135314941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.32601261138916
1 1.8075084234 	 5.3260125238 	 5.3260125238
epoch_time;  31.422176122665405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23743829131126404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5609384775161743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.977457523345947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.319368839263916
2 1.7068590127 	 5.3193689295 	 5.3193689295
epoch_time;  31.482445240020752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19915160536766052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4450298547744751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.821759223937988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0675787925720215
3 1.6526267349 	 5.0675787848 	 5.0675787848
epoch_time;  31.016017198562622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22896939516067505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4684118628501892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.753300666809082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.9907636642456055
4 1.6244356831 	 4.9907638962 	 4.9907638962
epoch_time;  31.158026218414307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19004933536052704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43262407183647156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.689925193786621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.95481538772583
5 1.6033981003 	 4.9548155089 	 4.9548155089
epoch_time;  31.094524383544922
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1937333643436432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4176061153411865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9681782722473145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.233407974243164
6 1.5905950155 	 5.2334080052 	 5.2334080052
epoch_time;  30.95569920539856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16790057718753815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3820497989654541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.712798118591309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.013622283935547
7 1.5794366153 	 5.013622387 	 5.013622387
epoch_time;  31.221719980239868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18858268857002258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39449575543403625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.640819549560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.888218402862549
8 1.5748290189 	 4.8882185652 	 4.8882185652
epoch_time;  31.277751922607422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19587388634681702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43435725569725037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.679375648498535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.972739219665527
9 1.566364761 	 4.9727393898 	 4.9727393898
epoch_time;  30.967491388320923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1726352572441101
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37614905834198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.397458076477051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.628237724304199
10 1.5601524813 	 4.6282378326 	 4.6282378326
epoch_time;  31.17545223236084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22067420184612274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.416666179895401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600101470947266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.852478504180908
11 1.5515123495 	 4.8524783573 	 4.8524783573
epoch_time;  31.283246755599976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17019812762737274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35600829124450684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6709065437316895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.90516996383667
12 1.5495547616 	 4.9051698427 	 4.9051698427
epoch_time;  31.36920142173767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15631669759750366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3484725058078766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.721582889556885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.001628398895264
13 1.5398112861 	 5.001628484 	 5.001628484
epoch_time;  30.70484232902527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16047409176826477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3353726863861084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5931572914123535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.831735134124756
14 1.5477461043 	 4.831735312 	 4.831735312
epoch_time;  31.19051504135132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18485882878303528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36167415976524353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.536252975463867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.761573791503906
15 1.5369936685 	 4.7615739152 	 4.7615739152
epoch_time;  30.893978357315063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1733025163412094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36199212074279785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.522829532623291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.763484954833984
16 1.531946057 	 4.7634848105 	 4.7634848105
epoch_time;  30.82793402671814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15622371435165405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31711143255233765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4334282875061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.652652740478516
17 1.5303126124 	 4.6526528848 	 4.6526528848
epoch_time;  30.814340353012085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20965778827667236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4046128988265991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.44625186920166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.706968307495117
18 1.5286307364 	 4.7069682353 	 4.7069682353
epoch_time;  30.743998527526855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15973825752735138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34840014576911926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600353240966797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.874826431274414
19 1.5245748945 	 4.8748264622 	 4.8748264622
epoch_time;  30.943530321121216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1597100794315338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3485018312931061
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600589275360107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8746161460876465
It took 679.6360681056976 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ƒâ–â–â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–„â–â–ƒâ–â–‚â–â–ƒâ–‚â–â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‚â–â–‚â–ƒâ–…â–…â–…â–†â–…â–†â–ƒâ–„â–ƒâ–„â–…â–‚â–„â–‚â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–‚â–‚â–ƒâ–†â–â–„â–â–‚â–â–‡â–‚â–‚â–‚â–ƒâ–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.05591
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32381
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.94853
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15858
wandb:                         Train loss 1.52006
wandb: 
wandb: ðŸš€ View run crimson-rabbit-1189 at: https://wandb.ai/nreints/thesis/runs/tmkiniy9
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_123821-tmkiniy9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124940-c3cvd6jd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-noodles-1196
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/c3cvd6jd
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29391342401504517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7660719752311707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.500298023223877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.057121753692627
0 3.2912648898 	 6.057121648 	 6.057121648
epoch_time;  31.42954397201538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24271085858345032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.610134482383728
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.839558124542236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.126905918121338
1 1.7932072706 	 5.1269059465 	 5.1269059465
epoch_time;  31.129989862442017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2085123360157013
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47630569338798523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.697841167449951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.890214443206787
2 1.6939188984 	 4.8902145798 	 4.8902145798
epoch_time;  31.188669443130493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18593984842300415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41838493943214417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.819484233856201
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.934641361236572
3 1.6489467737 	 4.9346412452 	 4.9346412452
epoch_time;  31.265000581741333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17303945124149323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3818258047103882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.911707878112793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.01200532913208
4 1.6178635563 	 5.0120054503 	 5.0120054503
epoch_time;  31.3990581035614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18163149058818817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37437987327575684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1197919845581055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.195919036865234
5 1.6013464595 	 5.1959192224 	 5.1959192224
epoch_time;  31.210596561431885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18559737503528595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4046473801136017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1280670166015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.263819217681885
6 1.5860768436 	 5.2638193491 	 5.2638193491
epoch_time;  31.09959363937378
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26090747117996216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4953811764717102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.169304370880127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.305629253387451
7 1.5751149792 	 5.3056294209 	 5.3056294209
epoch_time;  31.186582803726196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15475168824195862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3296089768409729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.263524055480957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.378260612487793
8 1.5655996107 	 5.378260597 	 5.378260597
epoch_time;  31.24976348876953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21622997522354126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4174540042877197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.126833438873291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.252651214599609
9 1.5571100626 	 5.2526512352 	 5.2526512352
epoch_time;  31.19316029548645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15771430730819702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32039740681648254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.218436241149902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.29825496673584
10 1.5522170018 	 5.2982550544 	 5.2982550544
epoch_time;  31.14566946029663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17571929097175598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37622034549713135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.928240776062012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.04671049118042
11 1.5474140268 	 5.0467107 	 5.0467107
epoch_time;  31.06691074371338
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15809129178524017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31903624534606934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.004263401031494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.096228122711182
12 1.5496570791 	 5.0962280273 	 5.0962280273
epoch_time;  31.143913745880127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2711530029773712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47036638855934143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.979458332061768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.098677635192871
13 1.5328669151 	 5.0986776816 	 5.0986776816
epoch_time;  31.145667552947998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17150840163230896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34838104248046875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.021081447601318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.117145538330078
14 1.5322920193 	 5.1171456002 	 5.1171456002
epoch_time;  31.2254376411438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16508491337299347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3386589586734772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.116727828979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.234111785888672
15 1.5338408329 	 5.234111724 	 5.234111724
epoch_time;  31.095510482788086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16842491924762726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3489876687526703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.848773956298828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.970890522003174
16 1.5301182256 	 4.9708905194 	 4.9708905194
epoch_time;  30.842214584350586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18495307862758636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37537020444869995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.017286777496338
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.15553092956543
17 1.5247788684 	 5.1555311048 	 5.1555311048
epoch_time;  31.069172382354736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16846275329589844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3140016198158264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.787590980529785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.861074924468994
18 1.5264432232 	 4.8610747466 	 4.8610747466
epoch_time;  31.3529109954834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15859542787075043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.323639452457428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.949858665466309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.054960250854492
19 1.520056392 	 5.0549600137 	 5.0549600137
epoch_time;  31.3851261138916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1585824340581894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3238064646720886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.94852876663208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.055907249450684
It took 678.3212134838104 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–…â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–‡â–„â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–†â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–…â–„â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.26902
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.33944
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.13232
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15545
wandb:                         Train loss 1.52838
wandb: 
wandb: ðŸš€ View run resplendent-noodles-1196 at: https://wandb.ai/nreints/thesis/runs/c3cvd6jd
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124940-c3cvd6jd/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_130057-40a6wsd7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-moon-1203
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/40a6wsd7
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25478044152259827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7687042951583862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.336758136749268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.9137163162231445
0 3.1655468486 	 5.9137160842 	 5.9137160842
epoch_time;  30.973644971847534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.214735209941864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5173764824867249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.921941757202148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.14017915725708
1 1.7976492415 	 5.1401789485 	 5.1401789485
epoch_time;  31.063757181167603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2286653220653534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5663998126983643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.157648086547852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.422971725463867
2 1.699165525 	 5.4229716533 	 5.4229716533
epoch_time;  30.93170142173767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1605994999408722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3495066165924072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.701981544494629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.795444965362549
3 1.6534699044 	 4.7954447978 	 4.7954447978
epoch_time;  31.023332834243774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17018966376781464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39313915371894836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.521642684936523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.673305034637451
4 1.6257385378 	 4.6733048723 	 4.6733048723
epoch_time;  31.100296020507812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19064541161060333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3957330584526062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.615625858306885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.7535786628723145
5 1.6049523686 	 4.7535786397 	 4.7535786397
epoch_time;  31.377607345581055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20482227206230164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42140302062034607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.394519329071045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.554177761077881
6 1.5972843364 	 4.554177774 	 4.554177774
epoch_time;  31.3928325176239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17697446048259735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4140976071357727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.369316101074219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.547887802124023
7 1.5833369147 	 4.5478878537 	 4.5478878537
epoch_time;  31.227100372314453
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1754436194896698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3975656032562256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.377699851989746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.54289436340332
8 1.5739111361 	 4.5428941881 	 4.5428941881
epoch_time;  31.107247591018677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20622257888317108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4237116277217865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.386192798614502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.529078960418701
9 1.5643405591 	 4.529078798 	 4.529078798
epoch_time;  31.393078565597534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19111116230487823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39262303709983826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.338256359100342
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.505256652832031
10 1.5621937085 	 4.5052566116 	 4.5052566116
epoch_time;  31.091304302215576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.180916890501976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41287851333618164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.346541404724121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.541123390197754
11 1.5565669035 	 4.5411235088 	 4.5411235088
epoch_time;  31.387043714523315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1573784053325653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33952558040618896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2801289558410645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.413079261779785
12 1.551531689 	 4.4130793391 	 4.4130793391
epoch_time;  31.090507984161377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15626001358032227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3588365316390991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.397875785827637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.580756664276123
13 1.5468426022 	 4.58075644 	 4.58075644
epoch_time;  31.033432483673096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15538574755191803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3101053237915039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.298421382904053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.418962001800537
14 1.5393694527 	 4.4189618085 	 4.4189618085
epoch_time;  31.541061639785767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16873010993003845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34713059663772583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2269287109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3539838790893555
15 1.5397461599 	 4.3539841111 	 4.3539841111
epoch_time;  31.068482875823975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1809815764427185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3685954809188843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.242217540740967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.385471343994141
16 1.5354232013 	 4.3854713234 	 4.3854713234
epoch_time;  31.29207420349121
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14121466875076294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.304158091545105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2313666343688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.353908538818359
17 1.5346663158 	 4.3539085594 	 4.3539085594
epoch_time;  30.981906414031982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16330161690711975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3356025516986847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.233170032501221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.383042335510254
18 1.5267285222 	 4.3830421242 	 4.3830421242
epoch_time;  30.943267583847046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15546025335788727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3394184112548828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.130697727203369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.268970489501953
19 1.5283803146 	 4.2689703864 	 4.2689703864
epoch_time;  31.264067888259888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15544869005680084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3394400179386139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.13231897354126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.269019603729248
It took 677.2143630981445 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–ƒâ–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–…â–‡â–ƒâ–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–„â–â–„â–‚â–‚â–‚â–‚â–†â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.58059
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32496
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.48468
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.15266
wandb:                         Train loss 1.52671
wandb: 
wandb: ðŸš€ View run red-moon-1203 at: https://wandb.ai/nreints/thesis/runs/40a6wsd7
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_130057-40a6wsd7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131210-lg54fv88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-rooster-1209
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/lg54fv88
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28371766209602356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8483472466468811
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.455132007598877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.136479377746582
0 3.3287051852 	 6.1364792283 	 6.1364792283
epoch_time;  31.02386236190796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19842009246349335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5420877933502197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.761889934539795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.026646137237549
1 1.8084459763 	 5.0266459697 	 5.0266459697
epoch_time;  31.03139615058899
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21443568170070648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5317421555519104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.663276195526123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.857426166534424
2 1.7076765614 	 4.857426164 	 4.857426164
epoch_time;  31.131314516067505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25701001286506653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5263150930404663
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.3710126876831055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.529976844787598
3 1.6591466285 	 4.5299768396 	 4.5299768396
epoch_time;  30.857780694961548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17662447690963745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4268808662891388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.591991901397705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.741425514221191
4 1.6329842414 	 4.7414253853 	 4.7414253853
epoch_time;  30.83487892150879
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17041179537773132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38378065824508667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.610727787017822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.725982666015625
5 1.6085602612 	 4.725982831 	 4.725982831
epoch_time;  30.92603611946106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2104610800743103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4520156979560852
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.244896411895752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.401210308074951
6 1.5974992198 	 4.4012104756 	 4.4012104756
epoch_time;  30.97510313987732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17389483749866486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4059165418148041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.427274703979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.574398994445801
7 1.5858345213 	 4.5743988862 	 4.5743988862
epoch_time;  31.36367130279541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17458762228488922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38245058059692383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4343061447143555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.536797523498535
8 1.5723555454 	 4.5367976008 	 4.5367976008
epoch_time;  31.22146439552307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16044682264328003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3631201684474945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.417219638824463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.52153205871582
9 1.5641547676 	 4.5215318834 	 4.5215318834
epoch_time;  30.898078680038452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17035551369190216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36614900827407837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.387962341308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.473733901977539
10 1.5622816454 	 4.4737340979 	 4.4737340979
epoch_time;  30.650493621826172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19659669697284698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38646525144577026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.39418888092041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.508780479431152
11 1.5541512832 	 4.5087804846 	 4.5087804846
epoch_time;  30.857002019882202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13696901500225067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3028221130371094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.385984897613525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4475603103637695
12 1.5515746657 	 4.4475602433 	 4.4475602433
epoch_time;  31.359838724136353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19769686460494995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3833937644958496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.42843770980835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5189714431762695
13 1.5476656057 	 4.5189713762 	 4.5189713762
epoch_time;  31.259894132614136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16179805994033813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3441231846809387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.378238677978516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4341230392456055
14 1.5368596143 	 4.4341229413 	 4.4341229413
epoch_time;  30.980203866958618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16363045573234558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3545050621032715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.568169116973877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.670701026916504
15 1.5357132939 	 4.6707008156 	 4.6707008156
epoch_time;  31.039764642715454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1596709042787552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34107649326324463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6276702880859375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.717094898223877
16 1.5336120647 	 4.7170947925 	 4.7170947925
epoch_time;  31.056960105895996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15149791538715363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3220634162425995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5652947425842285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.642605781555176
17 1.5307068408 	 4.6426058383 	 4.6426058383
epoch_time;  31.21501636505127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2415304034948349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4284683167934418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.517895698547363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.621338367462158
18 1.528720161 	 4.6213385505 	 4.6213385505
epoch_time;  31.00572371482849
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15266256034374237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32496878504753113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.485389232635498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.580564975738525
19 1.5267055462 	 4.5805647567 	 4.5805647567
epoch_time;  30.89354133605957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15265575051307678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32495516538619995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.484679698944092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.580589294433594
It took 672.9256751537323 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–…â–…â–ˆâ–…â–„â–‚â–‚â–ƒâ–‚â–â–„â–‚â–„â–‚â–‚â–‚â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.59155
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32374
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.47986
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14236
wandb:                         Train loss 1.53547
wandb: 
wandb: ðŸš€ View run auspicious-rooster-1209 at: https://wandb.ai/nreints/thesis/runs/lg54fv88
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131210-lg54fv88/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132325-w6irkskj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-moon-1215
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/w6irkskj
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26055771112442017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7928516864776611
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.489752769470215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.105069637298584
0 3.3229162259 	 6.1050695471 	 6.1050695471
epoch_time;  31.2965726852417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21869777143001556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5748319625854492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.805482864379883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.05897331237793
1 1.8118138607 	 5.0589731577 	 5.0589731577
epoch_time;  31.079962015151978
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20316122472286224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48362451791763306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.658170700073242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.838333606719971
2 1.7145610957 	 4.8383337072 	 4.8383337072
epoch_time;  31.28745698928833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20485779643058777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49513548612594604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7143473625183105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.890282154083252
3 1.6646651653 	 4.8902822134 	 4.8902822134
epoch_time;  31.532105684280396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2583930492401123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5252504348754883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5599284172058105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.666383743286133
4 1.6301843105 	 4.6663838155 	 4.6663838155
epoch_time;  31.404534578323364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20178855955600739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43241754174232483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.519780158996582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.593492031097412
5 1.6113915983 	 4.5934920027 	 4.5934920027
epoch_time;  31.326874494552612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19928188621997833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4418323338031769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6286492347717285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.729889392852783
6 1.5973193789 	 4.7298894109 	 4.7298894109
epoch_time;  31.00415563583374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15456566214561462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3680504560470581
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5455780029296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.632504463195801
7 1.5845819162 	 4.6325043549 	 4.6325043549
epoch_time;  31.18617558479309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15154516696929932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3661781847476959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.496511936187744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.58130407333374
8 1.5757050821 	 4.5813041068 	 4.5813041068
epoch_time;  31.089982509613037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16774465143680573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36873576045036316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.40541410446167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.469732284545898
9 1.5723986874 	 4.4697325011 	 4.4697325011
epoch_time;  31.622292518615723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.159891739487648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3354991674423218
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.469522953033447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.539207458496094
10 1.5628462929 	 4.5392076647 	 4.5392076647
epoch_time;  31.361406326293945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14342831075191498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31114423274993896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.495694637298584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5430097579956055
11 1.5586983817 	 4.54300999 	 4.54300999
epoch_time;  31.30222749710083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19861657917499542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4017322361469269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.445111274719238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.531608581542969
12 1.5556394336 	 4.5316086228 	 4.5316086228
epoch_time;  31.048828601837158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16269338130950928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35202813148498535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5314621925354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.632339000701904
13 1.55231403 	 4.6323390651 	 4.6323390651
epoch_time;  31.030009746551514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1936601996421814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3882856070995331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4893622398376465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.583818435668945
14 1.5455007379 	 4.5838184254 	 4.5838184254
epoch_time;  30.98406672477722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16185976564884186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3369554579257965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.350127220153809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.442540168762207
15 1.5421128528 	 4.4425401842 	 4.4425401842
epoch_time;  31.340483903884888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15301039814949036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3522028625011444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.557866096496582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.697494029998779
16 1.5393149054 	 4.6974939295 	 4.6974939295
epoch_time;  31.008310794830322
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15850138664245605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34137436747550964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.415378570556641
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.512139797210693
17 1.5359883447 	 4.5121397276 	 4.5121397276
epoch_time;  31.04619002342224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14158156514167786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3218262195587158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.539890766143799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.662647724151611
18 1.5367673798 	 4.6626478041 	 4.6626478041
epoch_time;  31.122690200805664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14235436916351318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3238653242588043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.479848384857178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.593381404876709
19 1.5354665086 	 4.5933814796 	 4.5933814796
epoch_time;  30.930222034454346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14235787093639374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3237365186214447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.479863166809082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.591548442840576
It took 675.3310222625732 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ˆâ–ˆâ–„â–„â–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–†â–…â–„â–ƒâ–ƒâ–…â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–…â–†â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.605
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29364
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.53062
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1442
wandb:                         Train loss 1.52465
wandb: 
wandb: ðŸš€ View run lunar-moon-1215 at: https://wandb.ai/nreints/thesis/runs/w6irkskj
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132325-w6irkskj/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_133441-5p83ddbq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-rocket-1222
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/5p83ddbq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2564205825328827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7872733473777771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.1546173095703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.928493022918701
0 3.1717682195 	 5.9284931905 	 5.9284931905
epoch_time;  30.724684238433838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.224271759390831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5639668107032776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8114142417907715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.148431777954102
1 1.8238268357 	 5.1484318914 	 5.1484318914
epoch_time;  31.124396562576294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22346222400665283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5461645722389221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.16871976852417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.391860485076904
2 1.7185898292 	 5.3918605495 	 5.3918605495
epoch_time;  31.359076976776123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20738506317138672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4615781605243683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.123950004577637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.273294448852539
3 1.6655055248 	 5.2732943148 	 5.2732943148
epoch_time;  31.10712718963623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1885196417570114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4088365435600281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8169097900390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.890616416931152
4 1.6368022239 	 4.8906164221 	 4.8906164221
epoch_time;  30.80201768875122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18137596547603607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39079949259757996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.822290897369385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.926577568054199
5 1.6156001727 	 4.9265776763 	 4.9265776763
epoch_time;  30.887089252471924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16921845078468323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3676486313343048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.663301467895508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.726801872253418
6 1.6002500582 	 4.7268020217 	 4.7268020217
epoch_time;  31.216740608215332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21514928340911865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4041860103607178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7541351318359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.829162120819092
7 1.5881233603 	 4.8291619378 	 4.8291619378
epoch_time;  31.416677951812744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15866141021251678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3578716814517975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.670161247253418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.779013156890869
8 1.5798310091 	 4.779013144 	 4.779013144
epoch_time;  31.05133867263794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1774759441614151
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3717910349369049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.813284873962402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.926571369171143
9 1.5747485203 	 4.9265714078 	 4.9265714078
epoch_time;  31.154406785964966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14591075479984283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3053515553474426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.643842697143555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.704551696777344
10 1.5674110779 	 4.704551903 	 4.704551903
epoch_time;  30.881190299987793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16018839180469513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35681644082069397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.733250141143799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.854977130889893
11 1.5587588796 	 4.8549771696 	 4.8549771696
epoch_time;  31.04114580154419
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15120534598827362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33198031783103943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.719793796539307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.828656196594238
12 1.5491278047 	 4.8286561708 	 4.8286561708
epoch_time;  31.091711044311523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16444043815135956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3294004797935486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.599848747253418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.676060676574707
13 1.5455240445 	 4.676060692 	 4.676060692
epoch_time;  31.292247772216797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16216008365154266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3498832881450653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6646318435668945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.791287899017334
14 1.5447134381 	 4.7912878088 	 4.7912878088
epoch_time;  31.25315523147583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15985406935214996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3380717635154724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.726353168487549
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.842438697814941
15 1.5415128322 	 4.8424388989 	 4.8424388989
epoch_time;  31.154237747192383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20912566781044006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40184223651885986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600763320922852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.732393264770508
16 1.5388203429 	 4.732393172 	 4.732393172
epoch_time;  30.867149114608765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21654289960861206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41145777702331543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.728140354156494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.852648735046387
17 1.5341054701 	 4.8526489258 	 4.8526489258
epoch_time;  31.199718952178955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15553714334964752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31884661316871643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.717524528503418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.816167831420898
18 1.5324163211 	 4.816167718 	 4.816167718
epoch_time;  31.135382890701294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14419466257095337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2936517894268036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.528160095214844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.602328777313232
19 1.5246494425 	 4.6023285737 	 4.6023285737
epoch_time;  31.03313136100769
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1441980004310608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2936410903930664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.530616760253906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.605004787445068
It took 675.3709781169891 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–…â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–…â–†â–†â–ˆâ–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–„â–â–ƒâ–‚â–„â–â–â–ƒâ–ƒâ–â–‚â–ƒâ–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.46263
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.30395
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.41426
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14784
wandb:                         Train loss 1.52417
wandb: 
wandb: ðŸš€ View run sparkling-rocket-1222 at: https://wandb.ai/nreints/thesis/runs/5p83ddbq
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_133441-5p83ddbq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134554-ko6bnuvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-dragon-1228
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/ko6bnuvg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3036259412765503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8924654722213745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.141082286834717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.766451835632324
0 3.2421108031 	 5.7664517789 	 5.7664517789
epoch_time;  31.256407499313354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2283598631620407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5718550086021423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.959296703338623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.139408111572266
1 1.8158163124 	 5.1394082559 	 5.1394082559
epoch_time;  30.974002599716187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22323448956012726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5365614891052246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.169929027557373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.264733791351318
2 1.7145812616 	 5.2647335568 	 5.2647335568
epoch_time;  30.66702175140381
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19600312411785126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4326256513595581
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.140296936035156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.173819065093994
3 1.6649390591 	 5.1738188872 	 5.1738188872
epoch_time;  30.754558563232422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2081216722726822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40332257747650146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.436705112457275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.465797424316406
4 1.6283645785 	 5.465797548 	 5.465797548
epoch_time;  30.916921615600586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20026272535324097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4057917892932892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.967000961303711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.01384162902832
5 1.6141201218 	 5.0138414538 	 5.0138414538
epoch_time;  30.843459367752075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17644140124320984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3639155924320221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8925933837890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.920907497406006
6 1.5946216002 	 4.9209076753 	 4.9209076753
epoch_time;  31.06963562965393
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21139729022979736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4195900857448578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.76721715927124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.846482276916504
7 1.5870983664 	 4.8464823955 	 4.8464823955
epoch_time;  31.226814031600952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15613487362861633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3407132029533386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.855276584625244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.920950889587402
8 1.5780944204 	 4.9209508947 	 4.9209508947
epoch_time;  31.199141025543213
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2014543116092682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.409402459859848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.692793369293213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.786417007446289
9 1.5668408104 	 4.7864168734 	 4.7864168734
epoch_time;  31.3823881149292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16003720462322235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34491413831710815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.67529296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.767257213592529
10 1.5585082925 	 4.767257443 	 4.767257443
epoch_time;  31.049397945404053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20361021161079407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36946210265159607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.603668689727783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.6511664390563965
11 1.5593714627 	 4.6511665963 	 4.6511665963
epoch_time;  30.74192500114441
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1586151272058487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32162022590637207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.672072410583496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.713413715362549
12 1.5487914295 	 4.7134138777 	 4.7134138777
epoch_time;  30.800079345703125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14897222816944122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3146009147167206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6552581787109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.695403099060059
13 1.5422366587 	 4.6954032279 	 4.6954032279
epoch_time;  30.975704193115234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19969098269939423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37036198377609253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.634395122528076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.693957328796387
14 1.5369721802 	 4.6939571896 	 4.6939571896
epoch_time;  31.042918920516968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19596782326698303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3945109248161316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.547645568847656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.625430107116699
15 1.5372999601 	 4.6254302154 	 4.6254302154
epoch_time;  30.9260835647583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1513071060180664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32193607091903687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.518567085266113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.602936744689941
16 1.5352977425 	 4.6029369457 	 4.6029369457
epoch_time;  30.629865646362305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1803213357925415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3559747338294983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.610635757446289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.682589054107666
17 1.5301883106 	 4.6825891443 	 4.6825891443
epoch_time;  31.12593126296997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19503477215766907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3595432937145233
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.584836006164551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.641468048095703
18 1.5253218623 	 4.6414682749 	 4.6414682749
epoch_time;  31.021167278289795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14785738289356232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3042258024215698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4154157638549805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.462615966796875
19 1.5241737237 	 4.4626158018 	 4.4626158018
epoch_time;  30.84835958480835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14783529937267303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3039480149745941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.414258003234863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.46263313293457
It took 673.4208672046661 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–„â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–†â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–â–ƒâ–â–„â–„
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.17983
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3984
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.06979
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2007
wandb:                         Train loss 1.52055
wandb: 
wandb: ðŸš€ View run crimson-dragon-1228 at: https://wandb.ai/nreints/thesis/runs/ko6bnuvg
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134554-ko6bnuvg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135707-1f0ekeb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-bao-1235
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/1f0ekeb1
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30110371112823486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8472856879234314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.339691162109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.031805038452148
0 3.2322790602 	 6.031804925 	 6.031804925
epoch_time;  30.881507396697998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23350326716899872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6183478832244873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.768015384674072
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.01090669631958
1 1.8108873826 	 5.0109064875 	 5.0109064875
epoch_time;  31.178431510925293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24896539747714996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5757756233215332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.76930570602417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.958528518676758
2 1.7079618605 	 4.9585287558 	 4.9585287558
epoch_time;  31.198116302490234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18567056953907013
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42332640290260315
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.570980548858643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.642498016357422
3 1.660193772 	 4.6424979545 	 4.6424979545
epoch_time;  30.95482850074768
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16931921243667603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3754316568374634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.255575180053711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2805609703063965
4 1.6264331071 	 4.2805607976 	 4.2805607976
epoch_time;  30.86600661277771
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19242413341999054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3997320532798767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.120012283325195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.168360233306885
5 1.6073542694 	 4.1683600348 	 4.1683600348
epoch_time;  31.113243579864502
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1924571543931961
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3961055874824524
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.492271423339844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5450005531311035
6 1.5937209081 	 4.5450007258 	 4.5450007258
epoch_time;  31.127866744995117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18724313378334045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4178365468978882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.370193958282471
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.452308177947998
7 1.5764675876 	 4.4523081187 	 4.4523081187
epoch_time;  31.06742548942566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1628265529870987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3534599244594574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.34364652633667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.402163505554199
8 1.5670097418 	 4.4021636138 	 4.4021636138
epoch_time;  30.917872667312622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16699573397636414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3444339632987976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.366253852844238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.399791717529297
9 1.5618203072 	 4.3997914907 	 4.3997914907
epoch_time;  30.77880883216858
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1939207762479782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35564759373664856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.331882953643799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3555073738098145
10 1.5563384659 	 4.3555073506 	 4.3555073506
epoch_time;  30.8984694480896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15063689649105072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32031679153442383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.355342864990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.418980121612549
11 1.5500803995 	 4.418980284 	 4.418980284
epoch_time;  30.90822148323059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15543122589588165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3228020966053009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2440996170043945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.305830478668213
12 1.5421561601 	 4.3058303421 	 4.3058303421
epoch_time;  30.930187702178955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16634497046470642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31853702664375305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.154363632202148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2021870613098145
13 1.5410982725 	 4.2021870381 	 4.2021870381
epoch_time;  30.808186292648315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15878267586231232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3120310604572296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.23445463180542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.299246311187744
14 1.538025109 	 4.2992461333 	 4.2992461333
epoch_time;  30.815098762512207
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1468932181596756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29484066367149353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0710835456848145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.126811504364014
15 1.5336960345 	 4.1268115894 	 4.1268115894
epoch_time;  31.128595113754272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14360260963439941
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29128870368003845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.280538558959961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.327244758605957
16 1.529945327 	 4.3272447741 	 4.3272447741
epoch_time;  31.23391032218933
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1994660198688507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34601396322250366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.161899566650391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.209328651428223
17 1.5281077432 	 4.2093284813 	 4.2093284813
epoch_time;  31.413724422454834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15236331522464752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3044999837875366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.074807643890381
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.122121334075928
18 1.5244266249 	 4.1221214501 	 4.1221214501
epoch_time;  30.72850227355957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2008013129234314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39849209785461426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.070976734161377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.180759906768799
19 1.520551781 	 4.1807600692 	 4.1807600692
epoch_time;  30.677022457122803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20070087909698486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3983991742134094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.069789886474609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.17982816696167
It took 673.2593393325806 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–…â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–…â–…
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–†â–ƒâ–‚â–ƒâ–‚â–…â–â–„â–ƒâ–ƒâ–â–‚â–„â–„â–‚â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.18257
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34503
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.01797
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17454
wandb:                         Train loss 1.51041
wandb: 
wandb: ðŸš€ View run dancing-bao-1235 at: https://wandb.ai/nreints/thesis/runs/1f0ekeb1
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135707-1f0ekeb1/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3000803291797638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8101710081100464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.239450931549072
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.983532428741455
0 3.1568589466 	 5.9835323849 	 5.9835323849
epoch_time;  31.098865747451782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21719428896903992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5609235167503357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.709741592407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.105885982513428
1 1.7971079006 	 5.1058860985 	 5.1058860985
epoch_time;  30.861512422561646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2569536566734314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5743698477745056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.871365070343018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.200761318206787
2 1.6975051374 	 5.2007614548 	 5.2007614548
epoch_time;  30.94937825202942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18430165946483612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4006591737270355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.835820198059082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.038540363311768
3 1.6522272171 	 5.038540237 	 5.038540237
epoch_time;  31.251292943954468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16792041063308716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38047170639038086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.88732385635376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0745930671691895
4 1.6246418059 	 5.074592879 	 5.074592879
epoch_time;  31.050671339035034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18254797160625458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4190344214439392
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8704986572265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.078057289123535
5 1.6050812987 	 5.0780573664 	 5.0780573664
epoch_time;  30.922852039337158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17709597945213318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3959491550922394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.875970840454102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.099709510803223
6 1.5867094274 	 5.0997093407 	 5.0997093407
epoch_time;  30.706182956695557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22958943247795105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4341585040092468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.878326416015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.066171169281006
7 1.5735068247 	 5.0661713471 	 5.0661713471
epoch_time;  30.908998727798462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14533133804798126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30052387714385986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.827620029449463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.963706016540527
8 1.5649095043 	 4.9637058567 	 4.9637058567
epoch_time;  31.28527021408081
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2080399990081787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40318062901496887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.886070251464844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.051871299743652
9 1.5629996866 	 5.0518713049 	 5.0518713049
epoch_time;  31.350894451141357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19792449474334717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3845272958278656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.811471462249756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.994449615478516
10 1.5511311794 	 4.9944494299 	 4.9944494299
epoch_time;  31.199495553970337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18727822601795197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40335094928741455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9272894859313965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.164272308349609
11 1.5450559292 	 5.164272329 	 5.164272329
epoch_time;  31.186382293701172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15490345656871796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3201978802680969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.913202285766602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0635085105896
12 1.5399094713 	 5.0635085647 	 5.0635085647
epoch_time;  31.150933265686035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1688610315322876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3591015934944153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.936527729034424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.139956474304199
13 1.5353703069 	 5.1399565826 	 5.1399565826
epoch_time;  30.964548110961914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21154876053333282
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39885976910591125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.891367435455322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.087584018707275
14 1.5270609304 	 5.0875841295 	 5.0875841295
epoch_time;  30.718053102493286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2104445993900299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.403279185295105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.825717449188232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.990026473999023
15 1.5256863376 	 4.9900265255 	 4.9900265255
epoch_time;  30.79403567314148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15763771533966064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3405954837799072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.850512504577637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.043971061706543
16 1.5204709517 	 5.0439710462 	 5.0439710462
epoch_time;  31.237614631652832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15328449010849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3180291950702667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9270195960998535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1017746925354
17 1.5188550284 	 5.1017746384 	 5.1017746384
epoch_time;  31.371116876602173
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15231476724147797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3127627968788147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.864398956298828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.008913516998291
18 1.514759681 	 5.0089134423 	 5.0089134423
epoch_time;  31.51014256477356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1744748055934906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3449709415435791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.016404628753662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.181304931640625
19 1.5104132506 	 5.1813047667 	 5.1813047667
epoch_time;  31.081157445907593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17454484105110168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.345034122467041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.017972469329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1825737953186035
It took 678.4610297679901 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137932
Array Job ID: 2137927_2
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-10:10:12 core-walltime
Job Wall-clock time: 01:53:54
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
