/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_003819-n4gvsly9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-noodles-1440
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/n4gvsly9
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(0,', '0)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e4d4dff40>, <torch.utils.data.dataloader.DataLoader object at 0x148e465e84c0>, <torch.utils.data.dataloader.DataLoader object at 0x148e465e8730>, <torch.utils.data.dataloader.DataLoader object at 0x148e465e8670>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05647887662053108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8101449012756348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.571945190429688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.799903869628906
0 0.7736865382 	 44.7999049991
epoch_time;  44.06009364128113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010339382104575634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.578113853931427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.675107955932617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.90080261230469
1 0.0227635721 	 41.9008036726
epoch_time;  43.28169894218445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053854649886488914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4857666492462158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.644580841064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.081993103027344
2 0.0126754738 	 40.0819928956
epoch_time;  43.06251335144043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003427265677601099
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4357739984989166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.575672149658203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.27385711669922
3 0.0091856733 	 38.2738564483
epoch_time;  42.90370464324951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005692030303180218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4134853482246399
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.723161697387695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.72339630126953
4 0.0066517426 	 36.7233967853
epoch_time;  43.09012150764465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022154448088258505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3980441391468048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.467132568359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.99457931518555
5 0.0066302716 	 35.9945802256
epoch_time;  43.07678723335266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002493569627404213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4138859808444977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.298870086669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.70539855957031
6 0.0046401067 	 35.705399712
epoch_time;  43.20704126358032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024301072116941214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42435720562934875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.37639045715332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.64398956298828
7 0.0040979822 	 35.6439883875
epoch_time;  43.25231599807739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004614127799868584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43125462532043457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.241525650024414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.28697204589844
8 0.0035304399 	 35.2869736594
epoch_time;  43.177016735076904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012847059406340122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4242341220378876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.221237182617188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.03919219970703
9 0.0030366347 	 35.039192315
epoch_time;  42.90920543670654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009475281112827361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41572266817092896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.022424697875977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.8049201965332
10 0.0028560667 	 34.8049205769
epoch_time;  42.85449457168579
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010146999265998602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43820011615753174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.221540451049805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.09888458251953
11 0.0030809185 	 35.0988835914
epoch_time;  43.255056858062744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002952833427116275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40931281447410583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.309022903442383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.23691940307617
12 0.0021649304 	 35.236918193
epoch_time;  43.05750060081482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008815694600343704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4056497812271118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.006969451904297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.1309700012207
13 0.0026149531 	 36.1309685376
epoch_time;  43.044025182724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007472288561984897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3943750560283661
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.575288772583008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.74214553833008
14 0.0028267291 	 36.7421461952
epoch_time;  42.83424472808838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000709950109012425
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39132773876190186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.964519500732422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.16647720336914
15 0.0021280405 	 37.166475878
epoch_time;  43.26729774475098
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009906713385134935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37360671162605286
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.094436645507812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.259742736816406
16 0.0015304751 	 37.2597420223
epoch_time;  43.05509948730469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005613799439743161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3713117241859436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.037979125976562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.238548278808594
17 0.0019064224 	 37.2385467806
epoch_time;  43.273375511169434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019630081951618195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36850032210350037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.032991409301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.30949401855469
18 0.001563893 	 37.3094936037
epoch_time;  44.0682156085968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009323733975179493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37303635478019714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.81144905090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.390281677246094
19 0.0033978019 	 37.3902798102
epoch_time;  43.87483525276184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008529419428668916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.370491623878479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.38775062561035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.60543441772461
20 0.0009872463 	 36.6054333459
epoch_time;  43.069952964782715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009937860304489732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36200475692749023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.85053825378418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.0054931640625
21 0.0014429384 	 37.0054935329
epoch_time;  43.177440881729126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002016412792727351
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3620011806488037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.613601684570312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.7399787902832
22 0.0015562368 	 36.7399776954
epoch_time;  43.099767208099365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002075221622362733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34940463304519653
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▃▂▂▂▂▁▁▁▁▁▂▂▃▃▃▃▃▂▃▂▂▃▃▅▄▄▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▂▂▁▁▁▁▁▁▁▂▃▃▄▄▄▃▃▃▃▃▄▄▆▅▅▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 39.14499
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34209
wandb:    Test loss t(0, 0)_r(-5, 5)_none 22.34366
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00045
wandb:                         Train loss 0.00107
wandb: 
wandb: 🚀 View run auspicious-noodles-1440 at: https://wandb.ai/nreints/thesis/runs/n4gvsly9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_003819-n4gvsly9/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_010117-auck9579
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-moon-1450
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/auck9579
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.817928314208984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.80705261230469
23 0.0013796397 	 36.8070507222
epoch_time;  43.34150266647339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012896275147795677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3647981584072113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.305870056152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.64973068237305
24 0.0014886165 	 37.6497297489
epoch_time;  42.99402856826782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004738252318929881
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33015528321266174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.5023193359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.90981674194336
25 0.0011933065 	 37.9098169609
epoch_time;  42.813053607940674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006754413479939103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37938541173934937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.72759437561035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.05903244018555
26 0.0024634898 	 40.0590333506
epoch_time;  43.20003700256348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011604144237935543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3561789393424988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.938003540039062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.72644805908203
27 0.000695015 	 38.7264474367
epoch_time;  43.04698705673218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004291471850592643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3454374074935913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.301319122314453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.16383361816406
28 0.0012582662 	 39.1638323735
epoch_time;  42.931578636169434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00044671876821666956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34166577458381653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.36166763305664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.17676544189453
29 0.0010683766 	 39.1767637132
epoch_time;  43.15536975860596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004467768012546003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34208938479423523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.343656539916992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.14498519897461
It took  1378.5996770858765  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e4eec9450>, <torch.utils.data.dataloader.DataLoader object at 0x148e406ee4a0>, <torch.utils.data.dataloader.DataLoader object at 0x148e406941f0>, <torch.utils.data.dataloader.DataLoader object at 0x148e406942b0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023082394152879715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5732060670852661
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.92919158935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 49.04972457885742
0 0.7327850365 	 49.0497250283
epoch_time;  43.19321036338806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010035911574959755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37803879380226135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.780179977416992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.43706512451172
1 0.0233761226 	 46.4370663
epoch_time;  43.25999736785889
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00980977714061737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3125763535499573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.36343002319336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.560020446777344
2 0.0131575608 	 44.5600217145
epoch_time;  43.21499538421631
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003527682973071933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2838132679462433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.39891815185547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.83643341064453
3 0.0099273793 	 42.8364331571
epoch_time;  43.20709252357483
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006008321885019541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2597905993461609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.756404876708984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.95131301879883
4 0.0071580051 	 41.9513134913
epoch_time;  42.974820613861084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00219126814045012
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2452695667743683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.39293670654297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.60138702392578
5 0.0057044405 	 41.6013854796
epoch_time;  42.84711837768555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034709384199231863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24042828381061554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.942453384399414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.061336517333984
6 0.004411192 	 41.0613346157
epoch_time;  43.507184743881226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011776945320889354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2269509732723236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.959449768066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.06690216064453
7 0.0045640207 	 41.0669019071
epoch_time;  44.30567383766174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026092524640262127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29210689663887024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.30828094482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.16731262207031
8 0.0140727633 	 42.1673137745
epoch_time;  43.40407967567444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014317769091576338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22154396772384644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.47792625427246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.12150955200195
9 0.0017808469 	 41.1215097479
epoch_time;  42.97850823402405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0076647596433758736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20839492976665497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.406137466430664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.86077880859375
10 0.0025825051 	 40.8607793618
epoch_time;  43.245901107788086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009497550781816244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1747487634420395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.1193790435791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.28945541381836
11 0.0026400198 	 40.2894548952
epoch_time;  43.139946699142456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017965581500902772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1679777055978775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.838239669799805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.585262298583984
12 0.0022456274 	 39.5852618722
epoch_time;  43.096083879470825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012048373464494944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.160332590341568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.506261825561523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.763797760009766
13 0.002436154 	 38.7637987396
epoch_time;  42.899569272994995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007352940738201141
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15557900071144104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.6047306060791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.980838775634766
14 0.0022976537 	 38.9808404928
epoch_time;  43.38104176521301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006801955751143396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14921268820762634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.210107803344727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.07633590698242
15 0.0016562018 	 38.076337094
epoch_time;  43.2640380859375
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▄▃▃▄▃▃▃▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▃▃▃▄▃▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▃▂▂▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▂▃▂▂▁▂▁▃▁▁▁▁▁▂▁▁▂▁▁▁▃▂▁▁▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.33802
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.1545
wandb:    Test loss t(0, 0)_r(-5, 5)_none 20.62187
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0052
wandb:                         Train loss 0.00171
wandb: 
wandb: 🚀 View run scintillating-moon-1450 at: https://wandb.ai/nreints/thesis/runs/auck9579
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_010117-auck9579/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_012404-lbgpsg1y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-firecracker-1458
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/lbgpsg1y
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004093321040272713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15342730283737183
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.017597198486328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.88530731201172
16 0.0019080217 	 37.8853055372
epoch_time;  43.258410930633545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017981770215556026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1616516262292862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.267967224121094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.14309310913086
17 0.0016536016 	 38.1430944345
epoch_time;  43.59668588638306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006410458008758724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13728685677051544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.21826934814453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.03820037841797
18 0.0016855396 	 38.0382010008
epoch_time;  43.12422060966492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021634383592754602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14106222987174988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.161300659179688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.948158264160156
19 0.002107593 	 37.9481566276
epoch_time;  43.13135123252869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012903298484161496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1368374228477478
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.316370010375977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.0734748840332
20 0.0011824554 	 38.0734752644
epoch_time;  42.941150188446045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019367861095815897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1339365839958191
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.345956802368164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.86797332763672
21 0.0013506683 	 37.8679722904
epoch_time;  42.97492790222168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001157292048446834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1789524257183075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.322965621948242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.11055374145508
22 0.007373116 	 39.1105521856
epoch_time;  43.14796304702759
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005791790783405304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16994090378284454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.651025772094727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.02535629272461
23 0.0010725613 	 38.0253581713
epoch_time;  43.33293080329895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025898602325469255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14312507212162018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.204740524291992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.48493957519531
24 0.0013235912 	 37.4849414653
epoch_time;  43.29406690597534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006082672043703496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1204664334654808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.075469970703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.4332160949707
25 0.001263016 	 37.4332161065
epoch_time;  43.224223613739014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007623324054293334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11378704756498337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.029788970947266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.19218063354492
26 0.001225803 	 37.192179239
epoch_time;  43.23410987854004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010975990444421768
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12184891849756241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.126195907592773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.177711486816406
27 0.0013185468 	 37.1777107723
epoch_time;  43.905539989471436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005221685860306025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11471812427043915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.32967185974121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.18391799926758
28 0.0011164075 	 37.1839182874
epoch_time;  43.87895321846008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005198410712182522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15449950098991394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.597139358520508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.38992691040039
29 0.0017117254 	 37.3899287198
epoch_time;  43.35090756416321
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00519731966778636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15450401604175568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.621871948242188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.33802032470703
It took  1367.918491601944  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e4d536710>, <torch.utils.data.dataloader.DataLoader object at 0x148e406ee710>, <torch.utils.data.dataloader.DataLoader object at 0x148e46f92d10>, <torch.utils.data.dataloader.DataLoader object at 0x148e46f92e90>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04308243468403816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48525699973106384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.386383056640625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.54237747192383
0 0.7459270007 	 46.542378682
epoch_time;  43.46042323112488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010152756236493587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2838287651538849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.7392520904541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.54436492919922
1 0.0219349367 	 43.5443642608
epoch_time;  43.20212984085083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01637301780283451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22280992567539215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.618534088134766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.123680114746094
2 0.0140147545 	 41.1236811981
epoch_time;  42.95179772377014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029558202251791954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22019831836223602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.674997329711914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.741111755371094
3 0.0117114296 	 40.7411135763
epoch_time;  43.129462480545044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0045857359655201435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15293963253498077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.99542808532715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.32221984863281
4 0.0072290116 	 39.3222213699
epoch_time;  43.015764236450195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022367616184055805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1315685659646988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.815702438354492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.141090393066406
5 0.0064811772 	 39.1410911537
epoch_time;  43.15741205215454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017489372985437512
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11847895383834839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.253929138183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.380958557128906
6 0.004615962 	 38.3809567362
epoch_time;  43.20707035064697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015679104253649712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11092205345630646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.695817947387695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.59373092651367
7 0.0042212513 	 37.5937293476
epoch_time;  43.65892934799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038509361911565065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1113746240735054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.724937438964844
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▆▅▅▅▄▄▃▃▃▃▅▄▃▃▂▁▂▁▁▁▁▁▁▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▂▂▂▂▂▂▁▁▁▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▆▅▅▅▄▃▃▃▃▃▆▄▄▃▂▁▁▁▁▁▁▁▁▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▄▆▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 32.63457
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08349
wandb:    Test loss t(0, 0)_r(-5, 5)_none 16.26981
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00601
wandb:                         Train loss 0.00113
wandb: 
wandb: 🚀 View run lambent-firecracker-1458 at: https://wandb.ai/nreints/thesis/runs/lbgpsg1y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_012404-lbgpsg1y/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_014647-txcx0555
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-rooster-1465
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/txcx0555
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.16154479980469
8 0.003743124 	 36.1615458601
epoch_time;  43.01763892173767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017961319535970688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10852718353271484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.41361427307129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.59245681762695
9 0.0036965539 	 35.5924577511
epoch_time;  43.3033971786499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012244018726050854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09908488392829895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.96695327758789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.74247360229492
10 0.0026944317 	 34.742473683
epoch_time;  42.96240425109863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007377027650363743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09541920572519302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.7122745513916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.15119934082031
11 0.002721482 	 34.1511990181
epoch_time;  43.253273010253906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013073742156848311
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09601476788520813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.644025802612305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.95956039428711
12 0.0025598091 	 33.9595596913
epoch_time;  43.2501962184906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002713535912334919
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16517330706119537
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.85165786743164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.291744232177734
13 0.0156879671 	 40.291744359
epoch_time;  43.203092098236084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014545280719175935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11615379899740219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.60601234436035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.3253173828125
14 0.0019503761 	 36.3253192268
epoch_time;  43.16330122947693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003096244530752301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11014121025800705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.813138961791992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.87257766723633
15 0.0021300689 	 34.872577771
epoch_time;  43.286338090896606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004166066646575928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10862084478139877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.98366355895996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.00935363769531
16 0.0023681175 	 34.0093555278
epoch_time;  43.370370864868164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006609777919948101
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08445954322814941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.939262390136719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.58392333984375
17 0.0017116191 	 32.5839224179
epoch_time;  44.47363758087158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008186086779460311
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08372672647237778
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.042234420776367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.17237091064453
18 0.0019031224 	 31.1723706571
epoch_time;  43.62083888053894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011211137752979994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0833241418004036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.99642562866211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.195005416870117
19 0.0016522628 	 31.1950056647
epoch_time;  43.43912053108215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005240521859377623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08453431725502014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.664702415466309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.41863250732422
20 0.0017555931 	 30.4186325765
epoch_time;  42.80695676803589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024263423401862383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09439436346292496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.595027923583984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.00012969970703
21 0.0015145512 	 30.000129815
epoch_time;  42.99008131027222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004897155449725688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08871046453714371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.62891674041748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.19611167907715
22 0.0015172368 	 30.1961120421
epoch_time;  43.49273180961609
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008167835185304284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0867779478430748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.6016845703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.14914894104004
23 0.0013788594 	 30.1491485319
epoch_time;  43.342573404312134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005517288809642196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08126373589038849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.997849464416504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.631555557250977
24 0.0013842794 	 30.6315556552
epoch_time;  43.24505662918091
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00046919132000766695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08058484643697739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.046972274780273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.788122177124023
25 0.0013738409 	 30.7881213416
epoch_time;  43.21239614486694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005599122494459152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10026231408119202
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.610913276672363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.5035343170166
26 0.0015854005 	 31.5035345072
epoch_time;  43.29072713851929
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008196273702196777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0759897455573082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.648353576660156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.648691177368164
27 0.0011282692 	 31.6486912292
epoch_time;  42.91695857048035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004288032359909266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0749969482421875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.273195266723633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.54781723022461
28 0.0016203049 	 32.5478161584
epoch_time;  43.28361701965332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006006175186485052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08347290009260178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.287860870361328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.58625411987305
29 0.0011319459 	 32.5862531864
epoch_time;  43.08363342285156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006012203637510538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08348523825407028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.269813537597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.63456726074219
It took  1362.8963820934296  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e46f91060>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e5f5420>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e5f6e00>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e5f6fb0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020823946222662926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.913699209690094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.135391235351562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.1591796875
0 0.7675198621 	 46.1591796875
epoch_time;  43.33807063102722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00909955520182848
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7565791010856628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.89429473876953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.65871047973633
1 0.0228402443 	 43.6587105835
epoch_time;  43.27936148643494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00617134477943182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6793792247772217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.677148818969727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.30219268798828
2 0.0132989631 	 41.3021915125
epoch_time;  43.2661554813385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003600856987759471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6354100704193115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.894145965576172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.77341842651367
3 0.0095840818 	 39.773419798
epoch_time;  42.817928075790405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003557007061317563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6207785606384277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.478851318359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.88557434082031
4 0.0074521064 	 38.8855740181
epoch_time;  43.25860905647278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002700807061046362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.587168276309967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.875375747680664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.579017639160156
5 0.006394426 	 37.579018953
epoch_time;  42.63463354110718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002816147170960903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.558749258518219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.654090881347656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.15312576293945
6 0.0045804537 	 37.1531255901
epoch_time;  44.81622552871704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032586476299911737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.536758303642273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.409482955932617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.78179168701172
7 0.0040985063 	 36.7817899122
epoch_time;  44.20587968826294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011260564206168056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5092348456382751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.130830764770508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.225372314453125
8 0.0040293032 	 36.225373513
epoch_time;  43.0580050945282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008793318993411958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5310987234115601
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.2407283782959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.23900604248047
9 0.0038080049 	 36.2390070336
epoch_time;  43.4741735458374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004837700165808201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5204082727432251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.9716854095459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.49018096923828
10 0.0025135487 	 35.4901812689
epoch_time;  43.15245509147644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007748900097794831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5089879035949707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.92928695678711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.20160675048828
11 0.0025847646 	 35.201605575
epoch_time;  43.26680874824524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006745129358023405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.505707323551178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.110868453979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.43538284301758
12 0.0023674689 	 35.435381656
epoch_time;  43.02725696563721
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001902600983157754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6424266695976257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.377275466918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.34132766723633
13 0.0147629665 	 37.341327771
epoch_time;  43.26228928565979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009887432679533958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5834992527961731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.968595504760742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.89830780029297
14 0.0018222049 	 36.898307685
epoch_time;  43.19611883163452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012245088582858443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5716285109519958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.283559799194336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.2993049621582
15 0.0020221368 	 37.2993060801
epoch_time;  43.1387403011322
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015607213135808706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5627986788749695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.557653427124023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.574310302734375
16 0.0019373524 	 37.5743102105
epoch_time;  43.267035245895386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005378553178161383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5602639317512512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.764297485351562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.67339324951172
17 0.0018312705 	 37.673394425
epoch_time;  43.700931787490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006177873583510518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5452808737754822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.796772003173828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.76239013671875
18 0.0016817902 	 37.7623884772
epoch_time;  42.98650002479553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008013765327632427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5480175018310547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.775693893432617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.46921157836914
19 0.0016415735 	 37.469210253
epoch_time;  43.12408447265625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006638357299380004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5052579641342163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.837562561035156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.3611946105957
20 0.0025010676 	 37.3611953597
epoch_time;  43.1636323928833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00230237003415823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.514275074005127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.790494918823242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.779083251953125
21 0.000961561 	 36.7790815002
epoch_time;  43.289750814437866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005744703230448067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4879177212715149
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.585811614990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.451751708984375
22 0.0015027733 	 36.451753092
epoch_time;  43.2428674697876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002757834969088435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4812905490398407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.193408966064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.6137580871582
23 0.0012646135 	 35.6137562547
epoch_time;  43.16704797744751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006456036935560405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4729316830635071
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.288677215576172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.569969177246094
24 0.0015019465 	 35.5699673102
epoch_time;  43.189127683639526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005763668450526893
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.462935209274292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.668676376342773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.5959587097168
25 0.0012135186 	 34.5959598046
epoch_time;  42.920207500457764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006704165716655552
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▅▄▃▃▃▃▃▂▂▂▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▄▄▃▃▂▂▂▂▂▂▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▃▂▂▂▁▁▁▁▁▃▂▃▃▃▄▃▄▄▃▃▃▂▂▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▃▁▁▁▂▁▂▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 33.8348
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.535
wandb:    Test loss t(0, 0)_r(-5, 5)_none 20.56416
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00063
wandb:                         Train loss 0.00381
wandb: 
wandb: 🚀 View run dancing-rooster-1465 at: https://wandb.ai/nreints/thesis/runs/txcx0555
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_014647-txcx0555/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_020937-tr0eeavu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-pig-1474
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/tr0eeavu
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4585482180118561
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.38677978515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.04304122924805
26 0.0012039283 	 34.0430395582
epoch_time;  43.14410853385925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006440082215704024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45743346214294434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.133981704711914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.500186920166016
27 0.0011723976 	 33.5001858714
epoch_time;  45.150545835494995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007367965299636126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4752642512321472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.98868179321289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.28385543823242
28 0.0011646125 	 33.2838551501
epoch_time;  44.527249574661255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006338734528981149
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5349640846252441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.560100555419922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.803062438964844
29 0.0038100773 	 33.8030618627
epoch_time;  42.92029333114624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006340256077237427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.534999668598175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.564157485961914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.83479690551758
It took  1369.4116492271423  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e1e5f5180>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4a9360>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aad40>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aaef0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023998448625206947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6841704845428467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.869619369506836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.1612434387207
0 0.7803003281 	 44.1612449254
epoch_time;  43.03899550437927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010907016694545746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4890281558036804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.8527774810791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.05823516845703
1 0.0232399674 	 41.0582367589
epoch_time;  42.75836515426636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04423647001385689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46632319688796997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.672456741333008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.22140884399414
2 0.0136308605 	 39.2214082562
epoch_time;  43.184170722961426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004433366935700178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3457964062690735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.8936710357666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.8327522277832
3 0.0101119893 	 37.8327540833
epoch_time;  43.103607416152954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01694263145327568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33521294593811035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.142486572265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.96076202392578
4 0.0073192504 	 36.9607634299
epoch_time;  43.19239115715027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017939239041879773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2832911014556885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.728330612182617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.12217330932617
5 0.0075719416 	 36.1221735744
epoch_time;  43.112515449523926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014034187188372016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26745733618736267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.024463653564453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.209205627441406
6 0.0038227605 	 35.2092056505
epoch_time;  42.926398038864136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025490853004157543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25701895356178284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.51998519897461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.503662109375
7 0.0040453612 	 34.5036613718
epoch_time;  43.15749454498291
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008463007980026305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2443731278181076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.25218391418457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.94734191894531
8 0.003934657 	 33.9473423338
epoch_time;  43.09082555770874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018819415709003806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23466457426548004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.939170837402344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.48430252075195
9 0.0030916544 	 33.4843041918
epoch_time;  42.82350420951843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034453386906534433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31612449884414673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.78821563720703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.138031005859375
10 0.013857718 	 36.1380316512
epoch_time;  42.929882287979126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010270548518747091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22036953270435333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.062549591064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.877662658691406
11 0.0015550728 	 34.8776612066
epoch_time;  42.80010104179382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002848745556548238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20798151195049286
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.79282569885254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.651634216308594
12 0.0020276181 	 34.6516327181
epoch_time;  42.98809003829956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0044343480840325356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2048656940460205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.54404067993164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.931087493896484
13 0.0020523337 	 33.9310889114
epoch_time;  43.00797462463379
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000633189978543669
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19895847141742706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.333772659301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.58007049560547
14 0.0020157477 	 33.5800722243
epoch_time;  43.22197723388672
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009534865617752075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20935025811195374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.23523712158203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.22431564331055
15 0.0019706918 	 33.2243172914
epoch_time;  43.22149968147278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027492183726280928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18647804856300354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.84706497192383
16 0.0018823802 	 32.8470632317
epoch_time;  44.284738063812256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000537868298124522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17463050782680511
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.721874237060547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.12224578857422
17 0.001824808 	 32.1222473329
epoch_time;  43.71363425254822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005851048976182938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17958875000476837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.932361602783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.426387786865234
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▂▂▂▃▃▂▂▂▂▁▁▁▁▁▁▂▂▂▁▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▄▃▃▃▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▃▃▂▂▂▁▃▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▃█▂▄▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▃▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 32.61373
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.15049
wandb:    Test loss t(0, 0)_r(-5, 5)_none 18.2528
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00042
wandb:                         Train loss 0.00115
wandb: 
wandb: 🚀 View run sweet-pig-1474 at: https://wandb.ai/nreints/thesis/runs/tr0eeavu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_020937-tr0eeavu/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_023213-fhkg1jom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-rabbit-1482
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/fhkg1jom
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
18 0.001908253 	 32.42638902
epoch_time;  42.71547055244446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002514227293431759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16168931126594543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.15311050415039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.621116638183594
19 0.0013931681 	 32.6211173527
epoch_time;  42.81525635719299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007777426508255303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16721636056900024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.19509506225586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.67499923706055
20 0.0017134683 	 32.6749994099
epoch_time;  42.861644983291626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000597554724663496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15968336164951324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.287569046020508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.97880172729492
21 0.001446821 	 32.978801808
epoch_time;  42.873159408569336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028264818247407675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1687101572751999
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.622425079345703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.48687744140625
22 0.0019437998 	 33.4868768882
epoch_time;  42.46778440475464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010343522299081087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16228869557380676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.545913696289062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.284908294677734
23 0.0011361572 	 33.2849084215
epoch_time;  42.45036959648132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004304934700485319
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16797547042369843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.41465950012207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.100528717041016
24 0.0014918518 	 33.100529881
epoch_time;  42.63687562942505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00048117394908331335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16091704368591309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.42143440246582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.977664947509766
25 0.0011849515 	 32.9776659271
epoch_time;  42.605419635772705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025646856520324945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16193225979804993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.513166427612305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.00882339477539
26 0.0011995472 	 33.0088244666
epoch_time;  42.45076060295105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01146166492253542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18688151240348816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.412391662597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.8411979675293
27 0.00123712 	 32.841197956
epoch_time;  42.361711502075195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004192118358332664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14747147262096405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.51447868347168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.89997863769531
28 0.0011884493 	 32.8999775774
epoch_time;  42.2921347618103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000423285091528669
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16272398829460144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.25506019592285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.74762725830078
29 0.0011513876 	 32.7476279267
epoch_time;  42.5773708820343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004235437954775989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1504942625761032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.252803802490234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.61372756958008
It took  1356.6644403934479  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e46fcee00>, <torch.utils.data.dataloader.DataLoader object at 0x148e46fcf940>, <torch.utils.data.dataloader.DataLoader object at 0x148e46f90f70>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aab90>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02756606787443161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9492501020431519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.4494686126709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.724456787109375
0 0.7653679685 	 46.7244559573
epoch_time;  42.3340220451355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03747125342488289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.75406414270401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.553199768066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.65586471557617
1 0.0229384128 	 44.6558635055
epoch_time;  42.11167883872986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006222063675522804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6143091320991516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.297685623168945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.06452560424805
2 0.0152292779 	 43.0645268835
epoch_time;  42.238287687301636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006964147090911865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5577329993247986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.3884334564209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.025657653808594
3 0.0084833778 	 42.0256591059
epoch_time;  41.96172046661377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004241263959556818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5090025663375854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.514801025390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.70675277709961
4 0.0065388474 	 40.7067539181
epoch_time;  42.141488552093506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002655775984749198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48692989349365234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.047853469848633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.98442077636719
5 0.0058624295 	 39.9844192551
epoch_time;  42.44487690925598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032528021838515997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46621981263160706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.508066177368164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.331748962402344
6 0.0051467514 	 39.3317480174
epoch_time;  43.719679832458496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002813659841194749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45465755462646484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.47398567199707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.388214111328125
7 0.0044222463 	 39.3882145723
epoch_time;  42.44804525375366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001936062821187079
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43769371509552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.18600845336914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.8917121887207
8 0.0034230086 	 38.8917107251
epoch_time;  42.182618856430054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011859548278152943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.515091598033905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.7569580078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.14173889160156
9 0.0047174128 	 40.1417402285
epoch_time;  42.31311821937561
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009705701377242804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4028744101524353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.057588577270508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.941341400146484
10 0.0023443143 	 38.9413413425
epoch_time;  42.30269646644592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016950885765254498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36783087253570557
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▄▄▃▃▃▃▄▂▂▂▄▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆█▂▂▂▁▂▁▁▃▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 34.56755
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.2612
wandb:    Test loss t(0, 0)_r(-5, 5)_none 17.0299
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00073
wandb:                         Train loss 0.00122
wandb: 
wandb: 🚀 View run twinkling-rabbit-1482 at: https://wandb.ai/nreints/thesis/runs/fhkg1jom
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_023213-fhkg1jom/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_025437-9vf79cnu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-envelope-1490
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9vf79cnu
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.77250099182129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.40361022949219
11 0.0023779523 	 38.4036094458
epoch_time;  42.657387495040894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009244520333595574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3667563498020172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.69109344482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.381412506103516
12 0.0023203809 	 38.3814110886
epoch_time;  42.46000075340271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004426863975822926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5228503346443176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.74359703063965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.08250427246094
13 0.0081431954 	 38.0825033044
epoch_time;  42.26668572425842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012431173818185925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41263195872306824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.84804344177246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.0677490234375
14 0.001565447 	 38.0677486546
epoch_time;  42.10881280899048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011730188271030784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3671986162662506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.276384353637695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.32689666748047
15 0.0020009795 	 37.3268976586
epoch_time;  42.25994110107422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001014130306430161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33479297161102295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.98736572265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.86491394042969
16 0.0018850486 	 36.864912788
epoch_time;  43.502527952194214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014615085674449801
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3368711769580841
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.726974487304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.62222671508789
17 0.0020434219 	 36.6222266805
epoch_time;  42.11140012741089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007021818310022354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31348979473114014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.426128387451172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.23596954345703
18 0.0015340167 	 36.2359681835
epoch_time;  42.38263297080994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006711728638038039
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32253965735435486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.276845932006836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.17154312133789
19 0.0015652283 	 36.1715416116
epoch_time;  42.14868497848511
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004037015605717897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3380393385887146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.347545623779297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.3079719543457
20 0.001651704 	 36.3079712283
epoch_time;  42.22625470161438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005947906756773591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3189637362957001
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.231182098388672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.11732482910156
21 0.0013754303 	 36.117326166
epoch_time;  42.43187475204468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010054948506876826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30754077434539795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.00049591064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.72574996948242
22 0.0015556975 	 35.7257511565
epoch_time;  42.46696758270264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019251095363870263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2940463125705719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.969463348388672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.52635192871094
23 0.0013782473 	 35.5263524358
epoch_time;  42.32422184944153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006000930443406105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3709631860256195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.625804901123047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.010894775390625
24 0.0034279051 	 36.0108956052
epoch_time;  42.25830602645874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005195964477024972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3046577572822571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.128320693969727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.30280685424805
25 0.0009862354 	 35.3028081335
epoch_time;  42.5043625831604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004776740970555693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2871958911418915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.992576599121094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.88393783569336
26 0.0014493637 	 34.8839395298
epoch_time;  42.87204194068909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007320467848330736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2694486975669861
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.80202865600586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.546226501464844
27 0.0010302841 	 34.5462259252
epoch_time;  44.87293887138367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006442274316214025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26593852043151855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.814237594604492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.44112777709961
28 0.0011279709 	 34.4411289181
epoch_time;  43.37401795387268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000730573374312371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2814578711986542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.004188537597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.57286834716797
29 0.0012191453 	 34.5728674943
epoch_time;  42.39739942550659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007309979991987348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2611958086490631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.02989959716797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.56755065917969
It took  1343.0437841415405  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e46f90310>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4ab6a0>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aa7d0>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4a9c30>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021656673401594162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7793871164321899
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.171293258666992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.48214340209961
0 0.7436505418 	 47.4821415927
epoch_time;  42.339972496032715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009606417268514633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5765867829322815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.55612564086914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.97404098510742
1 0.0245843233 	 44.9740399594
epoch_time;  42.390798568725586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006366063375025988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4842372536659241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.314130783081055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.68628692626953
2 0.0121455629 	 42.6862874103
epoch_time;  42.206852436065674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02477526292204857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4552052319049835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.40265464782715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.240203857421875
3 0.0108354844 	 41.2402048716
epoch_time;  42.28639888763428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033096715342253447
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3962995111942291
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.510231018066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.9067268371582
4 0.0066994624 	 39.9067250047
epoch_time;  42.65059518814087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017438913928344846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37632542848587036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.926376342773438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.910343170166016
5 0.0063769047 	 38.9103450718
epoch_time;  42.77555441856384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012858245521783829
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3880588114261627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.46882438659668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.967384338378906
6 0.004827451 	 37.9673839926
epoch_time;  42.25655961036682
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004636857658624649
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3638562560081482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.29710578918457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.46637725830078
7 0.0048111227 	 37.4663779267
epoch_time;  41.95532011985779
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013358354335650802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34840691089630127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.029260635375977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.50019454956055
8 0.0033007105 	 36.5001947224
epoch_time;  42.200655460357666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001146627007983625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33649957180023193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.935407638549805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.99378967285156
9 0.0032226048 	 35.9937895346
epoch_time;  42.32529044151306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000892236246727407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3297899067401886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.735246658325195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.73277282714844
10 0.0028273473 	 35.7327729654
epoch_time;  42.4482057094574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008917098166421056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33817023038864136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.750886917114258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.71662521362305
11 0.002795046 	 35.7166257553
epoch_time;  42.44755816459656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007698009139858186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35172051191329956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.645404815673828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.4559326171875
12 0.0026193077 	 35.4559337236
epoch_time;  42.34438681602478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009042895399034023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3817700743675232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.807477951049805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.62567138671875
13 0.0021184198 	 35.6256697272
epoch_time;  42.533087730407715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021401711273938417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36784204840660095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.691255569458008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.65766143798828
14 0.0023248268 	 35.6576632128
epoch_time;  42.532939195632935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009093269472941756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3764086067676544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.6407527923584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.91655349731445
15 0.0063226781 	 36.9165525869
epoch_time;  42.256526708602905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008256400469690561
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3427702784538269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.395431518554688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.948307037353516
16 0.0011343988 	 36.948307095
epoch_time;  42.768293142318726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006440104334615171
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3159951865673065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.306913375854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.895416259765625
17 0.0022989594 	 36.895416352
epoch_time;  43.00987148284912
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006065928027965128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31085634231567383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.295915603637695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.86931610107422
18 0.0013483287 	 36.8693146951
epoch_time;  42.440659284591675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007170425378717482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3076155185699463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.156532287597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.745113372802734
19 0.0016834144 	 36.7451142372
epoch_time;  42.30682849884033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014262263430282474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30943772196769714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.90375328063965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.57146453857422
20 0.0014958836 	 36.5714660829
epoch_time;  42.42324161529541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000538553693331778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31439295411109924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.06289291381836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.848724365234375
21 0.0019941579 	 36.848724273
epoch_time;  42.4248993396759
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005136175896041095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3021576404571533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.145183563232422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.80204391479492
22 0.0011313551 	 36.8020439955
epoch_time;  42.155659675598145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001729927258566022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30002379417419434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.15756607055664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.82892608642578
23 0.0014159947 	 36.8289274924
epoch_time;  41.88345217704773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005869848537258804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2971293032169342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.379610061645508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.94936752319336
24 0.0013059167 	 36.9493692173
epoch_time;  42.0069854259491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007733553065918386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2978596091270447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.417430877685547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.113929748535156
25 0.0013017234 	 37.1139303248
epoch_time;  42.275331258773804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004553358303382993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30299800634384155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.432846069335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.34840393066406
26 0.0015226488 	 37.3484056363
epoch_time;  42.19425415992737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004818844608962536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3039035201072693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.7019100189209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.56178283691406
27 0.0011382477 	 37.5617830674
epoch_time;  42.44139814376831
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004965187981724739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29959726333618164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.636558532714844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.568302154541016
28 0.0011186198 	 37.5683003682
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▄▄▃▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▃▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▄▃█▂▁▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.5307
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28946
wandb:    Test loss t(0, 0)_r(-5, 5)_none 19.62326
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0008
wandb:                         Train loss 0.00122
wandb: 
wandb: 🚀 View run sweet-envelope-1490 at: https://wandb.ai/nreints/thesis/runs/9vf79cnu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_025437-9vf79cnu/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_031651-ed1z3kwr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-cake-1497
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ed1z3kwr
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  41.886624336242676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000804142386186868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2894632816314697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.63622283935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.53508377075195
29 0.0012158366 	 37.5350824915
epoch_time;  42.34906339645386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008046634029597044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2894597351551056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.623262405395508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.53070068359375
It took  1334.810435295105  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e46f92110>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aa1a0>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4a88e0>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4aa260>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022438885644078255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7008283734321594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 26.86687469482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.530189514160156
0 0.7495698431 	 47.5301878776
epoch_time;  42.624683141708374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04158477857708931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4919509291648865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.066566467285156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.73356628417969
1 0.0225018668 	 44.7335666069
epoch_time;  42.41223096847534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005767648108303547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3249586522579193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.96643829345703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.49052810668945
2 0.0142756584 	 42.4905264586
epoch_time;  41.9279522895813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038082152605056763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2502990663051605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.835674285888672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.44915008544922
3 0.0094845589 	 40.4491508922
epoch_time;  41.866339921951294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00477695232257247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20518657565116882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.107873916625977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.95427322387695
4 0.007420516 	 38.9542726822
epoch_time;  42.19976091384888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019560791552066803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16385464370250702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.24498176574707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.60443878173828
5 0.0060894394 	 37.6044390814
epoch_time;  42.266674280166626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03275671601295471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1911473274230957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.848173141479492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.91731643676758
6 0.004594765 	 36.9173167249
epoch_time;  43.33814215660095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006400499492883682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14980413019657135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.705917358398438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.762062072753906
7 0.0043665827 	 36.7620609894
epoch_time;  44.073310136795044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004447666462510824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14234592020511627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.600196838378906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.24182891845703
8 0.0037137486 	 36.2418305089
epoch_time;  42.68501114845276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009142731432802975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12744057178497314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.469234466552734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.70952224731445
9 0.0029510064 	 35.7095213369
epoch_time;  42.26936221122742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006059542298316956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12992694973945618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.8922176361084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.200584411621094
10 0.0031169044 	 36.2005847574
epoch_time;  41.85253143310547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005816498771309853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3707228899002075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.480236053466797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 43.131473541259766
11 0.0229739406 	 43.1314730457
epoch_time;  42.00384497642517
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027350413147360086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23721228539943695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.199356079101562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.11601257324219
12 0.0023775135 	 41.1160132647
epoch_time;  42.38390111923218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027697491459548473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20726244151592255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.923812866210938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.3568229675293
13 0.0023842087 	 40.356822956
epoch_time;  42.435222864151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014666226925328374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1672414094209671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.59356689453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.60243606567383
14 0.0025067093 	 39.6024358006
epoch_time;  42.307945728302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013974630273878574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.170528843998909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.446990966796875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.0163688659668
15 0.0021543362 	 39.0163684856
epoch_time;  42.031636476516724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009531082469038665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14198382198810577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.071664810180664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.156455993652344
16 0.0021131358 	 38.1564565238
epoch_time;  42.098487854003906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007589880842715502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13205300271511078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.058385848999023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.81354904174805
17 0.0019661045 	 37.8135473707
epoch_time;  42.599008560180664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010142059065401554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13651561737060547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.944482803344727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.47257995605469
18 0.00186999 	 37.4725795412
epoch_time;  42.16957879066467
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006595078157261014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10982266813516617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.005149841308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.508392333984375
19 0.0015728488 	 37.5083907666
epoch_time;  42.512956857681274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021274983882904053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11205445230007172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.96231460571289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.28022003173828
20 0.0017261594 	 37.2802203314
epoch_time;  42.053648233413696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011364207603037357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10307175666093826
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▃▂▂▂▁▁▁▅▄▄▃▃▂▂▂▂▂▃▂▃▃▃▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▃▂▂▂▂▂▁▁▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▄▃▂▁▁▁▁▁▅▄▄▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▂▂▂▁▆▂▂▁▂▂▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 36.72013
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.09534
wandb:    Test loss t(0, 0)_r(-5, 5)_none 21.64668
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00051
wandb:                         Train loss 0.00113
wandb: 
wandb: 🚀 View run beaming-cake-1497 at: https://wandb.ai/nreints/thesis/runs/ed1z3kwr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_031651-ed1z3kwr/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_033909-pcvv4ctq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-mandu-1505
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/pcvv4ctq
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.38106346130371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.26939010620117
21 0.0016078223 	 38.2693896337
epoch_time;  42.29474973678589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005721347988583148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10415425896644592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.848888397216797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.360511779785156
22 0.0013077237 	 37.3605108809
epoch_time;  42.43351101875305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006246404955163598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11007073521614075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.503334045410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.80960464477539
23 0.0034130304 	 38.8096027662
epoch_time;  42.029234647750854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00087328115478158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10515137016773224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.76481056213379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.008544921875
24 0.001430146 	 39.0085441843
epoch_time;  42.2632052898407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013952003791928291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10381192713975906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.62043571472168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.57518768310547
25 0.0012497049 	 38.5751864615
epoch_time;  42.081398248672485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007008325774222612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09957454353570938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.23090934753418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.744956970214844
26 0.0013051538 	 37.7449578691
epoch_time;  42.08614110946655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020861735101789236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09925436973571777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.12422752380371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.552940368652344
27 0.001186924 	 37.5529408988
epoch_time;  42.68439984321594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00046569510595873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09566227346658707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.87922477722168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.09728240966797
28 0.0012184908 	 37.0972815568
epoch_time;  43.272955656051636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005121931317262352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09532098472118378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.720203399658203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.769981384277344
29 0.0011301349 	 36.769982652
epoch_time;  42.3638391494751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005120870191603899
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09534286707639694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.646682739257812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.72012710571289
It took  1337.795203447342  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e1e5f7fd0>, <torch.utils.data.dataloader.DataLoader object at 0x148e465e95a0>, <torch.utils.data.dataloader.DataLoader object at 0x148e465e9720>, <torch.utils.data.dataloader.DataLoader object at 0x148e4d4a8400>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024931911379098892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7787343859672546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.282245635986328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.38616943359375
0 0.7791738806 	 45.3861699868
epoch_time;  42.38708782196045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010949758812785149
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5636324286460876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.638134002685547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.978118896484375
1 0.0228586219 	 42.9781202795
epoch_time;  42.46159768104553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005628490820527077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4756269156932831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.560474395751953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 41.216609954833984
2 0.0129290963 	 41.2166110036
epoch_time;  42.2602915763855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030809720046818256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4466407001018524
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.89971160888672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.76688003540039
3 0.0092410673 	 39.7668818448
epoch_time;  42.34632325172424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024384288117289543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4327241778373718
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.29312515258789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.42885971069336
4 0.0072848856 	 38.4288614048
epoch_time;  42.33867263793945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02005579322576523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4531795382499695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.071382522583008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.58102035522461
5 0.0058089288 	 37.5810192834
epoch_time;  42.3273549079895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012276052730157971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39023280143737793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.316429138183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.51144027709961
6 0.0051477838 	 37.5114384677
epoch_time;  42.29155445098877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001182746491394937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37313583493232727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.953948974609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.08745574951172
7 0.003857104 	 37.0874539747
epoch_time;  42.61050820350647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013065877137705684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36119288206100464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.717267990112305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.52914810180664
8 0.0034914962 	 36.529149358
epoch_time;  42.382768869400024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008296832675114274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3758018910884857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.640789031982422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.36973190307617
9 0.0036757947 	 36.369730693
epoch_time;  42.716980934143066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016349388752132654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33361539244651794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.218460083007812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.51133346557617
10 0.0025292546 	 35.5113322555
epoch_time;  42.164201974868774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009201177163049579
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33217543363571167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.411846160888672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.302913665771484
11 0.0027871659 	 35.3029143457
epoch_time;  42.20605230331421
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016428915783762932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39887455105781555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.975284576416016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.83738708496094
12 0.0032174214 	 35.8373861169
epoch_time;  42.42441940307617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006566258962266147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3357708156108856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.680932998657227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.22200393676758
13 0.0017358459 	 35.2220042249
epoch_time;  42.41427540779114
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▄▃▄▃▃▃▃▂▂▃▂▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▃▂▃▂▂▂▁▁▂▂▁▁▁▁▂▃▃▂▃▃▃▃▃▃▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▂▂▂▇▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 34.76262
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24351
wandb:    Test loss t(0, 0)_r(-5, 5)_none 20.72199
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0036
wandb:                         Train loss 0.00107
wandb: 
wandb: 🚀 View run festive-mandu-1505 at: https://wandb.ai/nreints/thesis/runs/pcvv4ctq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_033909-pcvv4ctq/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_040128-jrnbkpc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-fish-1512
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/jrnbkpc9
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017944639548659325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4863307476043701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.1563777923584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.41884994506836
14 0.0228242188 	 36.4188509016
epoch_time;  42.382755279541016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011821232037618756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4196808636188507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.138200759887695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.10064697265625
15 0.001782951 	 36.1006478946
epoch_time;  42.29023027420044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013249682961031795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35540929436683655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.318622589111328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.88971710205078
16 0.0018852853 	 35.8897162953
epoch_time;  42.464423418045044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015159901231527328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31499892473220825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.457712173461914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.550357818603516
17 0.0018670819 	 35.5503593514
epoch_time;  43.270782709121704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001464987057261169
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3026256859302521
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.826950073242188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.57931900024414
18 0.0018175002 	 35.5793198877
epoch_time;  43.1525673866272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000543516653124243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2958499491214752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.49851417541504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.55715560913086
19 0.0017635777 	 35.5571569345
epoch_time;  42.30858111381531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005405058618634939
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2892114818096161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.402210235595703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.94614791870117
20 0.0015348338 	 34.9461474462
epoch_time;  42.07500338554382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017172561958432198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31773698329925537
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.802379608154297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.167327880859375
21 0.0016946899 	 34.1673285262
epoch_time;  42.43342876434326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009123924537561834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2728728950023651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.76964569091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.90122985839844
22 0.0013345335 	 34.9012285215
epoch_time;  42.30468964576721
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005247488734312356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29632681608200073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.745107650756836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.15889358520508
23 0.0023540817 	 35.1588935045
epoch_time;  42.096033573150635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002255996223539114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2650812268257141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.854782104492188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.14519119262695
24 0.0009964493 	 35.1451921261
epoch_time;  42.0793879032135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013529759598895907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24721606075763702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.495471954345703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.642452239990234
25 0.0013228459 	 34.6424512604
epoch_time;  42.27021527290344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001452914671972394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2628858983516693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.936487197875977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.16452407836914
26 0.0012123619 	 35.1645257034
epoch_time;  42.410292863845825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002166577847674489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2655252516269684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.59821319580078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.68171691894531
27 0.0017337063 	 34.6817173338
epoch_time;  42.45906639099121
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013217923697084188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24251742660999298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.322952270507812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.16414260864258
28 0.0011250752 	 34.1641421592
epoch_time;  42.51284980773926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035955754574388266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24355198442935944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.857765197753906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.77550506591797
29 0.0010741836 	 34.7755056883
epoch_time;  42.50699162483215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00359858269803226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2435091882944107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.721988677978516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.76261520385742
It took  1338.7856726646423  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x148e1e5f7fd0>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e3c80a0>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e3c92d0>, <torch.utils.data.dataloader.DataLoader object at 0x148e1e3ca5c0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023159118369221687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8478911519050598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 24.860029220581055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.55451965332031
0 0.7605336238 	 45.5545193306
epoch_time;  42.664592027664185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012851880863308907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7154985666275024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 23.225261688232422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 42.960411071777344
1 0.0239530617 	 42.9604123395
epoch_time;  42.065133810043335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009844459593296051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6565477848052979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.210010528564453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.92658233642578
2 0.0150174409 	 40.9265837424
epoch_time;  42.30101752281189
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005507051944732666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.619255006313324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.65519905090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.908626556396484
3 0.010262376 	 39.9086250236
epoch_time;  42.36016917228699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003154875710606575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5904219150543213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.202669143676758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.32479476928711
4 0.0085387746 	 39.3247940663
epoch_time;  42.33364939689636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020652238745242357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5470830202102661
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.88029670715332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.91952896118164
5 0.0060296759 	 38.9195294798
epoch_time;  42.28389286994934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016082156216725707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5421168804168701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.484724044799805
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▂▂▂▂▁▃▁▁▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▃▂▁▁▁▁▁▂▂▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 32.00277
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.58736
wandb:    Test loss t(0, 0)_r(-5, 5)_none 16.82386
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00059
wandb:                         Train loss 0.00109
wandb: 
wandb: 🚀 View run radiant-fish-1512 at: https://wandb.ai/nreints/thesis/runs/jrnbkpc9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_040128-jrnbkpc9/logs
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.33035659790039
6 0.0050106402 	 38.3303554569
epoch_time;  42.42893052101135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00122488581109792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5428280830383301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.113061904907227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.72867202758789
7 0.0044301969 	 37.728671993
epoch_time;  43.3701868057251
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010567107237875462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5323774814605713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.948766708374023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.46316146850586
8 0.003832686 	 37.4631620563
epoch_time;  42.81021690368652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009147559176199138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5197169780731201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.984636306762695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.64451599121094
9 0.0031788098 	 37.6445164983
epoch_time;  41.97355008125305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00334189529530704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49359020590782166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.986238479614258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.594966888427734
10 0.002902154 	 37.5949684904
epoch_time;  42.410481452941895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00409550778567791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6183561086654663
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.539623260498047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.72890853881836
11 0.0036847496 	 38.7289080202
epoch_time;  42.53128266334534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001022550743073225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5119598507881165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.9030818939209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.552391052246094
12 0.0019220674 	 37.5523921356
epoch_time;  42.7961790561676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013894822914153337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49252426624298096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.79248809814453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.39005661010742
13 0.0019306173 	 37.3900555844
epoch_time;  42.41533708572388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015476603293791413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6617355942726135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.832447052001953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 38.44465637207031
14 0.0126946801 	 38.4446545742
epoch_time;  42.08271074295044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004278444219380617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6343952417373657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.476343154907227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.62600326538086
15 0.0012610968 	 37.6260031156
epoch_time;  42.3826060295105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000905841588973999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5995216965675354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.135934829711914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.20363235473633
16 0.0019664829 	 37.2036324585
epoch_time;  42.17296814918518
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012431705836206675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5912799835205078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.512880325317383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.3255615234375
17 0.0017303431 	 36.3255611546
epoch_time;  42.14878797531128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021577831357717514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5782264471054077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.36227798461914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.22914123535156
18 0.001787512 	 36.2291410971
epoch_time;  42.0791130065918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036373462062329054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5785666108131409
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.193227767944336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.0645637512207
19 0.0017207934 	 36.0645652379
epoch_time;  42.2147696018219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015556277940049767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5804405808448792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.21731185913086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.98408508300781
20 0.001368369 	 35.9840858667
epoch_time;  42.241130352020264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006668978021480143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5981882214546204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.367958068847656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.32468032836914
21 0.0015980623 	 36.324679003
epoch_time;  42.304717779159546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000605183478910476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6013416647911072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.998445510864258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.54801559448242
22 0.0014165343 	 35.5480138312
epoch_time;  42.43893313407898
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009333958732895553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6035541296005249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.618206024169922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.051700592041016
23 0.0013400693 	 35.051701756
epoch_time;  42.577324628829956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002560921711847186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6006430983543396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.12558937072754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.39141082763672
24 0.0015551031 	 34.3914097904
epoch_time;  42.02396512031555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010317261330783367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5811442136764526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.972274780273438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.059200286865234
25 0.0012495677 	 34.05920152
epoch_time;  42.18765139579773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009566733497194946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5858900547027588
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.66230583190918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.504573822021484
26 0.001258345 	 33.5045730268
epoch_time;  42.166907787323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008797242189757526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6026807427406311
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.366487503051758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.34479904174805
27 0.0017335507 	 33.344800321
epoch_time;  42.1209397315979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001120733330026269
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5730282664299011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.10099220275879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.49809265136719
28 0.0008271555 	 32.4980940804
epoch_time;  43.277024269104004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005941215204074979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5873593688011169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.81679916381836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.932693481445312
29 0.0010860945 	 31.9326938963
epoch_time;  42.91688919067383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005937890964560211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5873584747314453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.823862075805664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.00276565551758
It took  1339.5244762897491  seconds.

JOB STATISTICS
==============
Job ID: 2141606
Array Job ID: 2141141_6
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 22:03:30
CPU Efficiency: 32.57% of 2-19:44:06 core-walltime
Job Wall-clock time: 03:45:47
Memory Utilized: 14.80 GB
Memory Efficiency: 47.36% of 31.25 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
