wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_170033-zozppcby
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-fire-1249
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/zozppcby
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run smooth-fire-1249 at: https://wandb.ai/nreints/ThesisFinal2/runs/zozppcby
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_170033-zozppcby/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_170558-kgtlobp0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-totem-1263
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/kgtlobp0
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–…â–„â–ƒâ–‚â–ƒâ–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run radiant-totem-1263 at: https://wandb.ai/nreints/ThesisFinal2/runs/kgtlobp0
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_170558-kgtlobp0/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_171112-kyrn3se9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-feather-1274
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/kyrn3se9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–…â–…â–„â–ƒâ–‚â–‚â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run worthy-feather-1274 at: https://wandb.ai/nreints/ThesisFinal2/runs/kyrn3se9
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_171112-kyrn3se9/logs
Training on dataset: data_tennis_pNone_gNone_tennisEffect
Testing on 1 datasets: ['data_tennis_pNone_gNone_tennisEffect']
Focussing on identity: False
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 52.612061738967896 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 13.148675203323364 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0027055754 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 5.84517e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 25.638721466064453
Epoch 1/9
	 Logging train Loss: 3.71418e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.04303e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.29642677307129
Epoch 2/9
	 Logging train Loss: 2.95835e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.72201e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 23.967945098876953
Epoch 3/9
	 Logging train Loss: 2.86304e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.61804e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.100910663604736
Epoch 4/9
	 Logging train Loss: 2.62239e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.29254e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.210340976715088
Epoch 5/9
	 Logging train Loss: 2.38071e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.81998e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.03817868232727
Epoch 6/9
	 Logging train Loss: 1.88704e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.45627e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.83658194541931
Epoch 7/9
	 Logging train Loss: 1.52754e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.0495e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.496756315231323
Epoch 8/9
	 Logging train Loss: 1.28281e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.60527e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.312103986740112
Epoch 9/9
	 Logging train Loss: 1.14842e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.1375e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.100606679916382
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  326.56059098243713  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.76457333564758 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.684774398803711 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0021708361 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.44437e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.224643230438232
Epoch 1/9
	 Logging train Loss: 3.4959e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.16888e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.06351923942566
Epoch 2/9
	 Logging train Loss: 3.11342e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.0022e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.49068307876587
Epoch 3/9
	 Logging train Loss: 2.85787e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.68844e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.426278829574585
Epoch 4/9
	 Logging train Loss: 2.51959e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.16606e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.52719521522522
Epoch 5/9
	 Logging train Loss: 2.22873e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.69767e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.279265880584717
Epoch 6/9
	 Logging train Loss: 1.76206e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.87593e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.501055479049683
Epoch 7/9
	 Logging train Loss: 1.41787e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.06311e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.273550748825073
Epoch 8/9
	 Logging train Loss: 1.20099e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.05631e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.20449423789978
Epoch 9/9
	 Logging train Loss: 1.12994e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.01234e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.312636375427246
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  313.59427762031555  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.19795036315918 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.268497228622437 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.002297376 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 5.14154e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.244284868240356
Epoch 1/9
	 Logging train Loss: 3.44372e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.18339e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.057215452194214
Epoch 2/9
	 Logging train Loss: 3.0068e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.40894e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.164944410324097
Epoch 3/9
	 Logging train Loss: 2.8334e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.17318e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.3305504322052
Epoch 4/9
	 Logging train Loss: 2.57106e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.45866e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.315446376800537
Epoch 5/9
	 Logging train Loss: 2.27651e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.92215e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.320120096206665
Epoch 6/9
	 Logging train Loss: 1.88002e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.61476e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.41803812980652
Epoch 7/9
	 Logging train Loss: 1.5069e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.65866e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.304232358932495
Epoch 8/9
	 Logging train Loss: 1.27338e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.6043e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.117880821228027
Epoch 9/9
	 Logging train Loss: 1.10157e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.8724e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.341179132461548
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_171622-lnruhkmg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-totem-1288
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/lnruhkmg
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–‡â–„â–„â–ƒâ–‚â–â–â–‚
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run devoted-totem-1288 at: https://wandb.ai/nreints/ThesisFinal2/runs/lnruhkmg
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_171622-lnruhkmg/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_172134-s2tvz20r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-wind-1301
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/s2tvz20r
wandb: Waiting for W&B process to finish... (success).
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–‡â–†â–„â–…â–‚â–ƒâ–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run glamorous-wind-1301 at: https://wandb.ai/nreints/ThesisFinal2/runs/s2tvz20r
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_172134-s2tvz20r/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_172647-jn9cltq5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-armadillo-1315
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/jn9cltq5
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–„â–„â–„â–ƒâ–‚â–â–â–‚
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 2e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run fiery-armadillo-1315 at: https://wandb.ai/nreints/ThesisFinal2/runs/jn9cltq5
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_172647-jn9cltq5/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_173201-o8pn965p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-bush-1327
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/o8pn965p
It took  310.03348875045776  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.11389684677124 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.232862949371338 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0024792962 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.82451e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.36331272125244
Epoch 1/9
	 Logging train Loss: 3.36445e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.00604e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.268274068832397
Epoch 2/9
	 Logging train Loss: 2.98392e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.32038e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.301422834396362
Epoch 3/9
	 Logging train Loss: 2.77328e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.42062e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.24989604949951
Epoch 4/9
	 Logging train Loss: 2.51823e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.50329e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.518982410430908
Epoch 5/9
	 Logging train Loss: 2.1923e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.87391e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.486168384552002
Epoch 6/9
	 Logging train Loss: 1.72965e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.59736e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.34750986099243
Epoch 7/9
	 Logging train Loss: 1.3583e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.8404e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 25.011264085769653
Epoch 8/9
	 Logging train Loss: 1.20442e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.00223e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.6064772605896
Epoch 9/9
	 Logging train Loss: 1.10857e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.26206e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.21483063697815
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  312.1525058746338  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 44.97809362411499 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.186030149459839 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0018659969 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.53741e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.081867218017578
Epoch 1/9
	 Logging train Loss: 3.165e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.11571e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.12837553024292
Epoch 2/9
	 Logging train Loss: 2.98075e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.66782e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.0537006855011
Epoch 3/9
	 Logging train Loss: 2.69793e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.1443e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.34324860572815
Epoch 4/9
	 Logging train Loss: 2.32516e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.22463e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.28415322303772
Epoch 5/9
	 Logging train Loss: 1.83659e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.30988e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.486584424972534
Epoch 6/9
	 Logging train Loss: 1.47137e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.77981e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.24880051612854
Epoch 7/9
	 Logging train Loss: 1.25287e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.00079e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.346777200698853
Epoch 8/9
	 Logging train Loss: 1.14688e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 8.9353e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.093401432037354
Epoch 9/9
	 Logging train Loss: 1.11766e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.4564e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.195664644241333
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  313.15584802627563  seconds.
----- ITERATION 6/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 44.959351539611816 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.277706861495972 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0024863069 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.68478e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.36245322227478
Epoch 1/9
	 Logging train Loss: 3.3835e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.1005e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.064040899276733
Epoch 2/9
	 Logging train Loss: 2.9857e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.84333e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.197861671447754
Epoch 3/9
	 Logging train Loss: 2.76752e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.52831e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.31403088569641
Epoch 4/9
	 Logging train Loss: 2.5123e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.75791e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.394096851348877
Epoch 5/9
	 Logging train Loss: 2.12029e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.08946e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.39814305305481
Epoch 6/9
	 Logging train Loss: 1.73512e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.50762e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.22281837463379
Epoch 7/9
	 Logging train Loss: 1.37226e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22377e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.21566152572632
Epoch 8/9
	 Logging train Loss: 1.20553e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.44138e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.214184284210205
Epoch 9/9
	 Logging train Loss: 1.14032e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.67828e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.250657320022583
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  313.3622612953186  seconds.
----- ITERATION 7/10 ------
Number of train simulations:  1920
Number of test simulations:  480
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–‡â–ˆâ–„â–„â–„â–ƒâ–‚â–‚â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run expert-bush-1327 at: https://wandb.ai/nreints/ThesisFinal2/runs/o8pn965p
wandb: Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_173201-o8pn965p/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_173713-vf8mib0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-dawn-1341
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/vf8mib0l
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–‡â–…â–†â–ˆâ–ƒâ–‚â–‚â–â–‚â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run dry-dawn-1341 at: https://wandb.ai/nreints/ThesisFinal2/runs/vf8mib0l
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_173713-vf8mib0l/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_174224-giqffn48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-cloud-1352
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/giqffn48
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–„â–„â–ƒâ–„â–ƒâ–‚â–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run colorful-cloud-1352 at: https://wandb.ai/nreints/ThesisFinal2/runs/giqffn48
wandb: Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_174224-giqffn48/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230710_174734-9hsiczdu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-waterfall-1362
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2
wandb: ğŸš€ View run at https://wandb.ai/nreints/ThesisFinal2/runs/9hsiczdu
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.026679277420044 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.271132946014404 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0014719559 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.9337e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.35176157951355
Epoch 1/9
	 Logging train Loss: 3.54128e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.6207e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.186821460723877
Epoch 2/9
	 Logging train Loss: 3.18658e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.48182e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.381250858306885
Epoch 3/9
	 Logging train Loss: 2.89516e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.73586e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.395122528076172
Epoch 4/9
	 Logging train Loss: 2.45288e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.27599e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.35318374633789
Epoch 5/9
	 Logging train Loss: 2.06687e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.84428e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.30982279777527
Epoch 6/9
	 Logging train Loss: 1.62211e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22018e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.577218294143677
Epoch 7/9
	 Logging train Loss: 1.29737e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.49221e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.65234136581421
Epoch 8/9
	 Logging train Loss: 1.17424e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.015e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.197747230529785
Epoch 9/9
	 Logging train Loss: 1.11303e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 8.5244e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.53163743019104
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  312.37519001960754  seconds.
----- ITERATION 8/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.1580970287323 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.260647296905518 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0020387073 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.80658e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.327330112457275
Epoch 1/9
	 Logging train Loss: 3.29104e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.66881e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.2076895236969
Epoch 2/9
	 Logging train Loss: 3.02902e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.94839e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.492424488067627
Epoch 3/9
	 Logging train Loss: 2.84214e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.02833e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.162816762924194
Epoch 4/9
	 Logging train Loss: 2.53087e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.86892e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.37414288520813
Epoch 5/9
	 Logging train Loss: 2.13014e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.39374e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 23.984750032424927
Epoch 6/9
	 Logging train Loss: 1.66351e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.52439e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.22664451599121
Epoch 7/9
	 Logging train Loss: 1.33868e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.4555e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.003480911254883
Epoch 8/9
	 Logging train Loss: 1.18927e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.22334e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.00158166885376
Epoch 9/9
	 Logging train Loss: 1.1103e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.3912e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.045356035232544
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  310.96679496765137  seconds.
----- ITERATION 9/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 45.108824729919434 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.240655422210693 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0028069473 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 6.0692e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.51336145401001
Epoch 1/9
	 Logging train Loss: 3.48561e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.25466e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.40554189682007
Epoch 2/9
	 Logging train Loss: 2.82695e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.16304e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.37910485267639
Epoch 3/9
	 Logging train Loss: 2.69532e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.62647e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.10007119178772
Epoch 4/9
	 Logging train Loss: 2.51583e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.37621e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.449491500854492
Epoch 5/9
	 Logging train Loss: 2.18548e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.28194e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.136668920516968
Epoch 6/9
	 Logging train Loss: 1.76506e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.73279e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.07368803024292
Epoch 7/9
	 Logging train Loss: 1.37809e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.08962e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.15011429786682
Epoch 8/9
	 Logging train Loss: 1.22348e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 9.9336e-06 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.21557879447937
Epoch 9/9
	 Logging train Loss: 1.09925e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.33044e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.298545598983765
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  310.0148174762726  seconds.
----- ITERATION 10/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 44.89233064651489 seconds.
-- Finished Train Dataloader --
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                     Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss tennis_pNone_gNone_tennisEffect â–ˆâ–…â–„â–„â–†â–‚â–â–â–â–
wandb:                                Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                     Epoch 9
wandb: Test loss tennis_pNone_gNone_tennisEffect 1e-05
wandb:                                Train loss 1e-05
wandb: 
wandb: ğŸš€ View run dauntless-waterfall-1362 at: https://wandb.ai/nreints/ThesisFinal2/runs/9hsiczdu
wandb: Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230710_174734-9hsiczdu/logs
The dataloader for data/data_tennis_pNone_gNone_tennisEffect took 11.361555576324463 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
Datatype: log_dualQ_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0028894555 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 4.60923e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.108514070510864
Epoch 1/9
	 Logging train Loss: 3.29406e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.00762e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.698182106018066
Epoch 2/9
	 Logging train Loss: 2.82476e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.68696e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.312467098236084
Epoch 3/9
	 Logging train Loss: 2.68109e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 2.55465e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.395626544952393
Epoch 4/9
	 Logging train Loss: 2.51993e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 3.50656e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.442311763763428
Epoch 5/9
	 Logging train Loss: 2.25965e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.78991e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.538904190063477
Epoch 6/9
	 Logging train Loss: 1.79579e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.32997e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.811392068862915
Epoch 7/9
	 Logging train Loss: 1.43114e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.30345e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.34920024871826
Epoch 8/9
	 Logging train Loss: 1.19236e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.27887e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.482014656066895
Epoch 9/9
	 Logging train Loss: 1.10591e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
	 Logging test loss: 1.09907e-05 [MSELoss(): tennis_pNone_gNone_tennisEffect]
		--> Epoch time; 24.287054538726807
Saved model in  trained_models/gru/data_tennis_pNone_gNone_tennisEffect/'log_dualQ_1'_'False'.pth
It took  318.36523938179016  seconds.

JOB STATISTICS
==============
Job ID: 3043754
Array Job ID: 3043750_43
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 15:46:30 core-walltime
Job Wall-clock time: 00:52:35
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
