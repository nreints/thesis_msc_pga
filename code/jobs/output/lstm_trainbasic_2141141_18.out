/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_065046-9d1x3qv5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-bao-1571
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9d1x3qv5
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14943647a8c0>, <torch.utils.data.dataloader.DataLoader object at 0x14941f6e04f0>, <torch.utils.data.dataloader.DataLoader object at 0x14941f6e0310>, <torch.utils.data.dataloader.DataLoader object at 0x14941f6e0640>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15552592277526855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.819186270236969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38888004422187805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1203349828720093
0 3.338602902 	 1.1203349595
epoch_time;  42.39292120933533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13305127620697021
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4444819390773773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25126802921295166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6041598320007324
1 0.2963341158 	 0.6041598594
epoch_time;  42.21031069755554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16394925117492676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34675759077072144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18457867205142975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39908573031425476
2 0.2124718804 	 0.399085745
epoch_time;  40.65325212478638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2343648076057434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34836292266845703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13815005123615265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2839592695236206
3 0.1548440776 	 0.2839592648
epoch_time;  40.74022626876831
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24458147585391998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32629644870758057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11220849305391312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22482502460479736
4 0.1187561174 	 0.2248250264
epoch_time;  39.97888970375061
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24142606556415558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31162044405937195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09811501950025558
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19595034420490265
5 0.10044229 	 0.1959503497
epoch_time;  40.40897512435913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2460525780916214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3122706711292267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09098262339830399
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17925003170967102
6 0.0884926483 	 0.1792500314
epoch_time;  40.28679585456848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34219151735305786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3931887149810791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15746116638183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23421098291873932
7 0.125799914 	 0.2342109795
epoch_time;  40.515469789505005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37385106086730957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.427403062582016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18279370665550232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2622073292732239
8 0.1655082847 	 0.2622073309
epoch_time;  40.10708212852478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3908917307853699
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5443096160888672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17061847448349
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35466650128364563
9 0.1787295923 	 0.354666488
epoch_time;  40.01177716255188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23167891800403595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34805482625961304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04065695405006409
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17215314507484436
10 0.0772780564 	 0.1721531387
epoch_time;  39.94389081001282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15930168330669403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2560395896434784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03225722908973694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1397726833820343
11 0.0341632245 	 0.139772686
epoch_time;  40.63393974304199
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12007100880146027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2186330407857895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02199157327413559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13062310218811035
12 0.028384478 	 0.1306231058
epoch_time;  39.939512968063354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09848307073116302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18496103584766388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02011146768927574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1107189953327179
13 0.022228267 	 0.1107189951
epoch_time;  44.64131808280945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08965861052274704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17439930140972137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020421452820301056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10527452081441879
14 0.0192198266 	 0.1052745231
epoch_time;  42.920870780944824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07837468385696411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2386803776025772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01617485284805298
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18827061355113983
15 0.0327916171 	 0.1882706149
epoch_time;  40.16809415817261
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07060815393924713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19461089372634888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014829042367637157
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15287762880325317
16 0.0160629235 	 0.1528776347
epoch_time;  39.71514010429382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06428202986717224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17063990235328674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01382491271942854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13051313161849976
17 0.01552143 	 0.1305131365
epoch_time;  39.509034633636475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06296621263027191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24121727049350739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015116721391677856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21089038252830505
18 0.0203129669 	 0.2108903868
epoch_time;  39.81071877479553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05839388817548752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16966471076011658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012782905250787735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13057537376880646
19 0.0129499111 	 0.1305753702
epoch_time;  40.08691048622131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05672459304332733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16750013828277588
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012445081025362015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1297326236963272
20 0.013679848 	 0.1297326218
epoch_time;  40.511096239089966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05285526439547539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14553560316562653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012188258580863476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10724704712629318
21 0.0122223752 	 0.1072470443
epoch_time;  39.80380201339722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.050396714359521866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14704501628875732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010773463174700737
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11334588378667831
22 0.0134943154 	 0.1133458809
epoch_time;  40.1798894405365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04927578195929527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13292863965034485
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▂▂▂▂▂▃▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▃▃▃▃▄▄▅▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▃▂▄▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▃▃▅▅▅▅▇██▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.11491
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.14688
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01264
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.04537
wandb:                         Train loss 0.01063
wandb: 
wandb: 🚀 View run crimson-bao-1571 at: https://wandb.ai/nreints/thesis/runs/9d1x3qv5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_065046-9d1x3qv5/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_071213-z4nw07t4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-fuse-1578
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/z4nw07t4
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010746902786195278
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09655895829200745
23 0.0114642503 	 0.0965589598
epoch_time;  39.83410668373108
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.046820443123579025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14493919909000397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010801596567034721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11420460045337677
24 0.0121988806 	 0.1142046027
epoch_time;  39.91591954231262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04786032810807228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1538846641778946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012073437683284283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12461863458156586
25 0.0139766335 	 0.124618634
epoch_time;  40.06319737434387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045151397585868835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1540500670671463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010794124566018581
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12014546990394592
26 0.0105811609 	 0.1201454693
epoch_time;  39.99363327026367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04683170095086098
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16048166155815125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01231922022998333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12330920249223709
27 0.0119990104 	 0.1233092017
epoch_time;  40.24044394493103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04257791116833687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15343482792377472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010319896973669529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11883483827114105
28 0.0101105721 	 0.1188348384
epoch_time;  40.45653700828552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04523668438196182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14690615236759186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012648925185203552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11350231617689133
29 0.0106340765 	 0.113502318
epoch_time;  39.95884084701538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0453699491918087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14688386023044586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012639384716749191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11490905284881592
It took  1287.5425653457642  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149436443f70>, <torch.utils.data.dataloader.DataLoader object at 0x14940813e3e0>, <torch.utils.data.dataloader.DataLoader object at 0x149408150070>, <torch.utils.data.dataloader.DataLoader object at 0x149430153910>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9115949273109436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2878354787826538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2533002197742462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9166271090507507
0 2.6884024517 	 0.9166270829
epoch_time;  40.28328013420105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6214607357978821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8032760620117188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13225919008255005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4493255615234375
1 0.1639368835 	 0.4493255615
epoch_time;  40.756420850753784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4647100269794464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5767821073532104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09742908179759979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29476237297058105
2 0.1067910436 	 0.2947623723
epoch_time;  39.75295686721802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3747485876083374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46246710419654846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07888303697109222
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22414207458496094
3 0.0814000721 	 0.2241420688
epoch_time;  39.849971532821655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30328619480133057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37889209389686584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06454198062419891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18451036512851715
4 0.0671182614 	 0.1845103722
epoch_time;  43.40557289123535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26096054911613464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3339642882347107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05731044337153435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16670824587345123
5 0.0576071382 	 0.1667082519
epoch_time;  41.22230839729309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22442276775836945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28711965680122375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04732209071516991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14060170948505402
6 0.0513004127 	 0.1406017084
epoch_time;  40.04921197891235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20296689867973328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2657259702682495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04461010918021202
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13739794492721558
7 0.0456558822 	 0.1373979505
epoch_time;  39.19995617866516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18016928434371948
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.232099249958992
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.040822844952344894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12338621914386749
8 0.0401926322 	 0.1233862217
epoch_time;  40.26459884643555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1640489250421524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21484999358654022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.040633756667375565
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11905483156442642
9 0.0360649045 	 0.1190548346
epoch_time;  39.94009447097778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14772860705852509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19355663657188416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03745576739311218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11468231678009033
10 0.0328132837 	 0.1146823157
epoch_time;  40.84312391281128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1341066062450409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18009096384048462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.033986303955316544
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1111140176653862
11 0.0304212346 	 0.1111140179
epoch_time;  40.11784291267395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12258873134851456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16701377928256989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03154231607913971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11049165576696396
12 0.028369482 	 0.1104916575
epoch_time;  40.087003231048584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10811173915863037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14819470047950745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026799092069268227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10104112327098846
13 0.0265498656 	 0.1010411196
epoch_time;  39.59314274787903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10004997998476028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13821525871753693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026438932865858078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09938965737819672
14 0.0251043177 	 0.0993896554
epoch_time;  41.0181450843811
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09196720272302628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13072536885738373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025822140276432037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10255949944257736
15 0.0238880877 	 0.1025594959
epoch_time;  39.88828110694885
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.08791
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08528
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01647
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.04516
wandb:                         Train loss 0.01521
wandb: 
wandb: 🚀 View run vivid-fuse-1578 at: https://wandb.ai/nreints/thesis/runs/z4nw07t4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_071213-z4nw07t4/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_073342-1iwuhrn7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-fuse-1586
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1iwuhrn7
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08524787425994873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12463536858558655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024371841922402382
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10416220873594284
16 0.0230460575 	 0.1041622104
epoch_time;  41.2742817401886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08299579471349716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12093380093574524
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02858038991689682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10315388441085815
17 0.0216979504 	 0.1031538857
epoch_time;  40.903000593185425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07683892548084259
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11652683466672897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024250837042927742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10000421106815338
18 0.0208672199 	 0.1000042135
epoch_time;  40.39612030982971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06803062558174133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10454247146844864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019681330770254135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08909361809492111
19 0.0200624112 	 0.0890936146
epoch_time;  40.839783906936646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06441430002450943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09949854016304016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019733639433979988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08881572633981705
20 0.0193029096 	 0.0888157237
epoch_time;  41.00108361244202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.061820436269044876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1000242531299591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020405594259500504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09100154787302017
21 0.0187961902 	 0.091001551
epoch_time;  40.97881817817688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05873851105570793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09744736552238464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019441692158579826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09065555781126022
22 0.0181615845 	 0.0906555545
epoch_time;  41.21546030044556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06081979349255562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11126641929149628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023578708991408348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10945779085159302
23 0.0177013706 	 0.1094577939
epoch_time;  41.15131425857544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05387547239661217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10051773488521576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018948256969451904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09639071673154831
24 0.0171112202 	 0.0963907155
epoch_time;  41.07055616378784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.052249740809202194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0956805944442749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016613703221082687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09146386384963989
25 0.0166999592 	 0.0914638669
epoch_time;  41.50425624847412
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04922468587756157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09139049053192139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017846444621682167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08858007937669754
26 0.0161668473 	 0.0885800768
epoch_time;  42.281320333480835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05639607086777687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10300152748823166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022698979824781418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0981784462928772
27 0.0159424819 	 0.0981784486
epoch_time;  42.63630151748657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0455823577940464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0893564522266388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016153039410710335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09111664444208145
28 0.015337513 	 0.091116643
epoch_time;  41.208117961883545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045174118131399155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08491255342960358
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016475901007652283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08823204785585403
29 0.015208245 	 0.0882320462
epoch_time;  41.04111075401306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045162491500377655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08528419584035873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01647072471678257
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08790963143110275
It took  1288.632167339325  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14940813e3b0>, <torch.utils.data.dataloader.DataLoader object at 0x1494081503d0>, <torch.utils.data.dataloader.DataLoader object at 0x14943011ac80>, <torch.utils.data.dataloader.DataLoader object at 0x14943011ae00>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9564822912216187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.269411325454712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22565653920173645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.72379469871521
0 2.7521466263 	 0.7237947124
epoch_time;  40.899046421051025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6457526087760925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7870846390724182
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12574075162410736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3545728623867035
1 0.1624785892 	 0.3545728609
epoch_time;  40.451634645462036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49530309438705444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5950481295585632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09568838775157928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2422715425491333
2 0.1072147205 	 0.2422715386
epoch_time;  40.31001424789429
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40289729833602905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4771662950515747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0808444619178772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1913471221923828
3 0.0833680863 	 0.191347128
epoch_time;  40.33788537979126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3497138023376465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4250850975513458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06757359206676483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16644293069839478
4 0.0715224954 	 0.1664429287
epoch_time;  39.43141150474548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29794633388519287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35842931270599365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.059132784605026245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13738632202148438
5 0.0586193584 	 0.137386322
epoch_time;  40.58896517753601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26389655470848083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31785428524017334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05042261630296707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11918844282627106
6 0.0511495055 	 0.1191884413
epoch_time;  40.32986354827881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23973290622234344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2838590741157532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04455188289284706
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10570073127746582
7 0.0453438221 	 0.105700732
epoch_time;  40.466190576553345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21679627895355225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2553211450576782
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.03632
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.09411
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01618
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.07396
wandb:                         Train loss 0.01534
wandb: 
wandb: 🚀 View run fortuitous-fuse-1586 at: https://wandb.ai/nreints/thesis/runs/1iwuhrn7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_073342-1iwuhrn7/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_075505-4mu77500
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-pig-1592
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/4mu77500
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.042532119899988174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09577718377113342
8 0.0408819662 	 0.0957771831
epoch_time;  40.90597677230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20282606780529022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2372860461473465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04044514521956444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09037059545516968
9 0.0369705125 	 0.0903705989
epoch_time;  40.59979009628296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18620891869068146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21844032406806946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.035625770688056946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08010225743055344
10 0.034121517 	 0.0801022579
epoch_time;  41.53878688812256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1742899864912033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2058631181716919
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0338498130440712
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07602889090776443
11 0.0314633689 	 0.0760288872
epoch_time;  40.37194752693176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1575687974691391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1847870796918869
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.030289076268672943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06699653714895248
12 0.0294046337 	 0.0669965369
epoch_time;  41.018370628356934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15126512944698334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18054457008838654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03604712337255478
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07337497174739838
13 0.0278545354 	 0.0733749701
epoch_time;  40.626320600509644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1369108259677887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16376544535160065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02744625322520733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06109187379479408
14 0.0258248796 	 0.0610918754
epoch_time;  42.44587063789368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12833045423030853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15327651798725128
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025467904284596443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05726725608110428
15 0.0244877084 	 0.0572672553
epoch_time;  40.6436665058136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12318672984838486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14591941237449646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024393871426582336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.054292913526296616
16 0.0232691104 	 0.0542929151
epoch_time;  41.68443489074707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11422601342201233
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13768896460533142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023044567555189133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05187226086854935
17 0.0220546708 	 0.0518722592
epoch_time;  41.07728123664856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10778523981571198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12996000051498413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023081321269273758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05078602582216263
18 0.0212659265 	 0.050786027
epoch_time;  41.45633101463318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10437241196632385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12684516608715057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0220058411359787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04824615642428398
19 0.02024489 	 0.0482461561
epoch_time;  40.332632064819336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0990593433380127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12187223881483078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019573591649532318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04574992507696152
20 0.0197058583 	 0.0457499265
epoch_time;  40.59366536140442
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09459389001131058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11638066172599792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019197270274162292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04315721243619919
21 0.0189461185 	 0.0431572116
epoch_time;  40.991514444351196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09143564105033875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11237253248691559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01881181262433529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04312460124492645
22 0.0183213484 	 0.0431245994
epoch_time;  40.514259815216064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08927999436855316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1117803305387497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01838715560734272
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04341832920908928
23 0.0178144326 	 0.0434183282
epoch_time;  40.43526220321655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08793887495994568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11031727492809296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022435171529650688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04690274968743324
24 0.0172225785 	 0.0469027488
epoch_time;  40.422632694244385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08289581537246704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10448139160871506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018803410232067108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.041261255741119385
25 0.0168253966 	 0.0412612552
epoch_time;  40.391358375549316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07958489656448364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10068528354167938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017305387184023857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03932908922433853
26 0.0164556092 	 0.0393290909
epoch_time;  40.171722650527954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07755988091230392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09653604030609131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016107600182294846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03662945702672005
27 0.0159689884 	 0.0366294578
epoch_time;  40.801626682281494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07768967747688293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09726350009441376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017671648412942886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03744177520275116
28 0.0156678352 	 0.0374417752
epoch_time;  40.23850083351135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07395745068788528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09421364963054657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016184097155928612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03630775213241577
29 0.0153448204 	 0.0363077527
epoch_time;  40.46566414833069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07396282255649567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09410969167947769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01618373952805996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.036316897720098495
It took  1283.4881393909454  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149430118fd0>, <torch.utils.data.dataloader.DataLoader object at 0x1494080f92d0>, <torch.utils.data.dataloader.DataLoader object at 0x1494080facb0>, <torch.utils.data.dataloader.DataLoader object at 0x1494080fae60>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9375742077827454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3249436616897583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22876743972301483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8818310499191284
0 2.6736671511 	 0.881831051
epoch_time;  41.53343462944031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6405279636383057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8033057451248169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13352002203464508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45342105627059937
1 0.166103054 	 0.4534210482
epoch_time;  41.68242859840393
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4949541389942169
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5876268148422241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09108293056488037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2948026955127716
2 0.1092324614 	 0.2948026859
epoch_time;  40.97247385978699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4090825021266937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48183539509773254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0752333477139473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23674479126930237
3 0.0860313802 	 0.2367447914
epoch_time;  41.25574851036072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3556342124938965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4200558364391327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07420540601015091
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20921745896339417
4 0.0725291019 	 0.2092174519
epoch_time;  41.16954851150513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3080809414386749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3720353841781616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05731938034296036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1798190474510193
5 0.0628266952 	 0.1798190437
epoch_time;  41.92816996574402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26852381229400635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33768412470817566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05049026012420654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16721582412719727
6 0.0553617514 	 0.1672158256
epoch_time;  41.868236780166626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23772215843200684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3030589818954468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04559972882270813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15132500231266022
7 0.0492031609 	 0.1513249953
epoch_time;  41.79705882072449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2195795774459839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2786274254322052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04104246199131012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13893388211727142
8 0.0441361109 	 0.138933879
epoch_time;  42.147265911102295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20198333263397217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26178085803985596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04148302227258682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14030255377292633
9 0.0400027348 	 0.1403025601
epoch_time;  40.89133596420288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18322326242923737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24015265703201294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03512735664844513
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13456331193447113
10 0.0373922632 	 0.1345633193
epoch_time;  41.531108379364014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16342328488826752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20761442184448242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03305715695023537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11543179303407669
11 0.0332427216 	 0.1154317942
epoch_time;  41.40393042564392
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14975979924201965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19202880561351776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029831307008862495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10802623629570007
12 0.030717008 	 0.1080262337
epoch_time;  40.959723711013794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13889233767986298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17645467817783356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028584342449903488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10005040466785431
13 0.0287818938 	 0.1000504047
epoch_time;  41.50169086456299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12998375296592712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1639987677335739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.027357663959264755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0953647792339325
14 0.0270115855 	 0.0953647809
epoch_time;  41.19514274597168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12864950299263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1628231555223465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03142044320702553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09834344685077667
15 0.026753783 	 0.0983434487
epoch_time;  39.82573056221008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11464901268482208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14707867801189423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02423200011253357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09014415740966797
16 0.0251790869 	 0.0901441603
epoch_time;  39.65547275543213
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11071328818798065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14146217703819275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025539681315422058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08678466081619263
17 0.0233291407 	 0.0867846624
epoch_time;  40.731104135513306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1044592410326004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13704922795295715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02503257617354393
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08612283319234848
18 0.022448321 	 0.0861228355
epoch_time;  40.788824796676636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10319265723228455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13234451413154602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02910437062382698
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08653847873210907
19 0.0215793945 	 0.0865384819
epoch_time;  41.2966992855072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09176742285490036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12079213559627533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023284541442990303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08128488808870316
20 0.0209487739 	 0.0812848889
epoch_time;  40.54535365104675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07797734439373016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10385365784168243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0189519040286541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06995667517185211
21 0.0202451297 	 0.0699566786
epoch_time;  40.961069107055664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07601802051067352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.101911760866642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02492021955549717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07638105750083923
22 0.0195219977 	 0.076381061
epoch_time;  41.28528165817261
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06655920296907425
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09183524549007416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024676501750946045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07307761907577515
23 0.0187703652 	 0.0730776196
epoch_time;  41.214503049850464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06005166098475456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08344350755214691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017921848222613335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06370124965906143
24 0.0182913709 	 0.0637012493
epoch_time;  41.14713668823242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05537300556898117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0800480917096138
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01761365868151188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06439154595136642
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.05995
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.07182
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01842
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.05051
wandb:                         Train loss 0.01643
wandb: 
wandb: 🚀 View run vibrant-pig-1592 at: https://wandb.ai/nreints/thesis/runs/4mu77500
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_075505-4mu77500/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_081643-brmfnnwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-rabbit-1601
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/brmfnnwl
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
25 0.0178388369 	 0.0643915482
epoch_time;  41.48605036735535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0534808412194252
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0784541442990303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01760687120258808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.061196036636829376
26 0.0173139825 	 0.0611960362
epoch_time;  41.16582632064819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05277347192168236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07724200189113617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01800359971821308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06266125291585922
27 0.0169716878 	 0.0626612545
epoch_time;  40.73073673248291
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.050084296613931656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07361704856157303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01609838753938675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.058462269604206085
28 0.0164899833 	 0.0584622697
epoch_time;  40.79342174530029
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05050373822450638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0717945322394371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018438519909977913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06001705676317215
29 0.0164342024 	 0.0600170585
epoch_time;  42.11446404457092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05051184073090553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07181840389966965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018419230356812477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.059950847178697586
It took  1298.1892063617706  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149430119870>, <torch.utils.data.dataloader.DataLoader object at 0x14940813f1f0>, <torch.utils.data.dataloader.DataLoader object at 0x14941f6e3130>, <torch.utils.data.dataloader.DataLoader object at 0x14941f6e34c0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2278845310211182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7402002811431885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3964807689189911
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1723281145095825
0 3.3922495239 	 1.1723280616
epoch_time;  40.31429147720337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7584787607192993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9859276413917542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17005504667758942
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5558474063873291
1 0.2427061833 	 0.5558473985
epoch_time;  40.443819522857666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5763246417045593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7000551223754883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1251971423625946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3517642617225647
2 0.13204196 	 0.3517642525
epoch_time;  40.14863157272339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45793798565864563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5436736345291138
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09206557273864746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2533194422721863
3 0.0966151986 	 0.253319432
epoch_time;  40.573718786239624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37828686833381653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4487060308456421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07517387717962265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20466335117816925
4 0.0758871675 	 0.2046633487
epoch_time;  40.13708519935608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3136914074420929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3786698579788208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06814011931419373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17739051580429077
5 0.0626133725 	 0.1773905221
epoch_time;  41.2704381942749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2658199667930603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3289741575717926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05149310082197189
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1492333561182022
6 0.0533011044 	 0.1492333542
epoch_time;  40.920127153396606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22781303524971008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28970715403556824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04916609451174736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13529732823371887
7 0.0458425598 	 0.1352973316
epoch_time;  40.38424825668335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1906438171863556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25568434596061707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04322739318013191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12441664189100266
8 0.0400725747 	 0.1244166394
epoch_time;  41.15598130226135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16100691258907318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22570569813251495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03439214825630188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11251289397478104
9 0.0352668964 	 0.1125128939
epoch_time;  40.500611543655396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1359347403049469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2010030299425125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.031059598550200462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1069813147187233
10 0.031725168 	 0.1069813178
epoch_time;  40.80585217475891
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.120206318795681
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17905405163764954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032064035534858704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10280057042837143
11 0.0286827665 	 0.1028005709
epoch_time;  40.722858905792236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10862711071968079
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18442384898662567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03496020287275314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11945583671331406
12 0.0278581739 	 0.1194558389
epoch_time;  40.917885065078735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09594516456127167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1549959033727646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02952870912849903
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09851392358541489
13 0.0245583799 	 0.098513923
epoch_time;  40.02958154678345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08270363509654999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13497209548950195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023353571072220802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08666354417800903
14 0.0233568153 	 0.0866635429
epoch_time;  41.07759356498718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07365746796131134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12559381127357483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021419376134872437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08463630825281143
15 0.0220892644 	 0.0846363079
epoch_time;  42.02165484428406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06875631213188171
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11836978048086166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020610500127077103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08157924562692642
16 0.0212871803 	 0.0815792487
epoch_time;  39.98051309585571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0704469084739685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11895480751991272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02809181436896324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09064195305109024
17 0.0200441463 	 0.0906419495
epoch_time;  40.392335653305054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05900269374251366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10105213522911072
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: | 0.143 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: / 0.143 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.05858
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.06919
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01471
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.03882
wandb:                         Train loss 0.01389
wandb: 
wandb: 🚀 View run golden-rabbit-1601 at: https://wandb.ai/nreints/thesis/runs/brmfnnwl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_081643-brmfnnwl/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_083803-g61ergib
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run abundant-laughter-1608
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g61ergib
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018554817885160446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07456784695386887
18 0.0191030769 	 0.0745678467
epoch_time;  40.42604064941406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0586593896150589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1014227494597435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020443398505449295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07622851431369781
19 0.0185083307 	 0.0762285135
epoch_time;  41.08059024810791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.054192084819078445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09338884055614471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01747702807188034
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07019316405057907
20 0.0179059103 	 0.0701931668
epoch_time;  41.74907636642456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05215252563357353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09053582698106766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018659302964806557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07121753692626953
21 0.0172506068 	 0.0712175398
epoch_time;  40.922935962677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05906830355525017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10277199000120163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02146422304213047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0745326578617096
22 0.0168386614 	 0.0745326616
epoch_time;  40.55272126197815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04795433580875397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08262813836336136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016811558976769447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06490128487348557
23 0.0163204444 	 0.0649012828
epoch_time;  40.04721450805664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04645690321922302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08128827810287476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01649438962340355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06378443539142609
24 0.0157586978 	 0.0637844351
epoch_time;  40.325382471084595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.043698351830244064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07742811739444733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015369652770459652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06175132095813751
25 0.0154018633 	 0.0617513224
epoch_time;  40.32623505592346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04318879917263985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07609730213880539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015905454754829407
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06208142638206482
26 0.015123028 	 0.0620814263
epoch_time;  40.35963749885559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.041692525148391724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07524560391902924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015798408538103104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06166037544608116
27 0.0144694442 	 0.0616603748
epoch_time;  40.813904762268066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0409623384475708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07611517608165741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01554164756089449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06494095176458359
28 0.0142761493 	 0.064940951
epoch_time;  40.571171045303345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03884647786617279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06930098682641983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014709493145346642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.058512620627880096
29 0.013894785 	 0.0585126214
epoch_time;  40.23301720619202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03881806507706642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0691903606057167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014714409597218037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.058579642325639725
It took  1279.488775253296  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14941f6e3490>, <torch.utils.data.dataloader.DataLoader object at 0x1494080f8d00>, <torch.utils.data.dataloader.DataLoader object at 0x1494080fb940>, <torch.utils.data.dataloader.DataLoader object at 0x1494080f9d20>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9911986589431763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2959747314453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24065163731575012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7673682570457458
0 2.6355656892 	 0.767368236
epoch_time;  41.18368411064148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6334042549133301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7754990458488464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1260838508605957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3687862157821655
1 0.1718248343 	 0.3687862154
epoch_time;  40.896811723709106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4793512523174286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5718435645103455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08928948640823364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24452263116836548
2 0.1102026132 	 0.2445226249
epoch_time;  41.53232264518738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39623796939849854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4877573847770691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07430470734834671
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20144687592983246
3 0.0868759791 	 0.2014468789
epoch_time;  41.111217737197876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3280891180038452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40178728103637695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06134026125073433
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16503174602985382
4 0.0694352987 	 0.1650317443
epoch_time;  41.0602707862854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2859826683998108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35018390417099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.052684150636196136
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14041881263256073
5 0.0606336052 	 0.1404188104
epoch_time;  40.60862755775452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2506657540798187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3119271695613861
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.048385292291641235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12916333973407745
6 0.0530420111 	 0.1291633445
epoch_time;  40.92046928405762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22678083181381226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2829396426677704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04459743574261665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11351834982633591
7 0.0485137951 	 0.113518349
epoch_time;  40.693164110183716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21332867443561554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26470813155174255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04635428264737129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10935797542333603
8 0.0426768058 	 0.1093579779
epoch_time;  40.39541554450989
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1931437999010086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23941296339035034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04213704541325569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09970857948064804
9 0.0392218423 	 0.0997085802
epoch_time;  41.59031534194946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1757308691740036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2167811542749405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03762837126851082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08912786841392517
10 0.0360892308 	 0.0891278662
epoch_time;  40.941245317459106
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.04299
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08806
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01567
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.06137
wandb:                         Train loss 0.01557
wandb: 
wandb: 🚀 View run abundant-laughter-1608 at: https://wandb.ai/nreints/thesis/runs/g61ergib
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_083803-g61ergib/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_085930-655ftrh1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-snake-1615
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/655ftrh1
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1591249406337738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19955900311470032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03230207413434982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0802455022931099
11 0.0343287783 	 0.0802454992
epoch_time;  41.4371280670166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14454759657382965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18068470060825348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029243500903248787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0730898380279541
12 0.0312358876 	 0.0730898359
epoch_time;  39.817853927612305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13445371389389038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1690751016139984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026150640100240707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06652219593524933
13 0.0301763663 	 0.0665221949
epoch_time;  39.99620461463928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12344531714916229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15785840153694153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02578864060342312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06491634994745255
14 0.0274779113 	 0.0649163514
epoch_time;  39.72887325286865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11443444341421127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14758019149303436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02407720312476158
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06146077811717987
15 0.0259766763 	 0.0614607774
epoch_time;  40.473612546920776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1134650707244873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1466057300567627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03140079975128174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06744784861803055
16 0.0244699055 	 0.0674478467
epoch_time;  42.33350491523743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10169752687215805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1327974498271942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022189144045114517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05606045201420784
17 0.0234330442 	 0.056060451
epoch_time;  39.4825177192688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09651894867420197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12635792791843414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022980470210313797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05613067001104355
18 0.0224521386 	 0.0561306714
epoch_time;  40.50863242149353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08964631706476212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11936388164758682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019824450835585594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0518278069794178
19 0.0213930836 	 0.0518278082
epoch_time;  40.7655234336853
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08482097834348679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11375751346349716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019747985526919365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.050639357417821884
20 0.0203710711 	 0.0506393571
epoch_time;  41.35677719116211
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08042106032371521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10852210968732834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01827799715101719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04915381968021393
21 0.019721764 	 0.0491538206
epoch_time;  41.34837889671326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07809604704380035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10808994621038437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01809348165988922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.050233758985996246
22 0.0190119039 	 0.0502337603
epoch_time;  40.838643312454224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07519359141588211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10381521284580231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01796913333237171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04722416400909424
23 0.0182495132 	 0.0472241629
epoch_time;  40.82900643348694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07276513427495956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10148372501134872
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0180536936968565
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04832056164741516
24 0.0177222533 	 0.0483205599
epoch_time;  40.912453174591064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07357064634561539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10374834388494492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02216428890824318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05279552564024925
25 0.017296093 	 0.0527955254
epoch_time;  40.85016131401062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06635304540395737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09486153721809387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0155548807233572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04374965280294418
26 0.0169653376 	 0.0437496537
epoch_time;  41.435590744018555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06380186229944229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09156997501850128
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014791551046073437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04295014962553978
27 0.0162070445 	 0.042950149
epoch_time;  40.49027395248413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06778277456760406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09518581628799438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01745624653995037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04545062407851219
28 0.0159291246 	 0.0454506255
epoch_time;  40.97601008415222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06134103611111641
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08817538619041443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015623010694980621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04232769459486008
29 0.015565081 	 0.0423276936
epoch_time;  41.21532917022705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.061371225863695145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0880555585026741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015666192397475243
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04299060255289078
It took  1286.5903580188751  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1494080f9780>, <torch.utils.data.dataloader.DataLoader object at 0x149430153a30>, <torch.utils.data.dataloader.DataLoader object at 0x14940813d8a0>, <torch.utils.data.dataloader.DataLoader object at 0x14940813de40>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9263907670974731
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.240356683731079
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2561756670475006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8328979015350342
0 2.7280877783 	 0.8328979123
epoch_time;  42.142972469329834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6380628347396851
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.754284143447876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13944144546985626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3997953534126282
1 0.1726744866 	 0.3997953478
epoch_time;  41.3376088142395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4940846264362335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5605482459068298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11542021483182907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.281004399061203
2 0.1162071257 	 0.2810043842
epoch_time;  41.15819764137268
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39439550042152405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.439664751291275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07958173006772995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2011234015226364
3 0.0920849843 	 0.2011234018
epoch_time;  41.02002787590027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3318033218383789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3711908161640167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06917060911655426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16896679997444153
4 0.0729493481 	 0.1689667947
epoch_time;  41.79354786872864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2857816517353058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3213368356227875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06283387541770935
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1464647650718689
5 0.0627378569 	 0.1464647598
epoch_time;  41.136685609817505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25999224185943604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30383315682411194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06088962033390999
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14476919174194336
6 0.0551655298 	 0.1447691903
epoch_time;  40.93270707130432
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22702793776988983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2604748010635376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04803749918937683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11818140745162964
7 0.0490115077 	 0.1181814073
epoch_time;  41.06657576560974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2068892866373062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23289965093135834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0424332320690155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1076182946562767
8 0.0446579476 	 0.1076182916
epoch_time;  40.75159215927124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1912374198436737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21790118515491486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03801305219531059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10136283189058304
9 0.0411398322 	 0.1013628334
epoch_time;  40.84466028213501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17525778710842133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20101335644721985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0370916947722435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09799464046955109
10 0.0374301705 	 0.0979946402
epoch_time;  41.4050772190094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1588505208492279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1878816783428192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.031465765088796616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0960247591137886
11 0.0360843274 	 0.0960247581
epoch_time;  40.95845437049866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15737615525722504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.184885635972023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03901872783899307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10049150139093399
12 0.0318941814 	 0.1004915036
epoch_time;  41.043848752975464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13734504580497742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1613597571849823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029544709250330925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08695529401302338
13 0.0300493406 	 0.0869552923
epoch_time;  40.73272967338562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12809960544109344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15232394635677338
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02769709937274456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0829901248216629
14 0.0283016289 	 0.0829901277
epoch_time;  40.84088754653931
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11998455971479416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14220714569091797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02454829216003418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07804761081933975
15 0.0268267618 	 0.0780476112
epoch_time;  42.03179478645325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11308235675096512
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13217443227767944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02426370419561863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07302902638912201
16 0.0250722552 	 0.0730290254
epoch_time;  41.70508360862732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10684806853532791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12669941782951355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022861197590827942
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07152619957923889
17 0.0242059767 	 0.0715262018
epoch_time;  40.068453550338745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10087857395410538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11909450590610504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021776624023914337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06922738254070282
18 0.0230026115 	 0.0692273857
epoch_time;  39.5497887134552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09450805932283401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11419348418712616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020910870283842087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06804835051298141
19 0.0219512841 	 0.0680483504
epoch_time;  40.26903295516968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09071771055459976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11000155657529831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020470425486564636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06827710568904877
20 0.0210492425 	 0.0682771055
epoch_time;  40.54558873176575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08744983375072479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10546766966581345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018815813586115837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0659530982375145
21 0.0205241643 	 0.0659530962
epoch_time;  39.48125720024109
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08792336285114288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10781436413526535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020616451278328896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07006418704986572
22 0.019542593 	 0.0700641874
epoch_time;  40.949766874313354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08035488426685333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09913761913776398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01829148642718792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06923562288284302
23 0.0191185119 	 0.0692356259
epoch_time;  41.17010712623596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0802643746137619
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09833744168281555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01902291551232338
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06387700885534286
24 0.0184167751 	 0.0638770078
epoch_time;  41.69702672958374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07514414936304092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09197884052991867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01670040562748909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06083250045776367
25 0.0180835334 	 0.060832499
epoch_time;  40.152748823165894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07373806089162827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09102334082126617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0173515472561121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06443995982408524
26 0.0175034511 	 0.064439958
epoch_time;  39.82846117019653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07022784650325775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08611438423395157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015851011499762535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.060699887573719025
27 0.017087086 	 0.0606998893
epoch_time;  41.00768709182739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07041902840137482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08893176168203354
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.06752
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08672
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.02021
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.06956
wandb:                         Train loss 0.01616
wandb: 
wandb: 🚀 View run legendary-snake-1615 at: https://wandb.ai/nreints/thesis/runs/655ftrh1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_085930-655ftrh1/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_092110-inwgtgul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-horse-1622
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/inwgtgul
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016882294788956642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06419458985328674
28 0.0169135923 	 0.0641945899
epoch_time;  40.46097707748413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06944230198860168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08680962771177292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020188113674521446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06748766452074051
29 0.0161592872 	 0.0674876648
epoch_time;  40.124003887176514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06956317275762558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08671575784683228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020214436575770378
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06752120703458786
It took  1300.4247093200684  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1494379f1480>, <torch.utils.data.dataloader.DataLoader object at 0x14943011a6b0>, <torch.utils.data.dataloader.DataLoader object at 0x14943011ae90>, <torch.utils.data.dataloader.DataLoader object at 0x14943011a5c0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10047663748264313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7757241725921631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25633564591407776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0640326738357544
0 3.0569156297 	 1.0640327016
epoch_time;  41.03150272369385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018522705882787704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3331024646759033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028528444468975067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40831458568573
1 0.1089846129 	 0.4083145925
epoch_time;  40.329813718795776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006959260907024145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20474125444889069
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01238672249019146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23974579572677612
2 0.0271938997 	 0.2397457941
epoch_time;  40.619683265686035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023609977215528488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16615615785121918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022489871829748154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18696936964988708
3 0.0170873791 	 0.1869693768
epoch_time;  41.00254559516907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026158438995480537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11364129185676575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005443070083856583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1337531954050064
4 0.01474037 	 0.1337531974
epoch_time;  41.15958571434021
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007607426960021257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08561939001083374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008383015170693398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09787049889564514
5 0.008124184 	 0.0978704954
epoch_time;  41.26998496055603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019056921591982245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05607409030199051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0034312857314944267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06933898478746414
6 0.0075178031 	 0.0693389858
epoch_time;  40.74855589866638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004154212307184935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18299470841884613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007317482028156519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22034341096878052
7 0.0330265602 	 0.220343414
epoch_time;  40.876168727874756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016600516391918063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09319638460874557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035510689485818148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1153448149561882
8 0.0060627516 	 0.1153448168
epoch_time;  40.700528621673584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015925596235319972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06241382285952568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0030806749127805233
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08194677531719208
9 0.004946312 	 0.0819467735
epoch_time;  40.73007345199585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012293619802221656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.046944018453359604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0023282631300389767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0629388689994812
10 0.004368585 	 0.0629388688
epoch_time;  40.98806166648865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003491678275167942
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04275406524538994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004244546871632338
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05707433074712753
11 0.0037667836 	 0.0570743307
epoch_time;  40.743674516677856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011145822703838348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.048569388687610626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00960230641067028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05954313278198242
12 0.0036814034 	 0.0595431313
epoch_time;  40.88485884666443
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5627885460853577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.272305965423584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4713800847530365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2843639850616455
13 0.0074218675 	 1.2843639915
epoch_time;  41.94914937019348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018430323107168078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07877783477306366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002766338177025318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09713220596313477
14 0.0077042821 	 0.0971322074
epoch_time;  41.66422200202942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001133791054598987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06037910282611847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0019517508335411549
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07581992447376251
15 0.0026376959 	 0.075819926
epoch_time;  41.44406867027283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013436410808935761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0482255220413208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0021379434037953615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06330160796642303
16 0.0028592311 	 0.0633016108
epoch_time;  41.643985986709595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010805099736899137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.039522845298051834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0018001823918893933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05172277241945267
17 0.0025864692 	 0.0517227715
epoch_time;  39.94484567642212
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013911721762269735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03600654751062393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0020054844208061695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04804651439189911
18 0.0025810351 	 0.0480465125
epoch_time;  40.19791269302368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006411029025912285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05198414623737335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.001532432739622891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06959380209445953
19 0.0053016625 	 0.0695938041
epoch_time;  40.294922828674316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009330830653198063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03388151526451111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0016748871421441436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0458373948931694
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▃▂▂▂▁▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▃▂▂▂▁▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.02939
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.02253
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00233
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00205
wandb:                         Train loss 0.00194
wandb: 
wandb: 🚀 View run resplendent-horse-1622 at: https://wandb.ai/nreints/thesis/runs/inwgtgul
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_092110-inwgtgul/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_094235-l5ih6w9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-mandu-1630
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/l5ih6w9w
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
20 0.0018413567 	 0.0458373966
epoch_time;  40.464518785476685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015159634640440345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029867561534047127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0020792121067643166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.039581991732120514
21 0.0022017306 	 0.0395819903
epoch_time;  40.08030915260315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018688809359446168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.028910938650369644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0023679074365645647
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.038293998688459396
22 0.0022841908 	 0.0382939998
epoch_time;  40.234135150909424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001464757020585239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02719929814338684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0021405399311333895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.03640071302652359
23 0.0021019579 	 0.0364007143
epoch_time;  40.26454544067383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010293252998962998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.024592841044068336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.001626024255529046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0330972857773304
24 0.0021431574 	 0.0330972873
epoch_time;  39.62540912628174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009110081009566784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04255910962820053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0017993311630561948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.057373154908418655
25 0.0086723821 	 0.0573731564
epoch_time;  40.696447134017944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004067606292665005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03406970202922821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003990991041064262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.04190860688686371
26 0.0018125436 	 0.041908607
epoch_time;  40.64949870109558
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010566299315541983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.024256959557533264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0016550435684621334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.031305596232414246
27 0.0019860796 	 0.0313055955
epoch_time;  39.65983986854553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017655850388109684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.022552743554115295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0021625086665153503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.028844576328992844
28 0.0019582486 	 0.0288445769
epoch_time;  40.44566297531128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020468730945140123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02254422754049301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0023254542611539364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.029333969578146935
29 0.0019371253 	 0.0293339703
epoch_time;  39.95646381378174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002046378795057535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02252831868827343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002326970687136054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0293933916836977
It took  1284.6373009681702  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1494363e9060>, <torch.utils.data.dataloader.DataLoader object at 0x1494081517e0>, <torch.utils.data.dataloader.DataLoader object at 0x149430118640>, <torch.utils.data.dataloader.DataLoader object at 0x149430118af0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9024447798728943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2267056703567505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22055622935295105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.767594575881958
0 2.5421941903 	 0.7675945824
epoch_time;  41.353371143341064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.602534294128418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7513589262962341
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11600713431835175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3787088394165039
1 0.1566155294 	 0.3787088538
epoch_time;  41.4966504573822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45173242688179016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5504308938980103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08507724851369858
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2621210515499115
2 0.101153874 	 0.2621210565
epoch_time;  40.6450080871582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3801797330379486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4603503942489624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07529126852750778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21313245594501495
3 0.0793867378 	 0.2131324607
epoch_time;  42.47671937942505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30597659945487976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37628525495529175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05903470516204834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17442311346530914
4 0.066173077 	 0.1744231141
epoch_time;  42.80073523521423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27270787954330444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33840566873550415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05817732587456703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1584930270910263
5 0.0570016265 	 0.1584930305
epoch_time;  41.89068102836609
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24109628796577454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3102521300315857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04953470081090927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.145402193069458
6 0.0508509599 	 0.1454021996
epoch_time;  41.16178250312805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21905192732810974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.284101277589798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04323019087314606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13121724128723145
7 0.0460930621 	 0.131217242
epoch_time;  41.47889542579651
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20044425129890442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26212775707244873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04642048105597496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12859737873077393
8 0.041574624 	 0.1285973748
epoch_time;  40.90383434295654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1780867874622345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23322376608848572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03706376627087593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1120057925581932
9 0.0380467538 	 0.1120057927
epoch_time;  41.615357637405396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1605754941701889
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21648162603378296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03249315917491913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10457014292478561
10 0.0351823667 	 0.104570141
epoch_time;  41.62621593475342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15290667116641998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20665404200553894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03596372902393341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10781053453683853
11 0.0328997627 	 0.1078105362
epoch_time;  41.433672189712524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13376376032829285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18150873482227325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0271308496594429
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09114377945661545
12 0.0302960831 	 0.0911437781
epoch_time;  41.40401315689087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.124689981341362
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.05348
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.07319
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01633
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.04522
wandb:                         Train loss 0.0159
wandb: 
wandb: 🚀 View run chromatic-mandu-1630 at: https://wandb.ai/nreints/thesis/runs/l5ih6w9w
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_094235-l5ih6w9w/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_100414-c2xwa4jd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-wish-1639
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/c2xwa4jd
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17001812160015106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029982056468725204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08812862634658813
13 0.027945603 	 0.0881286229
epoch_time;  41.282185792922974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1117156371474266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15469224750995636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029602760449051857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08704012632369995
14 0.0264577757 	 0.0870401261
epoch_time;  41.3395094871521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10166417062282562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14235304296016693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024543197825551033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07813102751970291
15 0.0246863528 	 0.0781310275
epoch_time;  40.72753858566284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09138640016317368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13086389005184174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02194846235215664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07379747182130814
16 0.0236506971 	 0.0737974737
epoch_time;  41.616255044937134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08457250893115997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12263035029172897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023717572912573814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07337423413991928
17 0.0222055723 	 0.0733742325
epoch_time;  39.99951720237732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09613628685474396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13609932363033295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.047164492309093475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09866786003112793
18 0.0213808988 	 0.0986678593
epoch_time;  40.280752182006836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06817396730184555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10184352099895477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.019047977402806282
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06298286467790604
19 0.0204260521 	 0.0629828646
epoch_time;  40.591179847717285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06399301439523697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09894412010908127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018722284585237503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.06282780319452286
20 0.0198754434 	 0.0628278047
epoch_time;  41.19308114051819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06041417270898819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09146463125944138
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01913786493241787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05904310569167137
21 0.0188873184 	 0.0590431064
epoch_time;  40.46331262588501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.058466702699661255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08920307457447052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017396269366145134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05564954876899719
22 0.0182304007 	 0.0556495471
epoch_time;  41.60166049003601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05835265293717384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08993147313594818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021240470930933952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.061035726219415665
23 0.0178101848 	 0.0610357267
epoch_time;  40.35183548927307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05229838192462921
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08328750729560852
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01776117831468582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05468321964144707
24 0.0171963087 	 0.0546832185
epoch_time;  40.61023235321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05089539662003517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08124592900276184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018635746091604233
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05538877472281456
25 0.0166702848 	 0.0553887762
epoch_time;  41.8690881729126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04903191700577736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07821153849363327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016170045360922813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05280747264623642
26 0.016351437 	 0.0528074708
epoch_time;  41.36294174194336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04637099429965019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07287156581878662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016082655638456345
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.0494757778942585
27 0.0160904681 	 0.0494757765
epoch_time;  41.20896005630493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045763034373521805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07115951925516129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016694450750947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05006761848926544
28 0.0151539629 	 0.0500676193
epoch_time;  40.58916640281677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0451776348054409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0731690376996994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016297942027449608
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.05356083810329437
29 0.0159010053 	 0.053560839
epoch_time;  41.003419160842896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04521729052066803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07318766415119171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016333911567926407
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.053482506424188614
It took  1299.6208941936493  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14940813f490>, <torch.utils.data.dataloader.DataLoader object at 0x149408187670>, <torch.utils.data.dataloader.DataLoader object at 0x149408184640>, <torch.utils.data.dataloader.DataLoader object at 0x1494081862c0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9078658819198608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2825543880462646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23834262788295746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8982560634613037
0 2.5485704929 	 0.8982560541
epoch_time;  40.92945218086243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6151359677314758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8078557252883911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1281854212284088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4506361484527588
1 0.1688240161 	 0.4506361578
epoch_time;  40.5880286693573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47469282150268555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5942668914794922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09450695663690567
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2992534637451172
2 0.1138246061 	 0.299253458
epoch_time;  40.63685965538025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39764124155044556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49427878856658936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08147483319044113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2331279069185257
3 0.0902708459 	 0.2331279052
epoch_time;  40.743345499038696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3372466266155243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42580264806747437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06687217205762863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19612324237823486
4 0.0752066266 	 0.1961232442
epoch_time;  40.79757475852966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29429200291633606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37958288192749023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.059144701808691025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17841637134552002
5 0.0648999904 	 0.178416376
epoch_time;  40.120025634765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2636111080646515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34792065620422363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.054846134036779404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16892214119434357
6 0.0568302452 	 0.1689221362
epoch_time;  40.57370901107788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23699739575386047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3082751929759979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.051775239408016205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15336188673973083
7 0.0509076057 	 0.1533618823
epoch_time;  40.31738066673279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21276350319385529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2825503945350647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04237978905439377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14368031919002533
8 0.0475080145 	 0.143680319
epoch_time;  40.83055639266968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19724173843860626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26006919145584106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03905633091926575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1336628943681717
9 0.0423227811 	 0.1336629009
epoch_time;  40.7752730846405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17912983894348145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24124839901924133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03517117351293564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.128860741853714
10 0.038643045 	 0.1288607387
epoch_time;  40.37004256248474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16467073559761047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2208578735589981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032306842505931854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11836261302232742
11 0.0353245818 	 0.1183626112
epoch_time;  40.222651720047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15006116032600403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20279230177402496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.030023522675037384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1092805564403534
12 0.0326668207 	 0.1092805545
epoch_time;  40.11687994003296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13478906452655792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18603643774986267
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02827492542564869
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.10628610849380493
13 0.0300621275 	 0.1062861094
epoch_time;  40.05230760574341
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12100540101528168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16603581607341766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.027086572721600533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09726998955011368
14 0.0280035425 	 0.097269986
epoch_time;  40.0554883480072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10917764902114868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15176913142204285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024628382176160812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09146023541688919
15 0.0263478224 	 0.0914602366
epoch_time;  40.54345440864563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10018055140972137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1422126144170761
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.027606802061200142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09186755865812302
16 0.0247799356 	 0.0918675564
epoch_time;  42.60171890258789
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09730815887451172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13694968819618225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.029037490487098694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.09244725853204727
17 0.0234597601 	 0.0924472578
epoch_time;  41.010637044906616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08007186651229858
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11741974949836731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02095458284020424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08081739395856857
18 0.0224083648 	 0.0808173926
epoch_time;  39.9741952419281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07410489022731781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11065243929624557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02201935090124607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07917714864015579
19 0.0214885247 	 0.0791771477
epoch_time;  40.42888617515564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06987711787223816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10794658958911896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021767664700746536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08301268517971039
20 0.0206097972 	 0.0830126875
epoch_time;  40.10159969329834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06656245142221451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10049416869878769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022754354402422905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07905090600252151
21 0.0197424921 	 0.0790509054
epoch_time;  40.21685171127319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06163392961025238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09498341381549835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018731744959950447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.072398342192173
22 0.0189925402 	 0.0723983442
epoch_time;  39.8560471534729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.060538046061992645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09265179932117462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01939789392054081
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07287931442260742
23 0.0183226061 	 0.072879313
epoch_time;  39.87664818763733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05755585432052612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08743584156036377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.021737953647971153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07819471508264542
24 0.0179237536 	 0.0781947133
epoch_time;  39.94920110702515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05493339151144028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08618971705436707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.020825322717428207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07882919907569885
25 0.0172916386 	 0.0788291977
epoch_time;  39.884538888931274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.050934143364429474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07972265034914017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015515835955739021
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07446600496768951
26 0.0171110585 	 0.0744660081
epoch_time;  39.99354863166809
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05001164972782135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07883843034505844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015975775197148323
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07784758508205414
27 0.0164355221 	 0.0778475874
epoch_time;  39.81750988960266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04988449066877365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.078999824821949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017803534865379333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.07875683903694153
28 0.0159269141 	 0.0787568395
epoch_time;  39.782673358917236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04664359614253044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07626809179782867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013991125859320164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08117378503084183
29 0.015573698 	 0.0811737844
epoch_time;  41.06467890739441
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.046577878296375275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0761556401848793
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.08118
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.07616
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.014
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.04658
wandb:                         Train loss 0.01557
wandb: 
wandb: 🚀 View run vermilion-wish-1639 at: https://wandb.ai/nreints/thesis/runs/c2xwa4jd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_100414-c2xwa4jd/logs
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013999098911881447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.08118152618408203
It took  1276.5549166202545  seconds.

JOB STATISTICS
==============
Job ID: 2142226
Array Job ID: 2141141_18
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 22:02:49
CPU Efficiency: 34.18% of 2-16:29:42 core-walltime
Job Wall-clock time: 03:34:59
Memory Utilized: 15.30 GB
Memory Efficiency: 48.96% of 31.25 GB
