wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_124731-g85ibsfh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-monkey-475
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/g85ibsfh
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                                             Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.16928
wandb:                                             Train loss 0.18494
wandb: 
wandb: ðŸš€ View run fluent-monkey-475 at: https://wandb.ai/nreints/test/runs/g85ibsfh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_124731-g85ibsfh/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_125602-42q3c88l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-haze-497
wandb: â­ï¸ View project at https://wandb.ai/nreints/test
wandb: ðŸš€ View run at https://wandb.ai/nreints/test/runs/42q3c88l
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.079 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: / 0.079 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() â–ˆâ–…â–…â–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:                                             Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_full_pNone_gNone, MSELoss() 0.21653
wandb:                                             Train loss 0.23496
wandb: 
wandb: ðŸš€ View run major-haze-497 at: https://wandb.ai/nreints/test/runs/42q3c88l
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_125602-42q3c88l/logs
Running for data type: log_quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.3264385066 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.9035862684249878 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 49.48511266708374
Epoch 1
	 Logging train Loss: 0.8102212455 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.6867321729660034 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 46.40978455543518
Epoch 2
	 Logging train Loss: 0.661721325 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.561772882938385 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.507734060287476
Epoch 3
	 Logging train Loss: 0.5083458853 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.4056970179080963 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.495492935180664
Epoch 4
	 Logging train Loss: 0.3546319002 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.29314208030700684 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.49157929420471
Epoch 5
	 Logging train Loss: 0.2863242706 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.2559395432472229 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.322500467300415
Epoch 6
	 Logging train Loss: 0.2529454963 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.22830350697040558 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.29487204551697
Epoch 7
	 Logging train Loss: 0.2249225774 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.19944150745868683 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.35546660423279
Epoch 8
	 Logging train Loss: 0.2040399977 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.18217714130878448 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.68522572517395
Epoch 9
	 Logging train Loss: 0.184936666 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.1692044883966446 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.41180467605591
	 Logging test loss: 0.16927599906921387 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  512.4255037307739  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 4.2274891913 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 1.0610190629959106 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.76504945755005
Epoch 1
	 Logging train Loss: 0.8820283327 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.7530317306518555 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.60253429412842
Epoch 2
	 Logging train Loss: 0.7106273534 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.6426687240600586 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.60920166969299
Epoch 3
	 Logging train Loss: 0.5827701095 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.5088459849357605 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.26129627227783
Epoch 4
	 Logging train Loss: 0.4356605927 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.36782601475715637 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.39815640449524
Epoch 5
	 Logging train Loss: 0.355278555 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.3185817003250122 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.62568712234497
Epoch 6
	 Logging train Loss: 0.3203251715 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.3022979199886322 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.64336848258972
Epoch 7
	 Logging train Loss: 0.2792395476 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.25811392068862915 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.5004997253418
Epoch 8
	 Logging train Loss: 0.2528299859 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.23468905687332153 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.78182411193848
Epoch 9
	 Logging train Loss: 0.2349558119 (MSELoss(): data_t(0, 0)_r(5, 20)_full_pNone_gNone)
	 Logging test loss: 0.2167786806821823 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
     --> Epoch time; 43.56051850318909
	 Logging test loss: 0.21653404831886292 (MSELoss(): t(0, 0)_r(5, 20)_full_pNone_gNone)
It took  498.78399300575256  seconds.

JOB STATISTICS
==============
Job ID: 2514683
Array Job ID: 2514679_4
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:28:04
CPU Efficiency: 67.34% of 05:09:00 core-walltime
Job Wall-clock time: 00:17:10
Memory Utilized: 24.83 GB
Memory Efficiency: 79.45% of 31.25 GB
