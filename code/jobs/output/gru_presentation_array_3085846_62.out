wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203204-dm3yc1fj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-music-396
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/dm3yc1fj
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▁▂▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▁▂▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▁▁▂▁▁▁▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▁▂▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run amber-music-396 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/dm3yc1fj
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203204-dm3yc1fj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203830-0lx12kp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-shadow-405
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/0lx12kp4
Training on dataset: data_t(5,20)_r(0,0)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(5,20)_r(0,0)_tennis_pNone_gTrue', 'data_t(5,20)_r(0,0)_full_pNone_gTrue', 'data_t(5,20)_r(0,0)_semi_pNone_gTrue', 'data_t(5,20)_r(0,0)_combi_pNone_gTrue']
Focussing on identity: False
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 51.72613167762756 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 12.984633207321167 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 13.00954532623291 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 13.115046501159668 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 13.281256675720215 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 4.11002e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 8.057e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 8.261e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 7.948e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 7.951e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 27.159553289413452
Epoch 1/9
	 Logging train Loss: 3.782e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.218e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.251e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.197e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 2.174e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.573413848876953
Epoch 2/9
	 Logging train Loss: 1.665e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.476e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.519e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.485e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.454e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.5475115776062
Epoch 3/9
	 Logging train Loss: 1.544e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.449e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.503e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.488e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.442e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.509990453720093
Epoch 4/9
	 Logging train Loss: 1.645e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.978e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.012e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.993e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.965e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.62556791305542
Epoch 5/9
	 Logging train Loss: 1.755e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.808e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.851e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.825e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.779e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.71494698524475
Epoch 6/9
	 Logging train Loss: 1.783e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.485e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.559e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.521e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.479e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.893762826919556
Epoch 7/9
	 Logging train Loss: 1.82e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.472e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.515e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.488e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.466e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.517730951309204
Epoch 8/9
	 Logging train Loss: 1.784e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.759e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.807e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.803e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.753e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.415688037872314
Epoch 9/9
	 Logging train Loss: 1.773e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.649e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.729e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.669e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.657e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.74251914024353
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  387.0076322555542  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 51.140151023864746 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 13.341870546340942 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 13.305362701416016 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 13.3236243724823 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 12.767565488815308 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 4.21991e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 8.528e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 8.523e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 8.12e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 8.362e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.72194242477417
Epoch 1/9
	 Logging train Loss: 4.101e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.171e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.149e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.091e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 2.161e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.528963327407837
Epoch 2/9
	 Logging train Loss: 1.765e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.79e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.757e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.741e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.787e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.69227957725525
Epoch 3/9
	 Logging train Loss: 1.592e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.079e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.044e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.053e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 3.087e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.543898820877075
Epoch 4/9
	 Logging train Loss: 1.658e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▃▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▃▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▁▃▁▁▁▁▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▃▁▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run dulcet-shadow-405 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/0lx12kp4
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203830-0lx12kp4/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204454-amgmqo22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-firebrand-416
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/amgmqo22
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▁▁▁▁▁▁▂▂▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▁▁▁▁▁▁▂▂▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▁▁▁▁▁▁▂▂▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▁▁▁▁▁▁▂▂▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run deep-firebrand-416 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/amgmqo22
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204454-amgmqo22/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205118-gg9hrvma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-rain-426
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/gg9hrvma
	 Logging test loss: 1.619e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.6e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.593e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.64e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.6680269241333
Epoch 5/9
	 Logging train Loss: 1.755e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.525e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.505e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.5e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.548e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.614712953567505
Epoch 6/9
	 Logging train Loss: 1.814e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.613e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.593e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.588e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.635e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.61789846420288
Epoch 7/9
	 Logging train Loss: 1.764e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.867e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.846e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.841e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.883e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.53461265563965
Epoch 8/9
	 Logging train Loss: 1.755e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.53e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.501e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.495e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.542e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.547940254211426
Epoch 9/9
	 Logging train Loss: 1.72e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.617e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.598e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.594e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.628e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.703256368637085
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  383.45333075523376  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 50.743234634399414 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 13.198170900344849 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 13.186115026473999 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 13.193689584732056 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 12.752224206924438 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 3.69838e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 7.421e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 7.458e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 7.447e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 7.679e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.588474988937378
Epoch 1/9
	 Logging train Loss: 3.333e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.743e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.766e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.784e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.807e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.778830766677856
Epoch 2/9
	 Logging train Loss: 1.557e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.423e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.423e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.465e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.486e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.660104036331177
Epoch 3/9
	 Logging train Loss: 1.512e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.414e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.414e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.457e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.465e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.752563953399658
Epoch 4/9
	 Logging train Loss: 1.629e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.419e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.417e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.46e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.482e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.4073486328125
Epoch 5/9
	 Logging train Loss: 1.756e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.48e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.474e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.523e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.532e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.74623680114746
Epoch 6/9
	 Logging train Loss: 1.79e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.786e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.784e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.828e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.836e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.863335371017456
Epoch 7/9
	 Logging train Loss: 1.792e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.07e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.059e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.121e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 2.11e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.723431825637817
Epoch 8/9
	 Logging train Loss: 1.775e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.036e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.043e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.075e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 2.082e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.868335962295532
Epoch 9/9
	 Logging train Loss: 1.755e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.634e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.632e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.687e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.693e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.797761917114258
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  384.4523913860321  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 50.43500351905823 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 13.159899950027466 seconds.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run zany-rain-426 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/gg9hrvma
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205118-gg9hrvma/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205743-2r9in6mi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-pyramid-436
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/2r9in6mi
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 13.056214809417725 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 13.115861415863037 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 12.729013204574585 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 4.19966e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 7.598e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 7.987e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 7.436e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 7.831e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.733272314071655
Epoch 1/9
	 Logging train Loss: 3.745e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.909e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.919e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.898e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.931e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.35598087310791
Epoch 2/9
	 Logging train Loss: 1.626e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.47e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.465e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.502e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.483e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.50666069984436
Epoch 3/9
	 Logging train Loss: 1.518e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.5e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.484e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.532e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.515e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.773293018341064
Epoch 4/9
	 Logging train Loss: 1.645e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.546e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.547e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.561e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.555e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.6548855304718
Epoch 5/9
	 Logging train Loss: 1.781e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.718e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.704e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.747e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.727e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.639768600463867
Epoch 6/9
	 Logging train Loss: 1.823e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.582e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.565e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.585e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.578e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.71773409843445
Epoch 7/9
	 Logging train Loss: 1.813e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.6e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.585e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.619e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.611e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.621347188949585
Epoch 8/9
	 Logging train Loss: 1.755e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.729e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.696e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.715e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.707e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.653621673583984
Epoch 9/9
	 Logging train Loss: 1.74e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.489e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.474e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.507e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.499e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.830212831497192
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  384.9335136413574  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 50.36505341529846 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 13.034813642501831 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 13.063198566436768 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 13.04451298713684 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 12.761762857437134 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 3.89744e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 8.015e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 8.051e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 7.915e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 8.028e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 28.053306102752686
Epoch 1/9
	 Logging train Loss: 3.61e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.89e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.898e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.909e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.913e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.535553455352783
Epoch 2/9
	 Logging train Loss: 1.65e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.443e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.456e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.465e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.464e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.757115364074707
Epoch 3/9
	 Logging train Loss: 1.594e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.404e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.417e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.416e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 1.425e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.500205039978027
Epoch 4/9
	 Logging train Loss: 1.756e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.342e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.361e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.36e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
	 Logging test loss: 2.372e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
		--> Epoch time; 26.70011281967163
Epoch 5/9
	 Logging train Loss: 1.85e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.506e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.521e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.521e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▁▂▁▁▁▁▂
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▁▂▁▁▁▁▂
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▁▁▂▁▁▁▁▂
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▁▂▁▁▁▁▂
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run leafy-pyramid-436 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/2r9in6mi
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205743-2r9in6mi/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210406-11qm2q8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-pyramid-447
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/11qm2q8r
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 3085865 ON gcn22 CANCELLED AT 2023-07-16T21:09:10 ***
slurmstepd: error: *** STEP 3085865.0 ON gcn22 CANCELLED AT 2023-07-16T21:09:10 ***

JOB STATISTICS
==============
Job ID: 3085865
Array Job ID: 3085846_62
Cluster: snellius
User/Group: nreints/nreints
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:36:41
CPU Efficiency: 5.46% of 11:11:42 core-walltime
Job Wall-clock time: 00:37:19
Memory Utilized: 9.74 GB
Memory Efficiency: 0.00% of 0.00 MB
