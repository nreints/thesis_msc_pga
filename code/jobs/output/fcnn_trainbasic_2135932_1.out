wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195346-65pfveug
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run incandescent-cake-1163
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/65pfveug
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.33314
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.37203
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.16141
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20477
wandb:                         Train loss 1.92788
wandb: 
wandb: ðŸš€ View run incandescent-cake-1163 at: https://wandb.ai/nreints/thesis/runs/65pfveug
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195346-65pfveug/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4010317921638489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9191614389419556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0893075466156006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.71236515045166
0 4.2170663058 	 3.7123650628 	 3.7123650628
epoch_time;  31.337022304534912
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32763615250587463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8961254358291626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.84316349029541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4961507320404053
1 2.2188129774 	 3.4961508261 	 3.4961508261
epoch_time;  29.9620258808136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3015412986278534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7711848020553589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7924866676330566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.308377742767334
2 2.1414126304 	 3.3083776526 	 3.3083776526
epoch_time;  29.68181586265564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2976517081260681
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6278954744338989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.870222806930542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.23793625831604
3 2.0950054151 	 3.2379361539 	 3.2379361539
epoch_time;  31.028182983398438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2721191942691803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6123020052909851
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6871261596679688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0842862129211426
4 2.071159306 	 3.0842862516 	 3.0842862516
epoch_time;  29.663164854049683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26524606347084045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5514351725578308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.663018226623535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.004991054534912
5 2.0407337971 	 3.0049910262 	 3.0049910262
epoch_time;  30.19421100616455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2587246596813202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5336339473724365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.622358798980713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9390716552734375
6 2.0258803939 	 2.9390717378 	 2.9390717378
epoch_time;  29.809539794921875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26196298003196716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5259707570075989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5637354850769043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8505783081054688
7 2.0090993211 	 2.8505783493 	 2.8505783493
epoch_time;  30.26341962814331
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2296188622713089
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48127371072769165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.543626546859741
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8193960189819336
8 1.9963769002 	 2.8193959829 	 2.8193959829
epoch_time;  29.863482236862183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2282549887895584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4459400177001953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.490419864654541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.735379219055176
9 1.9864049871 	 2.7353791108 	 2.7353791108
epoch_time;  30.089820384979248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26817572116851807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5273258686065674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4893929958343506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7704553604125977
10 1.9765259698 	 2.7704553553 	 2.7704553553
epoch_time;  29.7303466796875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2889189124107361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5386228561401367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5300910472869873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7768290042877197
11 1.9713712471 	 2.7768290752 	 2.7768290752
epoch_time;  30.235742330551147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25581488013267517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5183810591697693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.380872964859009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6705381870269775
12 1.9586580828 	 2.6705381651 	 2.6705381651
epoch_time;  29.871142387390137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2324552834033966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44838669896125793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.310572385787964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5448482036590576
13 1.9569633138 	 2.5448483029 	 2.5448483029
epoch_time;  30.200243949890137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.234201580286026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4449501931667328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3390822410583496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.557466506958008
14 1.9517120589 	 2.5574665791 	 2.5574665791
epoch_time;  29.6713285446167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2633073627948761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5057397484779358
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3093879222869873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5564792156219482
15 1.9485450078 	 2.5564791293 	 2.5564791293
epoch_time;  30.27212166786194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24384252727031708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45604759454727173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.282994031906128
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.503044605255127
16 1.9414179859 	 2.5030444996 	 2.5030444996
epoch_time;  29.87991213798523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22833137214183807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4388984143733978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2186169624328613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.432244300842285
17 1.938177544 	 2.4322443782 	 2.4322443782
epoch_time;  30.129451513290405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22553811967372894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.438304603099823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.242612838745117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.471426248550415
18 1.9336238337 	 2.4714261442 	 2.4714261442
epoch_time;  30.76066255569458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2048071324825287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.371900737285614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1601343154907227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3328750133514404
19 1.9278844628 	 2.3328750198 	 2.3328750198
epoch_time;  30.164632320404053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2047746181488037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.372033029794693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1614105701446533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.333141326904297
It took 665.8341879844666 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn30: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135933.0

JOB STATISTICS
==============
Job ID: 2135933
Array Job ID: 2135932_1
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:24:18 core-walltime
Job Wall-clock time: 00:11:21
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
