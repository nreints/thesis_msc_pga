wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230620_120349-qvmg9xga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-field-123
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal/runs/qvmg9xga
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone █▃▂▂▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone █▃▂▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone █▂▂▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone █▃▂▂▂▁▁▁▁▁
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone █▃▂▂▂▁▁▁▁▁
wandb:                                   Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone 0.00109
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.00028
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone 0.00031
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 0.0018
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 0.00188
wandb:                                   Train loss 0.00143
wandb: 
wandb: 🚀 View run proud-field-123 at: https://wandb.ai/nreints/ThesisFinal/runs/qvmg9xga
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230620_120349-qvmg9xga/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230620_122245-obmc7ly0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-waterfall-143
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal/runs/obmc7ly0
Training on dataset: data_t(5,20)_r(5,20)_combi_pNone_gNone
Testing on 5 datasets: ['data_t(5,20)_r(5,20)_combi_pNone_gNone', 'data_t(5,20)_r(5,20)_full_pNone_gNone', 'data_t(5,20)_r(5,20)_semi_pNone_gNone', 'data_t(5,20)_r(5,20)_none_pNone_gNone', 'data_t(5,20)_r(5,20)_tennis_pNone_gNone']
Focussing on identity: False
Using extra input: inertia_body
Using start-fr as reference point.
----- ITERATION 1/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 89.34205985069275 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 22.323578357696533 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 22.50818657875061 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 22.72364115715027 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_none_pNone_gNone took 22.85820746421814 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 22.71517562866211 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat
-- Started Training --
Epoch 0/9
	 Logging train Loss: 2.9944882393 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0339524075 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0185320433 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0435356013 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0226239078 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0467979684 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 92.40863060951233
Epoch 1/9
	 Logging train Loss: 0.0176281221 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0108744828 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0042131133 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0154139316 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0050827735 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0172731951 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.09599161148071
Epoch 2/9
	 Logging train Loss: 0.008006854 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0057210107 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0019170382 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0086598881 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0023006385 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.009251005 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 90.8666763305664
Epoch 3/9
	 Logging train Loss: 0.0050929403 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0056535513 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.001266403 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0094687575 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0014957278 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0098300129 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.32845211029053
Epoch 4/9
	 Logging train Loss: 0.0037682296 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0034039787 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0008056033 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.005663889 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0009471761 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0059741968 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 92.47080898284912
Epoch 5/9
	 Logging train Loss: 0.0028291021 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0023482644 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0005690805 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0037595907 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0006665217 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0040123523 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 96.6730010509491
Epoch 6/9
	 Logging train Loss: 0.0022425586 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0018000747 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0005056891 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0028919219 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0005708504 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0030444916 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.23568487167358
Epoch 7/9
	 Logging train Loss: 0.0019621714 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0021875908 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0004834429 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0036818734 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0005366991 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0039561349 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.39081907272339
Epoch 8/9
	 Logging train Loss: 0.0016137304 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0011997517 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0003342011 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0019063054 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0003818282 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0020570266 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.45276951789856
Epoch 9/9
	 Logging train Loss: 0.0014309568 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0010938002 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0002799094 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0018003802 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0003117476 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0018815374 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.08601260185242
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combi_pNone_gNone/'quat'_'inertia_body'.pth
It took  1137.090824842453  seconds.
----- ITERATION 2/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 80.64974546432495 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 20.404034852981567 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 20.75505256652832 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 20.584001064300537 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_none_pNone_gNone took 20.645896673202515 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 20.586642742156982 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat
-- Started Training --
Epoch 0/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone █▃▃▂▂▂▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone █▃▂▂▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone █▃▂▂▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone █▄▃▂▂▂▁▁▁▁
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone █▄▃▂▂▂▁▁▁▁
wandb:                                   Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone 0.00151
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.00031
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone 0.00036
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 0.00274
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 0.00298
wandb:                                   Train loss 0.00166
wandb: 
wandb: 🚀 View run smooth-waterfall-143 at: https://wandb.ai/nreints/ThesisFinal/runs/obmc7ly0
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230620_122245-obmc7ly0/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230620_124126-r7gd5cqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-spaceship-162
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal/runs/r7gd5cqz
	 Logging train Loss: 3.2908861637 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.02863116 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0152932713 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0428516082 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0169895105 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0411853716 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 92.25596332550049
Epoch 1/9
	 Logging train Loss: 0.017544277 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0110422736 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0043398482 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0178849623 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0047515631 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0189736001 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.49731969833374
Epoch 2/9
	 Logging train Loss: 0.0087121753 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0079498189 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.002354719 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0140942782 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0025765868 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0147216646 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 94.99373841285706
Epoch 3/9
	 Logging train Loss: 0.0058324328 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0053489907 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.001528465 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0094973138 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0016964289 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0102164475 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 94.14026880264282
Epoch 4/9
	 Logging train Loss: 0.0043135541 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0042368569 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0011937832 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.007602971 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0013136477 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0079129143 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.67394161224365
Epoch 5/9
	 Logging train Loss: 0.0032043504 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0039525148 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0008652284 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0070404354 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0009497436 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0080997404 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 92.02102947235107
Epoch 6/9
	 Logging train Loss: 0.0026341251 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0023092448 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0005993314 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0041572843 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0006841926 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0042912252 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.74521017074585
Epoch 7/9
	 Logging train Loss: 0.0023979647 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0016845256 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0004956737 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0030188102 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0005670989 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.003033577 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.60186862945557
Epoch 8/9
	 Logging train Loss: 0.0017430183 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0014554709 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0006472177 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0023010813 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0007010754 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0023681959 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.52414679527283
Epoch 9/9
	 Logging train Loss: 0.001661813 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0015062479 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0003132253 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0027422495 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.0003598632 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0029798469 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.69998812675476
Saved model in  trained_models/gru/data_t(5,20)_r(5,20)_combi_pNone_gNone/'quat'_'inertia_body'.pth
It took  1121.084906578064  seconds.
----- ITERATION 3/10 ------
Total number of simulations in train dir:  2400
Checked number of simulations in each data directory.
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 80.6571991443634 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(5,20)_combi_pNone_gNone took 20.370226621627808 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_full_pNone_gNone took 20.555331230163574 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_semi_pNone_gNone took 20.51856541633606 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_none_pNone_gNone took 20.63820719718933 seconds.
The dataloader for data/data_t(5,20)_r(5,20)_tennis_pNone_gNone took 20.885558605194092 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (pre_hidden_lin_layer): Sequential(
    (0): Linear(in_features=3, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
    (3): ReLU()
  )
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat
-- Started Training --
Epoch 0/9
	 Logging train Loss: 3.137389183 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0319238156 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0181569364 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0415683575 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.016883472 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0454937182 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 97.57975482940674
Epoch 1/9
	 Logging train Loss: 0.0169095639 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0114371618 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0047461409 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0165283531 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.004386072 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0177399274 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 92.71674728393555
Epoch 2/9
	 Logging train Loss: 0.0083923889 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0079423338 [MSELoss(): t(5,20)_r(5,20)_combi_pNone_gNone]
	 Logging test loss: 0.0024937259 [MSELoss(): t(5,20)_r(5,20)_full_pNone_gNone]
	 Logging test loss: 0.0125041762 [MSELoss(): t(5,20)_r(5,20)_semi_pNone_gNone]
	 Logging test loss: 0.002364004 [MSELoss(): t(5,20)_r(5,20)_none_pNone_gNone]
	 Logging test loss: 0.0136799626 [MSELoss(): t(5,20)_r(5,20)_tennis_pNone_gNone]
		--> Epoch time; 91.87733507156372
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                        Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone █▃▃▂▂▁▂▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone █▃▂▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone █▃▂▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone █▄▃▂▂▁▂▁▁▁
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone █▄▃▂▂▁▂▁▁▁
wandb:                                   Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                        Epoch 9
wandb:  Test loss t(5,20)_r(5,20)_combi_pNone_gNone 0.00182
wandb:   Test loss t(5,20)_r(5,20)_full_pNone_gNone 0.00051
wandb:   Test loss t(5,20)_r(5,20)_none_pNone_gNone 0.0005
wandb:   Test loss t(5,20)_r(5,20)_semi_pNone_gNone 0.00283
wandb: Test loss t(5,20)_r(5,20)_tennis_pNone_gNone 0.00312
wandb:                                   Train loss 0.0015
wandb: 
wandb: 🚀 View run laced-spaceship-162 at: https://wandb.ai/nreints/ThesisFinal/runs/r7gd5cqz
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230620_124126-r7gd5cqz/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230620_130007-17xu4ao2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-galaxy-182
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal/runs/17xu4ao2
slurmstepd: error: *** STEP 2943918.0 ON gcn43 CANCELLED AT 2023-06-20T13:04:07 ***
slurmstepd: error: *** JOB 2943918 ON gcn43 CANCELLED AT 2023-06-20T13:04:07 ***

JOB STATISTICS
==============
Job ID: 2943918
Array Job ID: 2943913_3
Cluster: snellius
User/Group: nreints/nreints
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 18:09:00 core-walltime
Job Wall-clock time: 01:00:30
Memory Utilized: 6.63 MB
Memory Efficiency: 0.00% of 0.00 MB
