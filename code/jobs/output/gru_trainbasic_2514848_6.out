wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_135639-b8xcr29p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-wildflower-541
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/b8xcr29p
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() █▆▅▄▂▂▁▁▁▁▁
wandb:                                             Train loss █▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 3.31261
wandb:                                             Train loss 3.30976
wandb: 
wandb: 🚀 View run woven-wildflower-541 at: https://wandb.ai/nreints/test/runs/b8xcr29p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_135639-b8xcr29p/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_140605-5e6njk23
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-paper-557
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/5e6njk23
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() █▆▅▄▃▂▁▁▁▁▁
wandb:                                             Train loss █▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 3.12589
wandb:                                             Train loss 3.20767
wandb: 
wandb: 🚀 View run mild-paper-557 at: https://wandb.ai/nreints/test/runs/5e6njk23
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_140605-5e6njk23/logs
Running for data type: log_dualQ
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 133.3127607644 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 29.87824058532715 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 50.88829946517944
Epoch 1
	 Logging train Loss: 23.6649151994 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 21.47352409362793 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 48.481953382492065
Epoch 2
	 Logging train Loss: 19.0800520486 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 18.233510971069336 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 48.940053939819336
Epoch 3
	 Logging train Loss: 15.4750331171 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 13.630596160888672 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 48.86280155181885
Epoch 4
	 Logging train Loss: 10.1815982508 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 7.922128200531006 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.37038588523865
Epoch 5
	 Logging train Loss: 6.2994493961 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 5.634096622467041 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.20138597488403
Epoch 6
	 Logging train Loss: 4.8616619034 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 4.715026378631592 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 50.21540141105652
Epoch 7
	 Logging train Loss: 4.1961867125 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 4.238643169403076 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 50.38222908973694
Epoch 8
	 Logging train Loss: 3.6832919153 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.6866464614868164 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 50.17275333404541
Epoch 9
	 Logging train Loss: 3.3097574109 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.310073137283325 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 51.12864637374878
	 Logging test loss: 3.3126094341278076 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  567.2149980068207  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(6, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=6, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 137.9229389186 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 29.25869369506836 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 50.058560848236084
Epoch 1
	 Logging train Loss: 24.0580096379 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 20.519309997558594 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.46374011039734
Epoch 2
	 Logging train Loss: 19.4993793808 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 17.573341369628906 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.23642635345459
Epoch 3
	 Logging train Loss: 16.3607909504 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 13.909383773803711 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 48.92191410064697
Epoch 4
	 Logging train Loss: 11.1797318299 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 11.199114799499512 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.29466485977173
Epoch 5
	 Logging train Loss: 6.6177148959 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 5.417173385620117 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.40711808204651
Epoch 6
	 Logging train Loss: 4.9088491687 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 4.398695468902588 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.12067723274231
Epoch 7
	 Logging train Loss: 4.2078291889 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.857039213180542 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.739405393600464
Epoch 8
	 Logging train Loss: 3.614751635 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.3877978324890137 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.0875141620636
Epoch 9
	 Logging train Loss: 3.2076697967 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.130648136138916 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 49.52685284614563
	 Logging test loss: 3.125887632369995 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  559.2844507694244  seconds.

JOB STATISTICS
==============
Job ID: 2514854
Array Job ID: 2514848_6
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:59:01
CPU Efficiency: 69.83% of 05:42:18 core-walltime
Job Wall-clock time: 00:19:01
Memory Utilized: 24.79 GB
Memory Efficiency: 79.31% of 31.25 GB
