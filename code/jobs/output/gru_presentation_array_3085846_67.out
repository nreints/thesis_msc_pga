wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203203-62ef39v3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-puddle-389
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/62ef39v3
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▁▂▂▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▂▂▃▃▁▁▂▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▂▂▃▂▁▁▂▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run honest-puddle-389 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/62ef39v3
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203203-62ef39v3/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203931-ruvjxd5p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-valley-408
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/ruvjxd5p
Training on dataset: data_t(5,20)_r(0,0)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(5,20)_r(0,0)_tennis_pNone_gTrue', 'data_t(5,20)_r(0,0)_combi_pNone_gTrue', 'data_t(5,20)_r(0,0)_full_pNone_gTrue', 'data_t(5,20)_r(0,0)_semi_pNone_gTrue']
Focussing on identity: True
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 52.05052208900452 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 13.187514066696167 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 12.894701957702637 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 12.693952560424805 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 12.230591297149658 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0003027509 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.77702e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.73984e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.82348e-05 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.82516e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.91558027267456
Epoch 1/9
	 Logging train Loss: 1.24691e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 4.1936e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 4.046e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 4.0142e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 4.2669e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.81889200210571
Epoch 2/9
	 Logging train Loss: 4.2032e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.3655e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.1933e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.0912e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.3883e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.94006538391113
Epoch 3/9
	 Logging train Loss: 2.9807e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.4012e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.8803e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.351e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.4737e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.68695664405823
Epoch 4/9
	 Logging train Loss: 3.4691e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.1832e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.7029e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.0736e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 6.4646e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.85908246040344
Epoch 5/9
	 Logging train Loss: 2.2369e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.056e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.4944e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.505e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 6.4106e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.78883242607117
Epoch 6/9
	 Logging train Loss: 2.4937e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.886e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.75e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.636e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.964e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.827120780944824
Epoch 7/9
	 Logging train Loss: 1.0864e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.4519e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 9.077e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.22e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.5746e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.80909705162048
Epoch 8/9
	 Logging train Loss: 1.0803e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.5543e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.4202e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.892e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.8613e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.808584213256836
Epoch 9/9
	 Logging train Loss: 1.2021e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.368e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.341e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.341e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.418e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.92538285255432
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  448.9900257587433  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 45.26599979400635 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 11.331849098205566 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 11.375802516937256 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 11.281931638717651 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 11.346600532531738 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0001431055 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 9.8041e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 9.0726e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 9.011e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 8.9971e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.13268995285034
Epoch 1/9
	 Logging train Loss: 6.0826e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.3455e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.1597e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.1069e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.1951e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.16912865638733
Epoch 2/9
	 Logging train Loss: 2.5573e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.7496e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.339e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.0195e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.6535e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.07486367225647
Epoch 3/9
	 Logging train Loss: 1.9876e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.41862e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 6.8963e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.4738e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.38813e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.07698678970337
Epoch 4/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▃▂▆▁▁▂▁▁▃
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▃▂▂▁▁▁▁▁▂
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue ▆▂▂█▁▁▂▁▁▃
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue ▆▂▂█▁▁▂▁▁▃
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run stellar-valley-408 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/ruvjxd5p
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203931-ruvjxd5p/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204645-apz29b50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-thunder-419
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/apz29b50
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▁▁▁▄▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▁▁▁▂▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▁▁▁▁▆▁▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▁▁▁▆▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run lunar-thunder-419 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/apz29b50
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204645-apz29b50/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205359-ahc5992k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-wind-430
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/ahc5992k
	 Logging train Loss: 1.7823e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.657e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.528e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.479e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.519e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.12128043174744
Epoch 5/9
	 Logging train Loss: 1.6335e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.92e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.866e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.812e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.854e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.04641842842102
Epoch 6/9
	 Logging train Loss: 1.3438e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.5774e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.0481e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.438e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.5361e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.07536816596985
Epoch 7/9
	 Logging train Loss: 1.2275e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.0857e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 6.645e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.022e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 9.963e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.14042949676514
Epoch 8/9
	 Logging train Loss: 7.436e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.236e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 5.788e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.394e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 6.116e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.92489433288574
Epoch 9/9
	 Logging train Loss: 7.889e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 4.3787e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.673e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.4714e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 4.1787e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.0498423576355
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  433.95066142082214  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 45.406840324401855 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 11.327875852584839 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 11.301103591918945 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 11.323561668395996 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 11.268978357315063 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0004417638 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.79798e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.90346e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.70817e-05 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.84101e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.76868534088135
Epoch 1/9
	 Logging train Loss: 1.89615e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 7.3096e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 7.4103e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.9707e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 7.394e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.95665526390076
Epoch 2/9
	 Logging train Loss: 4.286e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.1248e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.9259e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.5624e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.1958e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.91155695915222
Epoch 3/9
	 Logging train Loss: 3.5639e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.7124e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.742e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.6675e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.7312e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.74078893661499
Epoch 4/9
	 Logging train Loss: 3.5154e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.0605e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.0621e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 9.972e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.0781e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.74509954452515
Epoch 5/9
	 Logging train Loss: 2.3228e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.748e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 5.786e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.448e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 5.852e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.11737251281738
Epoch 6/9
	 Logging train Loss: 2.5862e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.84819e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.58204e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.9744e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.79268e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.070956230163574
Epoch 7/9
	 Logging train Loss: 1.5231e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 9.245e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 6.502e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.599e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 9.366e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.98855638504028
Epoch 8/9
	 Logging train Loss: 1.9329e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.538e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 4.684e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.617e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 5.675e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.96437335014343
Epoch 9/9
	 Logging train Loss: 1.0026e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.2145e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 7.594e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.668e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.1497e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.8272180557251
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  433.71283435821533  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 45.3256459236145 seconds.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▂▁▁▁▂▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▂▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▂▂▁▁▃▁▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▂▂▁▁▃▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run atomic-wind-430 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/ahc5992k
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205359-ahc5992k/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210113-17o1lejb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-pyramid-442
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/17o1lejb
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 11.322968244552612 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 11.280974864959717 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 11.279130220413208 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 11.262665510177612 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.000411076 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.50572e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.47247e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.43152e-05 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.43136e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.949132204055786
Epoch 1/9
	 Logging train Loss: 9.8968e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.9047e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.81e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.6881e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.7837e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.00824809074402
Epoch 2/9
	 Logging train Loss: 2.7619e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.0709e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.0363e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.0084e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.9993e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.80543804168701
Epoch 3/9
	 Logging train Loss: 2.9703e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.4287e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.8365e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.3089e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.2832e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.038809061050415
Epoch 4/9
	 Logging train Loss: 1.6666e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.3485e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.1808e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.0151e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.3098e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.571500301361084
Epoch 5/9
	 Logging train Loss: 1.7031e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 4.4e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 4.188e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.949e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 4.275e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.64654231071472
Epoch 6/9
	 Logging train Loss: 1.9279e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 6.2076e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.44e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 8.147e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 5.9208e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.18152952194214
Epoch 7/9
	 Logging train Loss: 1.5294e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.9734e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.3707e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 8.373e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.8745e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 34.5783576965332
Epoch 8/9
	 Logging train Loss: 9.901e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.243e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.15e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.013e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.213e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.82525324821472
Epoch 9/9
	 Logging train Loss: 1.0907e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.656e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.639e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.578e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.666e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.836658000946045
Saved model in  trained_models/gru/data_t(5,20)_r(0,0)_combi_pNone_gTrue/'dual_quat_1'_'False'.pth
It took  434.12347292900085  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 45.36265707015991 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(5,20)_r(0,0)_tennis_pNone_gTrue took 11.303519248962402 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_combi_pNone_gTrue took 11.28597378730774 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_full_pNone_gTrue took 11.286665439605713 seconds.
The dataloader for data/data_t(5,20)_r(0,0)_semi_pNone_gTrue took 11.29370641708374 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
Datatype: dual_quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0004102603 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.18824e-05 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.14798e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.1599e-05 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.11039e-05 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.28288507461548
Epoch 1/9
	 Logging train Loss: 1.19293e-05 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.3983e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 3.3431e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 3.3414e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 3.3025e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.66900300979614
Epoch 2/9
	 Logging train Loss: 2.6266e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.072e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 2.0685e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 2.0682e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 2.0203e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.75377607345581
Epoch 3/9
	 Logging train Loss: 2.2471e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.5524e-06 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 1.4332e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 1.2648e-06 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 1.5016e-06 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 32.91176390647888
Epoch 4/9
	 Logging train Loss: 2.1124e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.413e-07 [MSELoss(): t(5,20)_r(0,0)_tennis_pNone_gTrue]
	 Logging test loss: 5.339e-07 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
	 Logging test loss: 5.186e-07 [MSELoss(): t(5,20)_r(0,0)_full_pNone_gTrue]
	 Logging test loss: 5.288e-07 [MSELoss(): t(5,20)_r(0,0)_semi_pNone_gTrue]
		--> Epoch time; 33.109665393829346
Epoch 5/9
	 Logging train Loss: 1.7377e-06 [MSELoss(): t(5,20)_r(0,0)_combi_pNone_gTrue]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue █▂▁▁▁▁▁▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(5,20)_r(0,0)_combi_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_full_pNone_gTrue 0.0
wandb:   Test loss t(5,20)_r(0,0)_semi_pNone_gTrue 0.0
wandb: Test loss t(5,20)_r(0,0)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run lucky-pyramid-442 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/17o1lejb
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_210113-17o1lejb/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210825-wq4jty08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-pyramid-453
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/wq4jty08
slurmstepd: error: *** STEP 3085870.0 ON gcn29 CANCELLED AT 2023-07-16T21:09:10 ***
slurmstepd: error: *** JOB 3085870 ON gcn29 CANCELLED AT 2023-07-16T21:09:10 ***

JOB STATISTICS
==============
Job ID: 3085870
Array Job ID: 3085846_67
Cluster: snellius
User/Group: nreints/nreints
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 11:11:42 core-walltime
Job Wall-clock time: 00:37:19
Memory Utilized: 6.69 MB
Memory Efficiency: 0.00% of 0.00 MB
