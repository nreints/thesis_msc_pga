wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163841-w5i2mx7d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-rat-1132
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/w5i2mx7d
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–…â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–ˆâ–†â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–„â–„â–„â–„â–„â–‚â–ƒâ–ƒâ–‚â–…â–ƒâ–‡â–ƒâ–‚â–„â–â–‚â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.53464
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.36878
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.36832
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17622
wandb:                         Train loss 1.52747
wandb: 
wandb: ðŸš€ View run vivid-rat-1132 at: https://wandb.ai/nreints/thesis/runs/w5i2mx7d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163841-w5i2mx7d/logs
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2666323781013489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8006631135940552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.137120723724365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.752648830413818
0 3.1858477118 	 5.7526485959 	 5.7526485959
epoch_time;  32.14809226989746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2527845799922943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6315209269523621
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.210751533508301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.572750568389893
1 1.809843545 	 5.5727506071 	 5.5727506071
epoch_time;  30.57926321029663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20136095583438873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48405930399894714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.922404766082764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.138638973236084
2 1.7060014625 	 5.138638883 	 5.138638883
epoch_time;  31.66680097579956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19338741898536682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44501182436943054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5736589431762695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.676353454589844
3 1.6617529991 	 4.6763533309 	 4.6763533309
epoch_time;  30.976053953170776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20314766466617584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44499677419662476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6647047996521
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.783069610595703
4 1.6320035466 	 4.7830698374 	 4.7830698374
epoch_time;  31.007908582687378
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1913650929927826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3957810699939728
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8206706047058105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.936639785766602
5 1.6100198156 	 4.9366398992 	 4.9366398992
epoch_time;  30.932737112045288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19135139882564545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4107723832130432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.485647678375244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.605726718902588
6 1.5890185977 	 4.6057267473 	 4.6057267473
epoch_time;  30.948169231414795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15533232688903809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33699360489845276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.551822662353516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.639347553253174
7 1.5812282472 	 4.6393475507 	 4.6393475507
epoch_time;  30.605872869491577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1837751716375351
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3815649449825287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.572876453399658
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.675309658050537
8 1.573181359 	 4.6753094647 	 4.6753094647
epoch_time;  30.65638566017151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1735246628522873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35236260294914246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.476611614227295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5637922286987305
9 1.5661062397 	 4.5637922957 	 4.5637922957
epoch_time;  30.44906258583069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16775153577327728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34572941064834595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.426504611968994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.502009391784668
10 1.557204446 	 4.5020095413 	 4.5020095413
epoch_time;  30.893444061279297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2137749344110489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4097878634929657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.492044448852539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.610438346862793
11 1.5555660426 	 4.6104383314 	 4.6104383314
epoch_time;  30.228039264678955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18145766854286194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3676035702228546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.504792213439941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.618490219116211
12 1.5492620242 	 4.6184900232 	 4.6184900232
epoch_time;  30.623637199401855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24288058280944824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4208652079105377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.528219223022461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.618348598480225
13 1.5462313415 	 4.6183488176 	 4.6183488176
epoch_time;  30.67011523246765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17877593636512756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3530500531196594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.3988847732543945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.505116939544678
14 1.5381482537 	 4.5051167256 	 4.5051167256
epoch_time;  30.80312991142273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16079077124595642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33355289697647095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.548679828643799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.672745227813721
15 1.5358785889 	 4.6727449984 	 4.6727449984
epoch_time;  30.472771406173706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19443398714065552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38146814703941345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.356230735778809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.486110210418701
16 1.5374675691 	 4.486110378 	 4.486110378
epoch_time;  30.89820098876953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14465276896953583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29145175218582153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2702484130859375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.367870807647705
17 1.5270128295 	 4.3678707638 	 4.3678707638
epoch_time;  30.6658616065979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15600581467151642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3096138536930084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.43184757232666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.555827617645264
18 1.5294296068 	 4.5558277027 	 4.5558277027
epoch_time;  30.52872347831726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17617012560367584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36869722604751587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.368703365325928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.534726142883301
19 1.5274718536 	 4.5347260346 	 4.5347260346
epoch_time;  30.326574325561523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17621538043022156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3687838613986969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.368321418762207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.534637451171875
It took 676.3397142887115 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn53: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135330.0

JOB STATISTICS
==============
Job ID: 2135330
Array Job ID: 2135328_2
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:27:36 core-walltime
Job Wall-clock time: 00:11:32
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
