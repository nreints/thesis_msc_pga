wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203637-7fmya0jj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-night-400
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7fmya0jj
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue â–ˆâ–…â–‚â–‚â–â–â–‚â–â–â–
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue â–ˆâ–ƒâ–„â–„â–â–â–„â–â–â–
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue â–ˆâ–…â–‚â–‚â–â–â–â–â–â–
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue â–ˆâ–…â–‚â–‚â–â–â–â–â–â–
wandb:                                  Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: ðŸš€ View run amber-night-400 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/7fmya0jj
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203637-7fmya0jj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204305-dibsk9ik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-flower-410
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/dibsk9ik
Training on dataset: data_t(0,0)_r(5,20)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(0,0)_r(5,20)_semi_pNone_gTrue', 'data_t(0,0)_r(5,20)_tennis_pNone_gTrue', 'data_t(0,0)_r(5,20)_combi_pNone_gTrue', 'data_t(0,0)_r(5,20)_full_pNone_gTrue']
Focussing on identity: False
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.39356064796448 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 13.359156131744385 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.855990648269653 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.899137258529663 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.912752389907837 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0001386254 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.37632e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.58649e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.8867e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.3743e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.95904779434204
Epoch 1/9
	 Logging train Loss: 6.0982e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.3588e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.8516e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.9235e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.883e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.505439519882202
Epoch 2/9
	 Logging train Loss: 3.6071e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.5912e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.4766e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.7786e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.853e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.43133306503296
Epoch 3/9
	 Logging train Loss: 2.0686e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.4775e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.2915e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.2262e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.69e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.499234199523926
Epoch 4/9
	 Logging train Loss: 1.7776e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8969e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.6155e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6986e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.322e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.65531063079834
Epoch 5/9
	 Logging train Loss: 1.6956e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8299e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.4796e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6309e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.856e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.711230993270874
Epoch 6/9
	 Logging train Loss: 1.6332e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.3166e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.9206e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.165e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.32e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.450156688690186
Epoch 7/9
	 Logging train Loss: 1.5846e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7288e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.2583e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.5428e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.723e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.775423288345337
Epoch 8/9
	 Logging train Loss: 1.5401e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7594e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.1596e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.5688e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.056e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.552279949188232
Epoch 9/9
	 Logging train Loss: 1.4959e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.6786e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.1009e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.4877e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.689e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.6135675907135
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  388.059166431427  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 50.785794258117676 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 13.204073905944824 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.33148980140686 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.697930097579956 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.810089826583862 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.000145666 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.39074e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.4346e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.7779e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.3852e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.919493436813354
Epoch 1/9
	 Logging train Loss: 6.3235e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.8154e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.0693e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.0389e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.588e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.534159898757935
Epoch 2/9
	 Logging train Loss: 3.9941e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.0831e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.1691e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.8836e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.394e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.60352373123169
Epoch 3/9
	 Logging train Loss: 2.2704e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.3475e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.7299e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.052e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.085e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.837169408798218
Epoch 4/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue â–ˆâ–„â–ƒâ–ƒâ–â–â–‚â–â–â–‚
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue â–ˆâ–…â–‚â–‚â–â–â–â–â–â–
wandb:                                  Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: ðŸš€ View run distinctive-flower-410 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/dibsk9ik
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204305-dibsk9ik/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204926-uv780f44
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-jazz-422
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/uv780f44
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb:                                  Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: ðŸš€ View run grateful-jazz-422 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/uv780f44
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204926-uv780f44/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205547-8sir6u5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-pond-432
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/8sir6u5h
	 Logging train Loss: 1.8182e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8431e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.1686e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6314e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.975e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.620203733444214
Epoch 5/9
	 Logging train Loss: 1.7041e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7583e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.0642e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.5754e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.809e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.764294385910034
Epoch 6/9
	 Logging train Loss: 1.644e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.835e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.0387e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6208e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.372e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.465378522872925
Epoch 7/9
	 Logging train Loss: 1.5916e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.6991e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.868e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.5058e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.68e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.585843086242676
Epoch 8/9
	 Logging train Loss: 1.542e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.6914e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.7486e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.4499e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.321e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.69271755218506
Epoch 9/9
	 Logging train Loss: 1.5007e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8673e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.8975e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6319e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.308e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.840462684631348
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  381.0822365283966  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 50.031373500823975 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 13.132099628448486 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.123404741287231 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.662868022918701 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 13.102774143218994 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.000143848 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.39544e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.60197e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.5539e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.3957e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.862144231796265
Epoch 1/9
	 Logging train Loss: 6.3195e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.9085e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.01121e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.9353e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.02e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.730841875076294
Epoch 2/9
	 Logging train Loss: 3.9021e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.8228e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.4295e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.6153e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.826e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.56761932373047
Epoch 3/9
	 Logging train Loss: 2.2145e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.1127e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.8243e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.7471e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.477e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.644378185272217
Epoch 4/9
	 Logging train Loss: 1.8136e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.0651e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.7113e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.7904e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.903e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.46003532409668
Epoch 5/9
	 Logging train Loss: 1.7103e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.9965e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.5664e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.736e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.804e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.75528907775879
Epoch 6/9
	 Logging train Loss: 1.6429e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.9986e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.4527e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.7087e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.825e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.524025917053223
Epoch 7/9
	 Logging train Loss: 1.5956e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7126e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.1118e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.43e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.122e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.63465642929077
Epoch 8/9
	 Logging train Loss: 1.5541e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8475e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.2208e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.5332e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.427e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.8543119430542
Epoch 9/9
	 Logging train Loss: 1.5093e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7312e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.0175e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.4402e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.927e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.677093982696533
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  380.99313735961914  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue â–ˆâ–…â–‚â–ƒâ–â–‚â–‚â–‚â–â–
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue â–ˆâ–…â–ƒâ–â–â–â–â–â–â–
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:                                  Train loss â–ˆâ–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: ðŸš€ View run helpful-pond-432 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/8sir6u5h
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205547-8sir6u5h/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210211-wfgoo8dv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-aardvark-443
wandb: â­ï¸ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: ðŸš€ View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/wfgoo8dv
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: ðŸš€ View run effortless-aardvark-443 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/wfgoo8dv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_210211-wfgoo8dv/logs
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 50.6513569355011 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 13.034220457077026 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.235410928726196 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.75993800163269 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 13.106094121932983 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
Datatype: pos_diff_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.000141107 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.39104e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.47581e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.6176e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.2556e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.88374137878418
Epoch 1/9
	 Logging train Loss: 5.9378e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.6307e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.3283e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.6786e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.066e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.548885345458984
Epoch 2/9
	 Logging train Loss: 3.8876e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.318e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.5495e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.1991e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.405e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.769661903381348
Epoch 3/9
	 Logging train Loss: 2.3033e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.4734e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.9438e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.2975e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.77e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 27.12760877609253
Epoch 4/9
	 Logging train Loss: 1.7921e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.9224e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.355e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.8326e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.761e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 27.93055820465088
Epoch 5/9
	 Logging train Loss: 1.6738e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.0003e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.3739e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.9235e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.231e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.740070581436157
Epoch 6/9
	 Logging train Loss: 1.616e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8781e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.229e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.8041e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.386e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.463731288909912
Epoch 7/9
	 Logging train Loss: 1.562e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8752e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.163e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.7933e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.596e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.792523860931396
Epoch 8/9
	 Logging train Loss: 1.5228e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.8585e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.0566e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.7193e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.077e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.47415590286255
Epoch 9/9
	 Logging train Loss: 1.4834e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.7379e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.9067e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.6374e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.564e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
		--> Epoch time; 26.774981021881104
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'pos_diff_1'_'False'.pth
It took  384.1740906238556  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/gru.py", line 295, in <module>
    model = model_pipeline(
  File "/gpfs/home2/nreints/MScThesis/code/utils.py", line 325, in model_pipeline
    ) = make(
  File "/gpfs/home2/nreints/MScThesis/code/utils.py", line 387, in make
    data_set_train = dataset_class(
  File "/gpfs/home2/nreints/MScThesis/code/dataset.py", line 29, in __init__
    self.collect_data()
  File "/gpfs/home2/nreints/MScThesis/code/dataset.py", line 35, in collect_data
    with open(f"{self.dir}/sim_{i}.pickle", "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'data/data_t(0,0)_r(5,20)_combi_pNone_gTrue/sim_1860.pickle'
srun: error: gcn36: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=3085904.0

JOB STATISTICS
==============
Job ID: 3085904
Array Job ID: 3085846_74
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:26:18
CPU Efficiency: 5.53% of 07:55:12 core-walltime
Job Wall-clock time: 00:26:24
Memory Utilized: 9.85 GB
Memory Efficiency: 0.00% of 0.00 MB
