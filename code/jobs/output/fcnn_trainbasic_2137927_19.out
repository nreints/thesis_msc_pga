wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162246-qtfnxc7g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-peony-1317
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/qtfnxc7g
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▇▃▄▂▅▅▂▂▃▁▃▃▃▁▂▂▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▁▃▃▄▅▂▅▂▃▁▄▃▃▃▄▆▅▆▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▁▂▁▃▄▂▃▂▁▂▂▃▁▂▁▁▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▃▂▃▃▁▄▂▄▁▃▂▃▂▃▃▂▄▃▃
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.73398
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.54826
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.8072
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.69425
wandb:                         Train loss 5.04072
wandb: 
wandb: 🚀 View run legendary-peony-1317 at: https://wandb.ai/nreints/thesis/runs/qtfnxc7g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162246-qtfnxc7g/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_163703-2t7cnhd9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-cake-1324
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2t7cnhd9
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0978624820709229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.070638656616211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2192755937576294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5020878314971924
0 8.6066291205 	 3.5020877323 	 3.5215117583
epoch_time;  40.200952768325806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5578787922859192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7502100467681885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9597718119621277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.320162057876587
1 6.0780522702 	 3.3201620566 	 3.3368853595
epoch_time;  39.76129412651062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6381197571754456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1312766075134277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9721322655677795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.410794973373413
2 5.755969792 	 3.4107949747 	 3.4237205712
epoch_time;  39.74262833595276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5582831501960754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1146745681762695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6995740532875061
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7767276763916016
3 5.5797038973 	 2.7767277898 	 2.7877401816
epoch_time;  39.43663311004639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6615010499954224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.360013008117676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8009622693061829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.954789876937866
4 5.4728813817 	 2.9547897751 	 2.9630328204
epoch_time;  39.34409785270691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6851377487182617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.403644561767578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7201052308082581
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7269985675811768
5 5.3981591433 	 2.726998489 	 2.7337412241
epoch_time;  39.31175661087036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4927109181880951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9911019802093506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8367499113082886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.119645118713379
6 5.3305905393 	 3.1196450723 	 3.1249396247
epoch_time;  39.331634283065796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7945932149887085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4407553672790527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.918713390827179
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0401318073272705
7 5.2639869272 	 3.04013177 	 3.0442626953
epoch_time;  39.22050189971924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5588457584381104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9327192306518555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7283934950828552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.726081132888794
8 5.2312469586 	 2.7260811471 	 2.7298268911
epoch_time;  39.38874125480652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7479387521743774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1646814346313477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.821124255657196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6868720054626465
9 5.1984636827 	 2.6868719977 	 2.6903356604
epoch_time;  40.14107584953308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5138722062110901
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7221083641052246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7667304873466492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.735886812210083
10 5.1646502888 	 2.7358868573 	 2.7391580118
epoch_time;  40.03403639793396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6481183171272278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3119704723358154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.696129322052002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.520673990249634
11 5.1382384717 	 2.5206740921 	 2.5241672825
epoch_time;  39.6815550327301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5592015385627747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0123651027679443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7652656435966492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7678232192993164
12 5.1168486454 	 2.7678232554 	 2.7713507575
epoch_time;  39.18172788619995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6564606428146362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1681129932403564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7963648438453674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.840841054916382
13 5.1128871869 	 2.8408410974 	 2.844823229
epoch_time;  39.48913860321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.568320631980896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.075385093688965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8110356330871582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8380842208862305
14 5.0931493182 	 2.8380842879 	 2.8422868058
epoch_time;  39.22085475921631
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6331824064254761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3688466548919678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6893883943557739
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.537670850753784
15 5.0685940938 	 2.5376708984 	 2.5421538482
epoch_time;  39.23628640174866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6811349391937256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6400749683380127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.769658625125885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7230756282806396
16 5.0708814452 	 2.723075743 	 2.7277422931
epoch_time;  38.93476414680481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6191027164459229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.467702865600586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7208058834075928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7009387016296387
17 5.0559483698 	 2.7009387867 	 2.7062026565
epoch_time;  39.16223192214966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7475643754005432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.71954345703125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6978811025619507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.625370740890503
18 5.05634078 	 2.6253706649 	 2.6309042441
epoch_time;  39.3888840675354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6943623423576355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5483975410461426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8072789311408997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7352583408355713
19 5.0407180941 	 2.7352583602 	 2.7408252613
epoch_time;  39.36086297035217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6942481994628906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5482635498046875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8071967959403992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.733980178833008
It took 857.2185409069061 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▅▃▄▂▄▂▃▃▄▂▃▅▃▂▁▅▄▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▃▃▅█▃▇▂▁▇▄▅▄▅▆▅▃▄▄▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▄▁▂▁▂▃▃▂▂▄▃▁▁▃▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▅▁▆▇▂▅▂▁█▃▄▂▃▅▂▂▃▁▃▃
wandb:                         Train loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.88657
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.50864
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.73088
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.62151
wandb:                         Train loss 5.04405
wandb: 
wandb: 🚀 View run vivid-cake-1324 at: https://wandb.ai/nreints/thesis/runs/2t7cnhd9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_163703-2t7cnhd9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165102-bzwjg491
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-wonton-1331
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/bzwjg491
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6998417377471924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9126715660095215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3288897275924683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.162963390350342
0 8.5945215968 	 4.1629635373 	 4.1842717351
epoch_time;  39.04043173789978
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7489933967590332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.285663366317749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9411107897758484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.253446102142334
1 6.1114292855 	 3.2534460119 	 3.2709185626
epoch_time;  39.1651611328125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5397264361381531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2467434406280518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8410714864730835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3974950313568115
2 5.792928418 	 3.3974949192 	 3.4119592615
epoch_time;  38.717358350753784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7997496128082275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5249133110046387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.854396402835846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.081801414489746
3 5.6114068054 	 3.0818012959 	 3.0935464395
epoch_time;  38.68447947502136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8986588716506958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9598546028137207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9196979999542236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.215641498565674
4 5.5096419844 	 3.215641496 	 3.2250211149
epoch_time;  38.90820550918579
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5513174533843994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1853225231170654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7006655335426331
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.930529832839966
5 5.4284051419 	 2.9305297852 	 2.9385085647
epoch_time;  39.21367621421814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7828760147094727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.736311912536621
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8037463426589966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.247401237487793
6 5.3600434527 	 3.247401222 	 3.2538026552
epoch_time;  39.19512462615967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5902203917503357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1246047019958496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6709514260292053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7274348735809326
7 5.3169275902 	 2.7274348079 	 2.7330803948
epoch_time;  38.710203647613525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5363866686820984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9524433612823486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7928714156150818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1574270725250244
8 5.2735327099 	 3.1574271537 	 3.161977737
epoch_time;  39.1504647731781
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9364799857139587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.762664318084717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8794304132461548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.002229690551758
9 5.2259290898 	 3.0022295978 	 3.0060767921
epoch_time;  38.91415596008301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.66705322265625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.374694585800171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8322528004646301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1693649291992188
10 5.1886795447 	 3.1693649704 	 3.1729907886
epoch_time;  39.29512166976929
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6835175156593323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.500701427459717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7686399817466736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8707427978515625
11 5.1640744012 	 2.8707427154 	 2.8743447793
epoch_time;  39.641377449035645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.585171103477478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3909754753112793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7667175531387329
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0309011936187744
12 5.138879816 	 3.0309012748 	 3.0343499261
epoch_time;  39.51175141334534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6307666897773743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.483172655105591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.920051634311676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.513643741607666
13 5.1089862263 	 3.5136438318 	 3.5173389332
epoch_time;  38.63217544555664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.732254683971405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.716801881790161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.865710973739624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.132399082183838
14 5.099602102 	 3.1323991105 	 3.1363489099
epoch_time;  38.70509934425354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5860258340835571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.499122142791748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6622684597969055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.733893632888794
15 5.0900685511 	 2.7338936471 	 2.738326284
epoch_time;  38.91844153404236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5834559798240662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2396340370178223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6683105230331421
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.601681709289551
16 5.0758019449 	 2.601681766 	 2.6064048973
epoch_time;  38.894400119781494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6144540905952454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.320434808731079
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8870615363121033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.488541603088379
17 5.0646488342 	 3.4885415567 	 3.4932825037
epoch_time;  39.25806665420532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5183880925178528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2925567626953125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8131197690963745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2592179775238037
18 5.0551800789 	 3.2592179582 	 3.2641670846
epoch_time;  39.25157117843628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6215581297874451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.503858804702759
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.73117595911026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.887214183807373
19 5.0440550683 	 2.8872142895 	 2.8927770666
epoch_time;  39.368324518203735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6215129494667053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.508638381958008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7308825254440308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8865692615509033
It took 838.7957010269165 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ███▇▆▄▃▅▃▄▆▁▃▃▃▅▄▅▂▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▁▇▅▄▃▃▆▂▂▁▃▅█▂▄▇▇█▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▃▃▂▂▃▃▃▄▁▂▃▁▄▃▃▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂█▃▃▂▃▆▂▂▂▄▄▇▁▃▆▄▆▃▃
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.97534
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.18662
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.72713
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.56143
wandb:                         Train loss 5.04811
wandb: 
wandb: 🚀 View run dancing-wonton-1331 at: https://wandb.ai/nreints/thesis/runs/bzwjg491
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165102-bzwjg491/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_170457-nwlwbxyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-fish-1340
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nwlwbxyg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.829456090927124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.039874792098999
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.129526138305664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3347229957580566
0 8.8314996615 	 3.3347230654 	 3.3548145191
epoch_time;  39.30034065246582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5032831430435181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.559054136276245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9337614178657532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3998639583587646
1 6.0910715608 	 3.3998640731 	 3.416192462
epoch_time;  39.177839040756226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.827996551990509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5336263179779053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.965084433555603
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3484911918640137
2 5.7750871435 	 3.3484912769 	 3.3614703204
epoch_time;  39.669761419296265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5795608758926392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.183682441711426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7894766926765442
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2365713119506836
3 5.5955219617 	 3.2365712759 	 3.2469997097
epoch_time;  39.372124910354614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5865091681480408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0443055629730225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7854610085487366
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.189790725708008
4 5.4890726822 	 3.1897906329 	 3.1983876821
epoch_time;  39.244781732559204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.534899890422821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9313337802886963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6845836043357849
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8439228534698486
5 5.4314859089 	 2.843922878 	 2.8508947424
epoch_time;  39.25560164451599
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5979010462760925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.917257070541382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7195050120353699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7337279319763184
6 5.3554320545 	 2.7337280273 	 2.739321487
epoch_time;  39.04843187332153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7222234010696411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3416531085968018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7982242703437805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0209479331970215
7 5.29770445 	 3.0209479255 	 3.0255483266
epoch_time;  38.72272777557373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5235993266105652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.674304246902466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.741277277469635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7716238498687744
8 5.2433181174 	 2.7716239311 	 2.7756634686
epoch_time;  38.429119348526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5196149349212646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7643327713012695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7457231879234314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8073790073394775
9 5.2341214376 	 2.8073789854 	 2.8109883076
epoch_time;  38.589499711990356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5006651878356934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.630157709121704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8335655331611633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.149118423461914
10 5.1759253872 	 3.1491184544 	 3.1525680954
epoch_time;  38.31039094924927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5984569191932678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.949679374694824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6148825287818909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.466776132583618
11 5.156301572 	 2.4667760901 	 2.4704230231
epoch_time;  38.65193963050842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6174539923667908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2051844596862793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6908804178237915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7498061656951904
12 5.129139977 	 2.7498061721 	 2.7535760003
epoch_time;  39.862465143203735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8006063103675842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.747408390045166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7454773783683777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7375099658966064
13 5.1149011404 	 2.7375098976 	 2.7413265744
epoch_time;  38.832685470581055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.469648540019989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7103028297424316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6259692907333374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6916847229003906
14 5.1029562669 	 2.6916847023 	 2.6959673907
epoch_time;  38.477370738983154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.549837052822113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0547049045562744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7990965247154236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9999217987060547
15 5.0813241183 	 2.999921809 	 3.0041474213
epoch_time;  38.394062757492065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7256153225898743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4942243099212646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7615970969200134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9006869792938232
16 5.0797485269 	 2.9006868929 	 2.9052764068
epoch_time;  38.38976788520813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.640275776386261
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.504181385040283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7887199521064758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0129196643829346
17 5.0781164047 	 3.0129196579 	 3.017615076
epoch_time;  38.76966381072998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7274225354194641
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7148821353912354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7077749967575073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.622513771057129
18 5.0555219089 	 2.6225138896 	 2.6276141192
epoch_time;  39.04521203041077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5617269277572632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.190563678741455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7274311184883118
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9764404296875
19 5.0481117367 	 2.9764404297 	 2.9819418417
epoch_time;  38.836955547332764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5614286065101624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1866157054901123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.727130115032196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.975342035293579
It took 834.8920443058014 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇█▅▅▄▁▃▂▄▃▆▁▂▂▄▂▂▄▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▆▄▆▅▅▁▂▂█▆▆▆▅▄▅▄▃▄▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▂▄▂▁▂▃▂▅▆▁▂▂▃▁▂▃▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▇▂▅▃▃▁▃▃█▆▅▅▄▃▄▃▃▃▃▃
wandb:                         Train loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.7122
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.31825
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.73815
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.55309
wandb:                         Train loss 5.0644
wandb: 
wandb: 🚀 View run thriving-fish-1340 at: https://wandb.ai/nreints/thesis/runs/nwlwbxyg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_170457-nwlwbxyg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171848-52xqd6w2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-rat-1348
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/52xqd6w2
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6465678215026855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.862051248550415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.069471836090088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3062307834625244
0 9.4366960054 	 3.3062308647 	 3.3281810864
epoch_time;  38.87362837791443
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8399317264556885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4147229194641113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1836978197097778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5007548332214355
1 6.1002731995 	 3.5007548564 	 3.5173428922
epoch_time;  38.35902976989746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5321481823921204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.12676739692688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7628767490386963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0593814849853516
2 5.7809875116 	 3.0593815984 	 3.0734084671
epoch_time;  38.50807499885559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7336854934692383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.376896381378174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8916492462158203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1128270626068115
3 5.6151299086 	 3.1128269505 	 3.1238040409
epoch_time;  38.34669637680054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5519888401031494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.163846015930176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7665240168571472
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8699755668640137
4 5.4957244811 	 2.8699756519 	 2.8784694362
epoch_time;  38.55026435852051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6096872091293335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.16159987449646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6760284900665283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.503969192504883
5 5.4174480709 	 2.5039692647 	 2.5108984045
epoch_time;  38.543882608413696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4395487606525421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.623919725418091
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7188557386398315
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7451014518737793
6 5.3519595384 	 2.7451013514 	 2.7506682525
epoch_time;  38.29675602912903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5629144310951233
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.810237169265747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7923644781112671
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.644726276397705
7 5.2977712292 	 2.6447262326 	 2.6494292388
epoch_time;  38.33473801612854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5714340806007385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7804465293884277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7445492148399353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.884864330291748
8 5.2368038438 	 2.884864271 	 2.8886788033
epoch_time;  38.931764125823975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9427922368049622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6846091747283936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.940072238445282
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.787661552429199
9 5.2059644365 	 2.7876616607 	 2.7912742821
epoch_time;  38.350064516067505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8249655365943909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3083231449127197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0094826221466064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.209148406982422
10 5.171860454 	 3.2091483451 	 3.2125488281
epoch_time;  38.598886251449585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6959144473075867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3797872066497803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7074366211891174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5366036891937256
11 5.1511268487 	 2.536603608 	 2.5406058647
epoch_time;  38.35113978385925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6937628388404846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3618969917297363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7234243154525757
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.580292224884033
12 5.1227228969 	 2.5802922429 	 2.5843055519
epoch_time;  38.2163827419281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6378551125526428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2409114837646484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7598304748535156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.629429817199707
13 5.1157005067 	 2.6294298327 	 2.6337803196
epoch_time;  38.381524324417114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5803133249282837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.009035587310791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.83601975440979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9121856689453125
14 5.0976172404 	 2.9121855865 	 2.9167087864
epoch_time;  40.30703520774841
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6277918815612793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.207233428955078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7098858952522278
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6143949031829834
15 5.092462683 	 2.6143948942 	 2.6194423366
epoch_time;  39.363985538482666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5591326951980591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.101306438446045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7512561082839966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.641313076019287
16 5.0808841031 	 2.6413130477 	 2.6467278558
epoch_time;  38.57003879547119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5566695928573608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9419825077056885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8250889182090759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9484126567840576
17 5.0732028473 	 2.948412756 	 2.9539088894
epoch_time;  38.77549147605896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5597307682037354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0504491329193115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.830188512802124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.793811082839966
18 5.0737797266 	 2.7938110352 	 2.7997063714
epoch_time;  38.563326358795166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5529963970184326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.317857265472412
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7377721667289734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7132246494293213
19 5.0644038671 	 2.7132246688 	 2.7193516087
epoch_time;  38.55645728111267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5530885457992554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.318246603012085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7381469011306763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.712197780609131
It took 830.8543095588684 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▁▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.4626
wandb:  Test loss t(-10, 10)_r(0, 0)_none 4.43028
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.15265
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.43774
wandb:                         Train loss 5.16175
wandb: 
wandb: 🚀 View run brilliant-rat-1348 at: https://wandb.ai/nreints/thesis/runs/52xqd6w2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171848-52xqd6w2/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173230-o5anm2lg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-rooster-1355
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/o5anm2lg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 8.132668495178223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 18.2778377532959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.302281379699707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.10077953338623
0 15.2958290136 	 11.1007799303 	 11.1233609586
epoch_time;  38.29854440689087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.145706653594971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.045249938964844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5984044075012207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.069778442382812
1 9.6692025114 	 8.0697786898 	 8.0904329867
epoch_time;  38.12197780609131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4601411819458008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.141180038452148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.185892105102539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.334680080413818
2 8.1982368981 	 5.3346798459 	 5.3531530432
epoch_time;  38.03737926483154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3978697061538696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.375586032867432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.282391309738159
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.804104328155518
3 6.3994755522 	 5.8041042019 	 5.8197061075
epoch_time;  38.002201318740845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4945613145828247
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.665754318237305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3886475563049316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.983824729919434
4 5.9776369171 	 5.9838246938 	 5.9972689242
epoch_time;  38.09597873687744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3889474868774414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.364664554595947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.391954183578491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.135803699493408
5 5.7460263097 	 6.1358035526 	 6.1484691723
epoch_time;  38.235830783843994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7677977085113525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.129917144775391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.296032428741455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.774145603179932
6 5.6277564584 	 5.7741455078 	 5.7862693993
epoch_time;  38.51929259300232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.419624924659729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.598906517028809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3895931243896484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.261207580566406
7 5.5496407149 	 6.2612073744 	 6.2725810283
epoch_time;  38.080076456069946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5198167562484741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.702566623687744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2046921253204346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.8272175788879395
8 5.5030189468 	 5.8272177207 	 5.838671875
epoch_time;  38.40518069267273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2252713441848755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.199743270874023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0347883701324463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.565778732299805
9 5.4492871821 	 5.5657787426 	 5.5766106683
epoch_time;  38.45858454704285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1134295463562012
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.069216728210449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0601718425750732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.6714935302734375
10 5.404870779 	 5.6714936128 	 5.6820688609
epoch_time;  38.51758670806885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1428282260894775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.0340189933776855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.164010763168335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.802888870239258
11 5.3595260796 	 5.8028887774 	 5.8124703072
epoch_time;  38.646952629089355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.286460280418396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.1071367263793945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9752252101898193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.439291477203369
12 5.3442729932 	 5.4392914643 	 5.4487185916
epoch_time;  37.99059748649597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0440032482147217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7098631858825684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.8975248336791992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.316962242126465
13 5.3005723172 	 5.3169621648 	 5.3256443333
epoch_time;  38.43416976928711
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2010658979415894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.966116189956665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.8497923612594604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.014540672302246
14 5.281091978 	 5.0145405537 	 5.0227958061
epoch_time;  38.41635799407959
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2127186059951782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.179287910461426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1151700019836426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.730203628540039
15 5.258066393 	 5.7302034945 	 5.7374452333
epoch_time;  38.562978744506836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2187304496765137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.0756516456604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.020587682723999
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.46243143081665
16 5.2298988131 	 5.4624313767 	 5.4692646748
epoch_time;  39.97170615196228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2949053049087524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.134340763092041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9311772584915161
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.127986431121826
17 5.2053042485 	 5.1279864337 	 5.1346861143
epoch_time;  38.97718381881714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.25985848903656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.289595127105713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.8352060317993164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.9900383949279785
18 5.1708739738 	 4.9900384027 	 4.9964084935
epoch_time;  38.62862968444824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.436927080154419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.429062843322754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1515614986419678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.462512969970703
19 5.161749032 	 5.4625131968 	 5.4683135162
epoch_time;  38.54447364807129
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4377351999282837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.4302849769592285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.152653932571411
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.462600231170654
It took 822.7983660697937 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▂▄▂▅▃▃▂▂▃▃▁▆▂▃▃▅▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆▁▇▃▃▂▅▇▄▂▃▅▃▃▃▄█▆▃▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▅▁▂▁▃▃▃▂▁▂▂▁▄▂▂▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▁█▂▁▁▂▇▄▁▂▃▂▁▂▂▄▂▁▁▁
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.82758
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.16713
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.68074
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.53513
wandb:                         Train loss 5.03499
wandb: 
wandb: 🚀 View run glistening-rooster-1355 at: https://wandb.ai/nreints/thesis/runs/o5anm2lg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173230-o5anm2lg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174622-1j5kx0ec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-orchid-1363
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1j5kx0ec
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9187952876091003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4459657669067383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3817667961120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8670895099639893
0 9.2967275569 	 3.8670895138 	 3.8879213075
epoch_time;  38.74057102203369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5370601415634155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.557713270187378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9852750301361084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.427521228790283
1 6.1116322677 	 3.4275212468 	 3.444843684
epoch_time;  38.848891258239746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.012770414352417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.684525728225708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.034314513206482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4380648136138916
2 5.7632288846 	 3.4380648226 	 3.4525318043
epoch_time;  38.89198350906372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5619807839393616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.985944986343384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7008985280990601
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.959843873977661
3 5.6047460805 	 2.959843816 	 2.9719894822
epoch_time;  38.65721368789673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5337064862251282
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.972567558288574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.787471354007721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.312490463256836
4 5.4923121324 	 3.3124904323 	 3.3219924514
epoch_time;  38.84415578842163
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5108071565628052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7210824489593506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7086523771286011
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9865784645080566
5 5.410829331 	 2.9865785341 	 2.9943121833
epoch_time;  38.51295208930969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6032294631004333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.180398464202881
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8482197523117065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.363826036453247
6 5.3548125291 	 3.3638259475 	 3.3701340134
epoch_time;  38.268131732940674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9278241991996765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.619623899459839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.838078498840332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.055354356765747
7 5.2953433771 	 3.0553542678 	 3.0607540646
epoch_time;  38.65895175933838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7083485126495361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.134586811065674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.840520441532135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0858216285705566
8 5.2537859153 	 3.0858216982 	 3.0903432485
epoch_time;  38.92726540565491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5234857797622681
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.703890323638916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7763431668281555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9679739475250244
9 5.2055687204 	 2.9679740287 	 2.9718934755
epoch_time;  38.598649978637695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5492116808891296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9033162593841553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7211282253265381
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9926910400390625
10 5.1868408038 	 2.9926909576 	 2.9963491079
epoch_time;  38.97594666481018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6880525350570679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2850470542907715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7365097403526306
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.006732225418091
11 5.1517539777 	 3.0067323427 	 3.0102103568
epoch_time;  39.03539562225342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5574452877044678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.946753978729248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8124470710754395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0681710243225098
12 5.1099208039 	 3.0681709908 	 3.0717822925
epoch_time;  38.9257493019104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5284202098846436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0006518363952637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6801461577415466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7580268383026123
13 5.1006385013 	 2.7580269478 	 2.7619256097
epoch_time;  39.08700966835022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5494776368141174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.977544069290161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9995750784873962
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5424914360046387
14 5.0821142281 	 3.5424913561 	 3.5462286872
epoch_time;  38.98257637023926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5491185188293457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1084420680999756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8117151856422424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.982006072998047
15 5.0784787312 	 2.9820061761 	 2.9861605258
epoch_time;  38.75524353981018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.73555588722229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7982563972473145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.819553792476654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1268484592437744
16 5.0574558596 	 3.1268485404 	 3.1311602618
epoch_time;  39.327823877334595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5876530408859253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4283223152160645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7500969171524048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0626351833343506
17 5.0456789258 	 3.0626352671 	 3.0672795476
epoch_time;  39.743250608444214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5154054760932922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9901633262634277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9080838561058044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3546528816223145
18 5.0455182591 	 3.3546528584 	 3.3597049198
epoch_time;  39.493757486343384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.535009503364563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1683883666992188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6804930567741394
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8284287452697754
19 5.0349856656 	 2.8284288561 	 2.8338725322
epoch_time;  38.56889843940735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5351336598396301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.167128801345825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6807357668876648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8275842666625977
It took 831.8950672149658 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9028195142745972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.292290210723877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1802715063095093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2126524448394775
0 9.4707209769 	 3.2126524229 	 3.2350444072
epoch_time;  38.50368094444275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5860512256622314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.797591209411621
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▃▇▆▄▄▂▃▄▅▄▆▂▂▁▃▂▃▁▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▁▅▆▅▁█▆▂▆▃▃▃▃█▄▃█▇▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▁▄▂▁▃▂▂▄▃▃▄▂▂▂▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▄▅▄▁▆▅▂▆▂▃▃▃▅▃▂▄▃▂▂
wandb:                         Train loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.61651
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.31881
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.69725
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.50674
wandb:                         Train loss 5.03877
wandb: 
wandb: 🚀 View run flashing-orchid-1363 at: https://wandb.ai/nreints/thesis/runs/1j5kx0ec
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174622-1j5kx0ec/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180009-7fihc6dp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-dumpling-1371
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7fihc6dp
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0312228202819824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0981907844543457
1 6.1289677397 	 3.09819072 	 3.1152350348
epoch_time;  38.590009689331055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.634161651134491
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.285381555557251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7225823402404785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.706139087677002
2 5.7854418797 	 2.706139147 	 2.7209215319
epoch_time;  38.40897512435913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7123376727104187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.415055274963379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.876091480255127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1416008472442627
3 5.6054475922 	 3.1416009027 	 3.1523114179
epoch_time;  38.966476917266846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.59234619140625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2861711978912354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.796280562877655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.048902988433838
4 5.484478144 	 3.0489030168 	 3.0578428526
epoch_time;  38.8492317199707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41972920298576355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7335705757141113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7069203853607178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8203630447387695
5 5.3975581309 	 2.8203629777 	 2.8272846944
epoch_time;  38.775354862213135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7558750510215759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.620696544647217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8377236723899841
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.818364381790161
6 5.3287108053 	 2.8183643238 	 2.8237888645
epoch_time;  38.455156326293945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6973337531089783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.358621120452881
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7908637523651123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.657168388366699
7 5.2776578104 	 2.6571684966 	 2.6615430677
epoch_time;  38.172019481658936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49925142526626587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.8275716304779053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7860842943191528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7205233573913574
8 5.2315772179 	 2.7205233187 	 2.7242048934
epoch_time;  38.98907995223999
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7409613728523254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4329819679260254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8810857534408569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.844670057296753
9 5.1893443609 	 2.8446701462 	 2.8482171136
epoch_time;  38.94282507896423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5195673108100891
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9850645065307617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8245435357093811
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9063808917999268
10 5.156963413 	 2.9063809781 	 2.9098058752
epoch_time;  38.89094400405884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5376999378204346
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.993671178817749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8273317217826843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8010659217834473
11 5.1250317366 	 2.8010659708 	 2.8044618349
epoch_time;  38.59568691253662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5519946813583374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.000391721725464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9016977548599243
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0310781002044678
12 5.109996509 	 3.0310781118 	 3.0348375475
epoch_time;  38.6454119682312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5325887203216553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.073770761489868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7784098982810974
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.684577465057373
13 5.0880003808 	 2.6845775707 	 2.6886410275
epoch_time;  38.30064129829407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.701740562915802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.682175636291504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7937333583831787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6469788551330566
14 5.0782717819 	 2.6469789247 	 2.6514127164
epoch_time;  38.68780827522278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.560407817363739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1760013103485107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7394075989723206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5617592334747314
15 5.0692777537 	 2.5617593301 	 2.5663866198
epoch_time;  38.4435453414917
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5106995701789856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.068995475769043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7927058935165405
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.745126724243164
16 5.0595676678 	 2.7451267552 	 2.7501088735
epoch_time;  38.72632908821106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6448331475257874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6865897178649902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7807695269584656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6863324642181396
17 5.0469470686 	 2.6863325789 	 2.6915519096
epoch_time;  38.92519521713257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5848790407180786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5543479919433594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7942218780517578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.777742385864258
18 5.0447317858 	 2.7777422931 	 2.7831932274
epoch_time;  39.105835914611816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5067429542541504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.31860089302063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6973264813423157
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.606261968612671
19 5.0387719115 	 2.6062618771 	 2.6121189427
epoch_time;  40.15530848503113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5067353248596191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3188114166259766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.697254478931427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6165125370025635
It took 826.6649296283722 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5973891615867615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.866427421569824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.076741099357605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3028337955474854
0 8.8084634976 	 3.3028336808 	 3.3245493296
epoch_time;  37.934935092926025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7126983404159546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2850584983825684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.86125248670578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0885493755340576
1 6.105781488 	 3.0885494748 	 3.1060487489
epoch_time;  38.226300954818726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6649141907691956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1878247261047363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.821137547492981
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.973743438720703
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▂▂▇▂▄▁▂▄▆▅▄▂▁▅▃▅▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▄▄▄▂█▆▅▃▃▇▅▁▄█▅▄▇▇▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▂▂▅▂▄▁▂▅▆▅▃▃▂▃▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▅▄▄▂▆▅▄▂▁█▄▂▂▆▃▂▃▂▃▃
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.72469
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.62077
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.66639
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.61064
wandb:                         Train loss 5.07207
wandb: 
wandb: 🚀 View run vibrant-dumpling-1371 at: https://wandb.ai/nreints/thesis/runs/7fihc6dp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180009-7fihc6dp/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_181327-lmjie1cp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-dragon-1379
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/lmjie1cp
2 5.78680391 	 2.9737433356 	 2.9887127851
epoch_time;  38.159995794296265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6409342288970947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.260637044906616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7294685244560242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.829617500305176
3 5.6176972696 	 2.829617557 	 2.842134053
epoch_time;  39.076706647872925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.551456868648529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.018573522567749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7311065793037415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.78277587890625
4 5.518845276 	 2.7827758789 	 2.7932821738
epoch_time;  38.633280754089355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7635490894317627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7119569778442383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9213072657585144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2337520122528076
5 5.4447739691 	 3.2337521115 	 3.2418367953
epoch_time;  38.45071601867676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6737782955169678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.47396183013916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7329328656196594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.826061964035034
6 5.3794292432 	 2.8260620117 	 2.8330480627
epoch_time;  38.59912395477295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6240386366844177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3233988285064697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8199099898338318
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.991196632385254
7 5.3276261399 	 2.991196751 	 2.9970419394
epoch_time;  38.288676738739014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5498226881027222
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1619977951049805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6917311549186707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7451274394989014
8 5.3042600473 	 2.745127415 	 2.7501925082
epoch_time;  38.226166009902954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47506359219551086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.113408327102661
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7327770590782166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.793461561203003
9 5.252607693 	 2.7934616501 	 2.7981653162
epoch_time;  38.27335548400879
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.864603579044342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6395742893218994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8970025181770325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.948225736618042
10 5.2263819979 	 2.9482256915 	 2.9521979255
epoch_time;  37.732667684555054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6694262623786926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3077945709228516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9354396462440491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.134880781173706
11 5.2028651564 	 3.134880767 	 3.138437632
epoch_time;  36.74153280258179
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5060224533081055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9068474769592285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8803645968437195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0821123123168945
12 5.1776831336 	 3.0821124103 	 3.0855815166
epoch_time;  35.93348670005798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5581071972846985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.233199119567871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7928907871246338
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9964468479156494
13 5.1432505448 	 2.9964467641 	 2.9998264622
epoch_time;  36.18370580673218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7362614274024963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7224724292755127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.778124988079071
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.837061882019043
14 5.1338403039 	 2.8370618666 	 2.8405860695
epoch_time;  36.093676805496216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5656982064247131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3747801780700684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7154392600059509
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.761641025543213
15 5.1194308829 	 2.7616410539 	 2.7654592153
epoch_time;  36.36623167991638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5323495268821716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.275667428970337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7867498397827148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.068958282470703
16 5.0958204844 	 3.0689581794 	 3.0728838946
epoch_time;  36.36104917526245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5829668045043945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.601173162460327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.748710572719574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.868487596511841
17 5.0854716989 	 2.8684877138 	 2.872668787
epoch_time;  36.14905405044556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5399901866912842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5423455238342285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7917919158935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.051435708999634
18 5.0847794437 	 3.0514358108 	 3.0557762352
epoch_time;  36.071449995040894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6102180480957031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.626406669616699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6661561131477356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.720440626144409
19 5.0720662793 	 2.7204405089 	 2.7252674
epoch_time;  36.24798107147217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6106443405151367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6207661628723145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6663939356803894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7246861457824707
It took 798.4263682365417 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.120407223701477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9538469314575195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2983107566833496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5170273780822754
0 9.2145158048 	 3.5170274889 	 3.53824001
epoch_time;  37.58660864830017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6773269772529602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3090999126434326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8695616126060486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8009188175201416
1 6.0562042464 	 2.8009188265 	 2.8173699456
epoch_time;  37.43806052207947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4735700786113739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0584003925323486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8186420202255249
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9641826152801514
2 5.7543485793 	 2.9641825908 	 2.9770365947
epoch_time;  36.9164981842041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7765994668006897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.972309112548828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8887195587158203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0968592166900635
3 5.5761500561 	 3.0968591639 	 3.1065040382
epoch_time;  36.03410863876343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4971149265766144
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▄▅▄▃▅▄▁▄▃▂▁▂▂▂▁▅▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▂█▁▁▇▂▂▂▆▇█▄▄▆▂▂▃▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▃▄▃▂▄▃▂▃▃▂▂▂▁▂▁▄▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▁▄▁▁▆▂▂▂▃▃▃▂▂▂▁▁▁▂▂
wandb:                         Train loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.63431
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.58658
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.76032
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.57593
wandb:                         Train loss 5.0434
wandb: 
wandb: 🚀 View run thriving-dragon-1379 at: https://wandb.ai/nreints/thesis/runs/lmjie1cp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_181327-lmjie1cp/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_182635-humlvbp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-noodles-1387
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/humlvbp4
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.965595006942749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.783316433429718
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.976452589035034
4 5.4613447867 	 2.9764526367 	 2.9843301309
epoch_time;  36.23046064376831
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47192612290382385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.990811824798584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7140061855316162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.781722068786621
5 5.3826111708 	 2.7817221152 	 2.7880331503
epoch_time;  35.94486427307129
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9102001190185547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8954710960388184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9603641629219055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9786016941070557
6 5.3234607074 	 2.9786017341 	 2.9832351272
epoch_time;  36.48287224769592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5535007119178772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1343600749969482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8392811417579651
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8546650409698486
7 5.2492105514 	 2.8546650655 	 2.8584878458
epoch_time;  36.2031672000885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5322794318199158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.174015522003174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7040226459503174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.460516929626465
8 5.2106988141 	 2.4605168523 	 2.4642002415
epoch_time;  36.3134446144104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5723937153816223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.159189224243164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8094226121902466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.850078582763672
9 5.1727163512 	 2.8500785209 	 2.8533952043
epoch_time;  35.967801570892334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6735953092575073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.637554407119751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8238798975944519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6804490089416504
10 5.1422333198 	 2.6804491198 	 2.6838349213
epoch_time;  36.55164813995361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6805976033210754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7741689682006836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7344515323638916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.542686939239502
11 5.1203350462 	 2.5426869985 	 2.5462900523
epoch_time;  36.23455548286438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6710272431373596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.987285852432251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7015814781188965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4942305088043213
12 5.1026243547 	 2.4942305281 	 2.4981503048
epoch_time;  37.2853946685791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5358359813690186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.356123924255371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.714603066444397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5156960487365723
13 5.0875547457 	 2.5156960977 	 2.5197885874
epoch_time;  36.4253876209259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5487531423568726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.403456687927246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6812477111816406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6391828060150146
14 5.0762148517 	 2.6391829207 	 2.643692594
epoch_time;  36.15723204612732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.594813883304596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.640458345413208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6949007511138916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5918128490448
15 5.0703630803 	 2.5918128761 	 2.5965793919
epoch_time;  36.209179401397705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.504206657409668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.129330635070801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6414185166358948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4422833919525146
16 5.0532996837 	 2.4422833417 	 2.4476080817
epoch_time;  36.431633949279785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4645231068134308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.056795597076416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9055764079093933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.084392786026001
17 5.0500284968 	 3.0843928157 	 3.0895824535
epoch_time;  36.39798545837402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4657369554042816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.282627820968628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6413712501525879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4359652996063232
18 5.0351698042 	 2.4359653782 	 2.4416228423
epoch_time;  36.74905443191528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.575605571269989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.583402156829834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7603313326835632
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.634869337081909
19 5.0433975251 	 2.6348692198 	 2.6406423208
epoch_time;  36.174763202667236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5759305953979492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.586582899093628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7603207230567932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.634308099746704
It took 787.9461257457733 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6166497468948364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.866703510284424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.025696039199829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4599480628967285
0 8.5175847368 	 3.4599480706 	 3.4814139701
epoch_time;  36.75081491470337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8743973970413208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5642359256744385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.04331636428833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.310365676879883
1 6.1121354255 	 3.310365749 	 3.3267112938
epoch_time;  36.42644000053406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6186195015907288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2781260013580322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8108963966369629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9778082370758057
2 5.8053295055 	 2.977808277 	 2.9913956926
epoch_time;  36.44816184043884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.983910322189331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.009552001953125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0788040161132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6743390560150146
3 5.6255441509 	 3.6743391707 	 3.6842489707
epoch_time;  36.107861042022705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6099775433540344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2179582118988037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6453428268432617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6110172271728516
4 5.5031425874 	 2.6110171756 	 2.6193041002
epoch_time;  36.930227518081665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.601327657699585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2943637371063232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7463488578796387
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▆▃█▁▂▃▄▂▇▂▁▄▄▄▄▅▄▅▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▅▄█▃▄▅▂▃▅▃▅▃▇▅▅▅▄▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▇▄█▁▃▃▄▃▇▂▁▄▄▄▅▄▄▄▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▆▃█▂▂▃▁▂▄▂▃▂▅▂▃▂▁▁▁▁
wandb:                         Train loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.39307
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.21037
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.82282
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.51639
wandb:                         Train loss 5.04848
wandb: 
wandb: 🚀 View run flashing-noodles-1387 at: https://wandb.ai/nreints/thesis/runs/humlvbp4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_182635-humlvbp4/logs
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.833864212036133
5 5.4136621557 	 2.8338642842 	 2.84025483
epoch_time;  36.92453598976135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6631911993026733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5564122200012207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7503053545951843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.900700092315674
6 5.3500665808 	 2.9007000897 	 2.9057382944
epoch_time;  36.25196957588196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5102490186691284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0881004333496094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8092690110206604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1057229042053223
7 5.285998593 	 3.1057227882 	 3.1097742029
epoch_time;  36.71878910064697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5585339069366455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.16066575050354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7257391214370728
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8372104167938232
8 5.2464792178 	 2.8372103304 	 2.8408015071
epoch_time;  36.38817095756531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.724753201007843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5245909690856934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0322939157485962
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4893152713775635
9 5.1944681735 	 3.4893152185 	 3.492504223
epoch_time;  36.55599021911621
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5576829314231873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.216249465942383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6795153021812439
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7713117599487305
10 5.1713725034 	 2.771311827 	 2.7746265308
epoch_time;  36.45889329910278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6302184462547302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4450185298919678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6279464960098267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6396234035491943
11 5.1395300863 	 2.6396233636 	 2.6433171453
epoch_time;  36.44163393974304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5562516450881958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.256463050842285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.827411949634552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0015602111816406
12 5.1231652262 	 3.0015601906 	 3.0053750528
epoch_time;  36.232237100601196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7989622950553894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.893000841140747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8412361741065979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.073847532272339
13 5.1081815705 	 3.0738475903 	 3.0778785499
epoch_time;  36.65397787094116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6044341921806335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4677088260650635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8073139786720276
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1024696826934814
14 5.1019053217 	 3.1024697793 	 3.1067570867
epoch_time;  36.60392427444458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6673937439918518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5407767295837402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.859981894493103
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.000666379928589
15 5.0879587265 	 3.0006664379 	 3.0052566116
epoch_time;  36.59406089782715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5940963625907898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.551931619644165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8093832731246948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.199096918106079
16 5.0819459578 	 3.1990970096 	 3.20390625
epoch_time;  36.17593765258789
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5222668051719666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4104461669921875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7956318855285645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.141120433807373
17 5.0629056338 	 3.1411205395 	 3.146393977
epoch_time;  36.53296136856079
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5206527709960938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4367616176605225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.835608184337616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2430763244628906
18 5.0584939119 	 3.2430763038 	 3.2485490129
epoch_time;  36.34510779380798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5163107514381409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.212958812713623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8225690722465515
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3925435543060303
19 5.0484793527 	 3.3925434834 	 3.3982461465
epoch_time;  36.593637228012085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5163910984992981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2103724479675293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8228246569633484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.393073320388794
It took 781.7538466453552 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139454
Array Job ID: 2137927_19
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-17:07:30 core-walltime
Job Wall-clock time: 02:17:05
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
