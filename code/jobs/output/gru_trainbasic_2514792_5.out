wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_133621-if5mpoz6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-snowball-513
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/if5mpoz6
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                    Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() █▂▂▂▁▂▁▁▁▁▁
wandb:                                               Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                    Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() 0.06165
wandb:                                               Train loss 0.04045
wandb: 
wandb: 🚀 View run ruby-snowball-513 at: https://wandb.ai/nreints/test/runs/if5mpoz6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_133621-if5mpoz6/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_134450-9ize58e9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-wildflower-527
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/9ize58e9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                    Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() █▃▄▂▂▁▁▁▁▁▁
wandb:                                               Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                    Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() 0.03158
wandb:                                               Train loss 0.03763
wandb: 
wandb: 🚀 View run solar-wildflower-527 at: https://wandb.ai/nreints/test/runs/9ize58e9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_134450-9ize58e9/logs
Running for data type: dual_quat
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 19.5109951289 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.5380179286003113 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 46.46383547782898
Epoch 1
	 Logging train Loss: 0.1864909438 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.10064000636339188 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.18745040893555
Epoch 2
	 Logging train Loss: 0.104546398 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.11477731168270111 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.80612754821777
Epoch 3
	 Logging train Loss: 0.0836530413 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.10957720875740051 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.13146376609802
Epoch 4
	 Logging train Loss: 0.0740558079 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.06921622157096863 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.604976654052734
Epoch 5
	 Logging train Loss: 0.0645493633 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.08781817555427551 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.59516930580139
Epoch 6
	 Logging train Loss: 0.0557406824 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.044434741139411926 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.964765310287476
Epoch 7
	 Logging train Loss: 0.0467092771 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.035572174936532974 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.150779724121094
Epoch 8
	 Logging train Loss: 0.0408688432 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.046578411012887955 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.93010115623474
Epoch 9
	 Logging train Loss: 0.0404547677 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.0616978220641613 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.84928750991821
	 Logging test loss: 0.061645932495594025 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
It took  509.9119281768799  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(8, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=8, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 16.5516913176 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.39875397086143494 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 45.2436900138855
Epoch 1
	 Logging train Loss: 0.1698765163 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.1208743304014206 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.60133504867554
Epoch 2
	 Logging train Loss: 0.1146144383 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.20017658174037933 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.31105160713196
Epoch 3
	 Logging train Loss: 0.0945146202 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.07377488911151886 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.54513430595398
Epoch 4
	 Logging train Loss: 0.0771575031 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.0642499178647995 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.32337284088135
Epoch 5
	 Logging train Loss: 0.06779645 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.046924982219934464 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.2030816078186
Epoch 6
	 Logging train Loss: 0.0562259435 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.047427963465452194 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.18835234642029
Epoch 7
	 Logging train Loss: 0.0490703761 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.04332117363810539 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.51357173919678
Epoch 8
	 Logging train Loss: 0.0418361745 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.04346957430243492 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 44.19321846961975
Epoch 9
	 Logging train Loss: 0.0376348534 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.03158630430698395 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 43.889134645462036
	 Logging test loss: 0.031584251672029495 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
It took  506.64436960220337  seconds.

JOB STATISTICS
==============
Job ID: 2514797
Array Job ID: 2514792_5
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 05:09:54 core-walltime
Job Wall-clock time: 00:17:13
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
