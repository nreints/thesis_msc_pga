/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_171404-5vqwgar5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-pig-1155
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/5vqwgar5
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–ƒâ–â–ƒâ–‚â–‚â–â–‚â–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‡â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–ƒâ–â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.23538
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.20598
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.65709
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.46083
wandb:                         Train loss 7.98864
wandb: 
wandb: ðŸš€ View run beaming-pig-1155 at: https://wandb.ai/nreints/thesis/runs/5vqwgar5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_171404-5vqwgar5/logs
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0797526836395264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5584933757781982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3676687479019165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.622406005859375
0 20.7689044516 	 2.6224060059 	 2.6556379988
epoch_time;  34.84947347640991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5429392457008362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.336491584777832
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.941845178604126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7919211387634277
1 10.386078728 	 1.7919210898 	 1.8213629645
epoch_time;  34.592435121536255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9376384615898132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8384978771209717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9933266043663025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6537328958511353
2 9.4804198227 	 1.6537328772 	 1.680546941
epoch_time;  34.73301458358765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5964348912239075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5295989513397217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8549188375473022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5482136011123657
3 9.0323991304 	 1.5482136494 	 1.5712366053
epoch_time;  34.656656980514526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5724678039550781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4787837266921997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7621216773986816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.428390622138977
4 8.7942692906 	 1.4283905854 	 1.450160341
epoch_time;  34.73550820350647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5968019962310791
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4339505434036255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7313075065612793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3250569105148315
5 8.6303548948 	 1.3250569112 	 1.344693653
epoch_time;  34.93195676803589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5310631394386292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.269539475440979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7738669514656067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4681178331375122
6 8.5229323587 	 1.4681178737 	 1.4854731379
epoch_time;  34.79168725013733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46631547808647156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.123998999595642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7751397490501404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4241883754730225
7 8.4565624736 	 1.4241883974 	 1.4403345056
epoch_time;  34.11392045021057
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40513527393341064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0260857343673706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7516464591026306
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3963621854782104
8 8.3657170362 	 1.3963621397 	 1.4109963907
epoch_time;  34.433573484420776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3900967538356781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1141109466552734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6558106541633606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.262273907661438
9 8.3116510452 	 1.2622739225 	 1.2757096574
epoch_time;  34.38617920875549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5727582573890686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3564952611923218
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7415928244590759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3248704671859741
10 8.2795196926 	 1.3248705065 	 1.3368164887
epoch_time;  34.57555031776428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3763737082481384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0031461715698242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7529391646385193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4500608444213867
11 8.2182309739 	 1.4500608702 	 1.4607037518
epoch_time;  34.17353558540344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.645257294178009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.50599205493927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7939227223396301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4277364015579224
12 8.1847527981 	 1.4277363545 	 1.4373361948
epoch_time;  34.39220571517944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5186769366264343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2044146060943604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7067533135414124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3745813369750977
13 8.1464645396 	 1.3745813318 	 1.3837241508
epoch_time;  34.507084131240845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48600831627845764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2398834228515625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6792522668838501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2496329545974731
14 8.0969071333 	 1.2496329643 	 1.2580698374
epoch_time;  34.58845829963684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4450906217098236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.066220760345459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.671327531337738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.250565528869629
15 8.0878595496 	 1.2505654825 	 1.259194039
epoch_time;  34.30451989173889
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5212773084640503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.098562479019165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7797127366065979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3873947858810425
16 8.0724963767 	 1.3873947556 	 1.3958327835
epoch_time;  34.53552579879761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5020952820777893
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.242927074432373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6730630993843079
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2115918397903442
17 8.0264432232 	 1.2115918959 	 1.2198740366
epoch_time;  35.68099117279053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4331586956977844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9601747393608093
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.696725606918335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3167340755462646
18 8.0184455928 	 1.3167340253 	 1.3255384291
epoch_time;  35.17693328857422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4608980417251587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2063485383987427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6572294235229492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2351083755493164
19 7.9886396094 	 1.2351084116 	 1.2438511204
epoch_time;  35.04409956932068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4608251750469208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2059801816940308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6570883393287659
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2353827953338623
It took 751.7873001098633 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn41: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135473.0

JOB STATISTICS
==============
Job ID: 2135473
Array Job ID: 2135328_26
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:49:30 core-walltime
Job Wall-clock time: 00:12:45
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
