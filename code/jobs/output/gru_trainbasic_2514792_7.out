wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_133618-ra6iex24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-dust-507
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/ra6iex24
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: / 0.031 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                    Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() █▃▂▁▁▁▁▁▁▁▁
wandb:                                               Train loss █▃▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                    Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() 0.33964
wandb:                                               Train loss 0.32711
wandb: 
wandb: 🚀 View run copper-dust-507 at: https://wandb.ai/nreints/test/runs/ra6iex24
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_133618-ra6iex24/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_134349-hpc1n39z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-plasma-520
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/hpc1n39z
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                    Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() █▃▂▁▁▁▁▁▁▁▁
wandb:                                               Train loss █▃▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                    Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_tennis_pNone_gNone, MSELoss() 0.34873
wandb:                                               Train loss 0.32182
wandb: 
wandb: 🚀 View run resilient-plasma-520 at: https://wandb.ai/nreints/test/runs/hpc1n39z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_134349-hpc1n39z/logs
Running for data type: pos_diff_start
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 294.5238860147 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 114.10794830322266 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 40.570133447647095
Epoch 1
	 Logging train Loss: 72.9622204606 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 39.92637252807617 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 37.828675508499146
Epoch 2
	 Logging train Loss: 27.6348308161 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 16.54608726501465 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.035693645477295
Epoch 3
	 Logging train Loss: 11.6623910005 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 7.475799083709717 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 37.98857140541077
Epoch 4
	 Logging train Loss: 5.1856456395 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 3.5475804805755615 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.20134234428406
Epoch 5
	 Logging train Loss: 2.4297880573 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 1.8492435216903687 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 37.80834698677063
Epoch 6
	 Logging train Loss: 1.2419665022 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 1.0480432510375977 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.024574995040894
Epoch 7
	 Logging train Loss: 0.7270915257 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.6643958687782288 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 37.98203110694885
Epoch 8
	 Logging train Loss: 0.4741047145 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.4757327735424042 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.189921140670776
Epoch 9
	 Logging train Loss: 0.3271103211 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.3391337990760803 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.28028178215027
	 Logging test loss: 0.3396419882774353 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
It took  451.76765513420105  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 293.9928654873 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 112.59420013427734 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 39.323017597198486
Epoch 1
	 Logging train Loss: 70.7624697513 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 38.37565994262695 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.67647647857666
Epoch 2
	 Logging train Loss: 26.2265911841 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 16.007869720458984 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.433507204055786
Epoch 3
	 Logging train Loss: 10.9450752044 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 7.140748977661133 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 37.923786640167236
Epoch 4
	 Logging train Loss: 4.8474883829 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 3.4742205142974854 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.37256836891174
Epoch 5
	 Logging train Loss: 2.2662633837 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 1.7806090116500854 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 40.648019313812256
Epoch 6
	 Logging train Loss: 1.1656278195 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 1.0357762575149536 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 40.236422300338745
Epoch 7
	 Logging train Loss: 0.6915869108 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.6905743479728699 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 39.98554873466492
Epoch 8
	 Logging train Loss: 0.4533991285 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.47559377551078796 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.53335452079773
Epoch 9
	 Logging train Loss: 0.3218174363 (MSELoss(): data_t(0, 0)_r(5, 20)_tennis_pNone_gNone)
	 Logging test loss: 0.3485163152217865 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
     --> Epoch time; 38.498340368270874
	 Logging test loss: 0.3487255871295929 (MSELoss(): t(0, 0)_r(5, 20)_tennis_pNone_gNone)
It took  455.4274916648865  seconds.

JOB STATISTICS
==============
Job ID: 2514799
Array Job ID: 2514792_7
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 02:56:22
CPU Efficiency: 63.83% of 04:36:18 core-walltime
Job Wall-clock time: 00:15:21
Memory Utilized: 28.22 GB
Memory Efficiency: 90.29% of 31.25 GB
