wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162420-1cx7zq6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-wish-1318
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/1cx7zq6t
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▆▆▅▃▅▄▆▃▆▄▃▂▁▂▄▄▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▇█▇█▆▄▆▅█▃█▅▄▂▁▂▅▅▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▇▆▇▅▃▅▄▆▃▆▄▃▂▁▂▄▄▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▅█▆▇▅▂▅▄▇▂▇▃▃▁▁▁▅▄▂▂
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 12.84426
wandb:  Test loss t(-10, 10)_r(0, 0)_none 6.81513
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.40988
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.5713
wandb:                         Train loss 10.76659
wandb: 
wandb: 🚀 View run cheerful-wish-1318 at: https://wandb.ai/nreints/thesis/runs/1cx7zq6t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162420-1cx7zq6t/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_163949-7qmx7si6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-ox-1325
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7qmx7si6
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7621612548828125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.535414695739746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.342375755310059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.269681930541992
0 22.239241283 	 20.2696816934 	 20.2696816934
epoch_time;  50.54987955093384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4004111289978027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.354473114013672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.516165733337402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.372556686401367
1 13.7872984724 	 18.3725572741 	 18.3725572741
epoch_time;  46.22978496551514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.1277284622192383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.006087303161621
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.735690116882324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.966434478759766
2 12.8749259479 	 17.9664352829 	 17.9664352829
epoch_time;  48.54442286491394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5387191772460938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.424979209899902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.703532695770264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.978641510009766
3 12.5446071534 	 16.9786409945 	 16.9786409945
epoch_time;  47.865731954574585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.867772340774536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.985838890075684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.45166301727295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.941965103149414
4 12.2231587466 	 17.9419644742 	 17.9419644742
epoch_time;  45.48383665084839
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.254452705383301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.909380912780762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.389993667602539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.623035430908203
5 11.8654485972 	 16.6230363176 	 16.6230363176
epoch_time;  47.548224687576294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3991475105285645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.8297271728515625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.8346171379089355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.095361709594727
6 11.8544664015 	 14.0953613281 	 14.0953613281
epoch_time;  47.70471143722534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.222208023071289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.768574714660645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.096872806549072
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.38031005859375
7 11.4520605006 	 16.3803103885 	 16.3803103885
epoch_time;  46.63130283355713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.03374981880188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.103607177734375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.410569190979004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.901123046875
8 11.4471122313 	 14.9011230469 	 14.9011230469
epoch_time;  48.11452841758728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.783864974975586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.883196830749512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.949038505554199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.25444984436035
9 11.4693781208 	 17.2544499578 	 17.2544499578
epoch_time;  47.41800332069397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4422779083251953
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.701879024505615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.437010765075684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.575026512145996
10 11.2824930179 	 13.5750263936 	 13.5750263936
epoch_time;  39.41636371612549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.747785806655884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.66877269744873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.833391189575195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.19697380065918
11 11.1622561569 	 17.1969739759 	 17.1969739759
epoch_time;  38.84092569351196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7375012636184692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.732771396636963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.119558334350586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.473272323608398
12 11.2538097178 	 14.4732725401 	 14.4732725401
epoch_time;  37.60196232795715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5861327648162842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.099215507507324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.5579023361206055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.497509956359863
13 11.0281312151 	 13.4975097656 	 13.4975097656
epoch_time;  37.30310940742493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2121481895446777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.61140251159668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.659698963165283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.855670928955078
14 10.8634293543 	 11.8556706609 	 11.8556706609
epoch_time;  38.799898624420166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1611907482147217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.92512845993042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0384416580200195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.934791564941406
15 10.8855431724 	 10.9347913587 	 10.9347913587
epoch_time;  38.60343050956726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.203453540802002
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.978583812713623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.459402084350586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.614934921264648
16 10.8377147512 	 11.6149348079 	 11.6149348079
epoch_time;  38.79355597496033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1905102729797363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.891239166259766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.276281833648682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.544468879699707
17 10.8538812574 	 14.5444692251 	 14.5444692251
epoch_time;  38.41228914260864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8864706754684448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.007946014404297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.404637813568115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.758406639099121
18 10.9061799149 	 14.7584063556 	 14.7584063556
epoch_time;  38.28133201599121
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5717291831970215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.8180108070373535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4076008796691895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.840088844299316
19 10.7665942895 	 12.8400892103 	 12.8400892103
epoch_time;  38.52421689033508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5712976455688477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.815129280090332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.409878730773926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.844256401062012
It took 929.069518327713 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▃▃▂▄▃▁▃▂▁▁▂▂▁▁▁▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▆▄█▆▅▄▆▆▃▄▃▅▁▂▄▄▂▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▄▆▄▄▃▄▄▂▃▂▄▁▂▃▂▁▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▅▃█▅▄▂▅▅▃▃▂▅▁▂▃▃▁▅▅
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 109.54911
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.5519
wandb:    Test loss t(0, 0)_r(-5, 5)_none 6.25359
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.09621
wandb:                         Train loss 10.98408
wandb: 
wandb: 🚀 View run golden-ox-1325 at: https://wandb.ai/nreints/thesis/runs/7qmx7si6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_163949-7qmx7si6/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165348-7viip4ke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-mandu-1335
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7viip4ke
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7707607746124268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.89182186126709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.132516860961914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 298.7142639160156
0 22.5158656694 	 298.7142736486 	 298.7142736486
epoch_time;  38.500927686691284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7626978158950806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.152644157409668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.970189094543457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 169.64724731445312
1 13.8501594766 	 169.6472445101 	 169.6472445101
epoch_time;  38.79595422744751
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.194483995437622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.063652992248535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.424617767333984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 92.33447265625
2 13.0547972558 	 92.3344700169 	 92.3344700169
epoch_time;  38.69733142852783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7818913459777832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.206625938415527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.166439056396484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 82.14691925048828
3 12.5251764028 	 82.1469225084 	 82.1469225084
epoch_time;  38.62425994873047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.758507013320923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.11489486694336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.852939128875732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 100.22747039794922
4 12.12961239 	 100.2274704392 	 100.2274704392
epoch_time;  38.32653307914734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.208585739135742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.13807201385498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.539806842803955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 60.114776611328125
5 12.0574353631 	 60.1147751267 	 60.1147751267
epoch_time;  38.3968505859375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9301890134811401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.595560073852539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.321751117706299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 143.94711303710938
6 11.9043233222 	 143.9471072635 	 143.9471072635
epoch_time;  38.6163911819458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5757337808609009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.731464385986328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.443786144256592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 92.0188980102539
7 11.7358520756 	 92.0188978041 	 92.0188978041
epoch_time;  38.458659410476685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.075334072113037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.925943374633789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.391998767852783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.3461799621582
8 11.6135853927 	 33.3461782095 	 33.3461782095
epoch_time;  38.600001096725464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1573150157928467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.030193328857422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.368960380554199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 103.38905334472656
9 11.5467864019 	 103.3890519426 	 103.3890519426
epoch_time;  38.62540411949158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.583184003829956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.376774787902832
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.011653900146484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 53.98332595825195
10 11.4787205907 	 53.9833245355 	 53.9833245355
epoch_time;  38.31272864341736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6903420686721802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.850804805755615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.446694850921631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.3564453125
11 11.2018753703 	 22.3564453125 	 22.3564453125
epoch_time;  38.39250898361206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4135044813156128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.204640865325928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.048536777496338
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.44187545776367
12 11.2953675102 	 32.4418760557 	 32.4418760557
epoch_time;  38.22446966171265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0685479640960693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.693387985229492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.337008953094482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.03337860107422
13 11.1207327459 	 39.033379962 	 39.033379962
epoch_time;  37.92961120605469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.250322699546814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.33595085144043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.359737396240234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.02711486816406
14 11.1921864421 	 63.0271167652 	 63.0271167652
epoch_time;  38.08500599861145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4406036138534546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.090935230255127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.297607421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.940996170043945
15 11.1467223997 	 18.9409958298 	 18.9409958298
epoch_time;  38.314369678497314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.78848135471344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.020380020141602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.5084123611450195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.778100967407227
16 11.0592582357 	 27.7781012458 	 27.7781012458
epoch_time;  38.27283835411072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.592968225479126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.843994140625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.377999305725098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.817567825317383
17 11.1086516693 	 26.8175675676 	 26.8175675676
epoch_time;  38.00385332107544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3102060556411743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.644688129425049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.616400718688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 46.0857048034668
18 10.9481602012 	 46.0857052365 	 46.0857052365
epoch_time;  38.23821449279785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0958306789398193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.552972793579102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.259561538696289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 109.5535888671875
19 10.9840781303 	 109.553589527 	 109.553589527
epoch_time;  37.96742606163025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0962092876434326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.551898956298828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.253593921661377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 109.54911041259766
It took 839.7355530261993 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▅▄▃▆▄▆▆▆▄▅▄▅▁▃▆▇▃▅▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄█▄▃▃▄▄▆▆▅▃▄▃▅▁▃▅▆▃▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▄▃▃▆▄▇▆▆▄▅▅▅▁▄▅▇▄▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃█▃▂▂▄▃▆█▅▂▄▃▆▁▂▅▇▃▅▅
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 13.02754
wandb:  Test loss t(-10, 10)_r(0, 0)_none 6.47603
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.22098
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.69306
wandb:                         Train loss 10.20814
wandb: 
wandb: 🚀 View run bright-mandu-1335 at: https://wandb.ai/nreints/thesis/runs/7viip4ke
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165348-7viip4ke/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_170732-kcug7tzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-rocket-1342
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kcug7tzg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3312197923660278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.330245018005371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.12745475769043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.893994331359863
0 19.572096363 	 14.8939941406 	 14.8939941406
epoch_time;  38.98943042755127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0177419185638428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.00091552734375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.015248775482178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.985289573669434
1 13.1605159318 	 14.9852895376 	 14.9852895376
epoch_time;  39.20706510543823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3734900951385498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.249258995056152
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.914052486419678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.303915023803711
2 12.268232693 	 13.3039154878 	 13.3039154878
epoch_time;  38.75376772880554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.22123384475708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.850647926330566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.400125503540039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.805310249328613
3 11.7995686463 	 12.8053103885 	 12.8053103885
epoch_time;  38.54209327697754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2209552526474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.872585773468018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.317387580871582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.250765800476074
4 11.6003578305 	 12.2507654139 	 12.2507654139
epoch_time;  38.54306364059448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5516409873962402
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.452967166900635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.531530857086182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.854430198669434
5 11.2847648316 	 13.8544301626 	 13.8544301626
epoch_time;  38.063231468200684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3649749755859375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.31149435043335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.848734378814697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.712621688842773
6 11.2061941964 	 12.7126214105 	 12.7126214105
epoch_time;  37.67162847518921
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.72837495803833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.024616718292236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.732671737670898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.844218254089355
7 10.9789255564 	 13.8442184861 	 13.8442184861
epoch_time;  38.4648756980896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9894461631774902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.299495697021484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.604280471801758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.012683868408203
8 10.9819868187 	 14.0126834354 	 14.0126834354
epoch_time;  38.05716514587402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6607520580291748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.755331039428711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.562071323394775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.660136222839355
9 10.728717946 	 13.6601364548 	 13.6601364548
epoch_time;  38.173720598220825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3063644170761108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.869069576263428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.780720233917236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.544520378112793
10 10.7066623117 	 12.5445206926 	 12.5445206926
epoch_time;  38.276886224746704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.46558678150177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.109134674072266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.15311336517334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.541680335998535
11 10.6017311008 	 13.5416807432 	 13.5416807432
epoch_time;  38.00921893119812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3150768280029297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.856337070465088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0225019454956055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.615316390991211
12 10.6082833922 	 12.6153161951 	 12.6153161951
epoch_time;  38.205546140670776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.740583896636963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.83935022354126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.270239353179932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.164031028747559
13 10.5976179016 	 13.1640308277 	 13.1640308277
epoch_time;  37.843491077423096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1188290119171143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.036713600158691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7729947566986084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.073934555053711
14 10.4130211419 	 11.073935019 	 11.073935019
epoch_time;  38.365336418151855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2455542087554932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.758692264556885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.744443416595459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.313772201538086
15 10.3565418447 	 12.3137721706 	 12.3137721706
epoch_time;  38.447160482406616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6692346334457397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.813058853149414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.214120864868164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.788337707519531
16 10.3039906165 	 13.7883379962 	 13.7883379962
epoch_time;  38.112765312194824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9228136539459229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.233348369598389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.921664714813232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.434467315673828
17 10.3587872112 	 14.4344673775 	 14.4344673775
epoch_time;  37.99024438858032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3321142196655273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.791881561279297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.631885528564453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.17335319519043
18 10.3053995906 	 12.1733530405 	 12.1733530405
epoch_time;  38.34361028671265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6933268308639526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.473003387451172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.223214149475098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.029664039611816
19 10.2081440155 	 13.0296637458 	 13.0296637458
epoch_time;  38.11479330062866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.693056344985962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.476032733917236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.220983028411865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.027544021606445
It took 823.766637802124 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▁▅▃▂▂▄▅▄▆▃▇▂▃▆▇▇█▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▁▅▆▅▃▂▆▃▂▄█▃▄▃▇▂▃▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▁▃▄▃▁▂▃▂▁▃▅▂▂▂▄▁▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▁▄▅▄▂▁▅▂▁▃█▂▃▂▆▂▂▂▁▁
wandb:                         Train loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 111.61476
wandb:  Test loss t(-10, 10)_r(0, 0)_none 6.60015
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.06262
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.36071
wandb:                         Train loss 10.78188
wandb: 
wandb: 🚀 View run dancing-rocket-1342 at: https://wandb.ai/nreints/thesis/runs/kcug7tzg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_170732-kcug7tzg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_172108-znjx7t65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run virtuous-springroll-1349
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/znjx7t65
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.231515884399414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.178508758544922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.452109336853027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 416.8547668457031
0 21.3444072127 	 416.8547719595 	 416.8547719595
epoch_time;  38.04092597961426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3506157398223877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.068718910217285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.035719394683838
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.153202056884766
1 13.704960435 	 25.1532015414 	 25.1532015414
epoch_time;  38.59905815124512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.166104316711426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.530219078063965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.897975444793701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 246.03030395507812
2 12.7028975542 	 246.0302998311 	 246.0302998311
epoch_time;  38.40670442581177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.3852803707122803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.226407051086426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.677961349487305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 143.19976806640625
3 12.1877697613 	 143.1997677365 	 143.1997677365
epoch_time;  38.019670724868774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.2344119548797607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.706815719604492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.599822044372559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77.08502197265625
4 12.0550471818 	 77.0850242821 	 77.0850242821
epoch_time;  38.103967905044556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6293610334396362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.312009811401367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.3894734382629395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 90.9390869140625
5 11.7579262229 	 90.9390836149 	 90.9390836149
epoch_time;  37.76096844673157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2904648780822754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.983593940734863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.879452705383301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 183.19703674316406
6 11.5206883146 	 183.1970439189 	 183.1970439189
epoch_time;  37.71822166442871
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4061777591705322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.071699142456055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.924653053283691
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 247.45692443847656
7 11.5047208235 	 247.4569256757 	 247.4569256757
epoch_time;  38.362844944000244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6561381816864014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.517736911773682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.721286296844482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 171.08302307128906
8 11.3716610422 	 171.0830236486 	 171.0830236486
epoch_time;  38.3808856010437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2642451524734497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.511474132537842
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.194878578186035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 324.3459167480469
9 11.1928277865 	 324.3459037162 	 324.3459037162
epoch_time;  38.12010908126831
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8157341480255127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.81108283996582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.652256488800049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 121.2125244140625
10 11.2188002497 	 121.2125211149 	 121.2125211149
epoch_time;  38.154757022857666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.3555097579956055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.398200988769531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.034502029418945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 367.1662902832031
11 11.1285280552 	 367.1663006757 	 367.1663006757
epoch_time;  37.80788540840149
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4894506931304932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.111271381378174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.542796611785889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 61.040367126464844
12 11.1216028584 	 61.0403663429 	 61.0403663429
epoch_time;  37.88617563247681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.759538173675537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.679626941680908
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.1624603271484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 137.37599182128906
13 11.0123693509 	 137.3759923986 	 137.3759923986
epoch_time;  38.07988786697388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6553713083267212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.593814849853516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.68508768081665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 323.6988220214844
14 10.9645098542 	 323.6988175676 	 323.6988175676
epoch_time;  38.14895009994507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.69270920753479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.634940147399902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.465786457061768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 347.04986572265625
15 10.8655649649 	 347.0498521959 	 347.0498521959
epoch_time;  38.031842947006226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4294798374176025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.819201946258545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.289580345153809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 340.46685791015625
16 10.9981103483 	 340.466870777 	 340.466870777
epoch_time;  37.98283576965332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6027016639709473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.045436382293701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.681958198547363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 412.7574462890625
17 10.7087661857 	 412.7574324324 	 412.7574324324
epoch_time;  37.896061420440674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.414318561553955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.954172611236572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.455281734466553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77.66458129882812
18 10.7495933744 	 77.6645850929 	 77.6645850929
epoch_time;  37.97333383560181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3601020574569702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.601147651672363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.060605049133301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 111.62409210205078
19 10.781875476 	 111.6240920608 	 111.6240920608
epoch_time;  37.96502685546875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3607131242752075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.600152492523193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.062615394592285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 111.61476135253906
It took 815.5383911132812 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▅▄▄█▃▂▂▂▂▁▆▁▃▂▂▂▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▅▅▃▄▆▃▂▂█▃▁▅▄▃▄▄▃▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▅▅▇▃▃▃█▃▁▅▅▄▄▄▃▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃▄▂▃▆▂▁▁█▂▁▄▃▂▃▃▂▂▂▂
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 27.51814
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.59799
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.33119
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.59197
wandb:                         Train loss 10.78823
wandb: 
wandb: 🚀 View run virtuous-springroll-1349 at: https://wandb.ai/nreints/thesis/runs/znjx7t65
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_172108-znjx7t65/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173447-h1rlqwmr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run virtuous-fuse-1357
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/h1rlqwmr
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5890989303588867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.671064376831055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.768634796142578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 174.88087463378906
0 20.3809691308 	 174.8808804899 	 174.8808804899
epoch_time;  37.97008275985718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.21458101272583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.194199562072754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.787624835968018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 258.4497375488281
1 13.7954190822 	 258.4497255068 	 258.4497255068
epoch_time;  37.80110430717468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5432348251342773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.574494361877441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.895761013031006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 173.64730834960938
2 13.0058686315 	 173.6473078547 	 173.6473078547
epoch_time;  37.79407620429993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.792494773864746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.095839500427246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.249396324157715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 180.1852569580078
3 12.4768111036 	 180.1852618243 	 180.1852618243
epoch_time;  37.87189769744873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1626980304718018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.879743576049805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.742877006530762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 405.72784423828125
4 12.1894703157 	 405.7278293919 	 405.7278293919
epoch_time;  38.7933874130249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.417041301727295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.592114448547363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.886044979095459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 130.40777587890625
5 11.8781445709 	 130.4077702703 	 130.4077702703
epoch_time;  39.21395301818848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.458270788192749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.305172443389893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.13148307800293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 76.62806701660156
6 11.9057990754 	 76.6280669341 	 76.6280669341
epoch_time;  37.88798546791077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.280490756034851
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.146121978759766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8679609298706055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 79.12308502197266
7 11.784280848 	 79.123083826 	 79.123083826
epoch_time;  38.18841624259949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3111175298690796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.914994239807129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.829804420471191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 58.85673522949219
8 11.4741610951 	 58.8567356419 	 58.8567356419
epoch_time;  38.2607843875885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.082753658294678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 12.35646915435791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.771523475646973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 71.62335968017578
9 11.3554713947 	 71.6233583193 	 71.6233583193
epoch_time;  38.295082092285156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5497570037841797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.279682159423828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.260878562927246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 47.03342056274414
10 11.3139334377 	 47.0334195524 	 47.0334195524
epoch_time;  38.46255016326904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2305042743682861
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.77107048034668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5766031742095947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 310.80963134765625
11 11.2621181026 	 310.8096283784 	 310.8096283784
epoch_time;  38.501243591308594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4193549156188965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.152863502502441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.304593086242676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.481346130371094
12 11.0934207325 	 34.4813450169 	 34.4813450169
epoch_time;  38.67684531211853
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1892929077148438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.807329177856445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.32836389541626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 136.58737182617188
13 10.996505665 	 136.5873733108 	 136.5873733108
epoch_time;  38.05701184272766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8409931659698486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.077828407287598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.864646911621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 73.9915542602539
14 10.9236317546 	 73.9915540541 	 73.9915540541
epoch_time;  38.32018733024597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8428374528884888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.301937103271484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.560111999511719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 93.81312561035156
15 10.9764606783 	 93.8131228885 	 93.8131228885
epoch_time;  38.41955065727234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.032953977584839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.564390182495117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.936078071594238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.97574615478516
16 10.9110541321 	 96.9757495777 	 96.9757495777
epoch_time;  38.04512333869934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6595841646194458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.777809143066406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.391556262969971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 44.20278549194336
17 10.8503591528 	 44.2027871622 	 44.2027871622
epoch_time;  38.18691921234131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7987728118896484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.721102237701416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0473737716674805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 63.22617721557617
18 10.7980472717 	 63.2261771537 	 63.2261771537
epoch_time;  38.20936059951782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.593027114868164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.607106685638428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.332419395446777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.516382217407227
19 10.7882347029 	 27.5163824958 	 27.5163824958
epoch_time;  38.429025173187256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.591968059539795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.597986221313477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.331189155578613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.518136978149414
It took 819.2051870822906 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▅▂▂▂▄▄▅▃▅▂▂▆▄▂▁▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆▆▄▆▃▂▂▅▅▆▃▇▃▃█▆▃▁▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▅▂▁▁▄▄▅▂▅▂▂▆▄▂▁▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▄▂▅▂▂▁▄▅▅▃▆▂▂█▄▂▁▃▂▂
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 13.26021
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.0243
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.42395
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.32529
wandb:                         Train loss 10.84526
wandb: 
wandb: 🚀 View run virtuous-fuse-1357 at: https://wandb.ai/nreints/thesis/runs/h1rlqwmr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173447-h1rlqwmr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174836-k2hchdsh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-rooster-1365
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/k2hchdsh
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.246709108352661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.946693420410156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.85679817199707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.852468490600586
0 20.0950948396 	 18.8524691195 	 18.8524691195
epoch_time;  38.78208923339844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8128747940063477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.489195823669434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.288393974304199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.52717399597168
1 13.5670846204 	 16.5271748311 	 16.5271748311
epoch_time;  38.63757658004761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4340558052062988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.522134780883789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4842119216918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.984257698059082
2 12.8964592502 	 15.9842575486 	 15.9842575486
epoch_time;  38.10208821296692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0165178775787354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.769336700439453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.16308069229126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.46076774597168
3 12.4706819672 	 16.4607685811 	 16.4607685811
epoch_time;  38.53181290626526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2951284646987915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.135553359985352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6140217781066895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.713651657104492
4 12.0942485295 	 13.7136520798 	 13.7136520798
epoch_time;  37.955252170562744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2476016283035278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.4790191650390625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.171056270599365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.223102569580078
5 11.8864053508 	 13.2231023015 	 13.2231023015
epoch_time;  38.17437195777893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1741762161254883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.416996002197266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.359472751617432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.384379386901855
6 11.6986355895 	 13.384378959 	 13.384378959
epoch_time;  38.81508731842041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7751812934875488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.991700649261475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.7306952476501465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.402660369873047
7 11.6263673197 	 15.402660473 	 15.402660473
epoch_time;  39.20981550216675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0303876399993896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.286920547485352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.9376325607299805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.664687156677246
8 11.4633865098 	 15.6646867082 	 15.6646867082
epoch_time;  38.36001014709473
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1398303508758545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.690115928649902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.356326103210449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.226755142211914
9 11.4123275643 	 16.2267551731 	 16.2267551731
epoch_time;  38.582093954086304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4768099784851074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.1144938468933105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.764087677001953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.986939430236816
10 11.2913012547 	 13.9869391364 	 13.9869391364
epoch_time;  38.385972023010254
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4200265407562256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.08246898651123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.314384460449219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.328645706176758
11 11.2972386489 	 16.3286462732 	 16.3286462732
epoch_time;  38.31388187408447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2444212436676025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.948544979095459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.570942401885986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.764755249023438
12 11.1462318043 	 13.7647553315 	 13.7647553315
epoch_time;  38.82317328453064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3580458164215088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.028702259063721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.529961585998535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.80383586883545
13 11.1518941478 	 13.803836307 	 13.803836307
epoch_time;  38.64703559875488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8774850368499756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.777997970581055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.875771522521973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.054555892944336
14 10.9695368039 	 17.0545555321 	 17.0545555321
epoch_time;  38.511452198028564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.923626184463501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.578531265258789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.623923301696777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.260604858398438
15 10.9858606709 	 15.2606049409 	 15.2606049409
epoch_time;  38.38222289085388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3436429500579834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.147762298583984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8329033851623535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.745107650756836
16 10.9601229265 	 13.7451079497 	 13.7451079497
epoch_time;  38.144304275512695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0883139371871948
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.068048000335693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.119913101196289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.645899772644043
17 10.7752021888 	 12.6458997572 	 12.6458997572
epoch_time;  38.16704750061035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5620734691619873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.591976165771484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.37003755569458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.55237102508545
18 10.740690589 	 14.5523714633 	 14.5523714633
epoch_time;  38.020602226257324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3255311250686646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.023524761199951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.42464017868042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.260835647583008
19 10.8452627793 	 13.2608358847 	 13.2608358847
epoch_time;  38.295405626297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3252899646759033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.024295806884766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4239501953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.260208129882812
It took 828.9414913654327 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▄▁▂▁▃▁▂▂▂▁▂▂▁▁▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅█▆▄▃▇▅▇▃█▄▆▃▃▃▇▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▅▅▃▂▄▃▄▁▅▂▃▂▂▂▄▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▄█▄▃▃▇▄▆▃▇▃▅▂▃▃▇▃▁▁
wandb:                         Train loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 37.77324
wandb:  Test loss t(-10, 10)_r(0, 0)_none 5.71394
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.45965
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.1429
wandb:                         Train loss 10.80847
wandb: 
wandb: 🚀 View run beaming-rooster-1365 at: https://wandb.ai/nreints/thesis/runs/k2hchdsh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174836-k2hchdsh/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180229-zg1ble0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-rooster-1373
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/zg1ble0l
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.297367572784424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.525068283081055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.60187816619873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 272.5448913574219
0 20.4761631474 	 272.5448902027 	 272.5448902027
epoch_time;  38.33920454978943
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9242085218429565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.19769287109375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.612100601196289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 140.62933349609375
1 13.5086958362 	 140.6293285473 	 140.6293285473
epoch_time;  38.45577120780945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6649106740951538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.2615885734558105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.694131851196289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.8016586303711
2 12.5353784064 	 89.8016575169 	 89.8016575169
epoch_time;  38.488744497299194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.2233071327209473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.605768203735352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.118248462677002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 113.06301879882812
3 12.1277703432 	 113.0630173142 	 113.0630173142
epoch_time;  38.652634143829346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7053372859954834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.735693454742432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.537583827972412
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.503376007080078
4 12.0133611205 	 30.503375739 	 30.503375739
epoch_time;  38.095457553863525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5396853685379028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.080538749694824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.383714199066162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.484920501708984
5 11.7078180539 	 45.4849187078 	 45.4849187078
epoch_time;  38.815465211868286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.400134801864624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.333662986755371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.789463043212891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.040319442749023
6 11.7232883378 	 22.0403188345 	 22.0403188345
epoch_time;  38.25897240638733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.161879539489746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.26163387298584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.503739356994629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 83.67861938476562
7 11.4882177767 	 83.6786159206 	 83.6786159206
epoch_time;  38.60356426239014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6821250915527344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.480465412139893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.626766204833984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.532983779907227
8 11.2925614633 	 22.5329840583 	 22.5329840583
epoch_time;  38.467355489730835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9612629413604736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.120776176452637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.117241382598877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.06813049316406
9 11.2530520058 	 35.0681323902 	 35.0681323902
epoch_time;  38.13246154785156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4526294469833374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.412992477416992
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.607571125030518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.16852951049805
10 11.1748093158 	 40.1685282939 	 40.1685282939
epoch_time;  38.25201058387756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.093608856201172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.437644958496094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.562534809112549
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.63822937011719
11 11.206861988 	 40.6382284628 	 40.6382284628
epoch_time;  38.21531891822815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4772282838821411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.899107456207275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.903620719909668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.26302719116211
12 11.0187127095 	 28.2630278716 	 28.2630278716
epoch_time;  38.14345693588257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8391441106796265
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.815346717834473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.565507411956787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 60.9080924987793
13 10.9912763943 	 60.908092272 	 60.908092272
epoch_time;  38.27452635765076
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.282407283782959
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.528281211853027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.922987461090088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 69.68952941894531
14 10.9893166575 	 69.689527027 	 69.689527027
epoch_time;  38.467081785202026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4122936725616455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.565669059753418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.326932907104492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.46479606628418
15 10.9834652167 	 27.4647962416 	 27.4647962416
epoch_time;  38.53195548057556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5097312927246094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.726742744445801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.164464950561523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.75619125366211
16 10.8315214804 	 18.7561906144 	 18.7561906144
epoch_time;  38.734009742736816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1081295013427734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.135514259338379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.331353664398193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.35301971435547
17 10.89559046 	 35.3530194257 	 35.3530194257
epoch_time;  38.589723348617554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4919296503067017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.747624397277832
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.876488208770752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.03127670288086
18 10.7309421547 	 15.0312763936 	 15.0312763936
epoch_time;  38.52702021598816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.142931580543518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.714542865753174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.460558891296387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.76126480102539
19 10.8084648199 	 37.7612647804 	 37.7612647804
epoch_time;  38.327348470687866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1428967714309692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.713937759399414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4596476554870605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 37.77324295043945
It took 832.8965876102448 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▂▃▁▃▂▁▃▄▃▂▅▆▄▂▄▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▆▅▅▂▃▂▂▂▂▄▆▅▄██▅▁▆▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▆▄▄▁▃▁▂▃▂▄▆▄▄▇█▅▁▆▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▅▄▄▁▂▁▂▂▂▃▅▄▃██▄▁▆▅▅
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 15.88556
wandb:  Test loss t(-10, 10)_r(0, 0)_none 9.09704
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.32888
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.42974
wandb:                         Train loss 10.81381
wandb: 
wandb: 🚀 View run sweet-rooster-1373 at: https://wandb.ai/nreints/thesis/runs/zg1ble0l
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180229-zg1ble0l/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_181624-dgj8h18j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-monkey-1380
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/dgj8h18j
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.253958225250244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.159478187561035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.807375907897949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.436443328857422
0 23.5021448671 	 22.4364442568 	 22.4364442568
epoch_time;  38.18819260597229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.690589666366577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.384063720703125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.5218610763549805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.1536808013916
1 14.0386697592 	 20.1536805849 	 20.1536805849
epoch_time;  38.40423917770386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.244868040084839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.425838470458984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.467512130737305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.941366195678711
2 12.8805459758 	 15.9413666596 	 15.9413666596
epoch_time;  38.17696523666382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.311748743057251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.357536315917969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.166808128356934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.098605155944824
3 12.3281699602 	 15.0986050992 	 15.0986050992
epoch_time;  38.23970937728882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3537700176239014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.091102600097656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.715286731719971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.172453880310059
4 12.1892706394 	 14.1724543391 	 14.1724543391
epoch_time;  38.58650517463684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5682728290557861
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.054512023925781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.756840705871582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.736871719360352
5 12.0286105704 	 14.7368718328 	 14.7368718328
epoch_time;  38.18071794509888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.342979907989502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.363119125366211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600513458251953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.174251556396484
6 11.8943421526 	 12.174251742 	 12.174251742
epoch_time;  38.336119413375854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4549744129180908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.450933456420898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.148490905761719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.753242492675781
7 11.6339642751 	 15.7532424514 	 15.7532424514
epoch_time;  38.38441252708435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4276453256607056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.577961444854736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.508996486663818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.544268608093262
8 11.631466338 	 13.5442686339 	 13.5442686339
epoch_time;  38.381086349487305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4595565795898438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.58034610748291
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.795596122741699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.615072250366211
9 11.3754681153 	 12.6150720545 	 12.6150720545
epoch_time;  39.26682806015015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9164451360702515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.733834743499756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.168933391571045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.512866973876953
10 11.2888033175 	 14.5128668708 	 14.5128668708
epoch_time;  38.412317991256714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.761831521987915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.295415878295898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.423760890960693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.919168472290039
11 11.1993285058 	 15.9191683383 	 15.9191683383
epoch_time;  39.14575481414795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.2346248626708984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.323784828186035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.44091796875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.51325511932373
12 11.1870887462 	 15.5132548564 	 15.5132548564
epoch_time;  38.52426624298096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9342927932739258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.642520904541016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.349650859832764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.15642261505127
13 11.0384085932 	 14.156422878 	 14.156422878
epoch_time;  38.41753077507019
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.64353346824646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.93593978881836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.570446014404297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.64396858215332
14 11.0268994372 	 17.6439690667 	 17.6439690667
epoch_time;  38.57665944099426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.5163919925689697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.679342269897461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.952520370483398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.295053482055664
15 10.8987654452 	 19.2950538429 	 19.2950538429
epoch_time;  38.49314260482788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.230609655380249
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.246280670166016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.833189964294434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.022281646728516
16 10.9267439277 	 16.0222814611 	 16.0222814611
epoch_time;  38.85724759101868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2109229564666748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.528377056121826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4570231437683105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.583165168762207
17 10.7987712635 	 13.5831648543 	 13.5831648543
epoch_time;  38.08888077735901
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7855756282806396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.662386894226074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.561692237854004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.452394485473633
18 10.8029413243 	 16.4523938978 	 16.4523938978
epoch_time;  39.079301595687866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.430058717727661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.104047775268555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.327220439910889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.891023635864258
19 10.8138117806 	 15.8910235431 	 15.8910235431
epoch_time;  38.26478409767151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4297425746917725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.097044944763184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.328876495361328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.88555908203125
It took 834.9520497322083 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▃▃▃▂▂▂▁▂▇▂▂▃▁▁▁▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆▅▆▄▃▄▃▂▂▁▁▂█▂▂▄▁▁▂██
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▄▂▃▂▂▂▁▁▂▇▂▂▃▁▁▁▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▃▄▃▂▂▂▁▂▁▁▁█▂▁▃▁▁▁██
wandb:                         Train loss █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 16.88095
wandb:  Test loss t(-10, 10)_r(0, 0)_none 11.54713
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.88115
wandb:     Test loss t(0, 0)_r(0, 0)_none 4.0882
wandb:                         Train loss 10.6129
wandb: 
wandb: 🚀 View run enchanting-monkey-1380 at: https://wandb.ai/nreints/thesis/runs/dgj8h18j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_181624-dgj8h18j/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_183001-i4lswz8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-chrysanthemum-1388
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/i4lswz8m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.0844075679779053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 10.308777809143066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.14448070526123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.212646484375
0 19.7418780679 	 20.2126464844 	 20.2126464844
epoch_time;  38.09101104736328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.2419943809509277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.149721145629883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.871939659118652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.960394859313965
1 13.4373545405 	 15.9603951119 	 15.9603951119
epoch_time;  37.907474756240845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.4408774375915527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.581522941589355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.046880722045898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.344545364379883
2 12.5638289713 	 16.3445457665 	 16.3445457665
epoch_time;  37.93890309333801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1160037517547607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.510310173034668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.559147834777832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.332680702209473
3 12.2174977255 	 15.3326805321 	 15.3326805321
epoch_time;  38.100672245025635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7543903589248657
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.836759090423584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.177704811096191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.587977409362793
4 11.9705536984 	 13.5879777238 	 13.5879777238
epoch_time;  38.2102267742157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.736659049987793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.87207555770874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.835971355438232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.314682960510254
5 11.6370414057 	 14.3146827492 	 14.3146827492
epoch_time;  38.030234813690186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.491304636001587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.2063093185424805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.128427028656006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.273083686828613
6 11.4161637293 	 13.273083826 	 13.273083826
epoch_time;  38.18086266517639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3081117868423462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.359421253204346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.54885721206665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.120299339294434
7 11.3001121361 	 12.1202993032 	 12.1202993032
epoch_time;  37.93097996711731
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.385030746459961
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.754693984985352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.846291542053223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.719359397888184
8 11.1862424361 	 12.7193596917 	 12.7193596917
epoch_time;  37.85999083518982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3037375211715698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.108272552490234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.379523754119873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.192399024963379
9 11.1637583573 	 12.1923986486 	 12.1923986486
epoch_time;  37.863617181777954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2403104305267334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.950521945953369
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.3726372718811035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.590655326843262
10 11.0379378332 	 11.5906553526 	 11.5906553526
epoch_time;  37.82803153991699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3288378715515137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.220123767852783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.57252836227417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.85115909576416
11 10.9259538179 	 11.8511586782 	 11.8511586782
epoch_time;  38.700878381729126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.19538688659668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.712811470031738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.27795124053955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.52533721923828
12 10.9721987136 	 18.5253378378 	 18.5253378378
epoch_time;  38.59940242767334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.564588189125061
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.88447380065918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.628096580505371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.431059837341309
13 10.8738793003 	 12.4310599662 	 12.4310599662
epoch_time;  39.045573234558105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3712594509124756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.495548248291016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.759149551391602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.381426811218262
14 10.7781279092 	 12.381426837 	 12.381426837
epoch_time;  38.55595016479492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0603222846984863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.138277053833008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.5137858390808105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.508113861083984
15 10.8463550482 	 13.5081133868 	 13.5081133868
epoch_time;  38.14595603942871
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.167466640472412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.130735397338867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2916693687438965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.591876029968262
16 10.7131676593 	 11.5918760557 	 11.5918760557
epoch_time;  37.76827335357666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1799371242523193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.708901882171631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.978508710861206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.102487564086914
17 10.6532013001 	 11.102487595 	 11.102487595
epoch_time;  38.36952996253967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2484875917434692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.290554523468018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.091330051422119
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.388236045837402
18 10.5987954627 	 11.3882363809 	 11.3882363809
epoch_time;  37.942294120788574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.088212013244629
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.542287826538086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.879298686981201
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.873233795166016
19 10.6129037164 	 16.8732329497 	 16.8732329497
epoch_time;  36.46204710006714
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.088196277618408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.547130584716797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.8811516761779785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.880950927734375
It took 817.2487659454346 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▂▃▂▁▁▄▂▃▁▂▅▁▁▅▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▄▃▂▄▂▂▂▅▂▅▂▂▇▂▁█▂▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▂▁▃▂▁▁▄▂▄▂▂▅▂▂▆▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃▂▂▃▂▂▁▄▂▄▂▂▇▂▁█▂▃▁▁
wandb:                         Train loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 11.45843
wandb:  Test loss t(-10, 10)_r(0, 0)_none 5.46844
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.11915
wandb:     Test loss t(0, 0)_r(0, 0)_none 1.07056
wandb:                         Train loss 10.49162
wandb: 
wandb: 🚀 View run brilliant-chrysanthemum-1388 at: https://wandb.ai/nreints/thesis/runs/i4lswz8m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_183001-i4lswz8m/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8815053701400757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.908724308013916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.010594367980957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.51446533203125
0 22.3390027294 	 19.5144650021 	 19.5144650021
epoch_time;  36.61624336242676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7455211877822876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.305191516876221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.632414817810059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.869489669799805
1 13.3778272046 	 16.8694890203 	 16.8694890203
epoch_time;  37.11820101737976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2736461162567139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.264067649841309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.114415645599365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.601800918579102
2 12.3122051138 	 13.6018013619 	 13.6018013619
epoch_time;  37.00978660583496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3578530550003052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.696788787841797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.345364570617676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.08053970336914
3 11.9758140445 	 12.0805400127 	 12.0805400127
epoch_time;  38.80562376976013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.710700511932373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.133743762969971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.548496246337891
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.543139457702637
4 11.6659587635 	 13.5431389886 	 13.5431389886
epoch_time;  38.01592659950256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3895890712738037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.093182563781738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.574607849121094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.239855766296387
5 11.410880903 	 12.2398556271 	 12.2398556271
epoch_time;  38.33623647689819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1715686321258545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.807682037353516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.400872707366943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.019386291503906
6 11.3249129887 	 12.0193860853 	 12.0193860853
epoch_time;  37.878825426101685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1557581424713135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.579201698303223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.442860126495361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.852895736694336
7 11.1417119795 	 11.8528953758 	 11.8528953758
epoch_time;  38.56787419319153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.9363958835601807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.711760997772217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.259915351867676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.889627456665039
8 11.1875 	 14.8896273226 	 14.8896273226
epoch_time;  36.84721350669861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.176699161529541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.607700347900391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.832789421081543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.319354057312012
9 11.0121002507 	 12.319354413 	 12.319354413
epoch_time;  37.200923442840576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.03879976272583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.762355327606201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.974465370178223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.171269416809082
10 10.922007236 	 14.1712692673 	 14.1712692673
epoch_time;  37.10038137435913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.1727571487426758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.576685905456543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.481367588043213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.888172149658203
11 10.8811304587 	 11.8881717166 	 11.8881717166
epoch_time;  36.91055464744568
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3238660097122192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.121454238891602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.790503978729248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.597347259521484
12 10.7915763054 	 12.5973474451 	 12.5973474451
epoch_time;  36.8838369846344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.735170841217041
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.196815490722656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.171302795410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.806781768798828
13 10.7105348415 	 15.8067818307 	 15.8067818307
epoch_time;  36.97302293777466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2414573431015015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.727870464324951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.470298767089844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.734271049499512
14 10.6706379327 	 11.7342707454 	 11.7342707454
epoch_time;  37.404589891433716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0258246660232544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.243711948394775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.491215705871582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.611180305480957
15 10.6630667633 	 11.6111803209 	 11.6111803209
epoch_time;  37.658345460891724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.0526905059814453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.505622863769531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.370631694793701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.141918182373047
16 10.6091971426 	 16.1419182855 	 16.1419182855
epoch_time;  37.04609537124634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2239354848861694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.934518814086914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.687661647796631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.032777786254883
17 10.5514477192 	 12.0327781883 	 12.0327781883
epoch_time;  37.02377700805664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.6258609294891357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.7689313888549805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.993212699890137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.726278305053711
18 10.4220118642 	 12.726278769 	 12.726278769
epoch_time;  36.98661255836487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0704548358917236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.467463970184326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.122559547424316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.466119766235352
19 10.4916175631 	 11.4661198796 	 11.4661198796
epoch_time;  37.06336998939514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0705562829971313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.46844482421875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.119149208068848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.458426475524902
It took 804.1628251075745 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139461
Array Job ID: 2137927_20
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-17:48:54 core-walltime
Job Wall-clock time: 02:19:23
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
