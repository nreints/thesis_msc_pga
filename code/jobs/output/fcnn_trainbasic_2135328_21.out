wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170423-pigyxuf9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-lamp-1150
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/pigyxuf9
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–ˆâ–ƒâ–…â–„â–ƒâ–ƒâ–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–†â–ˆâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–„â–‚â–‚â–‚â–„â–‚â–â–â–‚â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.79971
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.5304
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.29495
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13017
wandb:                         Train loss 2.00713
wandb: 
wandb: ðŸš€ View run scintillating-lamp-1150 at: https://wandb.ai/nreints/thesis/runs/pigyxuf9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170423-pigyxuf9/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19582200050354004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.251063585281372
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41534850001335144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7446658611297607
0 5.1238452496 	 3.7446658573 	 3.7446658573
epoch_time;  29.395384550094604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22874779999256134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.600311517715454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44737985730171204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1430492401123047
1 2.3573260039 	 3.1430492504 	 3.1430492504
epoch_time;  28.301597118377686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1336972862482071
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2832515239715576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3105381429195404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.740222215652466
2 2.2752835139 	 2.7402223329 	 2.7402223329
epoch_time;  28.052022695541382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1593679040670395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.142730474472046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3703216016292572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5933284759521484
3 2.2260333909 	 2.5933283625 	 2.5933283625
epoch_time;  28.109664916992188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12537841498851776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1193935871124268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33671319484710693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5584895610809326
4 2.1916507212 	 2.5584894954 	 2.5584894954
epoch_time;  28.13932967185974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13340680301189423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0740604400634766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30892303586006165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4855215549468994
5 2.1593426683 	 2.4855216361 	 2.4855216361
epoch_time;  28.19503927230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13073526322841644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8479115962982178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31577008962631226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2477359771728516
6 2.1359856669 	 2.2477359256 	 2.2477359256
epoch_time;  27.77445387840271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10674608498811722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8699872493743896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2759758532047272
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1785221099853516
7 2.1118612012 	 2.1785220584 	 2.1785220584
epoch_time;  28.001415014266968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09518605470657349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8091247081756592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28285959362983704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1601850986480713
8 2.0985129737 	 2.160185118 	 2.160185118
epoch_time;  27.746586322784424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11720819026231766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7655868530273438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29924315214157104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0859737396240234
9 2.0846080328 	 2.0859736262 	 2.0859736262
epoch_time;  27.63511824607849
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1456739902496338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.824028730392456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30533990263938904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.119633674621582
10 2.0731301175 	 2.1196336901 	 2.1196336901
epoch_time;  27.794201135635376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11597269773483276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.663167119026184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2721112370491028
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9446110725402832
11 2.0579836464 	 1.9446110906 	 1.9446110906
epoch_time;  27.996935606002808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11760525405406952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.72177255153656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3023620843887329
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.041766405105591
12 2.0506409146 	 2.0417665224 	 2.0417665224
epoch_time;  27.671212911605835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11052174866199493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7095121145248413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2979092001914978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0278992652893066
13 2.0426710538 	 2.0278993349 	 2.0278993349
epoch_time;  27.883546113967896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14495854079723358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.522329330444336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29912880063056946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7759467363357544
14 2.0345008556 	 1.7759467048 	 1.7759467048
epoch_time;  27.847582578659058
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12333030998706818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.583303451538086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2683529257774353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8463538885116577
15 2.0284209771 	 1.8463538917 	 1.8463538917
epoch_time;  27.962461948394775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10224734246730804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.466327428817749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2836514711380005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.755344033241272
16 2.0236446146 	 1.7553440403 	 1.7553440403
epoch_time;  27.821183443069458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1025470495223999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4557627439498901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2665647268295288
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7490406036376953
17 2.0143023099 	 1.7490405933 	 1.7490405933
epoch_time;  27.918442249298096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11491050571203232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.549517273902893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26297539472579956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.83714759349823
18 2.0135279031 	 1.8371476457 	 1.8371476457
epoch_time;  27.892271041870117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1301887035369873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5296686887741089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29491353034973145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.799994945526123
19 2.0071260301 	 1.7999948862 	 1.7999948862
epoch_time;  27.99821972846985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1301710158586502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.530396819114685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2949531674385071
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.799705982208252
It took 620.5132050514221 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn57: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135414.0

JOB STATISTICS
==============
Job ID: 2135414
Array Job ID: 2135328_21
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:10:30 core-walltime
Job Wall-clock time: 00:10:35
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
