wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_135637-7iy582kg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-snow-535
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/7iy582kg
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() █▃▂▁▁▁▁▁▁▁▁
wandb:                                             Train loss █▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 0.12135
wandb:                                             Train loss 0.11397
wandb: 
wandb: 🚀 View run gallant-snow-535 at: https://wandb.ai/nreints/test/runs/7iy582kg
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_135637-7iy582kg/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230329_140404-awmfyymz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-fog-544
wandb: ⭐️ View project at https://wandb.ai/nreints/test
wandb: 🚀 View run at https://wandb.ai/nreints/test/runs/awmfyymz
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: \ 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: | 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: / 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: - 0.031 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                                  Epoch ▁▂▃▃▄▅▆▆▇█
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() █▃▂▁▁▁▁▁▁▁▁
wandb:                                             Train loss █▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                                  Epoch 9
wandb: Test loss t(0, 0)_r(5, 20)_semi_pNone_gNone, MSELoss() 0.11513
wandb:                                             Train loss 0.11491
wandb: 
wandb: 🚀 View run balmy-fog-544 at: https://wandb.ai/nreints/test/runs/awmfyymz
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230329_140404-awmfyymz/logs
Running for data type: pos
----- ITERATION 1/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 104.403360731 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 27.076364517211914 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 39.50384283065796
Epoch 1
	 Logging train Loss: 12.9495694781 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 8.309789657592773 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.64615058898926
Epoch 2
	 Logging train Loss: 4.6410725368 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.251032829284668 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.4090530872345
Epoch 3
	 Logging train Loss: 1.8116572422 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 1.3305909633636475 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.26914644241333
Epoch 4
	 Logging train Loss: 0.8063406645 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.6524717211723328 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.302353382110596
Epoch 5
	 Logging train Loss: 0.4284355866 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.37898680567741394 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.62783598899841
Epoch 6
	 Logging train Loss: 0.2657011106 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.24726428091526031 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.78448820114136
Epoch 7
	 Logging train Loss: 0.187534592 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.18515029549598694 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.553098917007446
Epoch 8
	 Logging train Loss: 0.1427683302 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.14661192893981934 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 38.40422081947327
Epoch 9
	 Logging train Loss: 0.1139718783 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.12134937196969986 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 39.380741357803345
	 Logging test loss: 0.12134914845228195 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  447.7923746109009  seconds.
----- ITERATION 2/2 ------
Number of train simulations: 1600
Number of test simulations: 400
-- Finished Train Dataloader --
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(24, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=24, bias=True)
)
-- Started Training --
Epoch 0
	 Logging train Loss: 107.4874311582 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 27.397035598754883 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 38.53723764419556
Epoch 1
	 Logging train Loss: 13.9211813668 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 8.442488670349121 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.572365045547485
Epoch 2
	 Logging train Loss: 5.0715302695 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 3.3831048011779785 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.526646852493286
Epoch 3
	 Logging train Loss: 1.9929036241 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 1.4298888444900513 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.76974081993103
Epoch 4
	 Logging train Loss: 0.8822917862 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.7114099264144897 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 38.216832876205444
Epoch 5
	 Logging train Loss: 0.4702764086 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.415597140789032 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.20820474624634
Epoch 6
	 Logging train Loss: 0.2927437185 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.27464646100997925 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.69673800468445
Epoch 7
	 Logging train Loss: 0.2033453255 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.19992822408676147 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.822595834732056
Epoch 8
	 Logging train Loss: 0.149533026 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.14856809377670288 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.739356994628906
Epoch 9
	 Logging train Loss: 0.1149099271 (MSELoss(): data_t(0, 0)_r(5, 20)_semi_pNone_gNone)
	 Logging test loss: 0.11510252207517624 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
     --> Epoch time; 37.64190864562988
	 Logging test loss: 0.11513033509254456 (MSELoss(): t(0, 0)_r(5, 20)_semi_pNone_gNone)
It took  441.6798758506775  seconds.

JOB STATISTICS
==============
Job ID: 2514849
Array Job ID: 2514848_1
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 02:53:39
CPU Efficiency: 64.03% of 04:31:12 core-walltime
Job Wall-clock time: 00:15:04
Memory Utilized: 27.87 GB
Memory Efficiency: 89.18% of 31.25 GB
