wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_160559-kueb632y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-springroll-1307
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kueb632y
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▄▂▃▁▂▂▂▂▂▁▂▂▁▁▂▁▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▂▃▁▁▁▂▂▂▂▂▂▁▁▂▁▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▃▇▂▅▁▂▂▃▃▂▂▂▃▂▁▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▅▂▅▂▂▂▃▃▂▃▂▃▂▁▂▂▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.33836
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27121
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17528
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14299
wandb:                         Train loss 2.26566
wandb: 
wandb: 🚀 View run resplendent-springroll-1307 at: https://wandb.ai/nreints/thesis/runs/kueb632y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_160559-kueb632y/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_161920-9higublz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-rat-1314
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9higublz
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3087286353111267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8228222131729126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31962892413139343
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8243147730827332
0 6.0857239389 	 0.8243147567 	 0.8435657398
epoch_time;  38.782278537750244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1973821520805359
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5407435894012451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1940051168203354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5278468728065491
1 2.7770961382 	 0.5278468776 	 0.5430455801
epoch_time;  36.48862338066101
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24057219922542572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4691488742828369
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.289431095123291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5330007076263428
2 2.5619062606 	 0.533000678 	 0.548213237
epoch_time;  36.470253705978394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15985266864299774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3166339099407196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1805412769317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3588903844356537
3 2.4653593049 	 0.3588903891 	 0.3740264893
epoch_time;  36.18666052818298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22087208926677704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40087708830833435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25174060463905334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4559873044490814
4 2.4115146438 	 0.4559873014 	 0.4703654934
epoch_time;  36.01322364807129
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1375664621591568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26106011867523193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14775997400283813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2949497401714325
5 2.3799204997 	 0.2949497532 	 0.3089478158
epoch_time;  36.4686713218689
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15389570593833923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2618118226528168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17576690018177032
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31170403957366943
6 2.3612043191 	 0.311704048 	 0.3231710073
epoch_time;  36.13659477233887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15211015939712524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2565787732601166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17708545923233032
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31140223145484924
7 2.3444693636 	 0.3114022332 	 0.3247886658
epoch_time;  36.62198328971863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16988348960876465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30693358182907104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20108960568904877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3729483187198639
8 2.3336594051 	 0.3729483115 	 0.3844002801
epoch_time;  36.72510099411011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16617047786712646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30152174830436707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20088361203670502
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.362200528383255
9 2.3241081677 	 0.3622005153 	 0.3741456315
epoch_time;  35.86993741989136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14131920039653778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2835811674594879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17561502754688263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3456805348396301
10 2.3136279727 	 0.3456805255 	 0.358602163
epoch_time;  35.94202709197998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16674679517745972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28353285789489746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17109374701976776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.307931512594223
11 2.3017776479 	 0.3079314979 	 0.3193053581
epoch_time;  36.48976469039917
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1512024998664856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29455122351646423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18085093796253204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35974961519241333
12 2.3033309244 	 0.3597496239 	 0.3719401592
epoch_time;  36.3578405380249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16751229763031006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3288663625717163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18888702988624573
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38345515727996826
13 2.2966201152 	 0.3834551425 	 0.3942605921
epoch_time;  36.64803719520569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1465819925069809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24694852530956268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1696786880493164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2929379343986511
14 2.2859752864 	 0.2929379231 	 0.3045269116
epoch_time;  36.43217587471008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12209315598011017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2255384773015976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14874045550823212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27163076400756836
15 2.2797653209 	 0.2716307563 	 0.2838146931
epoch_time;  36.42271685600281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14945384860038757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27113285660743713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17578458786010742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3315459191799164
16 2.2702464812 	 0.3315459174 	 0.3422192135
epoch_time;  36.42299175262451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1427524983882904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21907123923301697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1758025735616684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27804040908813477
17 2.2731199684 	 0.2780404168 	 0.2884304975
epoch_time;  36.72550082206726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.137274369597435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26569557189941406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1725555956363678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33155760169029236
18 2.2669394254 	 0.3315576089 	 0.3419545973
epoch_time;  37.23615908622742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14300096035003662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27131783962249756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17526352405548096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33836185932159424
19 2.2656607037 	 0.3383618741 	 0.3488057317
epoch_time;  37.187182903289795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14298561215400696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27120888233184814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17528438568115234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3383597433567047
It took 800.6600074768066 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.035 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.035 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▄▄▂▂▂▁▁▁▂▂▂▄▂▁▂▃▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▃▃▂▂▂▁▁▁▂▁▂▂▂▁▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▅▆▃▃▃▂▁▂▂▄▂█▂▁▂▄▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ██▅▅▄▃▃▁▁▁▂▂▃▅▂▁▂▃▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.28977
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22672
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17479
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13179
wandb:                         Train loss 2.22707
wandb: 
wandb: 🚀 View run alight-rat-1314 at: https://wandb.ai/nreints/thesis/runs/9higublz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_161920-9higublz/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_163219-ync4dm02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-mandu-1322
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ync4dm02
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25892964005470276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7176060080528259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2678413689136505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6751950979232788
0 5.3461695873 	 0.6751951063 	 0.6917821523
epoch_time;  36.48696279525757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2549629509449005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6232489943504333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3050228953361511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6506209373474121
1 2.7546305724 	 0.6506209502 	 0.6676987107
epoch_time;  35.934680223464966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20284980535507202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37069952487945557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23749645054340363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42513513565063477
2 2.5485608431 	 0.4251351434 	 0.4383798548
epoch_time;  35.93513369560242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1937897950410843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3510044515132904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2549983561038971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43627282977104187
3 2.450573342 	 0.4362728222 	 0.4493599144
epoch_time;  36.136059284210205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17569734156131744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.313804030418396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1969219148159027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35096073150634766
4 2.402561774 	 0.3509607264 	 0.363850939
epoch_time;  35.84034609794617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1711554080247879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29080867767333984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19340187311172485
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33129921555519104
5 2.3720838667 	 0.3312992199 	 0.3427215164
epoch_time;  35.92105484008789
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16494864225387573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30068543553352356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19201841950416565
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34518736600875854
6 2.3431985761 	 0.3451873779 	 0.3580296697
epoch_time;  36.22716426849365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1279899775981903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22871039807796478
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17466209828853607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29514220356941223
7 2.3242146176 	 0.2951421995 	 0.3086662499
epoch_time;  36.0331974029541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12354863435029984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23845700919628143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15400195121765137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2822067439556122
8 2.3092360861 	 0.2822067467 	 0.29477038
epoch_time;  36.09100317955017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13082250952720642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24947959184646606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1667342633008957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29933592677116394
9 2.2940562254 	 0.2993359128 	 0.3106085494
epoch_time;  36.0241482257843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14170724153518677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26377931237220764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1768736094236374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30915147066116333
10 2.2846537534 	 0.3091514794 	 0.3202367628
epoch_time;  35.80859303474426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1443292498588562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25772860646247864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20937606692314148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34361812472343445
11 2.2779335223 	 0.3436181352 	 0.3546582196
epoch_time;  35.93013668060303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16028748452663422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3193792402744293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18431852757930756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3479241132736206
12 2.2667304926 	 0.3479241036 	 0.3589822718
epoch_time;  36.004600524902344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20379875600337982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3280067443847656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3003675639629364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4468514621257782
13 2.2597852951 	 0.446851452 	 0.4567574166
epoch_time;  35.71398210525513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14813727140426636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2960769534111023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1804954707622528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3155803978443146
14 2.2550469504 	 0.315580399 	 0.3262344154
epoch_time;  35.69460487365723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1302298605442047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2402622103691101
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15603817999362946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27446940541267395
15 2.2458362205 	 0.2744694065 	 0.2844251684
epoch_time;  36.219533920288086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14628097414970398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.293965220451355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.169358029961586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3135482966899872
16 2.2473184202 	 0.3135482994 	 0.3239172033
epoch_time;  36.06733250617981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15685896575450897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30665725469589233
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21601003408432007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3775692880153656
17 2.2406481082 	 0.3775692914 	 0.3874040346
epoch_time;  35.90326762199402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14622944593429565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2588742971420288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1770065724849701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2991928160190582
18 2.2306309043 	 0.2991928101 	 0.309047534
epoch_time;  35.9281210899353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1317923665046692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22671566903591156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17477388679981232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2897649109363556
19 2.2270689637 	 0.2897648992 	 0.2981224266
epoch_time;  36.16796135902405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13179172575473785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2267201542854309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17479199171066284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28977352380752563
It took 779.0992329120636 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▃▃▃▃▂▂▁▃▂▃▂▁▁▂▂▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▃▃▃▂▃▂▂▂▃▁▁▁▂▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▂▃▃▅▂▂▂▃▂▄▂▁▁▂▂▂▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▅▄▅▃▄▃▃▃▅▂▁▁▃▂▂▂▃▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.31592
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26028
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.1583
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12382
wandb:                         Train loss 2.26202
wandb: 
wandb: 🚀 View run festive-mandu-1322 at: https://wandb.ai/nreints/thesis/runs/ync4dm02
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_163219-ync4dm02/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_164526-p5c7an2m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-dragon-1329
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/p5c7an2m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26867613196372986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7243695259094238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30422139167785645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7880111336708069
0 5.0923469766 	 0.7880111282 	 0.8053339778
epoch_time;  38.70073437690735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17359571158885956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3702160120010376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17369265854358673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42644038796424866
1 2.7042118142 	 0.4264403884 	 0.4413599375
epoch_time;  37.0887336730957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1916540265083313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.343428373336792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19815869629383087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3724750876426697
2 2.5131563201 	 0.3724750828 	 0.3884728509
epoch_time;  36.312098026275635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18723733723163605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3395361304283142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2021864354610443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3845970034599304
3 2.4384654877 	 0.3845969948 	 0.3979044734
epoch_time;  35.9103684425354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20450296998023987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3231147825717926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23202620446681976
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3778925836086273
4 2.3993826565 	 0.3778925715 	 0.3898585139
epoch_time;  36.50824975967407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1456218957901001
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2781962454319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1816631555557251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3418392539024353
5 2.365341982 	 0.3418392491 	 0.3557140453
epoch_time;  35.940921783447266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.185502827167511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3338823616504669
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1766820102930069
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33610641956329346
6 2.3490559675 	 0.3361064189 	 0.3482426824
epoch_time;  36.09108352661133
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14737141132354736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24336214363574982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.162046417593956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27137935161590576
7 2.3420887859 	 0.2713793368 	 0.2859563879
epoch_time;  35.88818597793579
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16491933166980743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30523672699928284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20249268412590027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3780032694339752
8 2.3173223146 	 0.3780032596 	 0.3886331094
epoch_time;  35.96905875205994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1503373086452484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2872755229473114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.166513592004776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31802666187286377
9 2.3157381279 	 0.3180266509 	 0.3298709972
epoch_time;  36.10499310493469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1989392638206482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34348264336586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.225348100066185
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3975955545902252
10 2.3016621067 	 0.3975955448 	 0.4078032622
epoch_time;  35.95095872879028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12550915777683258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23191149532794952
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1672477275133133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2972995340824127
11 2.2978097429 	 0.2972995449 	 0.3098615492
epoch_time;  36.140398025512695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11689264327287674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21437229216098785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15917930006980896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27271291613578796
12 2.2895712778 	 0.2727129137 	 0.2834266663
epoch_time;  36.49605131149292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11031869053840637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2000923603773117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1506182700395584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25905752182006836
13 2.285069966 	 0.2590575347 	 0.2708681674
epoch_time;  36.40755033493042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14436465501785278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2704210877418518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17796793580055237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33500778675079346
14 2.2758208877 	 0.3350077861 	 0.3448515609
epoch_time;  36.655003786087036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12755431234836578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26079291105270386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17024599015712738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3320706784725189
15 2.2688981704 	 0.3320706754 	 0.3424133301
epoch_time;  36.34390902519226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14167556166648865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2791226804256439
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18161709606647491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35041412711143494
16 2.265878893 	 0.3504141318 	 0.3613072988
epoch_time;  36.085606813430786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1420854777097702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2639511227607727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17037206888198853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30799421668052673
17 2.2644879361 	 0.3079942239 	 0.3195609428
epoch_time;  36.10343074798584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15667086839675903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2679300308227539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19798213243484497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.329958975315094
18 2.2631918585 	 0.3299589827 	 0.3403404442
epoch_time;  36.419620990753174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12382365763187408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2601902186870575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15830343961715698
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31579259037971497
19 2.262015785 	 0.3157925992 	 0.3263088742
epoch_time;  36.18480443954468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12381935119628906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2602764070034027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15830372273921967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3159191310405731
It took 786.8274972438812 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▂▁▂▂▂▁▃▂▁▃▁▁▂▁▁▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▂▁▁▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▂▁▂▂▂▁▄▃▂▄▁▁▃▁▁▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▁▁▁▁▁▂▃▂▂▃▂▁▃▂▁▂▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.33491
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27839
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17925
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16416
wandb:                         Train loss 2.27058
wandb: 
wandb: 🚀 View run vermilion-dragon-1329 at: https://wandb.ai/nreints/thesis/runs/p5c7an2m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_164526-p5c7an2m/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165828-nzwg0dud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-mandu-1336
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nzwg0dud
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3772253096103668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.857906699180603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34770137071609497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8483201265335083
0 5.5796351346 	 0.848320131 	 0.8668775404
epoch_time;  36.214000940322876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18109671771526337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48000505566596985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23527082800865173
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5740967988967896
1 2.7434549816 	 0.5740968034 	 0.5895734632
epoch_time;  36.17296886444092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14070431888103485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3050672709941864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16227439045906067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36141955852508545
2 2.5334880935 	 0.361419554 	 0.3768717173
epoch_time;  35.85205340385437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12651346623897552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24003632366657257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15917658805847168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2978309094905853
3 2.454600092 	 0.2978309013 	 0.3125669531
epoch_time;  38.319809675216675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14289537072181702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2639780342578888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1692919135093689
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32646167278289795
4 2.4087157046 	 0.3264616683 	 0.3409959329
epoch_time;  36.62502861022949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14283868670463562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2847207486629486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17721590399742126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3523130714893341
5 2.3843485859 	 0.3523130675 	 0.3672469268
epoch_time;  36.98186731338501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1403406858444214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25954732298851013
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1838456690311432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33942729234695435
6 2.3628056965 	 0.3394272882 	 0.3515990799
epoch_time;  36.023277759552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15237917006015778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2838934361934662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1594601273536682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3204536736011505
7 2.345582625 	 0.3204536644 	 0.3337406055
epoch_time;  36.059691190719604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19707544147968292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33652907609939575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23473508656024933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41844606399536133
8 2.3332273242 	 0.4184460614 	 0.4297123265
epoch_time;  35.944648027420044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16730597615242004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2820579707622528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21393956243991852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36256569623947144
9 2.3189840246 	 0.3625656953 	 0.3755244075
epoch_time;  35.82316875457764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15111379325389862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2760741710662842
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16492852568626404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3205844759941101
10 2.3167981643 	 0.3205844776 	 0.3330800855
epoch_time;  35.78037142753601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21054895222187042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3494965732097626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22079746425151825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.416900098323822
11 2.3115827784 	 0.4169000986 	 0.4279931971
epoch_time;  35.90158987045288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14598509669303894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24678948521614075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14874586462974548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28113630414009094
12 2.3032019944 	 0.2811363014 	 0.2927984702
epoch_time;  35.77604675292969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14287984371185303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24992920458316803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16100259125232697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30630990862846375
13 2.2950873352 	 0.3063099011 	 0.3171198458
epoch_time;  36.3226637840271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2022920399904251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30825841426849365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2053772211074829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3600962460041046
14 2.2864232357 	 0.3600962458 	 0.3708312988
epoch_time;  35.81795358657837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14641937613487244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2566034495830536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15706510841846466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30400609970092773
15 2.2848828522 	 0.304006092 	 0.3149680473
epoch_time;  35.843032121658325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13873282074928284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2410035878419876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14722977578639984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2882952392101288
16 2.2822968128 	 0.2882952303 	 0.2985991401
epoch_time;  36.12522292137146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14593926072120667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2518671751022339
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16512781381607056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.309756875038147
17 2.2742124027 	 0.3097568615 	 0.320426178
epoch_time;  36.09183597564697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15611878037452698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.276958167552948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16435053944587708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31391850113868713
18 2.2737708999 	 0.3139185106 	 0.3255846178
epoch_time;  36.31384062767029
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1641533523797989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27847033739089966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17929871380329132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3351038694381714
19 2.2705802117 	 0.3351038752 	 0.3454444267
epoch_time;  36.29805660247803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16415883600711823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27838557958602905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.179249569773674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33491408824920654
It took 782.4160602092743 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▃▃▂▂▃▂▁▂▁▄▁▁▂▁▁▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▃▂▂▃▃▁▂▁▃▁▁▂▁▁▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▄▃▂▄▄▃▁▃▁▆▂▂▃▂▁▄▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▄▂▂▃▄▁▂▁▆▂▂▃▂▁▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.34472
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3114
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.20846
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17266
wandb:                         Train loss 2.26562
wandb: 
wandb: 🚀 View run red-mandu-1336 at: https://wandb.ai/nreints/thesis/runs/nzwg0dud
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165828-nzwg0dud/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171138-19yudrhi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-cake-1344
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/19yudrhi
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2831234931945801
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7903141379356384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32663801312446594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.75602126121521
0 5.2586383135 	 0.7560212831 	 0.7718723606
epoch_time;  36.46127271652222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21900874376296997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5329415798187256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24227158725261688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5292675495147705
1 2.7470147734 	 0.5292675534 	 0.5448211258
epoch_time;  36.5362184047699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20006044209003448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38270512223243713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2232869565486908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40117183327674866
2 2.545866205 	 0.4011718338 	 0.414741846
epoch_time;  36.12953424453735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19433549046516418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3600575625896454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22847126424312592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4140505790710449
3 2.4687752901 	 0.4140505816 	 0.4282971356
epoch_time;  35.98332738876343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19039051234722137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35563594102859497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20239657163619995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37373456358909607
4 2.4218730165 	 0.3737345515 	 0.3856918747
epoch_time;  36.537723779678345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15529212355613708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30403995513916016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1785169094800949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3269841969013214
5 2.391999758 	 0.3269841993 	 0.3396766456
epoch_time;  36.2888605594635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15760669112205505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2864226698875427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21245010197162628
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35271960496902466
6 2.3673964328 	 0.3527196111 	 0.3639134587
epoch_time;  37.72478461265564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16973115503787994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.357285737991333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22416739165782928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42368367314338684
7 2.3554320545 	 0.4236836614 	 0.4353001363
epoch_time;  37.96294045448303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1865590214729309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3385947644710541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1939823180437088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3438433110713959
8 2.3396293625 	 0.3438433054 	 0.3551955352
epoch_time;  37.51006507873535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1350494623184204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24573655426502228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1421753168106079
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2545927166938782
9 2.3252379586 	 0.2545927305 	 0.2658607689
epoch_time;  36.55936813354492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15397047996520996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2751009166240692
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19092802703380585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3171919286251068
10 2.3155358069 	 0.3171919333 	 0.327928574
epoch_time;  36.21878123283386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12719270586967468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23235321044921875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15417484939098358
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2660318613052368
11 2.3111105307 	 0.2660318735 	 0.2768495096
epoch_time;  36.431130170822144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23578333854675293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39802947640419006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2721419036388397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4409934878349304
12 2.304773784 	 0.4409934791 	 0.4509351576
epoch_time;  36.04922294616699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14655497670173645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24929523468017578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17105107009410858
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28084930777549744
13 2.300443883 	 0.2808493124 	 0.2910527616
epoch_time;  36.32364463806152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14158113300800323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24987566471099854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1605815440416336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.271990031003952
14 2.2909296716 	 0.2719900389 	 0.2823804907
epoch_time;  36.59497094154358
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16873984038829803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3119509816169739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18562094867229462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32926031947135925
15 2.2894730926 	 0.3292603158 	 0.3395736488
epoch_time;  36.35653018951416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1379190981388092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2308402955532074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17647810280323029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2783922553062439
16 2.2818070439 	 0.2783922556 	 0.2900076376
epoch_time;  35.859230279922485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12466652691364288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21526522934436798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14680762588977814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24666692316532135
17 2.2776652487 	 0.2466669237 	 0.2571600218
epoch_time;  36.52377438545227
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17262060940265656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3070853650569916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22539964318275452
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3662237226963043
18 2.2743915824 	 0.3662237219 	 0.3769955197
epoch_time;  36.3507878780365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17268390953540802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31132715940475464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20844455063343048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34467750787734985
19 2.2656173964 	 0.3446775076 	 0.353687142
epoch_time;  36.122711420059204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17266234755516052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3113979399204254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20845796167850494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34471750259399414
It took 789.9782598018646 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▃▃▃▂▃▂▄▃▁▂▁▁▁▂▁▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▂▂▂▃▂▂▂▃▃▂▂▁▁▁▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▃▂▃▃▄▂▅▃█▄▁▁▁▁▁▂▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▁▃▃▄▁▃▄▆▄▂▂▁▁▁▂▁▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.33636
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28703
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.16909
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14721
wandb:                         Train loss 2.24641
wandb: 
wandb: 🚀 View run twinkling-cake-1344 at: https://wandb.ai/nreints/thesis/runs/19yudrhi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171138-19yudrhi/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_172442-d4k6ai4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-rabbit-1351
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/d4k6ai4w
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25411897897720337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7066845297813416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24819396436214447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6952050924301147
0 5.34600297 	 0.6952051111 	 0.7135051006
epoch_time;  36.27172803878784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15712475776672363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4468899667263031
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19943909347057343
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5165551900863647
1 2.730361143 	 0.5165551675 	 0.532599908
epoch_time;  36.436418294906616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13368232548236847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2931603193283081
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17932651937007904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3813673257827759
2 2.5372893812 	 0.3813673277 	 0.397925547
epoch_time;  36.33293795585632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1601455956697464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32026126980781555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1923399567604065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38073447346687317
3 2.4496503351 	 0.3807344591 	 0.3955678579
epoch_time;  36.24575901031494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16805189847946167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30500131845474243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20414310693740845
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37551382184028625
4 2.4047006905 	 0.3755138088 	 0.389176776
epoch_time;  36.170888900756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17763766646385193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35267457365989685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21051685512065887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4210149943828583
5 2.3722060196 	 0.4210149817 	 0.4362038277
epoch_time;  36.071930170059204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1376006156206131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28963300585746765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16852517426013947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34444430470466614
6 2.349259115 	 0.3444443161 	 0.3600924105
epoch_time;  35.88877844810486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17426817119121552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2943646311759949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24145278334617615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39784419536590576
7 2.3238175792 	 0.3978441805 	 0.4089841276
epoch_time;  36.26724720001221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1765809953212738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3216814398765564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19867970049381256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3681234121322632
8 2.31107929 	 0.3681233999 	 0.3789721102
epoch_time;  36.04984450340271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22643882036209106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3797900080680847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2985573410987854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.478580117225647
9 2.3046828717 	 0.4785801243 	 0.4896900899
epoch_time;  36.53624725341797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18379496037960052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3580866754055023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2240985929965973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.413845956325531
10 2.2871100362 	 0.4138459489 	 0.4241978413
epoch_time;  38.17906045913696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1501348614692688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2862262725830078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1609613597393036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31161412596702576
11 2.2846038343 	 0.3116141242 	 0.3225186322
epoch_time;  37.930554151535034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1523476541042328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29240819811820984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16676084697246552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32683372497558594
12 2.2769848947 	 0.3268337147 	 0.3386610701
epoch_time;  35.99088525772095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13984888792037964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25697579979896545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16330750286579132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29653069376945496
13 2.2733234465 	 0.2965307081 	 0.307246234
epoch_time;  36.033193588256836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13703709840774536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25879526138305664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15732073783874512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2987775206565857
14 2.2665103197 	 0.2987775236 	 0.3089908291
epoch_time;  35.94683122634888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13213804364204407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2484969049692154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15783464908599854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.287422776222229
15 2.2585204582 	 0.2874227782 	 0.2980969816
epoch_time;  36.23467946052551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1476503163576126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28565219044685364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17066261172294617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32501763105392456
16 2.2578883704 	 0.3250176301 	 0.3352568549
epoch_time;  36.31825661659241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13176827132701874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25219282507896423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16164915263652802
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29654771089553833
17 2.2496742037 	 0.296547699 	 0.3070977288
epoch_time;  36.17709016799927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1374041736125946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25813087821006775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16332322359085083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2946913540363312
18 2.2475866939 	 0.2946913642 	 0.3045822144
epoch_time;  35.8599169254303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14717191457748413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28713148832321167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1691027730703354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3363312780857086
19 2.2464079757 	 0.3363312799 	 0.3465747318
epoch_time;  36.293068647384644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14720523357391357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2870262861251831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16909177601337433
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33635589480400085
It took 784.3725800514221 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▃▂▃▂▃▄▁▃▄▂▂▂▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▄▃▂▃▂▂▃▁▂▃▂▁▁▁▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▆▅▅▄▃▃▃▆█▁▅█▂▂▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇█▆▇▄▃▃▂▄█▂▄▇▂▁▂▁▃▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.28448
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24856
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.15326
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13264
wandb:                         Train loss 2.26827
wandb: 
wandb: 🚀 View run legendary-rabbit-1351 at: https://wandb.ai/nreints/thesis/runs/d4k6ai4w
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_172442-d4k6ai4w/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173742-ux7k73vp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-fuse-1359
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ux7k73vp
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22168543934822083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6666087508201599
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25108033418655396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6860225200653076
0 5.7324959271 	 0.6860225368 	 0.7034670443
epoch_time;  36.549843549728394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23134000599384308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5649809837341309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23091524839401245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5565878748893738
1 2.75390906 	 0.5565878791 	 0.5722811312
epoch_time;  36.061033725738525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20094959437847137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42257454991340637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.218626007437706
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45557519793510437
2 2.5593245189 	 0.4555751904 	 0.4686169599
epoch_time;  36.057175636291504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22629331052303314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3805667459964752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2048652470111847
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3723706901073456
3 2.4666884415 	 0.3723707044 	 0.3871724309
epoch_time;  36.501484870910645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16126056015491486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3192444145679474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19595585763454437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37347501516342163
4 2.4215897009 	 0.3734750284 	 0.3874116227
epoch_time;  35.98025727272034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1431649625301361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27844715118408203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1723792999982834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32235682010650635
5 2.3895866172 	 0.3223568066 	 0.3362153749
epoch_time;  35.82863116264343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15728986263275146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3173297643661499
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1833118498325348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3580310344696045
6 2.3637115127 	 0.3580310306 	 0.3713522421
epoch_time;  35.62187123298645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1416163444519043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25690945982933044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16778883337974548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3094026446342468
7 2.3464297616 	 0.3094026308 	 0.3217477747
epoch_time;  36.11969327926636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1604231894016266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2999294698238373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2290382832288742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3987061083316803
8 2.3413330574 	 0.3987060959 	 0.4107290319
epoch_time;  35.95821523666382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2365272343158722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3677469491958618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27072393894195557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4114066958427429
9 2.326180305 	 0.4114066871 	 0.4226967064
epoch_time;  35.828415870666504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1255054622888565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2189483642578125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1364031434059143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24085567891597748
10 2.3141825372 	 0.2408556758 	 0.2526603905
epoch_time;  36.04074835777283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1744128316640854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2974393963813782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.214926615357399
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36677882075309753
11 2.3131319226 	 0.3667788119 	 0.3778775189
epoch_time;  35.8197500705719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.218826562166214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3302488923072815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27312347292900085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4149477481842041
12 2.3038486282 	 0.4149477572 	 0.4249500996
epoch_time;  36.011091470718384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13094846904277802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2481619119644165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1477375477552414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28237465023994446
13 2.2953686672 	 0.2823746552 	 0.2927679526
epoch_time;  37.95358443260193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11621275544166565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21456748247146606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1550927311182022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27854233980178833
14 2.2901387353 	 0.2785423279 	 0.2894279892
epoch_time;  37.65106391906738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13172002136707306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22819246351718903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15654249489307404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27601680159568787
15 2.2832383328 	 0.2760168127 	 0.286070602
epoch_time;  36.194164514541626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12432455271482468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21438801288604736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16543865203857422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2835966944694519
16 2.2811539636 	 0.2835966987 	 0.2943666407
epoch_time;  35.969701290130615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14579567313194275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26053154468536377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16124670207500458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2916858494281769
17 2.2794272927 	 0.291685857 	 0.3029517509
epoch_time;  36.12005424499512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13598406314849854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26964494585990906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15226788818836212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2935159504413605
18 2.2737423039 	 0.2935159425 	 0.303982379
epoch_time;  35.805113315582275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1326926201581955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24861016869544983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15325048565864563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2844359278678894
19 2.2682721984 	 0.2844359321 	 0.2946753837
epoch_time;  36.32145833969116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13264380395412445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24855825304985046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1532607525587082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2844772934913635
It took 779.4046540260315 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▂▂▁▃▂▁▂▂▂▂▁▁▂▁▃▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▂▂▂▃▂▁▂▁▂▂▁▁▂▁▃▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▂▂▂▁▃▂▂▅▂▄▄▂▁▃▁▅▅▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▃▂▃▂▁▅▂▃▂▁▁▄▁▄▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.34794
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.31581
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.19807
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18143
wandb:                         Train loss 2.25966
wandb: 
wandb: 🚀 View run scintillating-fuse-1359 at: https://wandb.ai/nreints/thesis/runs/ux7k73vp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173742-ux7k73vp/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_175044-3yy00m3v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-fireworks-1367
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/3yy00m3v
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3076173663139343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7652872204780579
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31297144293785095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7732129096984863
0 5.2202736887 	 0.7732129071 	 0.7911057756
epoch_time;  36.46368384361267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23586799204349518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5493196249008179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24359475076198578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5891058444976807
1 2.7200434329 	 0.5891058432 	 0.6046745094
epoch_time;  35.90689253807068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16974279284477234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3597413897514343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18277376890182495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3957027196884155
2 2.5367384532 	 0.3957027126 	 0.4122780877
epoch_time;  35.971346616744995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15759406983852386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28518760204315186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17457538843154907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31799349188804626
3 2.4468847855 	 0.317993494 	 0.330609502
epoch_time;  36.059468507766724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18259494006633759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.317772775888443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18402449786663055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33313873410224915
4 2.4016187664 	 0.3331387288 	 0.3485605188
epoch_time;  35.927326917648315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15201811492443085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2770366668701172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15656469762325287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30032435059547424
5 2.3712544167 	 0.3003243524 	 0.3142195418
epoch_time;  36.44401693344116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17488957941532135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3601229786872864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21005822718143463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4173170328140259
6 2.3477314592 	 0.4173170347 	 0.4297529478
epoch_time;  35.90942883491516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1657998114824295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29380932450294495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1764359325170517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31651049852371216
7 2.3343224032 	 0.3165105046 	 0.3292556763
epoch_time;  36.15215849876404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13699384033679962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24098876118659973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17370858788490295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31035658717155457
8 2.3202771269 	 0.3103565732 	 0.3219767596
epoch_time;  35.974419832229614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22337864339351654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3341814875602722
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24133804440498352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3756321966648102
9 2.3079124043 	 0.3756322087 	 0.3855679074
epoch_time;  36.135340213775635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15690308809280396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26930245757102966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1873176544904709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3219550549983978
10 2.3017609531 	 0.3219550674 	 0.3320968422
epoch_time;  36.02468514442444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17592866718769073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3172561526298523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21555227041244507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3721604645252228
11 2.2917326744 	 0.3721604631 	 0.3818583308
epoch_time;  36.37190914154053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16383156180381775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2802962362766266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2116849571466446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34013813734054565
12 2.2877922084 	 0.3401381415 	 0.3507118431
epoch_time;  36.019864320755005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13821148872375488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2393074929714203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16566206514835358
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2877565324306488
13 2.2817353059 	 0.2877565332 	 0.2982538172
epoch_time;  36.00596809387207
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12888167798519135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25086551904678345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15736332535743713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30269119143486023
14 2.2785173441 	 0.3026911968 	 0.3136715863
epoch_time;  35.97004008293152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20375852286815643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33520638942718506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19834791123867035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33924391865730286
15 2.2708509648 	 0.3392439146 	 0.3485274031
epoch_time;  36.08986258506775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1378948837518692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24270033836364746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15325026214122772
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27790361642837524
16 2.2722240699 	 0.2779036032 	 0.2887924813
epoch_time;  38.669400453567505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20733456313610077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35283181071281433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2422085553407669
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4076083302497864
17 2.266398415 	 0.4076083209 	 0.41828238
epoch_time;  37.39308452606201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18585827946662903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3197508454322815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23671439290046692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39719295501708984
18 2.2614918 	 0.3971929602 	 0.4086843955
epoch_time;  36.670321226119995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18144936859607697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3156905472278595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19811852276325226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3480350077152252
19 2.2596578527 	 0.3480349979 	 0.3574043893
epoch_time;  36.17672896385193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18143267929553986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3158104419708252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.198067769408226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34794262051582336
It took 782.3724865913391 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▂▃▂▂▂▃▃▂▁▁▂▂▂▂▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▂▃▂▄▂▄▆▂▂▂▃▄▃▄▁▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▂▂▂▃▃▄▄▂▁▂▁▂▃▁▁▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.29076
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25986
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.16497
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14072
wandb:                         Train loss 2.27832
wandb: 
wandb: 🚀 View run thriving-fireworks-1367 at: https://wandb.ai/nreints/thesis/runs/3yy00m3v
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_175044-3yy00m3v/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_180345-7kqy01se
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-rocket-1374
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7kqy01se
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30030182003974915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8099648356437683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29803818464279175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7918671369552612
0 5.4582299689 	 0.7918671479 	 0.8088682432
epoch_time;  36.26832032203674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23408271372318268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5480139851570129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2559364140033722
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5956146121025085
1 2.7189612469 	 0.595614624 	 0.6127134581
epoch_time;  36.27276968955994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19406703114509583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39365580677986145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20333175361156464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4157896637916565
2 2.5320072161 	 0.4157896712 	 0.4305352701
epoch_time;  36.492539405822754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16189248859882355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32190975546836853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17848818004131317
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3454042077064514
3 2.4546643918 	 0.3454042177 	 0.3601415273
epoch_time;  36.293336629867554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16719765961170197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3434510827064514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19089345633983612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.388102263212204
4 2.4131072605 	 0.3881022685 	 0.4029497714
epoch_time;  35.87564778327942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15679283440113068
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2979138493537903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1721538007259369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3307734429836273
5 2.3828627497 	 0.3307734515 	 0.3435148497
epoch_time;  35.60400366783142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17050360143184662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30588027834892273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2194174826145172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3749575912952423
6 2.3627597445 	 0.3749576053 	 0.3874031685
epoch_time;  36.21439242362976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17228202521800995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3115333616733551
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1769762635231018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3280569911003113
7 2.3487017405 	 0.3280569953 	 0.3408255087
epoch_time;  36.09940266609192
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2048819214105606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36114004254341125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2190084159374237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38270118832588196
8 2.3347609306 	 0.3827011933 	 0.3945472923
epoch_time;  35.907042503356934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20675739645957947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3423886001110077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26070842146873474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40937307476997375
9 2.3213236091 	 0.4093730617 	 0.4194684415
epoch_time;  35.89857721328735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16051214933395386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2744818329811096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17144331336021423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30168774724006653
10 2.3157887081 	 0.3016877458 	 0.3118947003
epoch_time;  35.86938953399658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1398705095052719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24923497438430786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1733369380235672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2932076156139374
11 2.3067464138 	 0.2932076119 	 0.3049011024
epoch_time;  36.077616930007935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15024104714393616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26498207449913025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1673959344625473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2923680245876312
12 2.3043790596 	 0.292368028 	 0.3041067175
epoch_time;  36.31377053260803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13586679100990295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2574178874492645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18116657435894012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3270186483860016
13 2.2986091092 	 0.3270186347 	 0.3382741155
epoch_time;  35.84271311759949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16777591407299042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2730690836906433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2187729924917221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33458298444747925
14 2.2962548134 	 0.3345829732 	 0.3457956263
epoch_time;  36.27805519104004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17754265666007996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32377830147743225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20097190141677856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35330435633659363
15 2.2868592838 	 0.3533043526 	 0.3630012718
epoch_time;  35.95296049118042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13905327022075653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26429203152656555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20613974332809448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3575313985347748
16 2.2836006593 	 0.3575314084 	 0.3697993923
epoch_time;  36.43722677230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1339668333530426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.244787335395813
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1478785127401352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26223719120025635
17 2.2749306092 	 0.2622371777 	 0.2728744507
epoch_time;  35.992709159851074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1321127861738205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2552810609340668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1613314151763916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29418787360191345
18 2.2787927254 	 0.294187886 	 0.3048675331
epoch_time;  36.0000364780426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14074842631816864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2599186301231384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16495560109615326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.290830135345459
19 2.2783247755 	 0.2908301482 	 0.3012734078
epoch_time;  37.11043620109558
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14071796834468842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25986161828041077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1649676412343979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2907591760158539
It took 781.1165573596954 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▂▃▂▁▁▂▂▂▃▂▂▁▂▂▁▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▂▃▂▁▂▂▂▂▃▂▁▁▁▁▁▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▄▂▂▁▃▃▃▅▂▄▁▃▂▁▃▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▃▄▄▃▄▅▄▄▇▃▃▁▂▂▂▄▁▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.30501
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.2641
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.17735
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14058
wandb:                         Train loss 2.24169
wandb: 
wandb: 🚀 View run golden-rocket-1374 at: https://wandb.ai/nreints/thesis/runs/7kqy01se
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_180345-7kqy01se/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20558682084083557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6310688257217407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2853369116783142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7268527746200562
0 5.2434116743 	 0.7268527469 	 0.7448511072
epoch_time;  36.61206364631653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1963684856891632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5464794039726257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2337883561849594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5660735964775085
1 2.7332657057 	 0.5660735672 	 0.5828486675
epoch_time;  35.68390703201294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1469891220331192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3091779053211212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19476838409900665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36573895812034607
2 2.5302780658 	 0.365738946 	 0.3810581929
epoch_time;  35.98764133453369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16148829460144043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.343930721282959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20408855378627777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40450942516326904
3 2.4434699573 	 0.4045094258 	 0.4206770613
epoch_time;  36.09903383255005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15003179013729095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31168195605278015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17123116552829742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34220090508461
4 2.4036019751 	 0.342200903 	 0.3573433541
epoch_time;  36.2220938205719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13992716372013092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2588944435119629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1660398691892624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2965310513973236
5 2.3757977134 	 0.2965310586 	 0.309544022
epoch_time;  35.89172029495239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15901724994182587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3000556230545044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15555018186569214
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3015667498111725
6 2.3496315245 	 0.3015667477 	 0.3145314191
epoch_time;  35.857608795166016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16570225358009338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3069292902946472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18402260541915894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33894458413124084
7 2.3349748223 	 0.3389445949 	 0.353387245
epoch_time;  36.23020124435425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15707623958587646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2983038127422333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18962842226028442
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34162765741348267
8 2.320560773 	 0.3416276468 	 0.3542962976
epoch_time;  36.3821485042572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1569625288248062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29402509331703186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19568106532096863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3560163676738739
9 2.3113341748 	 0.3560163756 	 0.3683396623
epoch_time;  35.707077741622925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1894272267818451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3329702317714691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23266014456748962
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4035418629646301
10 2.2999537835 	 0.4035418536 	 0.4140392819
epoch_time;  36.13044810295105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13879355788230896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2739669680595398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16023042798042297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31035128235816956
11 2.2918832581 	 0.3103512738 	 0.3222495827
epoch_time;  35.99639821052551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14118386805057526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25569114089012146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.195993572473526
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32562291622161865
12 2.2837393418 	 0.3256229298 	 0.3371230873
epoch_time;  35.76626515388489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11853204667568207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2439737617969513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14619667828083038
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2724630534648895
13 2.2714473489 	 0.2724630407 	 0.2840797837
epoch_time;  35.92931365966797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13698716461658478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25866615772247314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17837266623973846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30734506249427795
14 2.2647610034 	 0.3073450656 	 0.3177501369
epoch_time;  35.945743560791016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1292022168636322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2576201558113098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16504867374897003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3050340414047241
15 2.262054464 	 0.3050340395 	 0.3163037481
epoch_time;  35.84829640388489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12835130095481873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26937583088874817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15531398355960846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30129560828208923
16 2.2521233788 	 0.3012955949 	 0.3115652961
epoch_time;  35.986629247665405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15705819427967072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30308470129966736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19146794080734253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3391449451446533
17 2.2552611726 	 0.3391449593 	 0.350329961
epoch_time;  35.922202587127686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1241663470864296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25572291016578674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16816860437393188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3077792823314667
18 2.2478386034 	 0.3077792812 	 0.3176670384
epoch_time;  36.068134784698486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14059095084667206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2641703486442566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17739266157150269
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.305125892162323
19 2.2416851687 	 0.3051258809 	 0.3163861043
epoch_time;  36.19065475463867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14057907462120056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2641012668609619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17734552919864655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30500927567481995
It took 780.6352801322937 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139390
Array Job ID: 2137927_17
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-15:21:00 core-walltime
Job Wall-clock time: 02:11:10
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
