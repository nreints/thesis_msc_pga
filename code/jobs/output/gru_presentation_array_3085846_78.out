wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_203637-uzwhobqb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-snowball-398
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/uzwhobqb
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▂▄▂▂▂▁▄▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▁▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▂▄▂▂▂▁▄▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▂▄▂▂▂▁▄▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run noble-snowball-398 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/uzwhobqb
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_203637-uzwhobqb/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_204327-6n482sqv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-cosmos-412
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/6n482sqv
Training on dataset: data_t(0,0)_r(5,20)_combi_pNone_gTrue
Testing on 4 datasets: ['data_t(0,0)_r(5,20)_full_pNone_gTrue', 'data_t(0,0)_r(5,20)_tennis_pNone_gTrue', 'data_t(0,0)_r(5,20)_semi_pNone_gTrue', 'data_t(0,0)_r(5,20)_combi_pNone_gTrue']
Focussing on identity: True
Using extra input: False
Using fr-fr as reference point.
----- ITERATION 1/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.81529903411865 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 13.493099212646484 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 13.043746709823608 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 13.05796766281128 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 13.24183464050293 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0006147163 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.4712e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.30649e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.82686e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.32263e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.612114429473877
Epoch 1/9
	 Logging train Loss: 1.07176e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.189e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 9.8634e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 9.3244e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.974e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.269144773483276
Epoch 2/9
	 Logging train Loss: 6.0303e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.97e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.8271e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.73344e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.4601e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.175017595291138
Epoch 3/9
	 Logging train Loss: 5.7309e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.757e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.08898e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.01961e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.5224e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.238743782043457
Epoch 4/9
	 Logging train Loss: 5.67e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.529e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.3782e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.0091e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.7627e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.098551511764526
Epoch 5/9
	 Logging train Loss: 4.7639e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.25e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.5132e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.1043e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.2489e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.353537797927856
Epoch 6/9
	 Logging train Loss: 4.6035e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.102e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.9175e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.5874e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.9583e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.330190420150757
Epoch 7/9
	 Logging train Loss: 3.9504e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.84e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.90752e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.71276e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.6911e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.137110471725464
Epoch 8/9
	 Logging train Loss: 3.2501e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.425e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.2926e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.8298e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.691e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 28.806923151016235
Epoch 9/9
	 Logging train Loss: 2.7329e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.05e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 3.3846e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.135e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.6923e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.233580112457275
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'quat_1'_'False'.pth
It took  411.08294892311096  seconds.
----- ITERATION 2/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.310075759887695 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 13.282722234725952 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.969270706176758 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.964643716812134 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.798285245895386 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0004843982 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.9637e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.90259e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.96059e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.75731e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.436778783798218
Epoch 1/9
	 Logging train Loss: 9.2267e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.361e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.44962e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.4582e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.8343e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.393263339996338
Epoch 2/9
	 Logging train Loss: 6.6123e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.966e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.9774e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.6678e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.8481e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.42749547958374
Epoch 3/9
	 Logging train Loss: 6.524e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.886e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.9752e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.3946e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3404e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.190611839294434
Epoch 4/9
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▄▂▂▃▂▂▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▄▂▃▄▂▂▁▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▄▂▂▄▂▂▁▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run lucky-cosmos-412 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/6n482sqv
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_204327-6n482sqv/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205017-u4m1g3ej
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-vortex-424
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/u4m1g3ej
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▅▃▃▃▂▁▂▁▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▃▁▁▁▁▁▁▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue █▅▃▃▃▂▁▂▁▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue █▅▃▃▃▂▁▂▁▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run worthy-vortex-424 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/u4m1g3ej
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205017-u4m1g3ej/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_205705-egaljoa8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-vortex-435
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/egaljoa8
	 Logging train Loss: 5.8698e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.017e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.31033e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.32174e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 7.0639e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.345750093460083
Epoch 5/9
	 Logging train Loss: 5.0246e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.452e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.0682e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.5059e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.7482e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.446885347366333
Epoch 6/9
	 Logging train Loss: 4.512e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.704e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.6009e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.8409e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.5612e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.572059631347656
Epoch 7/9
	 Logging train Loss: 3.7523e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.53e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.0435e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.2388e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.1491e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.171736478805542
Epoch 8/9
	 Logging train Loss: 3.1183e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.43e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.1377e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.1823e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.1912e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.287272930145264
Epoch 9/9
	 Logging train Loss: 2.7136e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.27e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.4199e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.4685e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.2637e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.25915765762329
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'quat_1'_'False'.pth
It took  410.0929775238037  seconds.
----- ITERATION 3/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.00198841094971 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.938417673110962 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.991089582443237 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.896127223968506 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.77777647972107 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0004847429 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 6.5541e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 3.54439e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.36121e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.96769e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.22189712524414
Epoch 1/9
	 Logging train Loss: 1.12703e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.52e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.14506e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.0718e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.09777e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.516998767852783
Epoch 2/9
	 Logging train Loss: 6.9438e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 4.204e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.16561e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.12738e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.6665e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.178825616836548
Epoch 3/9
	 Logging train Loss: 6.5021e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.994e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.20181e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.14238e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.8986e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.21994185447693
Epoch 4/9
	 Logging train Loss: 6.0474e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.752e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.21254e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.14799e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.8637e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.141265153884888
Epoch 5/9
	 Logging train Loss: 5.3908e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.853e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 8.9878e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.6194e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.4262e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.239063262939453
Epoch 6/9
	 Logging train Loss: 4.3693e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.246e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.4742e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.4617e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.1712e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.29609727859497
Epoch 7/9
	 Logging train Loss: 3.8045e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.9e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 6.4643e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 6.1172e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.1218e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.223782777786255
Epoch 8/9
	 Logging train Loss: 3.1631e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.31e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 3.3409e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.1961e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.5989e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.449826955795288
Epoch 9/9
	 Logging train Loss: 2.4362e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.05e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 3.7742e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.4898e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.8309e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.414658069610596
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'quat_1'_'False'.pth
It took  408.35283303260803  seconds.
----- ITERATION 4/10 ------
Number of train simulations:  1920
Number of test simulations:  480
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                                       Epoch ▁▂▃▃▄▅▆▆▇█
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue █▅▃▂█▃▂▅▂▁
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue █▂▁▁▂▁▁▂▁▁
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue ▇▅▃▂█▃▂▅▂▁
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue ▆▅▃▂█▃▂▅▂▁
wandb:                                  Train loss █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                       Epoch 9
wandb:  Test loss t(0,0)_r(5,20)_combi_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_full_pNone_gTrue 0.0
wandb:   Test loss t(0,0)_r(5,20)_semi_pNone_gTrue 0.0
wandb: Test loss t(0,0)_r(5,20)_tennis_pNone_gTrue 0.0
wandb:                                  Train loss 0.0
wandb: 
wandb: 🚀 View run lyric-vortex-435 at: https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/egaljoa8
wandb: Synced 7 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230716_205705-egaljoa8/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230716_210355-m9u81v09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-jazz-446
wandb: ⭐️ View project at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll
wandb: 🚀 View run at https://wandb.ai/nreints/ThesisFinal2Grav%2Bcoll/runs/m9u81v09
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 50.84353709220886 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.910900115966797 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.96928882598877 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.944272518157959 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.839591264724731 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.0004603892 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.9889e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.3323e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.29254e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.54402e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.68103528022766
Epoch 1/9
	 Logging train Loss: 1.01936e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.2962e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.75325e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.73016e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.00026e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 30.59299659729004
Epoch 2/9
	 Logging train Loss: 7.7289e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.37e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.02899e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.00189e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.7197e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.53651785850525
Epoch 3/9
	 Logging train Loss: 7.2914e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.331e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.6527e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.586e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.3321e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.56645441055298
Epoch 4/9
	 Logging train Loss: 6.5726e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 7.775e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.91385e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.84819e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.54875e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.337119579315186
Epoch 5/9
	 Logging train Loss: 5.4102e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.178e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 8.5104e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 8.2162e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.6221e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.405097007751465
Epoch 6/9
	 Logging train Loss: 4.4956e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.073e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 4.6822e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 4.6218e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 2.6553e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.276063919067383
Epoch 7/9
	 Logging train Loss: 3.7178e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.327e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.76751e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.67015e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 9.4607e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.413413047790527
Epoch 8/9
	 Logging train Loss: 3.2187e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 1.608e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 5.6236e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 5.3446e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 3.0706e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.397873878479004
Epoch 9/9
	 Logging train Loss: 2.7838e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.08e-08 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.5456e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.4376e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.4049e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.35887122154236
Saved model in  trained_models/gru/data_t(0,0)_r(5,20)_combi_pNone_gTrue/'quat_1'_'False'.pth
It took  409.99033546447754  seconds.
----- ITERATION 5/10 ------
Number of train simulations:  1920
Number of test simulations:  480
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 51.97179627418518 seconds.
-- Finished Train Dataloader --
The dataloader for data/data_t(0,0)_r(5,20)_full_pNone_gTrue took 12.896175384521484 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_tennis_pNone_gTrue took 12.940518379211426 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_semi_pNone_gTrue took 12.893564224243164 seconds.
The dataloader for data/data_t(0,0)_r(5,20)_combi_pNone_gTrue took 12.958974599838257 seconds.
-- Finished Test Dataloader(s) --
GRU(
  (rnn): GRU(7, 96, batch_first=True)
  (fc): Linear(in_features=96, out_features=7, bias=True)
)
Datatype: quat_1
-- Started Training --
Epoch 0/9
	 Logging train Loss: 0.000577647 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 5.6775e-06 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 3.11848e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 3.10903e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.86759e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.52482318878174
Epoch 1/9
	 Logging train Loss: 9.887e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 9.383e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.05809e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.11492e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.9394e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.45409107208252
Epoch 2/9
	 Logging train Loss: 6.6272e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 3.791e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 1.03295e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 1.07472e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 5.5402e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.5776629447937
Epoch 3/9
	 Logging train Loss: 6.3622e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 8.265e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 2.84661e-05 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 2.77786e-05 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 1.47144e-05 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.34838342666626
Epoch 4/9
	 Logging train Loss: 6.4791e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
	 Logging test loss: 2.662e-07 [MSELoss(): t(0,0)_r(5,20)_full_pNone_gTrue]
	 Logging test loss: 7.5552e-06 [MSELoss(): t(0,0)_r(5,20)_tennis_pNone_gTrue]
	 Logging test loss: 7.8501e-06 [MSELoss(): t(0,0)_r(5,20)_semi_pNone_gTrue]
	 Logging test loss: 4.0299e-06 [MSELoss(): t(0,0)_r(5,20)_combi_pNone_gTrue]
		--> Epoch time; 29.46127724647522
Epoch 5/9
slurmstepd: error: *** JOB 3085908 ON gcn22 CANCELLED AT 2023-07-16T21:09:10 ***
slurmstepd: error: *** STEP 3085908.0 ON gcn22 CANCELLED AT 2023-07-16T21:09:10 ***

JOB STATISTICS
==============
Job ID: 3085908
Array Job ID: 3085846_78
Cluster: snellius
User/Group: nreints/nreints
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 09:49:30 core-walltime
Job Wall-clock time: 00:32:45
Memory Utilized: 6.70 MB
Memory Efficiency: 0.00% of 0.00 MB
