wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_182400-pfkkcxse
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-fuse-1385
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/pfkkcxse
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–„â–„â–â–„â–†â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–…â–ƒâ–‚â–…â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–ˆâ–‚â–„â–‚â–ƒâ–‚â–‚â–ƒâ–†â–ƒâ–†â–„â–â–ƒâ–…â–‚â–„â–…â–ˆâ–ˆ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–ˆâ–ƒâ–„â–â–ƒâ–…â–â–ƒâ–„â–ƒâ–„â–ƒâ–â–â–†â–„â–‚â–…â–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ˆâ–â–„â–‚â–ƒâ–‚â–â–„â–…â–ƒâ–‡â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–„â–‡â–‡
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.2334
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.6744
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.71406
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.69806
wandb:                         Train loss 8.0536
wandb: 
wandb: ðŸš€ View run twinkling-fuse-1385 at: https://wandb.ai/nreints/thesis/runs/pfkkcxse
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_182400-pfkkcxse/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_184246-gg91hi3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-envelope-1391
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/gg91hi3q
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5939294099807739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6324455738067627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9048187136650085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6699626445770264
0 13.3869938008 	 1.6699626201 	 1.699642202
epoch_time;  39.838664531707764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7603219747543335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.730660319328308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8897506594657898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5485783815383911
1 9.488498117 	 1.5485783757 	 1.5735107422
epoch_time;  38.443970680236816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33651113510131836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9339119791984558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6670343279838562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2497197389602661
2 9.0403716359 	 1.2497197332 	 1.2713978535
epoch_time;  38.56172442436218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4863208532333374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2733482122421265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6825190186500549
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2758437395095825
3 8.816229715 	 1.2758437698 	 1.2951443234
epoch_time;  38.487300872802734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.376736581325531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9534303545951843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5483364462852478
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0057055950164795
4 8.6456255025 	 1.0057055499 	 1.0232110101
epoch_time;  38.6056182384491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43254658579826355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0711930990219116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6464639902114868
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2930930852890015
5 8.5510338207 	 1.2930931298 	 1.3084779482
epoch_time;  38.526477575302124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38201412558555603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9713571667671204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7669301629066467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4721506834030151
6 8.4835121604 	 1.472150648 	 1.4860245988
epoch_time;  38.34595489501953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3279859125614166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9343839287757874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5710952877998352
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1035258769989014
7 8.3931725256 	 1.1035258525 	 1.1160918365
epoch_time;  38.90460705757141
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4863719940185547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1731117963790894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.673557460308075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1706111431121826
8 8.3694659254 	 1.1706111599 	 1.1820664689
epoch_time;  38.46680212020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5546435117721558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4714939594268799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6786417961120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1977782249450684
9 8.3005637219 	 1.1977782378 	 1.2081365637
epoch_time;  38.20911264419556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4482653737068176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.058254599571228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6553429961204529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2275692224502563
10 8.265286476 	 1.2275691677 	 1.2370122136
epoch_time;  38.614023208618164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6858209371566772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5233741998672485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7184731960296631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2202625274658203
11 8.2354732196 	 1.2202625172 	 1.2292219007
epoch_time;  38.365249156951904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.468219131231308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.217291235923767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6389204263687134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2105809450149536
12 8.1831057993 	 1.2105809392 	 1.2191450789
epoch_time;  38.40128827095032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4433501362800598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8695653080940247
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5721707344055176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9960535764694214
13 8.1589588535 	 0.9960535823 	 1.0049365894
epoch_time;  38.369248390197754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4647626578807831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1705563068389893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.552660346031189
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0210753679275513
14 8.1338889006 	 1.021075357 	 1.0297650311
epoch_time;  38.35303854942322
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5829635262489319
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3705549240112305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.782480776309967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4117062091827393
15 8.1097862538 	 1.411706213 	 1.4202379381
epoch_time;  38.58417010307312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4255398213863373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.010692834854126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6928149461746216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2303613424301147
16 8.0899085456 	 1.2303613611 	 1.2395653802
epoch_time;  39.23432469367981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48388898372650146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2484163045883179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6174657940864563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1121872663497925
17 8.0993568043 	 1.1121873185 	 1.1221080883
epoch_time;  38.303300857543945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5193761587142944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.398484706878662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.735893964767456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.367066740989685
18 8.0783299657 	 1.3670667494 	 1.3767116237
epoch_time;  38.40291094779968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6979551315307617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6745965480804443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7141658663749695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2335128784179688
19 8.0536018428 	 1.2335128372 	 1.2435554298
epoch_time;  38.397342920303345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6980626583099365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6744035482406616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7140580415725708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.233396053314209
It took 1125.8331627845764 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‡â–ˆâ–‚â–‚â–ƒâ–â–„â–„â–ƒâ–‚â–…â–…â–…â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ˆâ–‚â–â–…â–ƒâ–…â–‡â–…â–ƒâ–…â–ƒâ–„â–„â–…â–„â–‚â–ƒâ–‚â–†â–†
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–ˆâ–‚â–‚â–ƒâ–â–„â–„â–ƒâ–‚â–…â–„â–†â–„â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–ˆâ–‚â–â–„â–‚â–…â–ˆâ–†â–‚â–…â–„â–…â–…â–„â–„â–„â–„â–ƒâ–†â–†
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.26304
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.42785
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.73416
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.59888
wandb:                         Train loss 8.06975
wandb: 
wandb: ðŸš€ View run luminous-envelope-1391 at: https://wandb.ai/nreints/thesis/runs/gg91hi3q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_184246-gg91hi3q/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_185628-xcv8mgic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glittering-rabbit-1394
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/xcv8mgic
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4574921429157257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0925122499465942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8636869788169861
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.591741919517517
0 15.2690115627 	 1.5917419434 	 1.6261161186
epoch_time;  38.3695855140686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.746820330619812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7184813022613525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9538024663925171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6800075769424438
1 9.5488082896 	 1.6800075222 	 1.7064957902
epoch_time;  38.403852224349976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3772103786468506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.808934211730957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6065499186515808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1414798498153687
2 9.0546855165 	 1.1414798221 	 1.1643637167
epoch_time;  37.765758991241455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28910139203071594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7336263060569763
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6127378344535828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1858489513397217
3 8.8171077617 	 1.185848999 	 1.205532672
epoch_time;  37.868043661117554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4826958179473877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.245518445968628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.704962968826294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2606961727142334
4 8.6660394275 	 1.2606961637 	 1.27815593
epoch_time;  37.96598792076111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3499232828617096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9550256729125977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.568291425704956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0713589191436768
5 8.5736144317 	 1.071358923 	 1.0866305789
epoch_time;  38.41758966445923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5438326597213745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.348991870880127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7207228541374207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.341181993484497
6 8.486460361 	 1.341181987 	 1.3549723032
epoch_time;  38.29994463920593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7289813756942749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.570813536643982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7535992860794067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3164585828781128
7 8.4199840259 	 1.3164586248 	 1.3294004698
epoch_time;  38.53526711463928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5881490707397461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3058496713638306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6905650496482849
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2527967691421509
8 8.3633394338 	 1.2527968123 	 1.2645616686
epoch_time;  37.71387052536011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37383198738098145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9614313244819641
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6459031105041504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1578319072723389
9 8.3049942874 	 1.1578319653 	 1.1685766272
epoch_time;  38.05734205245972
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5635841488838196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.241349220275879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7883472442626953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3938769102096558
10 8.2681533514 	 1.3938768541 	 1.4034948401
epoch_time;  39.09402084350586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46484631299972534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9831977486610413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7508376836776733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.405314564704895
11 8.2465783948 	 1.4053145125 	 1.4145996094
epoch_time;  38.317336559295654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5544291734695435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2249387502670288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8328696489334106
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4117423295974731
12 8.208285508 	 1.4117423393 	 1.4204283018
epoch_time;  37.62355399131775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5352344512939453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1235864162445068
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7139605283737183
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2540265321731567
13 8.1684950491 	 1.2540265058 	 1.2630462646
epoch_time;  37.47124671936035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49640175700187683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2528915405273438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5989119410514832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0645520687103271
14 8.162032017 	 1.0645520184 	 1.0735760663
epoch_time;  37.45801877975464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46359285712242126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.095895767211914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6661570072174072
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1651228666305542
15 8.134243293 	 1.1651228621 	 1.1740922258
epoch_time;  37.548136472702026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4588308334350586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9438936114311218
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6872122287750244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2293452024459839
16 8.122609174 	 1.2293452082 	 1.2385574753
epoch_time;  37.880019426345825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48080024123191833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0371012687683105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7119102478027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2597005367279053
17 8.1162096151 	 1.2597005483 	 1.2692809234
epoch_time;  37.682332038879395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3980281949043274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.915434718132019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6583147048950195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1561404466629028
18 8.0851268936 	 1.1561404666 	 1.165947579
epoch_time;  37.537211418151855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5985792279243469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4273109436035156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7339064478874207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2631843090057373
19 8.0697504972 	 1.2631842536 	 1.2730945484
epoch_time;  37.893471002578735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5988751649856567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4278491735458374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7341558337211609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2630391120910645
It took 822.8514969348907 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–‚â–…â–…â–†â–â–‚â–„â–…â–„â–ƒâ–„â–„â–‚â–ƒâ–‚â–‚â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–…â–‚â–ˆâ–ƒâ–‚â–„â–ƒâ–„â–‡â–„â–‚â–…â–„â–‚â–…â–‚â–…â–â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–ˆâ–‚â–ˆâ–„â–†â–â–ƒâ–„â–†â–„â–‚â–„â–†â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ƒâ–„â–â–ˆâ–‚â–‚â–ƒâ–‚â–„â–†â–ƒâ–‚â–„â–„â–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.13347
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.16653
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.63774
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.4458
wandb:                         Train loss 8.09615
wandb: 
wandb: ðŸš€ View run glittering-rabbit-1394 at: https://wandb.ai/nreints/thesis/runs/xcv8mgic
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_185628-xcv8mgic/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_191016-6kbk27wu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-envelope-1398
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6kbk27wu
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47986581921577454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2046540975570679
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8053798079490662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5345629453659058
0 13.5191279303 	 1.5345628893 	 1.5657594423
epoch_time;  38.01947569847107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6192671656608582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4336414337158203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8601239919662476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4929240942001343
1 9.5307931248 	 1.4929240459 	 1.5191237991
epoch_time;  38.015296936035156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34724104404449463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.908775806427002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6283392906188965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.152146816253662
2 9.0863990035 	 1.1521467879 	 1.1752801844
epoch_time;  38.0524845123291
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9309796094894409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8163059949874878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8492258191108704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3573311567306519
3 8.8402351684 	 1.3573311471 	 1.3775481353
epoch_time;  37.98945355415344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46638229489326477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1517518758773804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7182157039642334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3294528722763062
4 8.7001913454 	 1.3294528446 	 1.3472527581
epoch_time;  37.79237723350525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.425495445728302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9277425408363342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7725172638893127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3921834230422974
5 8.6018593697 	 1.392183376 	 1.4079662426
epoch_time;  38.03167200088501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5034642815589905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.214247465133667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.582557201385498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.059152603149414
6 8.505873921 	 1.0591525516 	 1.0738853661
epoch_time;  38.25519156455994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4416658282279968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0833888053894043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6725291609764099
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1515971422195435
7 8.4388269878 	 1.1515971416 	 1.1643159609
epoch_time;  38.482882261276245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.565391480922699
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2837507724761963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7106447219848633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2421984672546387
8 8.3811886425 	 1.2421984698 	 1.2534060916
epoch_time;  37.77989649772644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7708955407142639
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6904933452606201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7947381138801575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3098905086517334
9 8.3350277167 	 1.3098904996 	 1.3199270712
epoch_time;  37.869887351989746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5554571747779846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.23952054977417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7124322056770325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2445827722549438
10 8.3018609565 	 1.2445828 	 1.2539973903
epoch_time;  38.24114632606506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42475807666778564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0154649019241333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.61578768491745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1894936561584473
11 8.2468541067 	 1.1894936227 	 1.198053061
epoch_time;  38.01858067512512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6247273087501526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3507509231567383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.70298171043396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2797737121582031
12 8.2024367119 	 1.2797736915 	 1.2881535711
epoch_time;  39.87865352630615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5839169025421143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2432891130447388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7626108527183533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2889422178268433
13 8.1969852848 	 1.2889422442 	 1.2974939295
epoch_time;  38.87556195259094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47530847787857056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.992743194103241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6257627606391907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.125282883644104
14 8.1657319789 	 1.1252829062 	 1.1340511013
epoch_time;  37.88600301742554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5540014505386353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.334334373474121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6747817993164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1669344902038574
15 8.1352800229 	 1.166934534 	 1.1756404567
epoch_time;  37.975542068481445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43687888979911804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9852428436279297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6127246618270874
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.103102207183838
16 8.1181885262 	 1.1031022355 	 1.1123463399
epoch_time;  37.619898080825806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.563666045665741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.401476502418518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.656500518321991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.154241681098938
17 8.107558078 	 1.1542416959 	 1.1639311095
epoch_time;  37.95790982246399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4203280806541443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8264521956443787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6077276468276978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0877084732055664
18 8.0848260568 	 1.0877085093 	 1.0975517479
epoch_time;  38.1972222328186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4459921717643738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1654311418533325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6375907063484192
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1335763931274414
19 8.096148099 	 1.1335764292 	 1.143460248
epoch_time;  37.856621980667114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4458036720752716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1665266752243042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6377365589141846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.133470058441162
It took 827.8617169857025 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–†â–ˆâ–‚â–ƒâ–„â–„â–‚â–„â–â–…â–‚â–…â–â–„â–ƒâ–„â–ƒâ–ƒâ–„â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–‡â–‚â–†â–ˆâ–„â–…â–†â–ƒâ–ˆâ–â–†â–‚â–†â–„â–„â–‚â–†â–ƒâ–…â–…
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–ˆâ–â–ƒâ–„â–„â–‚â–„â–â–†â–‚â–…â–â–„â–ƒâ–„â–‚â–ƒâ–„â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–…â–â–†â–ˆâ–‚â–ƒâ–ˆâ–‚â–ˆâ–‚â–…â–‚â–†â–„â–„â–‚â–†â–‚â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.06058
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.27554
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.58811
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.4735
wandb:                         Train loss 8.09049
wandb: 
wandb: ðŸš€ View run enchanting-envelope-1398 at: https://wandb.ai/nreints/thesis/runs/6kbk27wu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_191016-6kbk27wu/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_192405-440jdwd0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-rabbit-1401
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/440jdwd0
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5261806845664978
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3485263586044312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8496245741844177
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5422134399414062
0 14.4001539226 	 1.5422133987 	 1.5732291557
epoch_time;  37.78014945983887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5783516764640808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4631872177124023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9888355731964111
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7128944396972656
1 9.5811009701 	 1.7128944191 	 1.7381679226
epoch_time;  37.836639642715454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3763308525085449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.937445342540741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5988093018531799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0789273977279663
2 9.1032954521 	 1.0789273649 	 1.1021665006
epoch_time;  38.35030198097229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5996074080467224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3841785192489624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7092347741127014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1663434505462646
3 8.8718375772 	 1.1663434003 	 1.1868323249
epoch_time;  37.921006202697754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6882444024085999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6199336051940918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7226794958114624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2989470958709717
4 8.6918188262 	 1.2989471436 	 1.3161595525
epoch_time;  37.943068981170654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44398269057273865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1364754438400269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7286114692687988
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2879343032836914
5 8.5920216761 	 1.2879342569 	 1.3032449258
epoch_time;  37.78966403007507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4513530135154724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2521969079971313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.630544126033783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1314711570739746
6 8.5106284646 	 1.1314711287 	 1.1451098468
epoch_time;  37.99188041687012
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6843568086624146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4118664264678955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7661803960800171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3325871229171753
7 8.4424588482 	 1.3325870823 	 1.3446607435
epoch_time;  37.86494016647339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4188234508037567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.027569055557251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5829393267631531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9918174147605896
8 8.388123757 	 0.9918174125 	 1.0032819593
epoch_time;  38.04519295692444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7047132253646851
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6180475950241089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.840662956237793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4492595195770264
9 8.3374251545 	 1.4492594951 	 1.4587884027
epoch_time;  38.010584115982056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4039246439933777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.83357173204422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6048063635826111
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0815417766571045
10 8.3025472611 	 1.0815417316 	 1.0907961459
epoch_time;  37.97030305862427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5730974078178406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3683388233184814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7897742986679077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3702516555786133
11 8.2423832092 	 1.3702516298 	 1.3788161819
epoch_time;  37.66202902793884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.408643513917923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9630315899848938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5694311261177063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.016371488571167
12 8.2028367256 	 1.0163715259 	 1.0248800742
epoch_time;  37.93357181549072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.605095624923706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4421484470367432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7593365907669067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.272851586341858
13 8.1964874164 	 1.2728515625 	 1.281402258
epoch_time;  38.616501569747925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4968580901622772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1603412628173828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.664475679397583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1816359758377075
14 8.1686768735 	 1.1816360061 	 1.1900045859
epoch_time;  38.63364863395691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.530250072479248
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.199925184249878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7468835711479187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2997583150863647
15 8.1426997821 	 1.2997583338 	 1.3085886362
epoch_time;  38.150700092315674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4289104640483856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9921167492866516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6118967533111572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1516882181167603
16 8.1292156821 	 1.1516881994 	 1.1605876201
epoch_time;  38.14207720756531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.589137077331543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3743654489517212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6800062656402588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2282023429870605
17 8.113332822 	 1.2282023662 	 1.2373997044
epoch_time;  38.031428813934326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4416690170764923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0654892921447754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7337816953659058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2682042121887207
18 8.0988139758 	 1.2682042302 	 1.2774360451
epoch_time;  37.698084115982056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4734654724597931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2752149105072021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5882298946380615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0605313777923584
19 8.0904870779 	 1.0605313688 	 1.0702903954
epoch_time;  37.89717221260071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4735003411769867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.275537133216858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5881118178367615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0605803728103638
It took 828.8031933307648 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–†â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–â–„â–â–‚â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ƒâ–ƒâ–ƒâ–…â–†â–ˆâ–â–„â–‚â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–„â–„â–‡â–â–„â–ƒâ–ƒâ–ƒâ–‚â–…â–‚â–‚â–„â–â–ƒâ–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–ƒâ–‚â–‚â–…â–†â–ˆâ–â–„â–â–‚â–‚â–ƒâ–„â–â–‚â–‚â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.25606
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.13245
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.70072
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.49403
wandb:                         Train loss 8.06383
wandb: 
wandb: ðŸš€ View run flashing-rabbit-1401 at: https://wandb.ai/nreints/thesis/runs/440jdwd0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_192405-440jdwd0/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_193739-p3vkbsoc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-lamp-1404
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/p3vkbsoc
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.473080039024353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2645388841629028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9950752258300781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7908508777618408
0 13.937716867 	 1.7908508301 	 1.8199637748
epoch_time;  38.49971127510071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5174474716186523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2010895013809204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8946684002876282
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.581168532371521
1 9.4686733031 	 1.5811685098 	 1.6066635544
epoch_time;  37.83856129646301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46168482303619385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2157658338546753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7362139821052551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.352880835533142
2 9.0349883103 	 1.3528808594 	 1.3743907207
epoch_time;  37.82691192626953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4478948712348938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.162848711013794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7014598250389099
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2393254041671753
3 8.806692197 	 1.2393254461 	 1.2579450453
epoch_time;  38.28011226654053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6530795693397522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.417868971824646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7456987500190735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2873787879943848
4 8.6438925504 	 1.287378837 	 1.3032082223
epoch_time;  37.963231325149536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7385129332542419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5982615947723389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7235885262489319
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.174970030784607
5 8.5453675102 	 1.1749700598 	 1.1893816149
epoch_time;  38.0207200050354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9031075239181519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9637848138809204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9563941955566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5930640697479248
6 8.4403569577 	 1.5930640968 	 1.6049082163
epoch_time;  36.933446168899536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4012240767478943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8276883363723755
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5902355909347534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.046268343925476
7 8.4080715502 	 1.04626836 	 1.0574286384
epoch_time;  35.871389389038086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5891286134719849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2347681522369385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7565993070602417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2897955179214478
8 8.321004094 	 1.2897954992 	 1.2991057525
epoch_time;  35.90820503234863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4349575936794281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.019487738609314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6992236971855164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2796980142593384
9 8.2764101642 	 1.279697975 	 1.2881157128
epoch_time;  35.98470664024353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5084095597267151
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.239120364189148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6894410252571106
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2334084510803223
10 8.2430311654 	 1.2334084176 	 1.2416560817
epoch_time;  36.61017918586731
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44742727279663086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0220526456832886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.667733371257782
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2060256004333496
11 8.196272533 	 1.2060256546 	 1.214282309
epoch_time;  36.68876028060913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5154518485069275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.223958969116211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.644435703754425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1339495182037354
12 8.1626951803 	 1.133949486 	 1.1422595875
epoch_time;  35.97888135910034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5806806683540344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3587461709976196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7863180637359619
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3890776634216309
13 8.1307330897 	 1.3890776763 	 1.3972789868
epoch_time;  36.23020052909851
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4319784343242645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0884332656860352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6296084523200989
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.11232328414917
14 8.1044505332 	 1.1123232455 	 1.1211423307
epoch_time;  36.327890157699585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4769858717918396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1379002332687378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6249601244926453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0646255016326904
15 8.0975603789 	 1.0646255081 	 1.0738552609
epoch_time;  36.130109786987305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5057525038719177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0866856575012207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.746618926525116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3558911085128784
16 8.0823003502 	 1.3558911298 	 1.3651195629
epoch_time;  37.96985340118408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4116734266281128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1302884817123413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5599838495254517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0244511365890503
17 8.0677405107 	 1.024451096 	 1.0344041154
epoch_time;  38.598717212677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4709997773170471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1506019830703735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6723929047584534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1566071510314941
18 8.0568191435 	 1.1566071381 	 1.1668907372
epoch_time;  36.94129037857056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49391311407089233
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1323047876358032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7004772424697876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2557412385940552
19 8.063827649 	 1.2557412637 	 1.2664845235
epoch_time;  36.745904207229614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49403443932533264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1324477195739746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7007235288619995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2560629844665527
It took 813.3669302463531 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–„â–ƒâ–â–â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–ˆâ–…â–‚â–‚â–â–â–â–„â–‚â–‡â–†â–‚â–…â–ƒâ–†â–„â–„â–‚â–„â–„
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–†â–„â–ƒâ–â–â–‚â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ˆâ–„â–‚â–â–â–‚â–â–ƒâ–ƒâ–†â–„â–‚â–…â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.20157
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.22877
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.66524
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.4698
wandb:                         Train loss 8.12649
wandb: 
wandb: ðŸš€ View run flashing-lamp-1404 at: https://wandb.ai/nreints/thesis/runs/p3vkbsoc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_193739-p3vkbsoc/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_195114-b2ce5dmr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-pig-1407
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/b2ce5dmr
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5939817428588867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4653608798980713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.997432291507721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7633274793624878
0 13.6735372059 	 1.7633274388 	 1.7943780023
epoch_time;  37.21267509460449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7710729837417603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.590908408164978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9106997847557068
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5674527883529663
1 9.5223115109 	 1.5674527555 	 1.5922571131
epoch_time;  37.294830083847046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5386283993721008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.315843939781189
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8247769474983215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4700812101364136
2 9.0962353747 	 1.4700812262 	 1.4922272553
epoch_time;  37.56875944137573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3925226926803589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9315508008003235
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6902141571044922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3101874589920044
3 8.8787614516 	 1.3101874274 	 1.3303771148
epoch_time;  37.356953620910645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36710283160209656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9653868675231934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6175363659858704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1548000574111938
4 8.7263965439 	 1.1548000851 	 1.1722843479
epoch_time;  37.31192326545715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3402922749519348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.918676495552063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49394118785858154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8951228857040405
5 8.6209707706 	 0.8951228786 	 0.911371427
epoch_time;  37.577181577682495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3891226649284363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9170088768005371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5243564248085022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9438784122467041
6 8.5499018809 	 0.9438784213 	 0.9587093869
epoch_time;  36.896186113357544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36243173480033875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.868355929851532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5669518709182739
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0067765712738037
7 8.4755118854 	 1.0067765519 	 1.0199560217
epoch_time;  36.984867334365845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4681782126426697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2199585437774658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.741692841053009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2901095151901245
8 8.4125431089 	 1.2901095004 	 1.3013741158
epoch_time;  37.31303572654724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4619470238685608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9509419202804565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6562497615814209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1286109685897827
9 8.3728121562 	 1.1286109718 	 1.1390794909
epoch_time;  37.34289240837097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6499494314193726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4691864252090454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7555322647094727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2827798128128052
10 8.3388493356 	 1.2827798379 	 1.2923640071
epoch_time;  37.52993154525757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.548656702041626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3700629472732544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6489880681037903
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1200579404830933
11 8.3052263615 	 1.1200579669 	 1.1291623502
epoch_time;  37.12651085853577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4297025203704834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0151798725128174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5930145382881165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.081168532371521
12 8.273286751 	 1.0811685098 	 1.0898953
epoch_time;  36.793211460113525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5645667910575867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3303438425064087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6485334038734436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.096752643585205
13 8.220820815 	 1.0967525998 	 1.1051821322
epoch_time;  37.29797554016113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44396066665649414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1145505905151367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6047819256782532
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0701549053192139
14 8.2183870123 	 1.0701549633 	 1.078828224
epoch_time;  37.07378387451172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5679866671562195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.377121925354004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6485300660133362
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1135201454162598
15 8.1754658673 	 1.1135201944 	 1.1220793028
epoch_time;  36.923274517059326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44352293014526367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1372724771499634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6005958914756775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.141159176826477
16 8.1666338281 	 1.1411591401 	 1.1499421816
epoch_time;  37.64087963104248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4823300838470459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1600658893585205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6237894296646118
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0640980005264282
17 8.1473610994 	 1.0640979664 	 1.0733558449
epoch_time;  37.24880290031433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41354307532310486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9479243755340576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6682022213935852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1873223781585693
18 8.1310352488 	 1.1873223382 	 1.1967415475
epoch_time;  37.85289931297302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4699191153049469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.229323148727417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6654804944992065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2009215354919434
19 8.1264949274 	 1.2009215484 	 1.2107929127
epoch_time;  37.970850467681885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4697960913181305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2287732362747192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6652379035949707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2015732526779175
It took 815.0909585952759 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–…â–…â–ƒâ–â–‚â–â–â–„â–ƒâ–‚â–â–„â–‚â–†â–…â–‚â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ˆâ–„â–ƒâ–†â–„â–„â–…â–ƒâ–‚â–â–ƒâ–„â–„â–‚â–‚â–†â–ˆâ–â–…â–…
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–…â–…â–…â–„â–‚â–„â–‚â–â–ƒâ–ƒâ–„â–‚â–„â–ƒâ–‡â–†â–‚â–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ƒâ–ˆâ–„â–â–…â–ƒâ–ƒâ–†â–ƒâ–â–â–‚â–„â–ƒâ–‚â–‚â–‡â–ˆâ–â–„â–„
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.21144
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.35007
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.69326
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.56779
wandb:                         Train loss 8.10383
wandb: 
wandb: ðŸš€ View run twinkling-pig-1407 at: https://wandb.ai/nreints/thesis/runs/b2ce5dmr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_195114-b2ce5dmr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_200443-6y5nvgs2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glittering-rat-1410
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6y5nvgs2
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.497327595949173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.282437801361084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9063044190406799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6911729574203491
0 13.8493343242 	 1.6911729967 	 1.720756737
epoch_time;  37.14305019378662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8099044561386108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8208986520767212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8720711469650269
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4898747205734253
1 9.5381679238 	 1.4898747625 	 1.5163245949
epoch_time;  36.81581211090088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5647832751274109
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.241666316986084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7477678656578064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3777921199798584
2 9.08404322 	 1.377792111 	 1.401369332
epoch_time;  37.31510019302368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38875824213027954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0842208862304688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7236446142196655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3848607540130615
3 8.8509469416 	 1.3848608069 	 1.4067087534
epoch_time;  37.053046226501465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6124538779258728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5202878713607788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7406193017959595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3640528917312622
4 8.6767346712 	 1.3640529323 	 1.38370683
epoch_time;  37.02429413795471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5107198357582092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2556321620941162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6778939962387085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2331924438476562
5 8.5874945783 	 1.2331924851 	 1.2513284219
epoch_time;  37.007267475128174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5168607831001282
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.198701024055481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5987719297409058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.027988314628601
6 8.5254646771 	 1.0279883307 	 1.0449150292
epoch_time;  36.947824239730835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6626837849617004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4297406673431396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.669218897819519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1453886032104492
7 8.4607298896 	 1.145388629 	 1.1612845756
epoch_time;  37.12291932106018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4677778482437134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1413618326187134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5906586050987244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0492571592330933
8 8.4179568488 	 1.0492571857 	 1.0637650773
epoch_time;  37.675082206726074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38927921652793884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0172251462936401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5278396010398865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0391384363174438
9 8.3476536053 	 1.0391383815 	 1.0525927569
epoch_time;  37.43694996833801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35970360040664673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8469992876052856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6318316459655762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2703875303268433
10 8.3219793342 	 1.2703875567 	 1.2825213458
epoch_time;  37.21230435371399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.441967248916626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.132344126701355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6467275619506836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.17026948928833
11 8.287841301 	 1.170269528 	 1.1813292632
epoch_time;  36.89439821243286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5343075394630432
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.275354266166687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6782561540603638
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1255772113800049
12 8.2495887462 	 1.1255771946 	 1.1360255061
epoch_time;  37.15285325050354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.51332688331604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2668519020080566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5876819491386414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0468167066574097
13 8.2152583097 	 1.0468166867 	 1.0568187817
epoch_time;  36.92759561538696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4046372175216675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0179119110107422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6991779804229736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2876834869384766
14 8.2167552207 	 1.2876834354 	 1.296784602
epoch_time;  37.172977447509766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4069848954677582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9695361852645874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6225348711013794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1510002613067627
15 8.1738446545 	 1.1510003167 	 1.1599288528
epoch_time;  37.260294914245605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7394841313362122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5143393278121948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8640022277832031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4604175090789795
16 8.1449339085 	 1.4604175465 	 1.4687737542
epoch_time;  36.64017581939697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8189269304275513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.798835039138794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8238428235054016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3823423385620117
17 8.1235011055 	 1.3823423643 	 1.3907493798
epoch_time;  36.68889403343201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38528016209602356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8585149645805359
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6038834452629089
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0971426963806152
18 8.1069339243 	 1.0971427299 	 1.1060117155
epoch_time;  36.99691414833069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5680732131004333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3501752614974976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6930002570152283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2110953330993652
19 8.103828363 	 1.2110953666 	 1.2200396564
epoch_time;  36.99871778488159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5677944421768188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3500676155090332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.693261444568634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.211440920829773
It took 808.9839582443237 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–†â–„â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–‚â–ƒâ–„â–â–†â–ƒâ–ƒâ–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–â–‡â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–â–ƒâ–‚â–ˆâ–‚â–„â–‚â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–†â–„â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–â–â–â–‚â–‚â–ƒâ–â–‡â–ƒâ–ƒâ–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–â–‡â–â–‚â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–ˆâ–‚â–…â–ƒâ–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.12726
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.14322
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.62409
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.46494
wandb:                         Train loss 8.13929
wandb: 
wandb: ðŸš€ View run glittering-rat-1410 at: https://wandb.ai/nreints/thesis/runs/6y5nvgs2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_200443-6y5nvgs2/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_201809-ig3a8nd9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-chrysanthemum-1413
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/ig3a8nd9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5450778603553772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2404228448867798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8456457853317261
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4669777154922485
0 18.0163113046 	 1.466977671 	 1.4984536661
epoch_time;  37.92219114303589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3805944621562958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9282413125038147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.72739177942276
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2993175983428955
1 9.817149416 	 1.299317561 	 1.3267122011
epoch_time;  37.525548458099365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6928423047065735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6005237102508545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9585345983505249
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6760040521621704
2 9.2729918648 	 1.6760041108 	 1.6996473158
epoch_time;  36.67477512359619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3838164806365967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9748090505599976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6327501535415649
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1883838176727295
3 8.9853978451 	 1.188383855 	 1.2099723197
epoch_time;  37.132561445236206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44342362880706787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.142592191696167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6544690132141113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.139140248298645
4 8.8224243081 	 1.1391401961 	 1.1590684221
epoch_time;  37.201115131378174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3733725845813751
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1619372367858887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6439608335494995
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1529154777526855
5 8.7000055539 	 1.152915501 	 1.1705842714
epoch_time;  37.137749671936035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41599684953689575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1149457693099976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6560166478157043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1950364112854004
6 8.6090311865 	 1.1950363572 	 1.210824585
epoch_time;  36.951135873794556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41207173466682434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1106733083724976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6392928957939148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.115212082862854
7 8.5304433607 	 1.1152121054 	 1.1296639932
epoch_time;  36.52246928215027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44849061965942383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0669161081314087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5653870701789856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9778810143470764
8 8.468117251 	 0.9778810243 	 0.9912174534
epoch_time;  36.75419735908508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4738336205482483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1936143636703491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5859335660934448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0527265071868896
9 8.4044258051 	 1.0527264569 	 1.0645789894
epoch_time;  36.51974368095398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44947549700737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9813005328178406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5578619837760925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9917907118797302
10 8.3669990638 	 0.991790689 	 1.0029529468
epoch_time;  36.8229284286499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48971375823020935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1662285327911377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.600447416305542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0460189580917358
11 8.3200169526 	 1.0460189407 	 1.0560431403
epoch_time;  37.62123680114746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3937218189239502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9338619112968445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6367149353027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1561189889907837
12 8.2920239241 	 1.1561190219 	 1.1652290963
epoch_time;  36.68578600883484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4764370024204254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2230627536773682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.649332582950592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.244315505027771
13 8.2443363342 	 1.2443154825 	 1.252872034
epoch_time;  37.11432123184204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39819666743278503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.062048077583313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5720780491828918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0263333320617676
14 8.2234040443 	 1.0263333707 	 1.0349939295
epoch_time;  37.09643077850342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.717429518699646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7583554983139038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8833040595054626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4686686992645264
15 8.2122915961 	 1.4686686748 	 1.4770408837
epoch_time;  36.56093096733093
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4394212067127228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0081743001937866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6478768587112427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.131942868232727
16 8.1673677376 	 1.131942914 	 1.140629289
epoch_time;  37.088332653045654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5853768587112427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3356666564941406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6983986496925354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.195938229560852
17 8.1797787428 	 1.1959382753 	 1.2051273655
epoch_time;  37.28055286407471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4541774392127991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9959250092506409
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5902535319328308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.013619303703308
18 8.1612101705 	 1.0136193353 	 1.0228350665
epoch_time;  36.75251054763794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4650326371192932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1436697244644165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6242424845695496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1272591352462769
19 8.1392880945 	 1.1272591256 	 1.1368559141
epoch_time;  37.09357666969299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46494433283805847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1432181596755981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6240903735160828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1272600889205933
It took 806.3011031150818 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–…â–…â–ƒâ–„â–…â–â–„â–ƒâ–ƒâ–â–ƒâ–„â–‚â–„â–„
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–ƒâ–„â–ƒâ–†â–‚â–‡â–ˆâ–†â–ƒâ–…â–â–„â–„â–†â–„â–ƒâ–„â–…â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–‚â–ƒâ–â–…â–…â–‚â–„â–„â–â–ƒâ–ƒâ–ƒâ–â–ƒâ–„â–‚â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–‚â–‚â–â–„â–â–ˆâ–ˆâ–…â–ƒâ–…â–‚â–„â–„â–…â–ƒâ–„â–„â–„â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.17631
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.9577
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.66154
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.44369
wandb:                         Train loss 8.13056
wandb: 
wandb: ðŸš€ View run scintillating-chrysanthemum-1413 at: https://wandb.ai/nreints/thesis/runs/ig3a8nd9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_201809-ig3a8nd9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_203142-xubv95cy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-fish-1416
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/xubv95cy
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6522725224494934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5191210508346558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.995356559753418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7260850667953491
0 13.888991886 	 1.7260851061 	 1.7570866765
epoch_time;  37.7000412940979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4085552394390106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0074273347854614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7220671772956848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2463070154190063
1 9.5484135653 	 1.2463069606 	 1.2731364482
epoch_time;  39.11751937866211
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.401035338640213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1006879806518555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6430172324180603
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1959679126739502
2 9.1048763329 	 1.1959679681 	 1.2196196685
epoch_time;  38.92773699760437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35074013471603394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9107738733291626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5692034959793091
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0725586414337158
3 8.8797479318 	 1.0725586762 	 1.0938617603
epoch_time;  37.586673736572266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5065193772315979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2902719974517822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6329555511474609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.121183156967163
4 8.7334420754 	 1.1211831583 	 1.1398529878
epoch_time;  36.88429260253906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33121389150619507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.872778594493866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5193371176719666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0243724584579468
5 8.6043037513 	 1.0243724101 	 1.0420182512
epoch_time;  37.248703718185425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6916995644569397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4980828762054443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7489933967590332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3065568208694458
6 8.542571381 	 1.3065568254 	 1.3220571982
epoch_time;  37.322505950927734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7100474834442139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5934534072875977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7940266132354736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3890712261199951
7 8.4843776447 	 1.3890712429 	 1.4025192673
epoch_time;  37.26115036010742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5388737916946411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.356370449066162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5995439887046814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0542975664138794
8 8.4239537492 	 1.0542976173 	 1.0672686603
epoch_time;  36.97884821891785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44665321707725525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.965631902217865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6950328946113586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2836190462112427
9 8.3949253131 	 1.2836189889 	 1.2956889385
epoch_time;  36.99272179603577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5571362972259521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2482737302780151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7156771421432495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3096060752868652
10 8.3405769983 	 1.3096061088 	 1.3204279719
epoch_time;  36.962377309799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3682844042778015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7177751660346985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.505100667476654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8902561664581299
11 8.308527632 	 0.8902561497 	 0.9009552002
epoch_time;  37.30376076698303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4818524718284607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0969256162643433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6659431457519531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.210183024406433
12 8.2656560754 	 1.210183056 	 1.2196731155
epoch_time;  37.26836538314819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4944747984409332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.096142053604126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6236414909362793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.12367844581604
13 8.2544246149 	 1.1236784239 	 1.1327705177
epoch_time;  37.04284858703613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5597420930862427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3789448738098145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6104745268821716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1168527603149414
14 8.2213656271 	 1.1168527139 	 1.126015658
epoch_time;  37.27697944641113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4308774769306183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0507985353469849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49267295002937317
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8571489453315735
15 8.1851957885 	 0.8571489489 	 0.8664697596
epoch_time;  36.881980895996094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4831448495388031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0093567371368408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6190564632415771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1124358177185059
16 8.1712905171 	 1.1124358306 	 1.1215897844
epoch_time;  37.303574562072754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.486844539642334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.108835220336914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6863308548927307
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1957604885101318
17 8.1381621054 	 1.195760531 	 1.2049212482
epoch_time;  37.30202293395996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5131118893623352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2505037784576416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5765916705131531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0162352323532104
18 8.1359881464 	 1.0162351866 	 1.025501478
epoch_time;  36.826385498046875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44346895813941956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9573418498039246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6614595651626587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1759235858917236
19 8.1305644888 	 1.1759236104 	 1.185402865
epoch_time;  36.92109417915344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44369202852249146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9576972126960754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6615402698516846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.176311731338501
It took 812.9174644947052 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–†â–‡â–…â–„â–â–…â–…â–…â–…â–„â–…â–ƒâ–…â–ƒâ–„â–â–ƒâ–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–ƒâ–†â–ˆâ–†â–…â–ƒâ–„â–‚â–â–‡â–‡â–ˆâ–‚â–ƒâ–„â–„â–„â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–‡â–…â–„â–â–…â–„â–…â–…â–„â–†â–‚â–„â–ƒâ–„â–â–ƒâ–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–‚â–…â–‡â–„â–ƒâ–â–„â–â–‚â–‡â–†â–ˆâ–â–„â–ƒâ–ƒâ–ƒâ–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.14289
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.97655
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.61465
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.41504
wandb:                         Train loss 8.05472
wandb: 
wandb: ðŸš€ View run glowing-fish-1416 at: https://wandb.ai/nreints/thesis/runs/xubv95cy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_203142-xubv95cy/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5399265885353088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3010488748550415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8564509153366089
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5608792304992676
0 15.9041725732 	 1.5608792692 	 1.5895299963
epoch_time;  37.124669551849365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4357972741127014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1021547317504883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7543856501579285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4205577373504639
1 9.6161930856 	 1.4205577953 	 1.4453199232
epoch_time;  37.00907850265503
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5287097096443176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2745453119277954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7673637866973877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3729535341262817
2 9.1242409656 	 1.3729535902 	 1.3948525918
epoch_time;  37.55288815498352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5944722890853882
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.452265739440918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8143729567527771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4351295232772827
3 8.8613552334 	 1.4351295265 	 1.4537591058
epoch_time;  37.87396574020386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4984826147556305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2744312286376953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7075898051261902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2745620012283325
4 8.6954215947 	 1.274561949 	 1.2911463557
epoch_time;  38.25483727455139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47768786549568176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2013490200042725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6647196412086487
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2254139184951782
5 8.5801548219 	 1.2254139668 	 1.2402314882
epoch_time;  38.05620574951172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4046945571899414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0298794507980347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5419970154762268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.973534345626831
6 8.4593625698 	 0.9735343315 	 0.9865697912
epoch_time;  37.05504250526428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4970044195652008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1612025499343872
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7235674858093262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2917174100875854
7 8.4057468422 	 1.2917173643 	 1.3032328838
epoch_time;  37.325307846069336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40515393018722534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.948674201965332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6624993681907654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.288967251777649
8 8.3510679375 	 1.2889672357 	 1.2989977862
epoch_time;  36.859883069992065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4285680949687958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.908790647983551
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6903807520866394
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.261703372001648
9 8.2868766397 	 1.2617034087 	 1.2706101701
epoch_time;  37.06170868873596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6025163531303406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3515318632125854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7370413541793823
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3261590003967285
10 8.2638755184 	 1.3261590081 	 1.3348761481
epoch_time;  37.16816830635071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5553325414657593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3788373470306396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6553705334663391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1760609149932861
11 8.2048440674 	 1.1760609395 	 1.1847351074
epoch_time;  37.037970781326294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6220864653587341
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4629374742507935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7490378618240356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2994037866592407
12 8.197879861 	 1.2994037525 	 1.3079528809
epoch_time;  37.02582764625549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41288694739341736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9543012380599976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5799622535705566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0712825059890747
13 8.1497367182 	 1.0712824641 	 1.0800594845
epoch_time;  37.453819274902344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4882064759731293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0945085287094116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6847928762435913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2549360990524292
14 8.1369792548 	 1.2549360945 	 1.2639354809
epoch_time;  37.33152103424072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4707818031311035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1802209615707397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6060261726379395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0836316347122192
15 8.1204028172 	 1.0836316908 	 1.0931944151
epoch_time;  37.040642738342285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4768242835998535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1794456243515015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6838923096656799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2384028434753418
16 8.1051123741 	 1.2384028254 	 1.2478369636
epoch_time;  36.98469138145447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4611433744430542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1620533466339111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5225957632064819
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9349952340126038
17 8.0763179957 	 0.9349952492 	 0.9451347557
epoch_time;  37.08287692070007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40527859330177307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0362435579299927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6066441535949707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0949407815933228
18 8.0623505734 	 1.0949407629 	 1.1055146088
epoch_time;  37.38838720321655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4150454103946686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9753971695899963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6147179007530212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1431324481964111
19 8.0547172531 	 1.1431323902 	 1.1539819666
epoch_time;  37.28616666793823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41504475474357605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9765494465827942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6146484017372131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1428875923156738
It took 813.4649615287781 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2140421
Array Job ID: 2137927_26
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 10:40:51
CPU Efficiency: 25.16% of 1-18:27:00 core-walltime
Job Wall-clock time: 02:21:30
Memory Utilized: 5.17 GB
Memory Efficiency: 16.54% of 31.25 GB
