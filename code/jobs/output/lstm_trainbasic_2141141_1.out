/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_235636-817c0kd5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-moon-1431
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/817c0kd5
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(0,', '0)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a771167f40>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a470910>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a4703d0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a4701f0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04152165725827217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6655210256576538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2528412342071533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3223319053649902
0 1.4308053536 	 3.3223320076
epoch_time;  38.177536964416504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08745208382606506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3733561038970947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9043112993240356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.291028022766113
1 0.0972331969 	 5.2910277952
epoch_time;  38.08989405632019
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025227531790733337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6469442248344421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9306710958480835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8397579193115234
2 0.0364221433 	 2.8397580057
epoch_time;  36.97989201545715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024405336007475853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4653308391571045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6149598360061646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0545341968536377
3 0.0163408882 	 2.0545340823
epoch_time;  37.19701385498047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03042786382138729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9826866388320923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9578268527984619
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.163680076599121
4 0.0656631724 	 3.1636800622
epoch_time;  37.288644552230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010261687450110912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.526824414730072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5910084843635559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8326287269592285
5 0.0191192035 	 1.8326286938
epoch_time;  37.010027170181274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0074873617850244045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41670656204223633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4623735845088959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4886080026626587
6 0.0108414223 	 1.4886080001
epoch_time;  36.9317786693573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053041912615299225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37404513359069824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4057566821575165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3473666906356812
7 0.0092069018 	 1.3473666557
epoch_time;  36.99334669113159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031560007482767105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3256882131099701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3488643169403076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2384551763534546
8 0.0068056416 	 1.2384551357
epoch_time;  36.90862560272217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010646364651620388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5733451247215271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.524956226348877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.624576449394226
9 0.0359081265 	 1.6245764418
epoch_time;  37.228015184402466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005625646095722914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45490217208862305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4059670865535736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.307407021522522
10 0.0089463697 	 1.307407068
epoch_time;  37.05253195762634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01707731932401657
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4797891676425934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.360673189163208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2624882459640503
11 0.0073229763 	 1.2624882355
epoch_time;  37.0289146900177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009137364104390144
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3870765268802643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26673510670661926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9965550303459167
12 0.006347914 	 0.9965550172
epoch_time;  37.13956332206726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004631797317415476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45498034358024597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25941240787506104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1157560348510742
13 0.0082790554 	 1.115756032
epoch_time;  36.56654906272888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029528916347771883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35796430706977844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20060670375823975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8951225876808167
14 0.0047761057 	 0.8951226087
epoch_time;  37.27831792831421
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022906262893229723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32072556018829346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16636770963668823
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7850276231765747
15 0.0046115322 	 0.7850276336
epoch_time;  36.723020792007446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006901830900460482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5803900361061096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3072403371334076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2778674364089966
16 0.0179822894 	 1.2778674353
epoch_time;  36.54440093040466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004243918694555759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43532809615135193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21047109365463257
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9406006336212158
17 0.0065505427 	 0.9406006228
epoch_time;  38.83529615402222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009457994252443314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3696313500404358
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17381007969379425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7922406792640686
18 0.0050635948 	 0.7922406614
epoch_time;  39.62488865852356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00399878341704607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33283185958862305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14350588619709015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6951804161071777
19 0.0045266754 	 0.6951804262
epoch_time;  36.871222257614136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001953897764906287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29885560274124146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12626753747463226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6340163350105286
20 0.0041503932 	 0.6340163309
epoch_time;  37.117013454437256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012123308144509792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7067127227783203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2301788032054901
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.357573390007019
21 0.0080275258 	 1.3575734487
epoch_time;  37.38486313819885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00286606652662158
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4052198827266693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08156473189592361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7019608020782471
22 0.0043235951 	 0.7019608143
epoch_time;  36.93742609024048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031156254932284355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.063328742980957
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–ˆâ–„â–ƒâ–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–ƒâ–‚â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–ˆâ–ƒâ–‚â–…â–‚â–‚â–â–â–ƒâ–‚â–‚â–‚â–‚â–â–â–ƒâ–‚â–â–â–â–„â–‚â–†â–ƒâ–‚â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–…â–ˆâ–„â–ƒâ–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–â–ƒâ–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–ƒâ–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.5606
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.30746
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.08158
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00157
wandb:                         Train loss 0.0033
wandb: 
wandb: ğŸš€ View run legendary-moon-1431 at: https://wandb.ai/nreints/thesis/runs/817c0kd5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_235636-817c0kd5/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_001624-6b808ptb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-festival-1435
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/6b808ptb
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.549777626991272
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9748409986495972
23 0.0268706294 	 1.9748409767
epoch_time;  37.433077573776245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004786535166203976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5942977666854858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18766216933727264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.087807059288025
24 0.0109546468 	 1.0878070935
epoch_time;  38.35109305381775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030063011217862368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42610883712768555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1461407095193863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8318930268287659
25 0.0052099769 	 0.831893045
epoch_time;  37.79478907585144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005065619014203548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40588486194610596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1352490484714508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7634903788566589
26 0.0043063499 	 0.763490383
epoch_time;  36.89988565444946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003637084737420082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32483699917793274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10283926874399185
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6167183518409729
27 0.0038572556 	 0.6167183499
epoch_time;  36.964494943618774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002395698567852378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3057653605937958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09171455353498459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5680412650108337
28 0.003463199 	 0.5680412915
epoch_time;  36.26652240753174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015725864795967937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3074074685573578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08154778927564621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5590408444404602
29 0.003297926 	 0.5590408648
epoch_time;  36.60587120056152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015731457388028502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30745771527290344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08158064633607864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5606012940406799
It took  1189.1461424827576  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a7710bb2b0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a4dbe50>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a4dbf10>, <torch.utils.data.dataloader.DataLoader object at 0x14a76a544100>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04169128090143204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5929562449455261
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3331221342086792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4653143882751465
0 1.3435460566 	 3.4653143292
epoch_time;  36.245171546936035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0483560711145401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6793848872184753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1403247117996216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3151087760925293
1 0.0993878627 	 3.315108838
epoch_time;  36.70287895202637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03292486071586609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4714772701263428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7385838627815247
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.262580156326294
2 0.0224115457 	 2.2625802492
epoch_time;  36.92218470573425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017261240631341934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5202251076698303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7317827343940735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1444687843322754
3 0.0488824722 	 2.1444687397
epoch_time;  36.675803899765015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012235510163009167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3560486137866974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4830898642539978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4612371921539307
4 0.0143828728 	 1.4612372361
epoch_time;  36.60693407058716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012239630334079266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5695930123329163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45743757486343384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6248650550842285
5 0.0286580995 	 1.624865022
epoch_time;  37.09523129463196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005565020255744457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3844250738620758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.316336452960968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.17084538936615
6 0.0104320874 	 1.1708454236
epoch_time;  36.81621217727661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005048431921750307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3392080068588257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2779066264629364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0086370706558228
7 0.0095248486 	 1.0086370278
epoch_time;  36.47437810897827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008914371952414513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4952043294906616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3407513201236725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2898809909820557
8 0.0311467433 	 1.2898810349
epoch_time;  37.10081934928894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005213945172727108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3715498745441437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2369399517774582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9401057958602905
9 0.0088120416 	 0.9401057955
epoch_time;  36.8761522769928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006427438464015722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3608842194080353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2042447179555893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8559883832931519
10 0.0067523022 	 0.8559883786
epoch_time;  39.06918239593506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005788863170892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3159286379814148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18719518184661865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7698652148246765
11 0.0057534172 	 0.7698651916
epoch_time;  38.583051443099976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006326595321297646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30566585063934326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1488698273897171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7033029198646545
12 0.0049374838 	 0.7033028963
epoch_time;  37.26516628265381
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006444042082875967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41753119230270386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2492387890815735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0171688795089722
13 0.0194758402 	 1.0171688575
epoch_time;  37.176064014434814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0044425600208342075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34918463230133057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18183495104312897
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8330250382423401
14 0.0061481138 	 0.8330250535
epoch_time;  37.23836350440979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003558078547939658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32024946808815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16854141652584076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7439637184143066
15 0.0049650537 	 0.7439636968
epoch_time;  37.67770171165466
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–ˆâ–…â–…â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–„â–‚â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–‡â–„â–…â–‚â–…â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–ˆâ–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–ˆâ–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–…â–â–â–â–â–â–â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.49552
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27742
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.08416
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00552
wandb:                         Train loss 0.00305
wandb: 
wandb: ğŸš€ View run golden-festival-1435 at: https://wandb.ai/nreints/thesis/runs/6b808ptb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_001624-6b808ptb/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_003559-g0hf5nq3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-dumpling-1439
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/g0hf5nq3
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020742968190461397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3022710680961609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1614890843629837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7002822160720825
16 0.0057293656 	 0.7002822092
epoch_time;  37.23238134384155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004501607734709978
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.296231210231781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1383180022239685
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6653634905815125
17 0.0039333619 	 0.6653635077
epoch_time;  37.324453592300415
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002760808216407895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2762928307056427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12185963988304138
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6135780811309814
18 0.0041448469 	 0.6135780819
epoch_time;  37.245574712753296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016531463479623199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.253608375787735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1022435650229454
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5695857405662537
19 0.0037616499 	 0.5695857483
epoch_time;  37.19119381904602
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029076075181365013
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7723962068557739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5242190361022949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.714580774307251
20 0.0219243719 	 1.714580801
epoch_time;  36.58383822441101
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004097311291843653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3567846119403839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20010288059711456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.817574679851532
21 0.0093687723 	 0.8175746768
epoch_time;  36.67429518699646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038875704631209373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3134615421295166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1692962944507599
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6839872598648071
22 0.0057127665 	 0.6839872516
epoch_time;  36.37523150444031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002806341275572777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25530287623405457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10064185410737991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5193010568618774
23 0.0041638045 	 0.5193010313
epoch_time;  36.39561080932617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004331877455115318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2500793933868408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08691155910491943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4855929911136627
24 0.0034870405 	 0.4855929833
epoch_time;  36.55955648422241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019403054611757398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25717875361442566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0857471451163292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48592397570610046
25 0.003069808 	 0.4859239745
epoch_time;  36.6041796207428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003097661305218935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3207624852657318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.153555229306221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.635270893573761
26 0.0239341701 	 0.6352708707
epoch_time;  36.69384574890137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019065092783421278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2762313187122345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1043444275856018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5278757810592651
27 0.003557202 	 0.5278757793
epoch_time;  36.67727732658386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032073042821139097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2726428806781769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08703851699829102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4974316358566284
28 0.0032103637 	 0.4974316369
epoch_time;  36.702908515930176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005527745932340622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2774240970611572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08416371792554855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4934327304363251
29 0.0030482752 	 0.4934327278
epoch_time;  36.93828201293945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005524264182895422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2774238586425781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08416136354207993
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4955161213874817
It took  1174.6619849205017  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76ab8a710>, <torch.utils.data.dataloader.DataLoader object at 0x14a771131150>, <torch.utils.data.dataloader.DataLoader object at 0x14a771132b30>, <torch.utils.data.dataloader.DataLoader object at 0x14a771132ce0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.054983243346214294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9267756938934326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.4946460723876953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.644996404647827
0 1.4466342969 	 3.6449962973
epoch_time;  37.25853109359741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03300362452864647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9340102076530457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0925031900405884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.023756980895996
1 0.0695228606 	 3.0237568743
epoch_time;  36.97343182563782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013221017085015774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5643848180770874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6864302158355713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9534220695495605
2 0.0226641142 	 1.9534220623
epoch_time;  37.81146264076233
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012831226922571659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.596269428730011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5858232378959656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8024120330810547
3 0.0280837926 	 1.8024120504
epoch_time;  40.35828876495361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011605493724346161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5055466294288635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4044088125228882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3909564018249512
4 0.0125387352 	 1.39095636
epoch_time;  37.105952739715576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004863107576966286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42738139629364014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3074576258659363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1529158353805542
5 0.0116241348 	 1.1529158393
epoch_time;  37.11435890197754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007498106453567743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6408224105834961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3410532772541046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4386008977890015
6 0.0224056648 	 1.4386008456
epoch_time;  37.096768617630005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053637404926121235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45656657218933105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23367595672607422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0389397144317627
7 0.0074759223 	 1.0389396921
epoch_time;  37.38236403465271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009442509151995182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7221746444702148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2789565920829773
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–„â–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–ƒâ–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ˆâ–ƒâ–„â–‚â–â–„â–‚â–…â–ƒâ–‚â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–ˆâ–„â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–ƒâ–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–ƒâ–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.66675
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.46258
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.12604
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00295
wandb:                         Train loss 0.00373
wandb: 
wandb: ğŸš€ View run radiant-dumpling-1439 at: https://wandb.ai/nreints/thesis/runs/g0hf5nq3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_003559-g0hf5nq3/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_005537-sq7zy7lg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-springroll-1446
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/sq7zy7lg
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2948931455612183
8 0.0210107808 	 1.2948931092
epoch_time;  37.529608726501465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004609833471477032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.563978374004364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18361517786979675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9767418503761292
9 0.007430618 	 0.9767418253
epoch_time;  38.29340600967407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035219923593103886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47752466797828674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13710226118564606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8149089813232422
10 0.0060499727 	 0.8149089525
epoch_time;  37.401432275772095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018482722342014313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.89812171459198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44003963470458984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6732940673828125
11 0.0316292519 	 1.6732941135
epoch_time;  36.97492027282715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005256983917206526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6198298931121826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23631489276885986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0971267223358154
12 0.0100481198 	 1.0971267562
epoch_time;  36.742526054382324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037705523427575827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5269849896430969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20631492137908936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9507962465286255
13 0.0075891742 	 0.9507962599
epoch_time;  36.50516414642334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030619781464338303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5586218237876892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21339866518974304
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9958710074424744
14 0.0071750889 	 0.9958709993
epoch_time;  36.26815605163574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025809353683143854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47761061787605286
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17743019759655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8448426127433777
15 0.0051030156 	 0.8448426399
epoch_time;  36.93291759490967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025135576725006104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4592124819755554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16070257127285004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7779112458229065
16 0.0037539762 	 0.7779112294
epoch_time;  36.50394034385681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001764141139574349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48494598269462585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14231660962104797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7839542627334595
17 0.004251279 	 0.7839542631
epoch_time;  36.36159348487854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007478191051632166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5577913522720337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3186856806278229
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0584348440170288
18 0.0165184405 	 1.0584348004
epoch_time;  36.64053440093994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027918859850615263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49104994535446167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23598584532737732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9237089157104492
19 0.0053445742 	 0.9237089128
epoch_time;  36.75656485557556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021610469557344913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5111080408096313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2063324749469757
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9168432950973511
20 0.0055564914 	 0.9168432875
epoch_time;  36.65563988685608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007544008083641529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5047136545181274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18385963141918182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.84700608253479
21 0.0037237885 	 0.8470060688
epoch_time;  36.50297498703003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016208995366469026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4851244390010834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13317441940307617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7563347816467285
22 0.0054448169 	 0.7563347946
epoch_time;  36.682995319366455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004055571276694536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40730124711990356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13044781982898712
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6746489405632019
23 0.0031793762 	 0.6746489649
epoch_time;  36.5098078250885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038654243107885122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4261597692966461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12504854798316956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6736823916435242
24 0.0037248584 	 0.6736824059
epoch_time;  36.34127473831177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001852628542110324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4085950553417206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10983122140169144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6357390880584717
25 0.0028632851 	 0.6357390989
epoch_time;  36.754809856414795
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010221420787274837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8992186784744263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2729051411151886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4141664505004883
26 0.0309500424 	 1.4141664073
epoch_time;  36.092952728271484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004439841024577618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6599097847938538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21018926799297333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0118032693862915
27 0.0070398715 	 1.0118032957
epoch_time;  38.835920572280884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028658313676714897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5370470285415649
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14643964171409607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7823139429092407
28 0.0047395177 	 0.7823139663
epoch_time;  38.34500598907471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029530003666877747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.463400661945343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1259201318025589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6695005893707275
29 0.0037301834 	 0.6695005757
epoch_time;  36.86422538757324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029543412383645773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4625779688358307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12604455649852753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6667463779449463
It took  1178.4572007656097  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76a544250>, <torch.utils.data.dataloader.DataLoader object at 0x14a7711319f0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac52f50>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac0ad40>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.052551448345184326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.829021155834198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.48167085647583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.0948333740234375
0 1.4003963543 	 4.0948331435
epoch_time;  36.93233060836792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02145654521882534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44849127531051636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9704440236091614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7490618228912354
1 0.034801668 	 2.7490617919
epoch_time;  37.018049478530884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06404154002666473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2613606452941895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3841603994369507
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.270037651062012
2 0.0677453387 	 4.2700376021
epoch_time;  37.62450814247131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015524834394454956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6239203214645386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6799075603485107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4901552200317383
3 0.0273220516 	 2.490155269
epoch_time;  37.94398760795593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08603295683860779
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9320014715194702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0685261487960815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.99043607711792
4 0.0810808908 	 3.9904361045
epoch_time;  37.03963017463684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01702268235385418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8011778593063354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6262013912200928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.154021739959717
5 0.0339329595 	 2.1540217558
epoch_time;  36.777997732162476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009203224442899227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5577751398086548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5010738968849182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6176636219024658
6 0.0142377728 	 1.6176636111
epoch_time;  37.11949968338013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02735918201506138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46454888582229614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40891024470329285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3074322938919067
7 0.0103214655 	 1.3074323303
epoch_time;  36.879621744155884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028008801862597466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3940812647342682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3393433392047882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0923333168029785
8 0.00798653 	 1.0923333759
epoch_time;  36.800880432128906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008335507474839687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3670724928379059
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29497030377388
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0154610872268677
9 0.0071434235 	 1.0154610717
epoch_time;  36.77194094657898
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009399624541401863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5972190499305725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4044937789440155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2822210788726807
10 0.0337447167 	 1.2822210306
epoch_time;  37.01580214500427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010710288770496845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3884468078613281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3343105912208557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9380179047584534
11 0.0078793251 	 0.9380178768
epoch_time;  36.34949469566345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0070809354074299335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3666890859603882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26370254158973694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7980002760887146
12 0.0063289668 	 0.7980002781
epoch_time;  36.13322615623474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032272133976221085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3389286696910858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22643201053142548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.730463445186615
13 0.0054420006 	 0.7304634486
epoch_time;  36.80024766921997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035693810787051916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34271103143692017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.202239990234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7146943211555481
14 0.0049489658 	 0.7146943429
epoch_time;  36.33980321884155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028464950155466795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34885016083717346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1689019799232483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6962702870368958
15 0.0049778796 	 0.6962703002
epoch_time;  36.15215587615967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001918621826916933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3410911560058594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13765983283519745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6428956389427185
16 0.0045318138 	 0.6428956092
epoch_time;  36.761449098587036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021539919544011354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3116605579853058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12003780156373978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6073617339134216
17 0.0037537794 	 0.6073617618
epoch_time;  35.87603759765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00972410012036562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5481008887290955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3778464198112488
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.134516716003418
18 0.036972147 	 1.1345166901
epoch_time;  36.08223223686218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0072252643294632435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.407481849193573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25385749340057373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8266055583953857
19 0.0066714976 	 0.826605575
epoch_time;  39.547852516174316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034017718862742186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3656415343284607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22280290722846985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7246160507202148
20 0.0048467584 	 0.7246160594
epoch_time;  38.005905628204346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021972081158310175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3358451724052429
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18442858755588531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6492432355880737
21 0.004461091 	 0.6492432194
epoch_time;  36.80973792076111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023599015548825264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32307907938957214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15772956609725952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6113781929016113
22 0.0035526678 	 0.6113781886
epoch_time;  36.770808696746826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022059332113713026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30930617451667786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.145415797829628
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5883881449699402
23 0.0036361775 	 0.5883881261
epoch_time;  36.74243664741516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002258006250485778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3114496171474457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15938295423984528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.606361448764801
24 0.0058953165 	 0.6063614583
epoch_time;  36.835551023483276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021299354266375303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2825484871864319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1362600028514862
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5452468991279602
25 0.0030567641 	 0.5452469195
epoch_time;  37.53506660461426
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–ˆâ–…â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–‚â–…â–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–‡â–„â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ƒâ–†â–‚â–ˆâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.62909
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.38592
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.11901
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00466
wandb:                         Train loss 0.00294
wandb: 
wandb: ğŸš€ View run prosperous-springroll-1446 at: https://wandb.ai/nreints/thesis/runs/sq7zy7lg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_005537-sq7zy7lg/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_011513-pfh3kuun
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-rooster-1453
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/pfh3kuun
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00176787911914289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26348137855529785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12486927211284637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5134350657463074
26 0.0030497692 	 0.5134350641
epoch_time;  38.8893768787384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011681761825457215
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25767582654953003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12002773582935333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5082550644874573
27 0.0026919391 	 0.508255051
epoch_time;  38.36302447319031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020822344813495874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4387620687484741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14795435965061188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7099288702011108
28 0.0100739405 	 0.7099288987
epoch_time;  37.173113107681274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0046677314676344395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38917678594589233
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11903585493564606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6284050345420837
29 0.0029439045 	 0.6284050149
epoch_time;  37.180588722229004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004664480220526457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38591620326042175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1190144419670105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6290926933288574
It took  1176.1368935108185  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76ac08f10>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac09f00>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac0a140>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac0a080>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09793920814990997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2010834217071533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.8078033924102783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.710768222808838
0 1.432114121 	 4.7107682242
epoch_time;  37.0445442199707
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021965391933918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5143409371376038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1170015335083008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9129183292388916
1 0.0437055352 	 2.9129183213
epoch_time;  37.29606556892395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02962317503988743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7832125425338745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.139146089553833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.243170976638794
2 0.0808027153 	 3.2431710696
epoch_time;  37.04704284667969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013071879744529724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4794468581676483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6599101424217224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0463039875030518
3 0.0205511112 	 2.0463039248
epoch_time;  36.98665404319763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008203361183404922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3754166066646576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4721320569515228
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5863364934921265
4 0.0139921666 	 1.5863365335
epoch_time;  37.14671301841736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027006393298506737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7953040599822998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7413053512573242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3624351024627686
5 0.0589064743 	 2.3624350556
epoch_time;  36.87423133850098
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00948269758373499
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4596570134162903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46208426356315613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5059747695922852
6 0.0159817117 	 1.505974807
epoch_time;  37.07506346702576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006890458986163139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3459332287311554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3561321496963501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1820266246795654
7 0.0101096906 	 1.1820266585
epoch_time;  37.405805587768555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012030615471303463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48816928267478943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6439813375473022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6566660404205322
8 0.0485925956 	 1.6566659979
epoch_time;  37.0605845451355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006591408513486385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3731304109096527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5069826245307922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.312628149986267
9 0.0104596718 	 1.3126281554
epoch_time;  37.2946572303772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005026835482567549
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33674412965774536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4199245572090149
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1453328132629395
10 0.0077631544 	 1.1453328205
epoch_time;  37.42590856552124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004093541298061609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3110657334327698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3776779770851135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0382874011993408
11 0.006332666 	 1.0382873904
epoch_time;  37.084712982177734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004776489455252886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.360653817653656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4517281651496887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1702700853347778
12 0.0155558792 	 1.1702701073
epoch_time;  38.85585403442383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010356314480304718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2847001850605011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3707573413848877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9700010418891907
13 0.0059260048 	 0.9700010363
epoch_time;  39.13390374183655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041946531273424625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2591826915740967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31719815731048584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8684324622154236
14 0.0051472279 	 0.8684324512
epoch_time;  37.45001292228699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008409032598137856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24635297060012817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3012727200984955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8204379081726074
15 0.0045937467 	 0.8204378894
epoch_time;  37.234601974487305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0047015040181577206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2194952815771103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26414787769317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7727502584457397
16 0.0044435296 	 0.7727502552
epoch_time;  37.20633292198181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013979100622236729
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8036333322525024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.549496591091156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8236558437347412
17 0.0469772864 	 1.8236557883
epoch_time;  37.45086193084717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0054631284438073635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.554271936416626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35312697291374207
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–ƒâ–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ƒâ–…â–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–ƒâ–‚â–„â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‚â–ƒâ–‚â–â–ƒâ–‚â–â–‚â–â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.82179
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39106
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.13502
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00237
wandb:                         Train loss 0.00444
wandb: 
wandb: ğŸš€ View run sparkling-rooster-1453 at: https://wandb.ai/nreints/thesis/runs/pfh3kuun
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_011513-pfh3kuun/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_013457-czsi5cnf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-goat-1460
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/czsi5cnf
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2671103477478027
18 0.0094735307 	 1.2671103117
epoch_time;  37.89452052116394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035285865887999535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47374585270881653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2835751473903656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0881693363189697
19 0.0064396255 	 1.0881693399
epoch_time;  38.13384461402893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002622533356770873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.404345840215683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24288539588451385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9275585412979126
20 0.0046643012 	 0.9275585532
epoch_time;  37.93318724632263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001944553223438561
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3734804689884186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20829316973686218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8421285152435303
21 0.0043542196 	 0.8421285116
epoch_time;  37.54659295082092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019717493560165167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3558827340602875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1811719387769699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7882353663444519
22 0.0041304783 	 0.7882353907
epoch_time;  36.829261302948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004704798571765423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3141818642616272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.180708646774292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7459931373596191
23 0.0039820675 	 0.7459931618
epoch_time;  37.28504133224487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014747594250366092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29369640350341797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15586069226264954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6844092607498169
24 0.0034407292 	 0.6844092424
epoch_time;  36.65682578086853
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032191327773034573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3040970265865326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1377200037240982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6847816109657288
25 0.0040675682 	 0.6847816306
epoch_time;  36.914976358413696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013018089346587658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33453333377838135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12048406898975372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6620924472808838
26 0.0030350992 	 0.6620924566
epoch_time;  37.35244822502136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016197454184293747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7315869331359863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4656313955783844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7253957986831665
27 0.0187788508 	 1.725395825
epoch_time;  36.68960905075073
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033056018874049187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4569559693336487
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1733785718679428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9532036185264587
28 0.0074829229 	 0.953203645
epoch_time;  36.800827741622925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002365241525694728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39107823371887207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13519710302352905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8202979564666748
29 0.0044367817 	 0.8202979327
epoch_time;  36.74716639518738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023653933312743902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3910592496395111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1350153386592865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8217902779579163
It took  1183.5730438232422  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a7710b98a0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abf91b0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abfab90>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abfad40>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03954274207353592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5762939453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2214174270629883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.12385892868042
0 1.401282727 	 3.1238589561
epoch_time;  37.0230507850647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.043296437710523605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8334272503852844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0984851121902466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.359009265899658
1 0.089024725 	 3.3590091579
epoch_time;  37.829718351364136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024866830557584763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5049659013748169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6385473012924194
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0422260761260986
2 0.0239920504 	 2.0422261863
epoch_time;  36.98222470283508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06532446295022964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3073934316635132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2197086811065674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.593453884124756
3 0.0595523392 	 3.5934538596
epoch_time;  37.06077694892883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013623111881315708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6717698574066162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6418635845184326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9232137203216553
4 0.0267848348 	 1.9232137167
epoch_time;  39.83064532279968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008591067045927048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5238788723945618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5087937116622925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5181807279586792
5 0.0121344885 	 1.5181807319
epoch_time;  38.37385082244873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006282222922891378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46987149119377136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45518410205841064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3831838369369507
6 0.0091223338 	 1.3831837818
epoch_time;  36.882426023483276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003603634424507618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4378129243850708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42401930689811707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.259507656097412
7 0.0077364724 	 1.2595076547
epoch_time;  37.36828827857971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030011190101504326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4459695816040039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3931485414505005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2195740938186646
8 0.007049614 	 1.2195740668
epoch_time;  37.191203355789185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003194756107404828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4049210548400879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37405678629875183
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.105136513710022
9 0.0081911679 	 1.1051365602
epoch_time;  37.817606687545776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008616814389824867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4553082585334778
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30383065342903137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0920833349227905
10 0.0118195442 	 1.0920833346
epoch_time;  37.40699529647827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009924711659550667
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‡â–‡â–„â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‚â–„â–‚â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–„â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–†â–„â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.92863
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.49908
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.11831
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00304
wandb:                         Train loss 0.00521
wandb: 
wandb: ğŸš€ View run legendary-goat-1460 at: https://wandb.ai/nreints/thesis/runs/czsi5cnf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_013457-czsi5cnf/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_015441-ef4ux2x7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-bao-1468
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ef4ux2x7
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45018285512924194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26963165402412415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0184439420700073
11 0.0050866436 	 1.0184439576
epoch_time;  38.26775336265564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013434045948088169
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4345586895942688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23347258567810059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9794002771377563
12 0.0059696619 	 0.979400266
epoch_time;  37.74064755439758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028733129147440195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4244592785835266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1970086395740509
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.927247941493988
13 0.0054744154 	 0.9272479377
epoch_time;  37.145954608917236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012877285480499268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4333849549293518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19502967596054077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9209805130958557
14 0.0040507573 	 0.9209804938
epoch_time;  36.836456537246704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003482631640508771
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5783723592758179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24052205681800842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2459027767181396
15 0.0080364907 	 1.2459028077
epoch_time;  36.69185733795166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0044239885173738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43375375866889954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1455831378698349
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8768196105957031
16 0.0035038384 	 0.8768196221
epoch_time;  36.69664430618286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004991454537957907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.636092483997345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2191947102546692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2300654649734497
17 0.0208579532 	 1.2300654754
epoch_time;  37.12565326690674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01001939456909895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5001367926597595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1812223345041275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9702690839767456
18 0.0046617503 	 0.9702690562
epoch_time;  36.584574699401855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003243732964619994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4593071937561035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1651490181684494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.915986955165863
19 0.0068820756 	 0.9159869514
epoch_time;  36.57610869407654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024495236575603485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4286069869995117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14708849787712097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.869625985622406
20 0.0035609715 	 0.8696259559
epoch_time;  36.75185751914978
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002534399041905999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4580758512020111
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14767037332057953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8679264783859253
21 0.0051419802 	 0.8679264679
epoch_time;  36.47068119049072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020511543843895197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4292827844619751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12975694239139557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8025408387184143
22 0.0032743383 	 0.8025408511
epoch_time;  36.65870952606201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027161985635757446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4054456055164337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11889073252677917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7692222595214844
23 0.0029997051 	 0.769222248
epoch_time;  36.907504081726074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00462622195482254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4402950406074524
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10917971283197403
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7919191718101501
24 0.003658913 	 0.7919191666
epoch_time;  37.05364775657654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027677156031131744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4217873513698578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1089218333363533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7734677195549011
25 0.0028703153 	 0.773467741
epoch_time;  36.60565757751465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001990718999877572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45753780007362366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11699750274419785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8379417657852173
26 0.0060433392 	 0.837941795
epoch_time;  36.35088872909546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010732666123658419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41477537155151367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09420517832040787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7546907663345337
27 0.0027923315 	 0.7546907638
epoch_time;  36.91102647781372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028393378015607595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42390888929367065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0992792621254921
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7690089344978333
28 0.0026013885 	 0.7690089476
epoch_time;  39.34681177139282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003043754491955042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4986024498939514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11839136481285095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9250364899635315
29 0.0052079205 	 0.9250364736
epoch_time;  39.4026153087616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00304368045181036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4990833103656769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11830873042345047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9286274313926697
It took  1184.0893173217773  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76abf8f10>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac15870>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac15330>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac15ab0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04316871985793114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7093485593795776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3432289361953735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4458818435668945
0 1.3812571893 	 3.4458819156
epoch_time;  36.826037645339966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023438256233930588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6311054229736328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9632627367973328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.825556993484497
1 0.0707248454 	 2.8255570979
epoch_time;  37.045389890670776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.054075222462415695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1290537118911743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1298660039901733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.531627655029297
2 0.0448271977 	 3.5316276435
epoch_time;  37.45475792884827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010944224894046783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5107513666152954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6314201951026917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0151379108428955
3 0.0216770764 	 2.0151378251
epoch_time;  38.604716062545776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014855194836854935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5174627304077148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6073570251464844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.822767734527588
4 0.0309016786 	 1.822767736
epoch_time;  36.98378229141235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006582210306078196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3464401662349701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4356170892715454
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2629307508468628
5 0.010351202 	 1.2629307865
epoch_time;  38.05770993232727
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006097790785133839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30626803636550903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3914952576160431
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1355218887329102
6 0.0109302527 	 1.135521834
epoch_time;  37.17145657539368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01087812427431345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5037117600440979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3732993006706238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2687677145004272
7 0.0254275158 	 1.2687676652
epoch_time;  36.858402252197266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00842335820198059
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35657602548599243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33526137471199036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9435790181159973
8 0.0080805337 	 0.9435789909
epoch_time;  36.91000938415527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003818388096988201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3358374238014221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2858676612377167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8902136087417603
9 0.0131146895 	 0.890213612
epoch_time;  36.85277795791626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00470246747136116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3266904950141907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2360038161277771
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8254143595695496
10 0.0064918805 	 0.8254143752
epoch_time;  36.32636070251465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010570034384727478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2936943471431732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19792437553405762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.698920726776123
11 0.0053323284 	 0.6989207196
epoch_time;  36.652751207351685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002297736471518874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.282904714345932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1513320654630661
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6311172246932983
12 0.0051522324 	 0.631117207
epoch_time;  36.67055606842041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01913115568459034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8013461232185364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4113130271434784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6372023820877075
13 0.0161020638 	 1.6372024213
epoch_time;  36.6482937335968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005806730128824711
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.391183078289032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20801110565662384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8078092336654663
14 0.0078048287 	 0.8078092362
epoch_time;  37.16838526725769
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028192205354571342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3329780697822571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17521736025810242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.690946102142334
15 0.0053495704 	 0.6909460892
epoch_time;  36.68378567695618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002425702288746834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32297655940055847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16480572521686554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6720567941665649
16 0.0047126502 	 0.6720568147
epoch_time;  36.656898736953735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017850234871730208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3126465380191803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13222327828407288
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6045661568641663
17 0.0047280566 	 0.6045661304
epoch_time;  36.5223343372345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006113588809967041
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3120128810405731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12696632742881775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6010966300964355
18 0.0040712429 	 0.6010966229
epoch_time;  36.61925029754639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014691639225929976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30427277088165283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.132472425699234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5960022807121277
19 0.0037291705 	 0.5960023079
epoch_time;  36.486199617385864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021937850397080183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.317412793636322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13146796822547913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6502580642700195
20 0.0100778858 	 0.6502580441
epoch_time;  36.313257694244385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002741040661931038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.285395085811615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12015204131603241
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5511646270751953
21 0.0037901476 	 0.5511646559
epoch_time;  38.66756725311279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.047987524420022964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9835346937179565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42647600173950195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8290997743606567
22 0.0077938127 	 1.8290997185
epoch_time;  38.08825492858887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004040946252644062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4347633421421051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16803985834121704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7660787105560303
23 0.0060782956 	 0.766078707
epoch_time;  35.746195793151855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018730078591033816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3347935080528259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10149537771940231
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.559360682964325
24 0.0031337799 	 0.5593607001
epoch_time;  36.12383794784546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004608716815710068
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.313092976808548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09269026666879654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.527273952960968
25 0.0031537793 	 0.527273956
epoch_time;  36.51616811752319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012198486365377903
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2882526218891144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08338402956724167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4829651117324829
26 0.0028816116 	 0.4829651063
epoch_time;  37.077595710754395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018077150452882051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28779345750808716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08992788195610046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48749709129333496
27 0.0031498549 	 0.487497105
epoch_time;  37.31144070625305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009546337649226189
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5194975733757019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19298093020915985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8622380495071411
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–ˆâ–…â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–„â–‚â–â–â–â–â–â–â–â–„â–‚â–â–â–â–â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–…â–„â–ˆâ–ƒâ–ƒâ–‚â–â–ƒâ–‚â–â–â–â–â–…â–‚â–â–â–â–â–â–â–â–‡â–‚â–â–â–â–â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–‡â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–„â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–ƒâ–‚â–â–â–â–‚â–â–â–â–‡â–â–â–â–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.64436
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.40205
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.11265
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00353
wandb:                         Train loss 0.00387
wandb: 
wandb: ğŸš€ View run festive-bao-1468 at: https://wandb.ai/nreints/thesis/runs/ef4ux2x7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_015441-ef4ux2x7/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_021417-8xvghybv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-kumquat-1476
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/8xvghybv
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
28 0.0110589135 	 0.8622380283
epoch_time;  36.43407201766968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035317048896104097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40175774693489075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.112517811357975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6423558592796326
29 0.0038695086 	 0.6423558814
epoch_time;  37.1984224319458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035315509885549545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40204697847366333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11265379935503006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6443625092506409
It took  1175.9105603694916  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76a4c08b0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abfb040>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abf8cd0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abf8940>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04054443910717964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6715506911277771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.4643524885177612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.644113779067993
0 1.373423528 	 3.6441137769
epoch_time;  36.02406859397888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017434213310480118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3859381675720215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9714142680168152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4443328380584717
1 0.027385394 	 2.4443328028
epoch_time;  36.01608085632324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021837975829839706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5783385634422302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0273747444152832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.750920057296753
2 0.0815773963 	 2.7509199529
epoch_time;  36.18321371078491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01248396746814251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4492335021495819
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8274023532867432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.040072202682495
3 0.0235569324 	 2.0400722538
epoch_time;  35.96514439582825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14580436050891876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8260396718978882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.5488651990890503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2977070808410645
4 0.0501785047 	 4.2977069958
epoch_time;  36.3381028175354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012594041414558887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.572490394115448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.709529459476471
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.934102177619934
5 0.0294724521 	 1.9341021304
epoch_time;  36.23662972450256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01447390392422676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.388894259929657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5059913992881775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3757283687591553
6 0.0117158601 	 1.3757283652
epoch_time;  36.48318886756897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009068443439900875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.478154718875885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4870167672634125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4133399724960327
7 0.0200268184 	 1.4133399433
epoch_time;  36.143208265304565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009149492718279362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.326261967420578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37035736441612244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0139079093933105
8 0.0066215673 	 1.0139079022
epoch_time;  36.19273900985718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025399556383490562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6837475895881653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5260722637176514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7132673263549805
9 0.0149092808 	 1.7132673465
epoch_time;  36.329423904418945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00657862750813365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30239060521125793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.361876904964447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.914649248123169
10 0.0072603789 	 0.9146492488
epoch_time;  36.08077311515808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005200222600251436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25472402572631836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31386834383010864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.795673668384552
11 0.0065061005 	 0.7956736585
epoch_time;  36.10363173484802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002312332158908248
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22730296850204468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2659846246242523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7069122195243835
12 0.0054192872 	 0.7069122222
epoch_time;  36.401198387145996
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001986464951187372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22560852766036987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23900321125984192
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6775090098381042
13 0.004955291 	 0.6775089967
epoch_time;  36.20601296424866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005363416392356157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3568952977657318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19895687699317932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8640651106834412
14 0.0063215482 	 0.8640651184
epoch_time;  38.61756992340088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019113888265565038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21668307483196259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1307239532470703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5485734343528748
15 0.0030936162 	 0.5485734277
epoch_time;  37.9167377948761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017037928337231278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26838523149490356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12336920946836472
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5937827825546265
16 0.0050700745 	 0.5937827764
epoch_time;  36.425774574279785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018685080576688051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2839531898498535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10969655215740204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6168045997619629
17 0.0063392999 	 0.6168046012
epoch_time;  36.39137554168701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021898159757256508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5970728397369385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.434975802898407
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.501595139503479
18 0.0082756594 	 1.5015951197
epoch_time;  36.40999221801758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004636042285710573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2898065149784088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15631280839443207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6608133316040039
19 0.0046387917 	 0.660813346
epoch_time;  37.03937888145447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025969159323722124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.288743793964386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1683351993560791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6901074051856995
20 0.0063600396 	 0.690107409
epoch_time;  36.88454461097717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017205220647156239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2661433517932892
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‡â–…â–…â–„â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–‚â–ƒâ–‚â–ˆâ–ƒâ–‚â–‚â–â–ƒâ–â–â–â–â–‚â–â–â–â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–…â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.75861
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29949
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.23102
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00272
wandb:                         Train loss 0.00563
wandb: 
wandb: ğŸš€ View run vivid-kumquat-1476 at: https://wandb.ai/nreints/thesis/runs/8xvghybv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_021417-8xvghybv/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_023336-h16yj496
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-lamp-1483
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/h16yj496
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14463572204113007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6170896887779236
21 0.003146992 	 0.6170896778
epoch_time;  37.318058490753174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005222482141107321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35738709568977356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2414623200893402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8938306570053101
22 0.0124343248 	 0.8938306365
epoch_time;  37.34840989112854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030005755834281445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28000858426094055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15546450018882751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6872504949569702
23 0.0039851949 	 0.6872504658
epoch_time;  36.61539363861084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018092300742864609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2604486644268036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1441178172826767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6356116533279419
24 0.0034174684 	 0.635611635
epoch_time;  36.53791856765747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031930820550769567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25386741757392883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12414852529764175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5853942632675171
25 0.0032081864 	 0.5853942687
epoch_time;  36.73392939567566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023193489760160446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2503487169742584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14517270028591156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5993547439575195
26 0.0067816827 	 0.5993547238
epoch_time;  36.11370253562927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015064830658957362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2396421581506729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.123186856508255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5357059240341187
27 0.0034931083 	 0.5357059352
epoch_time;  36.03283190727234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001827129046432674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22246699035167694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10329313576221466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5040991306304932
28 0.0024420313 	 0.5040991285
epoch_time;  36.27121639251709
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027205501683056355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3006652295589447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2310924232006073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7584700584411621
29 0.005626241 	 0.758470057
epoch_time;  36.38788414001465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002721264027059078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29949215054512024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23102053999900818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7586125731468201
It took  1159.0975189208984  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a772bbc9d0>, <torch.utils.data.dataloader.DataLoader object at 0x14a76abf8c70>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac15960>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac15bd0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.043046291917562485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5984714031219482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0915378332138062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.940356969833374
0 1.3669323629 	 2.940356851
epoch_time;  36.29235529899597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020740175619721413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49050894379615784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.836996853351593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3940370082855225
1 0.0638438708 	 2.3940370681
epoch_time;  36.01357913017273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04020606726408005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8005379438400269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.085495948791504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0519587993621826
2 0.0750801959 	 3.0519588044
epoch_time;  35.826390981674194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01396962720900774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.467707097530365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6839065551757812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.013227939605713
3 0.0229873094 	 2.0132278488
epoch_time;  35.9225709438324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01736060529947281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46625378727912903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6001893877983093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.722203016281128
4 0.0440546681 	 1.722203004
epoch_time;  35.763556480407715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00906039122492075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3359912633895874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47294262051582336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2779461145401
5 0.0117277142 	 1.2779461725
epoch_time;  35.8690299987793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02898251637816429
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7075220942497253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8162797689437866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2590558528900146
6 0.0478222982 	 2.2590558839
epoch_time;  36.62518548965454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008208024315536022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39351487159729004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4554020166397095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3580769300460815
7 0.0146374295 	 1.3580769427
epoch_time;  39.76898741722107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005195354577153921
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2881423532962799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4064576327800751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0713152885437012
8 0.0090464867 	 1.071315339
epoch_time;  36.31310033798218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014510925859212875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27801594138145447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40091338753700256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0095911026000977
9 0.0070095113 	 1.009591094
epoch_time;  36.134724617004395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003395919455215335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2440701127052307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3915291428565979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9207151532173157
10 0.0061991668 	 0.9207151476
epoch_time;  36.297001123428345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007963880896568298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23521552979946136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3306661546230316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8169599771499634
11 0.0053021729 	 0.8169599919
epoch_time;  35.91875195503235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007211486343294382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22212454676628113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26308080554008484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7079741358757019
12 0.0050208646 	 0.7079741602
epoch_time;  36.61606454849243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017825454473495483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2621821463108063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2278042435646057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7110869884490967
13 0.0045660085 	 0.7110869993
epoch_time;  36.738346338272095
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–ˆâ–…â–„â–ƒâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–†â–†
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ƒâ–†â–ƒâ–ƒâ–‚â–…â–‚â–‚â–‚â–â–â–â–â–â–ƒâ–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–ˆâ–ˆ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–ˆâ–…â–…â–„â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–…â–…
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ƒâ–…â–‚â–ƒâ–‚â–„â–‚â–â–‚â–â–‚â–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆ
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.25055
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.10661
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.67123
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.07146
wandb:                         Train loss 0.0215
wandb: 
wandb: ğŸš€ View run bright-lamp-1483 at: https://wandb.ai/nreints/thesis/runs/h16yj496
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_023336-h16yj496/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_025250-h1up8ox0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-goat-1489
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/h1up8ox0
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023661023005843163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2112821638584137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19611437618732452
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5885382890701294
14 0.0040646762 	 0.5885383168
epoch_time;  37.10628390312195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009005667641758919
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47901326417922974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2559535801410675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1053783893585205
15 0.0127695663 	 1.1053783958
epoch_time;  36.88441061973572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005318833515048027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29135802388191223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1443478912115097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6569086313247681
16 0.0051072303 	 0.6569086173
epoch_time;  39.18925499916077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024668544065207243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24074018001556396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1245427206158638
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5547214150428772
17 0.0041024459 	 0.5547214289
epoch_time;  35.95802617073059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002525919582694769
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22724884748458862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.11777839809656143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5201351046562195
18 0.0036820207 	 0.5201351016
epoch_time;  36.055257081985474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035445194225758314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34727561473846436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.14372381567955017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.709829568862915
19 0.0109216645 	 0.7098295552
epoch_time;  36.03633999824524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031256056390702724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2508469820022583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12106888741254807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5354936718940735
20 0.0038660935 	 0.5354936951
epoch_time;  36.04765868186951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014947637682780623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23391258716583252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10568152368068695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49344590306282043
21 0.003573043 	 0.4934459122
epoch_time;  36.07555294036865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001594825997017324
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2332744598388672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.10217723250389099
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4813212454319
22 0.0034988369 	 0.4813212599
epoch_time;  35.69234609603882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005953111685812473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2841140329837799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1044066995382309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5607194900512695
23 0.0030442569 	 0.560719516
epoch_time;  36.06296110153198
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015943747712299228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23876017332077026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08682771027088165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4781607687473297
24 0.0027248679 	 0.4781607544
epoch_time;  35.84158229827881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022558097261935472
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22989341616630554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08406636863946915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45805466175079346
25 0.0028062412 	 0.4580546491
epoch_time;  35.6422164440155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012484309263527393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2516706883907318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07405200600624084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49090081453323364
26 0.0042218902 	 0.4909008291
epoch_time;  36.20484447479248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002291207667440176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2534848153591156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07011612504720688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48500341176986694
27 0.0030320245 	 0.4850034224
epoch_time;  36.16670250892639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003529100213199854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2568419873714447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06576389074325562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4746777415275574
28 0.0021639462 	 0.4746777399
epoch_time;  36.04947018623352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07142309844493866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.106390118598938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6718114614486694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2550055980682373
29 0.0214996554 	 2.2550056204
epoch_time;  35.92327952384949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07145975530147552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.106614112854004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6712294816970825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2505481243133545
It took  1154.1141204833984  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14a76a473f40>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac17f40>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac17e20>, <torch.utils.data.dataloader.DataLoader object at 0x14a76ac17d00>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04856032878160477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6741926670074463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2101446390151978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1740856170654297
0 1.4044161731 	 3.1740855422
epoch_time;  38.93845176696777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022714678198099136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5670653581619263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8639858365058899
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.415539264678955
1 0.0788218103 	 2.4155393295
epoch_time;  36.2259407043457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015643106773495674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.445597767829895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6781795620918274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9069464206695557
2 0.0259154736 	 1.9069464646
epoch_time;  36.12641358375549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.046569984406232834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.158567190170288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0383529663085938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2072019577026367
3 0.0620749717 	 3.2072020009
epoch_time;  36.14702534675598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01209967490285635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6111123561859131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5745922327041626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7556672096252441
4 0.0232306227 	 1.7556672341
epoch_time;  36.1687650680542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02055896259844303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7195543646812439
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6602326035499573
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9111875295639038
5 0.0263942167 	 1.9111875782
epoch_time;  36.47779703140259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007473164703696966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4842197000980377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40634021162986755
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–ˆâ–„â–„
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‚â–â–â–ƒâ–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–ˆâ–„â–„
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–ˆâ–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–‚â–‚
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.78722
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.57611
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.88801
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.08967
wandb:                         Train loss 0.18923
wandb: 
wandb: ğŸš€ View run glowing-goat-1489 at: https://wandb.ai/nreints/thesis/runs/h1up8ox0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_025250-h1up8ox0/logs
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2370892763137817
6 0.011769146 	 1.2370893127
epoch_time;  37.75917959213257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010813471861183643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4431372284889221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36956164240837097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.138465404510498
7 0.0108427052 	 1.1384654434
epoch_time;  36.22101092338562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006539273541420698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3889513313770294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3181706666946411
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0147029161453247
8 0.0073127913 	 1.0147029266
epoch_time;  37.099278688430786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009834948927164078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6888924241065979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4561663269996643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5001599788665771
9 0.0198119883 	 1.5001599637
epoch_time;  36.10087537765503
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0055447835475206375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49012401700019836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2857989966869354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0568668842315674
10 0.0075449998 	 1.0568668792
epoch_time;  35.93747878074646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035707619972527027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44144096970558167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24433405697345734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9075295925140381
11 0.0070646049 	 0.9075296177
epoch_time;  36.35004901885986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00406164675951004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3853292465209961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22777137160301208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8222769498825073
12 0.0065738178 	 0.8222769654
epoch_time;  35.825875997543335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020827241241931915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3799465298652649
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2179536372423172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8086025714874268
13 0.0051852124 	 0.808602601
epoch_time;  35.73070406913757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00562893133610487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5888674855232239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32683447003364563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1629235744476318
14 0.0090613997 	 1.1629235766
epoch_time;  36.061458587646484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01639552041888237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4514475464820862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19144238531589508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7672953009605408
15 0.0044079181 	 0.7672953073
epoch_time;  36.02785325050354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01215856522321701
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45273011922836304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1474764049053192
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7284623384475708
16 0.0039035853 	 0.7284623345
epoch_time;  36.3753936290741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010511326603591442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6745837330818176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40632906556129456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.320540189743042
17 0.0159767362 	 1.3205401372
epoch_time;  36.42862057685852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00411190465092659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.487076997756958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22189633548259735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.904789388179779
18 0.0058615278 	 0.9047893974
epoch_time;  36.63014221191406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017371827736496925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4759249985218048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20272421836853027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8314772248268127
19 0.0043671678 	 0.8314772315
epoch_time;  36.199530601501465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030094420071691275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4557260572910309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21648886799812317
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.857377827167511
20 0.0070426798 	 0.8573778043
epoch_time;  35.91025185585022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028511909767985344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4389628469944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17272169888019562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7954749464988708
21 0.0036043781 	 0.7954749715
epoch_time;  36.587278604507446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001617577625438571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4318845868110657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15413200855255127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7577087879180908
22 0.0037333436 	 0.7577087771
epoch_time;  35.58145046234131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002629676600918174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4498029947280884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.13581353425979614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7451570630073547
23 0.0038004304 	 0.7451570632
epoch_time;  35.513700008392334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001702524023130536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45857053995132446
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1285819262266159
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7442309856414795
24 0.0044119522 	 0.7442309792
epoch_time;  39.04379343986511
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002086276886984706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6047813892364502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23090901970863342
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0430419445037842
25 0.0052802261 	 1.0430419553
epoch_time;  37.093425035476685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005156232044100761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4666445255279541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18636395037174225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8102909326553345
26 0.0030526523 	 0.810290933
epoch_time;  35.99316763877869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032252317760139704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4325248897075653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1596856415271759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7222028970718384
27 0.0029579404 	 0.7222029118
epoch_time;  36.27629208564758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5196800827980042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.410942792892456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5975112915039062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.734780788421631
28 0.7265259066 	 5.7347806717
epoch_time;  36.34159517288208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08966773748397827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5805916786193848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8885701894760132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7850871086120605
29 0.1892281474 	 2.7850871014
epoch_time;  36.19009351730347
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08967075496912003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5761065483093262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8880095481872559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.787224054336548
It took  1157.0801544189453  seconds.

JOB STATISTICS
==============
Job ID: 2141142
Array Job ID: 2141141_1
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 15:52:00
CPU Efficiency: 27.00% of 2-10:45:36 core-walltime
Job Wall-clock time: 03:15:52
Memory Utilized: 25.62 GB
Memory Efficiency: 81.99% of 31.25 GB
