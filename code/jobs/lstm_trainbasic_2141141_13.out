/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_041633-9x54htcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-fish-1518
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9x54htcn
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(-10,', '10)_r(0,', '0)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571b213f70>, <torch.utils.data.dataloader.DataLoader object at 0x145714520430>, <torch.utils.data.dataloader.DataLoader object at 0x1457145203a0>, <torch.utils.data.dataloader.DataLoader object at 0x1457145206a0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03111954592168331
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15307581424713135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.271921157836914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.393720626831055
0 1.5282370533 	 31.3937199065
epoch_time;  49.2150764465332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018745383247733116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10360196977853775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.232416152954102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.378652572631836
1 0.0948269921 	 30.3786525208
epoch_time;  47.8073947429657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00921713002026081
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06436458230018616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.505273818969727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.015214920043945
2 0.0701655201 	 28.0152149028
epoch_time;  46.791072607040405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010265372693538666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0675576776266098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.406517028808594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.382402420043945
3 0.0459661318 	 26.3824024028
epoch_time;  47.24574375152588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010396047495305538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07830241322517395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.352099418640137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.361658096313477
4 0.0550444355 	 26.3616585631
epoch_time;  46.60503387451172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011715038679540157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07273352146148682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.218607902526855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.558765411376953
5 0.0455367632 	 24.5587663449
epoch_time;  47.03925347328186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0047023240476846695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07820139080286026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.031292915344238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.900127410888672
6 0.0342214788 	 27.9001280448
epoch_time;  47.22996997833252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010145854204893112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07092306762933731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.374581336975098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.738473892211914
7 0.0356646299 	 24.7384730221
epoch_time;  47.229990005493164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024817006662487984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1029977947473526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.634712219238281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.840805053710938
8 0.0320796894 	 22.8408055608
epoch_time;  47.161279916763306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004909408278763294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06178276240825653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.785712242126465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.077287673950195
9 0.0318927635 	 24.0772885786
epoch_time;  47.56965708732605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01822328194975853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08502030372619629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.309584617614746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.149124145507812
10 0.0244299219 	 24.1491249292
epoch_time;  46.951255083084106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00561955850571394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07184286415576935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.751354217529297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.981176376342773
11 0.0261040052 	 26.9811768316
epoch_time;  46.783172369003296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004984784871339798
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058469559997320175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.788137435913086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.427452087402344
12 0.021242738 	 26.4274511424
epoch_time;  46.97495889663696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006889197044074535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05697837471961975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.949767112731934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.855810165405273
13 0.0266734173 	 23.8558109894
epoch_time;  47.95048189163208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005798026919364929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06278526782989502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.630810737609863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.061248779296875
14 0.0190524449 	 26.0612490559
epoch_time;  47.556403398513794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019129905849695206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04645661637187004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.111854553222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.759416580200195
15 0.0201861636 	 26.7594174849
epoch_time;  47.283138275146484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026393458247184753
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0522686205804348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.772371292114258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.637279510498047
16 0.0196866015 	 27.6372793146
epoch_time;  47.26654839515686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020784931257367134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03963133320212364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.722224235534668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.069995880126953
17 0.0156605841 	 28.0699968136
epoch_time;  47.45869016647339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014036008156836033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0805375799536705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.409822463989258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.40680694580078
18 0.0205576173 	 26.4068076142
epoch_time;  47.29106020927429
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003077420173212886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04681084305047989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.403830528259277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.06833267211914
19 0.0184318057 	 27.0683328219
epoch_time;  45.922149419784546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017725389916449785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03444597125053406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.677580833435059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.499601364135742
20 0.0126505165 	 26.4996017041
epoch_time;  46.560858726501465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029194606468081474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05522427335381508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.955520629882812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.08147430419922
21 0.0152975006 	 26.0814736358
epoch_time;  47.330081939697266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026404443196952343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04167664051055908
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.229816436767578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.9072322845459
22 0.0202182189 	 24.9072324632
epoch_time;  46.917972803115845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004582558758556843
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.143 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▅▄▃▃▂▄▂▁▂▂▃▃▂▃▃▄▄▃▃▃▃▂▂▃▄▇▇███
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▃▄▃▄▃▅▃▄▃▂▂▃▂▂▁▄▂▁▂▁▃▂▁▄▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▅▄▃▃▂▄▂▁▁▂▃▂▁▂▃▃▃▂▄▃▃▂▂▄▄▇▇███
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▃▃▃▂▃▆▂▅▂▂▂▂▁▁▁▄▁▁▁▁▂▂▁▃▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 35.16365
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05243
wandb:    Test loss t(0, 0)_r(-5, 5)_none 17.41205
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00757
wandb:                         Train loss 0.01242
wandb: 
wandb: 🚀 View run crimson-fish-1518 at: https://wandb.ai/nreints/thesis/runs/9x54htcn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_041633-9x54htcn/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_044125-5powhu6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-horse-1525
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5powhu6b
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06480616331100464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.670448303222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.084697723388672
23 0.0194185403 	 25.0846983573
epoch_time;  47.54848647117615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007304843980818987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0594019778072834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.85403823852539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.52899932861328
24 0.0122076391 	 26.5289988907
epoch_time;  46.85319185256958
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021635994780808687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042465940117836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.812495231628418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.65870475769043
25 0.018130036 	 28.6587046828
epoch_time;  46.889575719833374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009241371415555477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08589595556259155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.04693603515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.044342041015625
26 0.0120901314 	 34.0443436084
epoch_time;  46.87709951400757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023539888206869364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.036943674087524414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.706396102905273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.22209167480469
27 0.0123058596 	 34.2220897847
epoch_time;  46.53197503089905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030300195794552565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03946956247091293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.67515754699707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.9581184387207
28 0.0198072106 	 34.9581199254
epoch_time;  47.08295249938965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0075713954865932465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05243527889251709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.389202117919922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.084049224853516
29 0.0124182416 	 35.0840492825
epoch_time;  47.24633193016052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007572119124233723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052429117262363434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.41204833984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.16365051269531
It took  1492.9509601593018  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x145714520130>, <torch.utils.data.dataloader.DataLoader object at 0x14571459e6e0>, <torch.utils.data.dataloader.DataLoader object at 0x1456eeff0160>, <torch.utils.data.dataloader.DataLoader object at 0x1456eeff0310>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027249520644545555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33722415566444397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.180355072021484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.4511604309082
0 1.3822735794 	 32.4511600736
epoch_time;  46.58620095252991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018411537632346153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29388928413391113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.46866226196289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.27235984802246
1 0.0878259744 	 29.2723606259
epoch_time;  48.20355701446533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01883922703564167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24468927085399628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.001850128173828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.71786117553711
2 0.0630665273 	 27.7178619477
epoch_time;  47.83356332778931
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019456295296549797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3360798954963684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.89027214050293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.378271102905273
3 0.0542442385 	 27.3782719269
epoch_time;  47.180538177490234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019666267558932304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2236143797636032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.128878593444824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.219070434570312
4 0.0404426003 	 24.2190701119
epoch_time;  47.481950998306274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007327576633542776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26314011216163635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.25542163848877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.2847957611084
5 0.0308226499 	 23.2847963085
epoch_time;  47.48453211784363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009146657772362232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20467376708984375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.558494567871094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.32927703857422
6 0.0369127925 	 22.3292771077
epoch_time;  47.203996658325195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002360985614359379
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21335598826408386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.828402519226074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.499980926513672
7 0.0344368316 	 22.4999808228
epoch_time;  47.12663412094116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004761112853884697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16258519887924194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.743301391601562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.75885772705078
8 0.0271321137 	 20.7588569203
epoch_time;  46.8180992603302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005132435820996761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18038903176784515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.593239784240723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.276838302612305
9 0.0292863867 	 20.2768377667
epoch_time;  47.125991106033325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003861487377434969
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14279340207576752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.187833786010742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.583845138549805
10 0.0212657952 	 20.583845709
epoch_time;  47.512876749038696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0060485778376460075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1674145758152008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.291756629943848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.854223251342773
11 0.0265338935 	 21.8542237066
epoch_time;  45.75162720680237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005413842853158712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21365457773208618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.958362579345703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.115903854370117
12 0.0196845107 	 21.1159041022
epoch_time;  47.460012912750244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05493366718292236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24963952600955963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.044605255126953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.06943702697754
13 0.0222167723 	 21.0694377242
epoch_time;  46.34986424446106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006919736973941326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15511620044708252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.85962200164795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.552295684814453
14 0.0190206365 	 20.5522947744
epoch_time;  47.5345401763916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004272038582712412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14543721079826355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.378846168518066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.911481857299805
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▅▄▃▃▃▂▂▂▂▂▂▂▃▂▄▃▂▂▂▁▂▂▂▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▆█▅▆▄▅▃▄▃▃▅▆▃▃▃▄▂▃▂▁▂▄▁▂▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▆▃▃▃▃▂▂▂▂▂▂▂▃▂▅▄▂▂▂▁▃▂▂▃▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃▃▃▃▂▂▁▁▂▁▂▂█▂▁▂▅▁▁▁▂▂▂▁▂▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 19.48443
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.10556
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.86351
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00474
wandb:                         Train loss 0.01138
wandb: 
wandb: 🚀 View run chromatic-horse-1525 at: https://wandb.ai/nreints/thesis/runs/5powhu6b
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_044125-5powhu6b/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_050613-krnliqz3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-noodles-1533
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/krnliqz3
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
15 0.0183823129 	 22.9114809526
epoch_time;  46.67609977722168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006123839411884546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15731149911880493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.531806945800781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.394866943359375
16 0.0141637096 	 20.3948675887
epoch_time;  46.88276410102844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03557345271110535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19971579313278198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.642075538635254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.67499542236328
17 0.0203396597 	 24.6749949844
epoch_time;  47.329955101013184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038256936240941286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10644607245922089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.590043067932129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.21101188659668
18 0.0156902079 	 23.2110112585
epoch_time;  46.837395668029785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004738884512335062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1660458743572235
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.917447090148926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.105510711669922
19 0.0187068408 	 21.1055115299
epoch_time;  47.00656175613403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019992825109511614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11660661548376083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.02392292022705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.476179122924805
20 0.0173255522 	 21.4761789558
epoch_time;  50.133885860443115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0055672298185527325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08085920661687851
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.020557403564453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.275318145751953
21 0.010975816 	 21.2753183417
epoch_time;  47.51935124397278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006680961232632399
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09587719291448593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.005916595458984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.04498291015625
22 0.0150283276 	 19.044982357
epoch_time;  46.819578886032104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007562187034636736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18578732013702393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.611756324768066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.311857223510742
23 0.0109842324 	 21.3118568259
epoch_time;  46.67481184005737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035997694358229637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09004601836204529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.912640571594238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.43218421936035
24 0.0131831981 	 20.4321849627
epoch_time;  46.83048605918884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005708135664463043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09452161937952042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.73912239074707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.633602142333984
25 0.0126896338 	 20.6336017159
epoch_time;  47.42977976799011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00387197220697999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07931220531463623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.439835548400879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.991893768310547
26 0.0135361948 	 20.9918939412
epoch_time;  47.17506289482117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012152037816122174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08609707653522491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.864792823791504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.915372848510742
27 0.0128750494 	 20.9153724509
epoch_time;  47.022520542144775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031248589511960745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07451246678829193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.922602653503418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.401123046875
28 0.0117080379 	 19.4011237845
epoch_time;  47.12124681472778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004744652193039656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1055578738451004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.861254692077637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.461524963378906
29 0.0113819066 	 19.4615246176
epoch_time;  46.87308883666992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0047431946732103825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10556112229824066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.863511085510254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.48443031311035
It took  1488.0182585716248  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571b16b310>, <torch.utils.data.dataloader.DataLoader object at 0x14571459e9e0>, <torch.utils.data.dataloader.DataLoader object at 0x14571459e8f0>, <torch.utils.data.dataloader.DataLoader object at 0x1456eeff3fa0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1420787274837494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2637645900249481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.793760299682617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.523853302001953
0 1.4544118669 	 31.5238534979
epoch_time;  47.82122993469238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021048912778496742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09214480966329575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.06475067138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.143964767456055
1 0.0951794228 	 26.1439647847
epoch_time;  46.934900760650635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009608939290046692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06095476448535919
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.748276710510254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.760147094726562
2 0.0695289421 	 24.7601462188
epoch_time;  46.28934979438782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018592461943626404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05662691220641136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.027007102966309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.523277282714844
3 0.0496030486 	 22.5232767065
epoch_time;  46.65167188644409
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00810802262276411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04414552450180054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.280688285827637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.17144012451172
4 0.0457390317 	 21.1714398249
epoch_time;  46.64879083633423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007841105572879314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04353375732898712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.996365547180176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.813764572143555
5 0.0399170009 	 21.8137642206
epoch_time;  47.32132363319397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021472664549946785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054823070764541626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.275277137756348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.572097778320312
6 0.0389394854 	 19.5720974556
epoch_time;  47.36655569076538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024344196543097496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054574597626924515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.403986930847168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.118621826171875
7 0.0360439107 	 19.1186213652
epoch_time;  49.426560163497925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009417054243385792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.033281028270721436
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▃▃▂▂▂▁▂▃▃▂▂▂▄▃▃▂▃▂▂▂▁▁▂▁▁▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▄▃▃▂▂▂▁▂▃▂▂▃▂▃▃▃▁▂▂▁▁▁▂▂▁▁▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▁▂▁▁▂▂▁▁▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 21.44253
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.02703
wandb:    Test loss t(0, 0)_r(-5, 5)_none 13.82196
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00489
wandb:                         Train loss 0.01961
wandb: 
wandb: 🚀 View run glistening-noodles-1533 at: https://wandb.ai/nreints/thesis/runs/krnliqz3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_050613-krnliqz3/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_053057-nhol61jb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-pig-1543
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nhol61jb
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.317315101623535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.157703399658203
8 0.0261065059 	 19.1577030424
epoch_time;  47.68880105018616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005275187082588673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03022410348057747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.377939224243164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.67298126220703
9 0.0292929073 	 17.6729813775
epoch_time;  46.50260543823242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002751428633928299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017842881381511688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.705985069274902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.51579475402832
10 0.0266330381 	 19.5157946445
epoch_time;  47.07941436767578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011444482952356339
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04402362182736397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.988055229187012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.45073127746582
11 0.0306946915 	 21.4507307992
epoch_time;  46.88934779167175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006971175782382488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030937358736991882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.916762351989746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.722314834594727
12 0.0197205222 	 20.7223140106
epoch_time;  47.1033148765564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01579275354743004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04517873376607895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.812694549560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.097822189331055
13 0.0207429564 	 20.097821469
epoch_time;  47.2437584400177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013786026276648045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04148053377866745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.132234573364258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.299365997314453
14 0.0227002571 	 20.2993650869
epoch_time;  46.766494274139404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028466517105698586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023068372160196304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.769302368164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.304208755493164
15 0.0234854436 	 20.3042080698
epoch_time;  46.72896218299866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0069930292665958405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03436639904975891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.94999885559082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.693096160888672
16 0.0176032151 	 22.6930953196
epoch_time;  47.20747208595276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012980214320123196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04478169232606888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.580994606018066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.88775062561035
17 0.0170824714 	 21.8877498938
epoch_time;  46.99172019958496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005222362000495195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.025850806385278702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.305180549621582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.405122756958008
18 0.0167368399 	 22.4051229702
epoch_time;  47.016908168792725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004096797201782465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.021734042093157768
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.66707992553711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.0350284576416
19 0.0240743639 	 20.0350279102
epoch_time;  46.95052933692932
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006696890573948622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.025200311094522476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.344552040100098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.456239700317383
20 0.0125358561 	 20.4562390837
epoch_time;  46.40350389480591
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002033848315477371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.01927598938345909
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.172957420349121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.366588592529297
21 0.0188283928 	 19.366588581
epoch_time;  47.53016757965088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014215783448889852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.018055036664009094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.684489250183105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.69645881652832
22 0.0118885156 	 18.696458707
epoch_time;  46.92008423805237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00770713621750474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03826947882771492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.668413162231445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.7585391998291
23 0.0202521614 	 18.7585382836
epoch_time;  46.55592465400696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026018654461950064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02433105558156967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.417302131652832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.002971649169922
24 0.0137508184 	 18.0029724674
epoch_time;  46.940693855285645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0045603797771036625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02621293067932129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.078886985778809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.31013298034668
25 0.012194692 	 18.3101338274
epoch_time;  47.22940373420715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003171340562403202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023344602435827255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.350554466247559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.130475997924805
26 0.0138563666 	 19.1304758308
epoch_time;  47.98947763442993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019640091340988874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.014007865451276302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.556970596313477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.595443725585938
27 0.010550377 	 17.5954434951
epoch_time;  48.08182072639465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015693054301664233
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.014135668985545635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.525182723999023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.436199188232422
28 0.0152178338 	 17.4361989001
epoch_time;  46.96978783607483
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004889904987066984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027031004428863525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.79934310913086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.439899444580078
29 0.0196121008 	 21.4399001015
epoch_time;  46.64796566963196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004889651667326689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027028558775782585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.821955680847168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.442529678344727
It took  1484.2835338115692  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1456eeff0460>, <torch.utils.data.dataloader.DataLoader object at 0x14571456d300>, <torch.utils.data.dataloader.DataLoader object at 0x14571456ece0>, <torch.utils.data.dataloader.DataLoader object at 0x14571459c5b0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032417088747024536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1974163055419922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.191627502441406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.15252685546875
0 1.4564256112 	 30.1525266711
epoch_time;  46.84877014160156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04564034938812256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15399447083473206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.048643112182617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.35253143310547
1 0.0946226445 	 24.3525316867
epoch_time;  46.65956711769104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01760733313858509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12420595437288284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.975683212280273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.048892974853516
2 0.0689390242 	 26.0488930325
epoch_time;  47.080477714538574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014512727968394756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0909070298075676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.614095687866211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.36665916442871
3 0.0486577371 	 23.3666593892
epoch_time;  46.58770561218262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010324020870029926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08181578665971756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.159152030944824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.127464294433594
4 0.0455977117 	 21.1274635338
epoch_time;  46.74038791656494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02625933662056923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10712447762489319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.864632606506348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.265119552612305
5 0.0428898051 	 23.2651204919
epoch_time;  47.055898666381836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004959761165082455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052450571209192276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.538576126098633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.521520614624023
6 0.0333159141 	 20.5215212542
epoch_time;  46.67294526100159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014492694288492203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09274298697710037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.245096206665039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.687496185302734
7 0.0370069673 	 21.6874955745
epoch_time;  46.080315351486206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005279789213091135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04579862579703331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.937507629394531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.46363639831543
8 0.0279377368 	 19.4636355858
epoch_time;  47.30552840232849
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02541998215019703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11637764424085617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.295402526855469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.51336097717285
9 0.0269553555 	 23.5133606141
epoch_time;  46.328551292419434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009164385497570038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06379017978906631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.72671890258789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.702974319458008
10 0.0274855638 	 21.7029745327
epoch_time;  46.51150035858154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015911290422081947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0721307173371315
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.698256492614746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.557344436645508
11 0.0263197651 	 18.5573442811
epoch_time;  47.21147608757019
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00381773104891181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04985345900058746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.810063362121582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.880779266357422
12 0.0188938548 	 18.8807797158
epoch_time;  46.842084884643555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010375811718404293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05645541474223137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.326391220092773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.805681228637695
13 0.0245055286 	 19.8056817645
epoch_time;  47.320937156677246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025888478849083185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04070292040705681
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.471846580505371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.649185180664062
14 0.022121903 	 20.6491854112
epoch_time;  48.182817220687866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014190401881933212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06511243432760239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.381420135498047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.85439109802246
15 0.0177600624 	 20.8543918759
epoch_time;  48.31123065948486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01475045457482338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06728680431842804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.910636901855469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.19087028503418
16 0.0207412266 	 23.1908707633
epoch_time;  46.40826725959778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006547361612319946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0686972588300705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.177989959716797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.590070724487305
17 0.0138699678 	 22.5900709262
epoch_time;  47.01543664932251
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009623474441468716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05220988392829895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.803397178649902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.149185180664062
18 0.0140457846 	 20.1491854112
epoch_time;  45.7920777797699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012494053691625595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10241368412971497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.545297622680664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.292280197143555
19 0.0201279917 	 22.2922798456
epoch_time;  44.22161841392517
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0096410708501935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05566104128956795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.783219337463379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.925363540649414
20 0.0144536945 	 20.9253637769
epoch_time;  43.96099662780762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030703209340572357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06039626896381378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.195760726928711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.73895835876465
21 0.0152298208 	 19.738958353
epoch_time;  43.92029595375061
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002281269757077098
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03718741610646248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.77896785736084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.077178955078125
22 0.0134759884 	 20.0771794161
epoch_time;  44.46612763404846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01864738203585148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06587111204862595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.26037883758545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.900293350219727
23 0.0121486196 	 20.9002932638
epoch_time;  44.43600511550903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004686358384788036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05533028766512871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.504227638244629
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.921585083007812
24 0.019432888 	 22.9215843915
epoch_time;  44.61677098274231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005792523268610239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057021208107471466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.89155101776123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.434371948242188
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▆▄▃▄▂▃▂▄▃▁▁▂▂▂▄▃▂▃▂▂▂▂▄▃▃▄▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▅▃▃▄▂▃▁▄▂▃▂▂▁▂▂▂▂▄▂▂▁▂▂▂▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▆▅▃▄▃▃▂▄▃▁▁▂▃▃▄▃▂▄▃▂▂▂▄▃▄▄▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆█▄▃▂▅▂▃▂▅▂▃▁▂▁▃▃▂▂▃▂▁▁▄▁▂▁▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 20.62939
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.04158
wandb:    Test loss t(0, 0)_r(-5, 5)_none 13.6249
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00332
wandb:                         Train loss 0.01094
wandb: 
wandb: 🚀 View run resplendent-pig-1543 at: https://wandb.ai/nreints/thesis/runs/nhol61jb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_053057-nhol61jb/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_055507-u88lblmr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-lamp-1551
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/u88lblmr
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
25 0.0126415198 	 21.4343711646
epoch_time;  44.27719974517822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016208258457481861
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054988544434309006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.685662269592285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.54429054260254
26 0.0110454444 	 22.5442905023
epoch_time;  43.91661334037781
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004303694702684879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06441836804151535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.158455848693848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.014427185058594
27 0.013346732 	 23.014427162
epoch_time;  44.15776181221008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002112708054482937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04094850271940231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.455731391906738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.92443084716797
28 0.0116086486 	 20.9244314695
epoch_time;  44.160690784454346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003323037177324295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04159226268529892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.618982315063477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.63007354736328
29 0.0109415848 	 20.6300731094
epoch_time;  44.07549333572388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033247366081923246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.041579436510801315
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.624897956848145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.629392623901367
It took  1449.5152807235718  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571b16b940>, <torch.utils.data.dataloader.DataLoader object at 0x145714588d90>, <torch.utils.data.dataloader.DataLoader object at 0x14571458ab00>, <torch.utils.data.dataloader.DataLoader object at 0x14571456ef20>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022034257650375366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22370480000972748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.07609748840332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.522357940673828
0 1.4366054954 	 29.5223576756
epoch_time;  44.66807842254639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011848945170640945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19310788810253143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.655915260314941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.083934783935547
1 0.087056809 	 27.0839342192
epoch_time;  44.06424856185913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011774157173931599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18210379779338837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.268667221069336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.318830490112305
2 0.0640759638 	 25.3188314294
epoch_time;  44.414390563964844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00845725554972887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1604875922203064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.945236206054688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.569135665893555
3 0.0467703225 	 25.5691353144
epoch_time;  48.17732834815979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003963425289839506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14302211999893188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.097529411315918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.72358512878418
4 0.0430558354 	 22.7235856071
epoch_time;  44.365710735321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010979657992720604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1417877972126007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.639470100402832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.279809951782227
5 0.0341376132 	 20.2798102341
epoch_time;  44.27528095245361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005784482229501009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11317623406648636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.368413925170898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.431058883666992
6 0.0337603856 	 19.4310579329
epoch_time;  43.98295497894287
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0074053374119102955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14451579749584198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.333870887756348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.921676635742188
7 0.0280323303 	 19.9216773272
epoch_time;  44.41892671585083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011246158741414547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13811205327510834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.460391998291016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.143495559692383
8 0.0312167853 	 19.1434956807
epoch_time;  44.25343632698059
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002466249978169799
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10909144580364227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.614034652709961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.54568862915039
9 0.0212607517 	 19.5456889634
epoch_time;  44.193084478378296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004574490711092949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07956768572330475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.948016166687012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.37453269958496
10 0.0302518546 	 18.3745323711
epoch_time;  44.270118713378906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053478749468922615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1317155808210373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.162240028381348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.183351516723633
11 0.0228846273 	 19.1833518221
epoch_time;  43.85497307777405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004195531364530325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13095955550670624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.533814430236816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.113933563232422
12 0.0251685062 	 19.1139332751
epoch_time;  43.874054193496704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004321086686104536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1502164900302887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.01387882232666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.74517250061035
13 0.0191421754 	 19.745173244
epoch_time;  44.260732650756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038819918408989906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11221644282341003
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.003823280334473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.79564666748047
14 0.0199241234 	 20.7956461834
epoch_time;  44.34522080421448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038696203846484423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11866386979818344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.193438529968262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.76750373840332
15 0.0254088986 	 20.7675028913
epoch_time;  43.927321672439575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005362335126847029
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14677155017852783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.891881942749023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.550865173339844
16 0.0148218691 	 21.5508653347
epoch_time;  44.270251989364624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025233689229935408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1166314035654068
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.270201683044434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.841588973999023
17 0.0166998061 	 19.841588876
epoch_time;  44.55821752548218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00563725084066391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11085089296102524
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▆▄▂▂▂▁▂▁▂▁▂▃▃▃▂▃▃▃▄▅▆▆▄▃▄▃▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▆▅▄▄▃▄▄▂▁▄▃▄▃▃▄▃▃▄▅▃▄▄▄▂▅▃▃▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▅▆▃▂▁▃▂▂▁▁▂▂▂▂▃▃▄▃▃▄▆▆▇▅▄▄▃▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▃▂▄▃▃▄▁▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▄▂▂▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 24.29927
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.18759
wandb:    Test loss t(0, 0)_r(-5, 5)_none 13.67848
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00417
wandb:                         Train loss 0.01379
wandb: 
wandb: 🚀 View run flashing-lamp-1551 at: https://wandb.ai/nreints/thesis/runs/u88lblmr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_055507-u88lblmr/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_061825-4ysooppg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-dragon-1560
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/4ysooppg
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.209939956665039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.831090927124023
18 0.013414741 	 21.8310900916
epoch_time;  44.1491801738739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034744455479085445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15136392414569855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.64304256439209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.429840087890625
19 0.0199607485 	 21.4298394425
epoch_time;  43.955769062042236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002486987505108118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17016351222991943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.67555046081543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.428701400756836
20 0.014609931 	 21.4287020865
epoch_time;  44.98014807701111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022568502463400364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12723900377750397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.949995994567871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.866479873657227
21 0.0154918538 	 22.8664794184
epoch_time;  44.25253224372864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013796199345961213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13620701432228088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.170352935791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.19198989868164
22 0.0125457654 	 25.1919904173
epoch_time;  44.000757455825806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016107839765027165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13815127313137054
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.23910903930664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.588531494140625
23 0.012786305 	 25.5885308488
epoch_time;  48.00720143318176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002805368509143591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1464080810546875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.471973419189453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.948213577270508
24 0.0172151093 	 25.9482126841
epoch_time;  44.84205102920532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012118673184886575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0945088341832161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.247908592224121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.540157318115234
25 0.0125890294 	 22.5401570761
epoch_time;  44.27325201034546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011320509947836399
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16695162653923035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.259581565856934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.869436264038086
26 0.0122609279 	 21.869435659
epoch_time;  43.52178502082825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027531364466995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11729155480861664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.915602684020996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.60923194885254
27 0.0121363771 	 22.6092319085
epoch_time;  43.35081338882446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005156064871698618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12571276724338531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.678179740905762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.658613204956055
28 0.008458152 	 21.6586132222
epoch_time;  43.969499826431274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004166481085121632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18758977949619293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.698619842529297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.256059646606445
29 0.013786352 	 24.2560599981
epoch_time;  43.78243160247803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004167228005826473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18759189546108246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.678478240966797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.29926872253418
It took  1398.259212255478  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571456ed40>, <torch.utils.data.dataloader.DataLoader object at 0x14571458a9e0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b2cb0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b2e90>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.052641212940216064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3798724114894867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.70267677307129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.015987396240234
0 1.4854305709 	 29.0159878918
epoch_time;  43.79620981216431
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011833890341222286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2893262207508087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.821167945861816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.945981979370117
1 0.0875587146 	 26.9459822272
epoch_time;  43.991658210754395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014836456626653671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22035175561904907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.461771011352539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.940919876098633
2 0.058586161 	 24.9409194439
epoch_time;  43.913233518600464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010798577219247818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.165378138422966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.942191123962402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.31339454650879
3 0.0487624926 	 24.3133954282
epoch_time;  43.51315140724182
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018082857131958008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19353283941745758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.63833999633789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.928268432617188
4 0.0373671573 	 20.9282683865
epoch_time;  44.193557024002075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005981271155178547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18662197887897491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.08156967163086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.693117141723633
5 0.0391163245 	 21.6931174471
epoch_time;  43.2359573841095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005655411630868912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1346702128648758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.694294929504395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.341876983642578
6 0.0290268834 	 22.3418765342
epoch_time;  43.72209286689758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007691862527281046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1429610401391983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.48648452758789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.072580337524414
7 0.0325400001 	 20.0725798362
epoch_time;  43.69526386260986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0071192579343914986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.126530721783638
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.637752532958984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.021968841552734
8 0.0329156712 	 24.0219697059
epoch_time;  43.71803045272827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010811641812324524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1388489007949829
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.453661918640137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.451339721679688
9 0.0288313751 	 18.4513400444
epoch_time;  43.71942162513733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005493128206580877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14654682576656342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.2018404006958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.270172119140625
10 0.0280863569 	 18.2701714738
epoch_time;  43.63439464569092
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▅▃▃▄▂▅▁▁▂▄▂▃▅▃▄▂▃▄▄▂▃▄▅▄▄▄▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▃▄▄▃▃▂▃▃▂▂▁▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▅▃▄▄▂▅▁▁▂▄▂▃▄▃▅▃▄▆▅▃▄▅▅▄▅▄▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▃▂▃▂▂▂▂▂▂▁▂▁▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 23.64025
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.12916
wandb:    Test loss t(0, 0)_r(-5, 5)_none 14.69506
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00745
wandb:                         Train loss 0.01324
wandb: 
wandb: 🚀 View run dazzling-dragon-1560 at: https://wandb.ai/nreints/thesis/runs/4ysooppg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_061825-4ysooppg/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_064129-rr0wzrqi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-kumquat-1567
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rr0wzrqi
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022600414231419563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09046390652656555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.029326438903809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.45714569091797
11 0.0182537971 	 19.4571448381
epoch_time;  43.53210210800171
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005795793142169714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11986453086137772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.804034233093262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.089900970458984
12 0.0211417078 	 23.0899012816
epoch_time;  49.10822939872742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017122469143941998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06636880338191986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.23210334777832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.321874618530273
13 0.0255848223 	 20.321874705
epoch_time;  44.42571711540222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011135268025100231
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11570026725530624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.696038246154785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.658321380615234
14 0.0159806361 	 20.6583211386
epoch_time;  43.62952661514282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005054930690675974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09826529771089554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.33122444152832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.934494018554688
15 0.0209969269 	 23.9344936037
epoch_time;  43.938836336135864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002029563533142209
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.092006616294384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.86827278137207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.058856964111328
16 0.0133449156 	 21.0588578054
epoch_time;  43.43378233909607
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0072739520110189915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13456232845783234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.803605079650879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.144105911254883
17 0.018318273 	 23.1441064011
epoch_time;  43.5285804271698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0052442392334342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09392742067575455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.705039978027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.105899810791016
18 0.0181680632 	 20.1058994996
epoch_time;  43.31395387649536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003160050604492426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09232935309410095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.54118537902832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.321184158325195
19 0.0175302724 	 21.3211843254
epoch_time;  43.523263692855835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004345291294157505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10492919385433197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.781006813049316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.34215545654297
20 0.0185556188 	 23.3421553413
epoch_time;  43.50231623649597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006683649029582739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12158437073230743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.125113487243652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.365100860595703
21 0.0105434291 	 23.3651016097
epoch_time;  43.86909198760986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022990675643086433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10331970453262329
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.894579887390137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.481250762939453
22 0.0194891988 	 20.4812505901
epoch_time;  43.706910610198975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026877704076468945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12024296075105667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.009345054626465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.08186912536621
23 0.0103113057 	 22.0818689813
epoch_time;  43.858763456344604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031194190960377455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11025071144104004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.898571968078613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.55353355407715
24 0.0127984104 	 23.5535339171
epoch_time;  44.09669256210327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012894360115751624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12325996905565262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.859786987304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.702646255493164
25 0.0114534849 	 23.7026455698
epoch_time;  43.80875039100647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004372900351881981
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11022253334522247
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.335718154907227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.7087345123291
26 0.0128904353 	 22.7087335961
epoch_time;  43.630449056625366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030643439386039972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12726840376853943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.901572227478027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.865068435668945
27 0.0123760359 	 22.865069156
epoch_time;  43.60023331642151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022364649921655655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10211271792650223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.360742568969727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.664581298828125
28 0.0124454657 	 22.6645817598
epoch_time;  43.6404013633728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007451092824339867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12914443016052246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.68047046661377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.600238800048828
29 0.013240241 	 23.6002392726
epoch_time;  43.897791385650635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007452299818396568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.129158154129982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.695064544677734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.64025115966797
It took  1384.4500617980957  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571b24afb0>, <torch.utils.data.dataloader.DataLoader object at 0x145714cff6a0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a6d10>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a6bc0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02004874311387539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.282886803150177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.334638595581055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.372947692871094
0 1.4783477206 	 36.3729495138
epoch_time;  43.811784505844116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013279645703732967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2551303505897522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.48261833190918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.456649780273438
1 0.0788240628 	 31.4566506562
epoch_time;  49.646947622299194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12909771502017975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4253195822238922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.85253620147705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.980152130126953
2 0.0649202911 	 28.9801530636
epoch_time;  43.86036205291748
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01691274531185627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20308196544647217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.863728523254395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.99717903137207
3 0.0492265111 	 26.9971794751
epoch_time;  43.99798512458801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012823116034269333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1922910064458847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.051583290100098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.25300407409668
4 0.042804796 	 27.253003446
epoch_time;  43.48066711425781
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019344644621014595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20717120170593262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.707182884216309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.290634155273438
5 0.0401065922 	 25.2906350312
epoch_time;  43.04988884925842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003586955601349473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17758364975452423
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.624368667602539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.0015926361084
6 0.0334704281 	 27.0015931835
epoch_time;  43.799311876297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016098808497190475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15661269426345825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.422743797302246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.68867301940918
7 0.0346215946 	 24.6886727601
epoch_time;  43.16806507110596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02836688980460167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19025976955890656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.497655868530273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.800085067749023
8 0.0273917413 	 26.8000849698
epoch_time;  43.32786154747009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008927666582167149
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15921220183372498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.324660301208496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.96333885192871
9 0.0242881746 	 24.9633390767
epoch_time;  43.17776107788086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038700862787663937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1568787693977356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.786883354187012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.827770233154297
10 0.0243321346 	 23.8277709592
epoch_time;  43.570210695266724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003124627750366926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1462504118680954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.202003479003906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.335100173950195
11 0.0230203437 	 24.3351010786
epoch_time;  43.95666241645813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009891996160149574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1560966819524765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.375353813171387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.2347412109375
12 0.023348887 	 24.2347408421
epoch_time;  43.85901093482971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001871206215582788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1390957534313202
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.914068222045898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.720972061157227
13 0.0186002863 	 25.7209716059
epoch_time;  43.39450526237488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007226927205920219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15987570583820343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.496357917785645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.589691162109375
14 0.0239267125 	 22.5896903323
epoch_time;  43.597941637039185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01161983422935009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14449289441108704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.023099899291992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.943639755249023
15 0.0162104896 	 24.9436396573
epoch_time;  43.91227650642395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003399553941562772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1695079654455185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.741362571716309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.634061813354492
16 0.0183880291 	 24.6340619689
epoch_time;  43.27644157409668
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05846436694264412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22806991636753082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.56871509552002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.00173568725586
17 0.01678648 	 25.0017347998
epoch_time;  43.5030300617218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003281171666458249
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11706102639436722
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.816381454467773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.066884994506836
18 0.0151782068 	 28.0668842051
epoch_time;  43.583980321884155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004906901624053717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10701614618301392
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.964942932128906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.64593505859375
19 0.0183705655 	 27.6459356118
epoch_time;  43.64881229400635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022697312757372856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18218763172626495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.166200637817383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.777210235595703
20 0.020129307 	 29.7772109847
epoch_time;  43.40672564506531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002533181570470333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13256756961345673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.313611030578613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.221149444580078
21 0.0156679074 	 28.2211486263
epoch_time;  47.56333684921265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00834900327026844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16433154046535492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.90676498413086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.368486404418945
22 0.0119277808 	 27.3684856495
epoch_time;  45.5608606338501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012607024982571602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12482583522796631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.484435081481934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.228200912475586
23 0.0099800263 	 24.2282014138
epoch_time;  43.71475386619568
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003181390231475234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09861226379871368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.263285636901855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.323944091796875
24 0.0114039999 	 23.3239443684
epoch_time;  43.454599380493164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007723980583250523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10881040245294571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.187170028686523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.10133934020996
25 0.0115525594 	 24.1013397493
epoch_time;  43.82620692253113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004299594089388847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14717379212379456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.269803047180176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.884658813476562
26 0.0116569907 	 23.8846579376
epoch_time;  43.521796464920044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003025934100151062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.131228506565094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.079939842224121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.365816116333008
27 0.0113056675 	 24.365815592
epoch_time;  43.37124752998352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020623249001801014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10773824155330658
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▃▂▃▂▃▂▂▂▂▃▁▂▂▂▄▄▅▄▃▂▁▂▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▄█▃▃▃▃▂▃▂▂▂▂▂▂▂▃▄▁▁▃▂▂▂▁▁▂▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▄▂▃▂▃▂▂▂▂▃▁▃▂▂▄▃▄▄▃▂▂▂▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▂█▂▂▂▁▂▂▁▁▁▁▁▁▂▁▄▁▁▂▁▁▁▁▁▁▁▁▂▂
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.47569
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.1294
wandb:    Test loss t(0, 0)_r(-5, 5)_none 12.11282
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0133
wandb:                         Train loss 0.01059
wandb: 
wandb: 🚀 View run filigreed-kumquat-1567 at: https://wandb.ai/nreints/thesis/runs/rr0wzrqi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_064129-rr0wzrqi/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_070432-aut3lbn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-noodles-1576
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/aut3lbn4
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.901619911193848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.463075637817383
28 0.0127549153 	 23.4630750212
epoch_time;  43.34686803817749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013291125185787678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12939830124378204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.120591163635254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.578575134277344
29 0.0105877247 	 22.5785749268
epoch_time;  43.18354511260986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013295766897499561
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12940089404582977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.112817764282227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.47568702697754
It took  1383.0498538017273  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1457145892a0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b2da0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b1690>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b1a20>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02804489992558956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22398969531059265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.303722381591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.623329162597656
0 1.4409446745 	 32.6233301076
epoch_time;  43.57849407196045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02288101799786091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21179801225662231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.928491592407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.307395935058594
1 0.0873862879 	 30.307395912
epoch_time;  43.21138644218445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023085379973053932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19763785600662231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.04385757446289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.8279972076416
2 0.0685256623 	 30.8279981354
epoch_time;  43.29381227493286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007893121801316738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1390295773744583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.419710159301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.836261749267578
3 0.0453465366 	 31.8362620374
epoch_time;  43.65888285636902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04793702811002731
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18768294155597687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.624826431274414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.49899673461914
4 0.0462001249 	 32.4989968844
epoch_time;  43.40269160270691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01766844093799591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12253148853778839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.0330171585083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.192819595336914
5 0.0332331863 	 26.1928194628
epoch_time;  43.55190825462341
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01006015669554472
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1155153289437294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.637267112731934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.125398635864258
6 0.0347362463 	 28.1253982959
epoch_time;  43.351642370224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009691118262708187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10348740220069885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.260078430175781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.324369430541992
7 0.0239927594 	 27.3243692173
epoch_time;  43.366838693618774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004015972837805748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10032156109809875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.770342826843262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.890016555786133
8 0.0237398169 	 26.89001723
epoch_time;  43.75541090965271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005873912014067173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09266381710767746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.962369918823242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.278940200805664
9 0.0267685248 	 27.2789398839
epoch_time;  43.56230139732361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006911128759384155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10017607361078262
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.605691909790039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.817678451538086
10 0.0204979942 	 27.8176778465
epoch_time;  46.72250699996948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00912482850253582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1311238706111908
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.35372257232666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.262020111083984
11 0.0218168211 	 27.2620196847
epoch_time;  45.81934881210327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015209645964205265
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10768409818410873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.729386329650879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.954355239868164
12 0.0198972732 	 27.9543552917
epoch_time;  43.8207151889801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0075338464230299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08132551610469818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.550527572631836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.011314392089844
13 0.0199847297 	 30.0113145534
epoch_time;  43.380470514297485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004491167608648539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07794493436813354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.486997604370117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.56681251525879
14 0.0193392287 	 25.5668133969
epoch_time;  43.34171223640442
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002471866086125374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1111937090754509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.248947143554688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.554534912109375
15 0.017415996 	 28.5545340823
epoch_time;  43.349578619003296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0040779695846140385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08456236869096756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 17.35325050354004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.41383171081543
16 0.0138391932 	 30.4138323735
epoch_time;  43.64871311187744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005860897712409496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0956789031624794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.411012649536133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.338489532470703
17 0.0185647094 	 28.338489544
epoch_time;  43.265010833740234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018008193001151085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0787191092967987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.996248245239258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.23166275024414
18 0.0111165909 	 26.2316636377
epoch_time;  43.458136320114136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003903981763869524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06194004788994789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.411934852600098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.66358184814453
19 0.0140106351 	 23.6635815946
epoch_time;  43.59393048286438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015982724726200104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05584140121936798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.210508346557617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.09931755065918
20 0.0110629715 	 25.0993172914
epoch_time;  43.84423041343689
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▇▇█▃▅▄▄▄▅▄▅▆▃▅▆▅▃▂▃▄▁▂▅▃▄▃▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▇▄▆▄▃▃▃▃▃▄▃▂▂▃▂▃▂▁▁▃▂▂▂▂▃▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▆▇▇▂▄▄▃▃▄▄▄▅▃▅▇▅▃▁▂▅▁▁▄▃▄▃▂▂▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▄▄▂█▃▂▂▁▂▂▂▃▂▁▁▁▂▁▁▁▂▂▂▂▁▃▁▁▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 24.1697
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08265
wandb:    Test loss t(0, 0)_r(-5, 5)_none 13.75756
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00332
wandb:                         Train loss 0.0097
wandb: 
wandb: 🚀 View run vivid-noodles-1576 at: https://wandb.ai/nreints/thesis/runs/aut3lbn4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_070432-aut3lbn4/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_072726-g42ywslr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resplendent-orchid-1583
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/g42ywslr
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009227899834513664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10103051364421844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.516260147094727
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.433134078979492
21 0.0144331078 	 27.433133497
epoch_time;  43.573867082595825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005609638523310423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07643980532884598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.412322044372559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.851442337036133
22 0.0151894704 	 22.8514430112
epoch_time;  43.603028535842896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006260557100176811
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07363048195838928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.471550941467285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.5637264251709
23 0.0100997388 	 23.5637258662
epoch_time;  43.08605456352234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007395974826067686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07907797396183014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.69063663482666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.75551986694336
24 0.0135846293 	 27.7555200859
epoch_time;  43.33877682685852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004390069283545017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07214280217885971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.52437973022461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.974916458129883
25 0.0094557422 	 24.9749162103
epoch_time;  43.18916845321655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01266658678650856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10926007479429245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.563158988952637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.54949188232422
26 0.0153307225 	 26.5494919515
epoch_time;  43.37347984313965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002386543434113264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0682818740606308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.1272554397583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.088642120361328
27 0.0150393241 	 26.0886429617
epoch_time;  43.74312686920166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003625514218583703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06549379229545593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.878289222717285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.944005966186523
28 0.0082172586 	 23.9440054994
epoch_time;  43.279179096221924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033176280558109283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08265351504087448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.783424377441406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.24923324584961
29 0.0096996244 	 24.2492329116
epoch_time;  43.79282760620117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003319993382319808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0826503336429596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.757564544677734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.16970443725586
It took  1373.9603252410889  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14571458a980>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b19c0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a7250>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a4820>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04442218318581581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18798236548900604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.970490455627441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.85205841064453
0 1.5546011914 	 29.8520581571
epoch_time;  47.60210585594177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01888495869934559
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11538228392601013
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.27220916748047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 29.598756790161133
1 0.1025575787 	 29.5987567268
epoch_time;  43.873416900634766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01875353418290615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11926530301570892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.0295991897583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27.028915405273438
2 0.0690900123 	 27.0289162812
epoch_time;  43.781681299209595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013997426256537437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10666032880544662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.263072967529297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.20098114013672
3 0.0557481533 	 33.2009801029
epoch_time;  43.567607164382935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009472057223320007
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08897949010133743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.048873901367188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.09111213684082
4 0.0449734528 	 26.0911123961
epoch_time;  43.375633001327515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006017676088958979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.056361500173807144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.238214492797852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.898134231567383
5 0.0447091125 	 22.898133615
epoch_time;  43.591602087020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011764932423830032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0673561617732048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.374608039855957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.841548919677734
6 0.0382398855 	 26.8415490465
epoch_time;  43.69372582435608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027188179083168507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.062038421630859375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.214566230773926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.910602569580078
7 0.0314981315 	 26.9106017513
epoch_time;  43.435232162475586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008799215778708458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05129360780119896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.496039390563965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.978981018066406
8 0.0343717103 	 25.9789817787
epoch_time;  43.61694860458374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02195082977414131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08405622839927673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.82405948638916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.021484375
9 0.0298790827 	 25.021484375
epoch_time;  43.451316833496094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004627808928489685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06506815552711487
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.243118286132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.306045532226562
10 0.0309786136 	 26.3060446563
epoch_time;  43.803178548812866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020614812150597572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0724879652261734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.310563087463379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.038114547729492
11 0.0281810082 	 25.0381154409
epoch_time;  43.832921504974365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010454392060637474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0708528682589531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.474454879760742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.46309471130371
12 0.0197835817 	 23.4630941985
epoch_time;  43.455432176589966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009531807154417038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.062244657427072525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.082141876220703
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▆▄█▃▁▄▄▃▃▄▃▂▂▁▂▂▂▃▆▃▃▃▃▃▂▁▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▄▃▂▂▂▂▃▂▃▃▂▁▃▂▂▂▂▂▂▁▁▁▁▂▃▅▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▆▅█▄▂▄▄▃▃▄▂▁▃▂▁▁▂▂▅▂▃▃▃▂▁▁▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▃▂▂▃▁▂▄▁▄▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▂▇▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 22.9864
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05851
wandb:    Test loss t(0, 0)_r(-5, 5)_none 11.6917
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00476
wandb:                         Train loss 0.01258
wandb: 
wandb: 🚀 View run resplendent-orchid-1583 at: https://wandb.ai/nreints/thesis/runs/g42ywslr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_072726-g42ywslr/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_075030-555mb35g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-moon-1590
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/555mb35g
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.478309631347656
13 0.0248478089 	 24.4783091012
epoch_time;  43.62371277809143
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004169652238488197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04523378238081932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.319279670715332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.764610290527344
14 0.0288912349 	 22.7646100831
epoch_time;  43.4301917552948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005903025157749653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07021414488554001
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.689885139465332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.101736068725586
15 0.0207047055 	 23.1017365701
epoch_time;  43.50686502456665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041259233839809895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05659055709838867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.771907806396484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.874629974365234
16 0.028936171 	 23.8746297323
epoch_time;  43.49526381492615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023499552626162767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05414750799536705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.612245559692383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.268648147583008
17 0.0172790972 	 24.2686476232
epoch_time;  43.57168984413147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00561974011361599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054493553936481476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.810555458068848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.89864730834961
18 0.0229089507 	 24.8986469741
epoch_time;  43.62412238121033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010144623927772045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06461406499147415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.118158340454102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 30.12882423400879
19 0.0187011288 	 30.1288236405
epoch_time;  43.63101124763489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00765996053814888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05820274353027344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.837286949157715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.1699275970459
20 0.0176900666 	 26.1699277757
epoch_time;  46.83361053466797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003102438058704138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04883221536874771
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.550284385681152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 26.074893951416016
21 0.0172124194 	 26.0748943778
epoch_time;  45.22580003738403
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003459646599367261
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.036645520478487015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.21430492401123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.636865615844727
22 0.020647368 	 25.636866267
epoch_time;  43.84497594833374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027219969779253006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03640450909733772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.011405944824219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.028013229370117
23 0.0164133934 	 25.0280134772
epoch_time;  43.807340145111084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036281466018408537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0403812937438488
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.790063858032227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.318946838378906
24 0.0137313638 	 25.3189464926
epoch_time;  43.26885008811951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001945790252648294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0430758111178875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.742212295532227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.19388198852539
25 0.0145727052 	 23.1938815852
epoch_time;  43.39816617965698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041228169575333595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.047841187566518784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.905890464782715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.03097152709961
26 0.0230531281 	 23.0309711929
epoch_time;  43.62423849105835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007390802260488272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07133644819259644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.635603904724121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.294048309326172
27 0.0089797627 	 22.2940485744
epoch_time;  43.43608379364014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03626945614814758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11876635998487473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.222452163696289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.48785400390625
28 0.0225950063 	 24.4878549259
epoch_time;  43.96416258811951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004754104185849428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05850717052817345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.685356140136719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.98610496520996
29 0.0125755524 	 22.9861053743
epoch_time;  43.185622692108154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004757253918796778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05850624665617943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.69170093536377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.986404418945312
It took  1383.8507623672485  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x145714cffe50>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1b24a0>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a4f70>, <torch.utils.data.dataloader.DataLoader object at 0x14571b1a5ed0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03504742309451103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17274655401706696
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 16.2249755859375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28.993671417236328
0 1.5445414333 	 28.993671521
epoch_time;  43.88228249549866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017881250008940697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10436055064201355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.10271167755127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.96648406982422
1 0.0979752985 	 25.966484139
epoch_time;  43.36968231201172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015581618994474411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12800268828868866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.232605934143066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.62997055053711
2 0.0623038189 	 25.6299713227
epoch_time;  43.42747759819031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013540688902139664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0953686311841011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.878413200378418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 25.096052169799805
3 0.0557422624 	 25.0960512651
epoch_time;  43.50504016876221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008928386494517326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11263640224933624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.096946716308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.968843460083008
4 0.0499933778 	 24.9688444109
epoch_time;  43.52257800102234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01314693782478571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08878765255212784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.319157600402832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.539777755737305
5 0.0369021483 	 23.5397779574
epoch_time;  43.096076011657715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006840555462986231
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▆▅▅▄▃▂▃▂▁▂▂▂▂▂▃▃▂▁▂▂▅▃▃▂▃▃▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▆▄▅▄▄▅▅▆▇▅▅▅▂▂▂▂▁▁▂▁▁▄▁▂▁▂▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▇▆▇▅▃▃▃▁▁▂▂▂▂▂▅▄▂▁▃▁▆▄▄▃▃▄▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▄▃▃▂▃▂▇▅▂▆▄▂▃▂▁▁▁▁▁▁▁▁▂▁▂▁▂█▁▁
wandb:                         Train loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 20.34177
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05736
wandb:    Test loss t(0, 0)_r(-5, 5)_none 12.76537
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00256
wandb:                         Train loss 0.01625
wandb: 
wandb: 🚀 View run vermilion-moon-1590 at: https://wandb.ai/nreints/thesis/runs/555mb35g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_075030-555mb35g/logs
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08894824981689453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.60437297821045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.57066535949707
6 0.0327880247 	 21.5706650656
epoch_time;  43.39267373085022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03463109955191612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11124832928180695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.610365867614746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.274513244628906
7 0.0288382537 	 20.2745128989
epoch_time;  43.49077653884888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023897048085927963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10972080379724503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.635014533996582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.8308162689209
8 0.034838192 	 20.83081571
epoch_time;  43.70061659812927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004944935906678438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12318985164165497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.96021842956543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.079126358032227
9 0.0306812953 	 19.0791266404
epoch_time;  47.293668031692505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029126780107617378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15862062573432922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.01611328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.115550994873047
10 0.0246545386 	 18.1155500614
epoch_time;  45.848729372024536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01978784240782261
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10805214196443558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.821243286132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.21323585510254
11 0.026987958 	 20.2132358148
epoch_time;  43.388355016708374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005142437294125557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10022516548633575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.452950477600098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.072755813598633
12 0.0293287498 	 19.0727553814
epoch_time;  43.53294348716736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015577437356114388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10379268229007721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.872591018676758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.74662208557129
13 0.0190959991 	 19.7466218608
epoch_time;  43.628148555755615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0057595944963395596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044575098901987076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.649672508239746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.91956901550293
14 0.0183441659 	 18.9195693094
epoch_time;  43.736093282699585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035622185096144676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.043976180255413055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.024877548217773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.80026626586914
15 0.0187192479 	 19.8002664157
epoch_time;  43.68575882911682
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002688394393771887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03753187134861946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.867178916931152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.557111740112305
16 0.0160261193 	 21.5571126794
epoch_time;  43.64552569389343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00292326626367867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04447469487786293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.188250541687012
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.12291717529297
17 0.0236544959 	 21.12291706
epoch_time;  43.53567814826965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002695278264582157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.034233152866363525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.086658477783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.37474250793457
18 0.0142786487 	 19.3747418453
epoch_time;  43.642136096954346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003259895136579871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03451664745807648
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.079730987548828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.694665908813477
19 0.0178780812 	 18.6946663756
epoch_time;  43.666178941726685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001705501927062869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04321298375725746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.177806854248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.06369400024414
20 0.0162986449 	 20.0636934125
epoch_time;  43.56917929649353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002618001541122794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03316536173224449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.118931770324707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.94611930847168
21 0.0113806432 	 18.946119418
epoch_time;  43.87496209144592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002234832849353552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026812847703695297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.652112007141113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 23.820011138916016
22 0.017070031 	 23.8200115653
epoch_time;  43.82379221916199
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007948708720505238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08083588629961014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 13.412748336791992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.699750900268555
23 0.0155062679 	 20.6997512863
epoch_time;  43.16070604324341
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003749367780983448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023902293294668198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.948420524597168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.829872131347656
24 0.0125838604 	 20.8298730764
epoch_time;  43.51473569869995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005065915174782276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04554110765457153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.656608581542969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.125051498413086
25 0.0142336303 	 20.1250516309
epoch_time;  43.56315898895264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018891177605837584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030681390315294266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.811307907104492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.07687759399414
26 0.0127901573 	 21.0768784814
epoch_time;  43.53106665611267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005383287090808153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04477105289697647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.922582626342773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.021024703979492
27 0.0109935893 	 21.0210255971
epoch_time;  43.44851469993591
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04129540175199509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.097682423889637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.796140670776367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.13468360900879
28 0.0174035167 	 20.1346830155
epoch_time;  43.11465859413147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025577708147466183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05736052989959717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.757064819335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.310070037841797
29 0.016249204 	 20.3100703951
epoch_time;  44.20569086074829
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002556262305006385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057361338287591934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 12.765365600585938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.34177017211914
It took  1380.4402723312378  seconds.

JOB STATISTICS
==============
Job ID: 2142191
Array Job ID: 2141141_13
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-23:10:12 core-walltime
Job Wall-clock time: 03:57:14
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
