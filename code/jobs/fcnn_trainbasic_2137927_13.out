wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142046-pzpkgh9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-kumquat-1248
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/pzpkgh9j
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▁▄▄▂█▁▃▂▂▁▃▂▃▁▁▂▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▃█▂▂▃▁▃▂▃▂▂▁▁▁▁▁▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄▁▃▂▄█▂▃▁▂▂▃▃▂▁▁▂▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▃█▂▂▃▁▃▂▃▂▂▁▁▁▁▁▁▂▁▁
wandb:                         Train loss █▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 198.5761
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.68467
wandb:    Test loss t(0, 0)_r(-5, 5)_none 98.12203
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.37738
wandb:                         Train loss 3.88376
wandb: 
wandb: 🚀 View run dancing-kumquat-1248 at: https://wandb.ai/nreints/thesis/runs/pzpkgh9j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142046-pzpkgh9j/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_143428-04qgvsjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-wish-1256
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/04qgvsjr
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.728949785232544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1874029636383057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 244.9154815673828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 227.06776428222656
0 8.683601023 	 227.0677576014 	 227.0677576014
epoch_time;  40.65683889389038
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7019864320755005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3230071067810059
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 89.66448211669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 101.70304870605469
1 5.6264678191 	 101.703051098 	 101.703051098
epoch_time;  37.95480990409851
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3954532146453857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1373512744903564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 191.12808227539062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 314.1532287597656
2 5.3517925906 	 314.1532305743 	 314.1532305743
epoch_time;  37.55452013015747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4888896346092224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8781957030296326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 132.6329345703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 371.23077392578125
3 4.9774263314 	 371.230785473 	 371.230785473
epoch_time;  37.83854651451111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49895867705345154
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0174602270126343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 202.4756317138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 189.127197265625
4 4.6589373651 	 189.1271959459 	 189.1271959459
epoch_time;  37.78814220428467
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5731261968612671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1579023599624634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 440.6793518066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 660.3675537109375
5 4.5476227943 	 660.3675675676 	 660.3675675676
epoch_time;  37.533123254776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3883938789367676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7206910848617554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 124.17977142333984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.39513397216797
6 4.4688657065 	 96.3951330236 	 96.3951330236
epoch_time;  37.34549832344055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6443915367126465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2248176336288452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 172.05874633789062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 241.31748962402344
7 4.4496938077 	 241.3174831081 	 241.3174831081
epoch_time;  37.53069710731506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5462106466293335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1157296895980835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.59226989746094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 146.54873657226562
8 4.2811977668 	 146.5487436655 	 146.5487436655
epoch_time;  37.065101861953735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6541346907615662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2876794338226318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 103.0902099609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 184.6504669189453
9 4.190098767 	 184.650464527 	 184.650464527
epoch_time;  37.76172423362732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4789621829986572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8773724436759949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 130.23350524902344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 123.61598205566406
10 4.2708615437 	 123.6159839527 	 123.6159839527
epoch_time;  37.666770219802856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5384846329689026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1005932092666626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 179.70291137695312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 220.64044189453125
11 4.1371197555 	 220.6404349662 	 220.6404349662
epoch_time;  37.55135226249695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35083815455436707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6104932427406311
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 150.8634033203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 180.56617736816406
12 4.0740379173 	 180.5661739865 	 180.5661739865
epoch_time;  37.80503010749817
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33186960220336914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6218922734260559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.80686950683594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 242.13491821289062
13 4.0165301551 	 242.1349239865 	 242.1349239865
epoch_time;  37.93496012687683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35078442096710205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6577072739601135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.15702819824219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 131.313232421875
14 4.0203739236 	 131.3132390203 	 131.3132390203
epoch_time;  37.876734495162964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.351423054933548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6249401569366455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.57462310791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 136.00965881347656
15 3.9470943794 	 136.0096600507 	 136.0096600507
epoch_time;  37.807061433792114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35445353388786316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6648349761962891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 117.77678680419922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 189.20790100097656
16 3.9137527373 	 189.2078969595 	 189.2078969595
epoch_time;  37.42467403411865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34495314955711365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6035674810409546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 91.13402557373047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 183.94651794433594
17 3.9137560432 	 183.9465160473 	 183.9465160473
epoch_time;  37.75679659843445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4330217242240906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8687978982925415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.81329345703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 109.4811019897461
18 3.9425090978 	 109.4811021959 	 109.4811021959
epoch_time;  37.37492752075195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3774110972881317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6842848658561707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.12739562988281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 199.1461639404297
19 3.8837586482 	 199.1461570946 	 199.1461570946
epoch_time;  37.28993272781372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3773818910121918
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.684666633605957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.12203216552734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 198.5760955810547
It took 822.519228219986 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▅▁▁▆▂█▁▆▃▇▂█▂▃▁▁▄▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▄█▂▂▅▃▁▃▃▂▃▃▁▂▅▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄█▄▂▆▁▄▆▄█▂▃▆▂▆▂▁▆▆▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▆▄█▂▂▆▂▂▂▄▁▁▂▂▁▅▂▁▁▁
wandb:                         Train loss █▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1548.94861
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.03021
wandb:    Test loss t(0, 0)_r(-5, 5)_none 47.00851
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31518
wandb:                         Train loss 3.76965
wandb: 
wandb: 🚀 View run flashing-wish-1256 at: https://wandb.ai/nreints/thesis/runs/04qgvsjr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_143428-04qgvsjr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144750-wuezbx4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-dragon-1263
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/wuezbx4q
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6145420074462891
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7259390354156494
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.499385833740234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2624.70751953125
0 7.8844330516 	 2624.7074324324 	 2624.7074324324
epoch_time;  37.32040214538574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7720653414726257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3751955032348633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 85.42870330810547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8796.404296875
1 5.1502970681 	 8796.4040540541 	 8796.4040540541
epoch_time;  37.4085111618042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5454890131950378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6155648231506348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.2807502746582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1441.1136474609375
2 4.7451340476 	 1441.1136824324 	 1441.1136824324
epoch_time;  37.87314987182617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9376868009567261
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.673490047454834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 45.204593658447266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 400.6187438964844
3 4.7536576464 	 400.61875 	 400.61875
epoch_time;  37.7930748462677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4434327483177185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3096773624420166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.89295196533203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10738.103515625
4 4.5554600885 	 10738.1033783784 	 10738.1033783784
epoch_time;  37.32391810417175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3807157874107361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1472055912017822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 37.66920852661133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2333.491943359375
5 4.4150265001 	 2333.4918918919 	 2333.4918918919
epoch_time;  37.418559074401855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.771488606929779
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9369564056396484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.33050537109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15922.39453125
6 4.1972484343 	 15922.3945945946 	 15922.3945945946
epoch_time;  37.262083530426025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.391774982213974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4784871339797974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.41897583007812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1501.4278564453125
7 4.2656461578 	 1501.4278716216 	 1501.4278716216
epoch_time;  37.37645125389099
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38720765709877014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9970687627792358
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.7439079284668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11035.787109375
8 4.0775223347 	 11035.7871621622 	 11035.7871621622
epoch_time;  37.28883767127991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3966604173183441
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4998462200164795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 89.25699615478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4257.10107421875
9 4.0004492716 	 4257.1010135135 	 4257.1010135135
epoch_time;  37.10797572135925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5900460481643677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4967504739761353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 44.56772994995117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12708.8447265625
10 4.1276586038 	 12708.8445945946 	 12708.8445945946
epoch_time;  37.446882009506226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34751608967781067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1977108716964722
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.152889251708984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2721.384521484375
11 3.9430125862 	 2721.3846283784 	 2721.3846283784
epoch_time;  37.05764293670654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3303827941417694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5245085954666138
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.82205200195312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15473.3544921875
12 3.9187734058 	 15473.3540540541 	 15473.3540540541
epoch_time;  36.90647602081299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36081263422966003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3886147737503052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 44.01852798461914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1515.423095703125
13 3.920497432 	 1515.4231418919 	 1515.4231418919
epoch_time;  37.269952058792114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40063056349754333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9677691459655762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.04794311523438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4424.11328125
14 3.9572160497 	 4424.1135135135 	 4424.1135135135
epoch_time;  36.88133668899536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3397432565689087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3055102825164795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 39.54085159301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1189.0682373046875
15 3.8250108433 	 1189.0682432432 	 1189.0682432432
epoch_time;  36.894750118255615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6643783450126648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0684382915496826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 34.22990036010742
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1346.9888916015625
16 3.8391858761 	 1346.9888513514 	 1346.9888513514
epoch_time;  37.239115476608276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3746846318244934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0002747774124146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.6163558959961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6486.0390625
17 3.7905937923 	 6486.0391891892 	 6486.0391891892
epoch_time;  37.22796010971069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34969934821128845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.24024498462677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.70999908447266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3090.285400390625
18 3.8207164808 	 3090.285472973 	 3090.285472973
epoch_time;  37.464046239852905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31513941287994385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.030402660369873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.97917175292969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1548.9453125
19 3.7696522459 	 1548.9452702703 	 1548.9452702703
epoch_time;  37.233566999435425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3151755928993225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0302115678787231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.00851058959961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1548.9486083984375
It took 801.7605819702148 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▄▄▃▇▃▁▅▄▂▆█▂▅▂▃▂▂▃▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▂▂▄▅▂▃▄▂▃▁▂▄▃▃▁▂▁▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▃▃▂▅▂▁▂█▂▄▂▁▁▂▂▂▁▆▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▄▄▄▁▂▂▁▄▁▂▁▂▂▁▁▁▁▁▁
wandb:                         Train loss █▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 316.12265
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.93579
wandb:    Test loss t(0, 0)_r(-5, 5)_none 302.49258
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31702
wandb:                         Train loss 3.73465
wandb: 
wandb: 🚀 View run crimson-dragon-1263 at: https://wandb.ai/nreints/thesis/runs/wuezbx4q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144750-wuezbx4q/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150108-exq0883o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-pig-1269
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/exq0883o
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4964796304702759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.326358795166016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 511.3189697265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 202.5859375
0 7.9764573724 	 202.5859375 	 202.5859375
epoch_time;  37.235814809799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6337119340896606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9091163873672485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 227.9006805419922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6497.2109375
1 5.3917774363 	 6497.2108108108 	 6497.2108108108
epoch_time;  37.355740785598755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7470481991767883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8448164463043213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 262.30029296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7183.7001953125
2 4.7748358952 	 7183.7 	 7183.7
epoch_time;  37.342814922332764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7573167085647583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.664154291152954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 115.88567352294922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5152.26904296875
3 4.7794634923 	 5152.2692567568 	 5152.2692567568
epoch_time;  37.36921215057373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.858147382736206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.984393358230591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 456.94915771484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13978.0244140625
4 4.5379937029 	 13978.0243243243 	 13978.0243243243
epoch_time;  38.42789554595947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3975594937801361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.525728464126587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 110.0969467163086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3577.76318359375
5 4.3535830653 	 3577.7631756757 	 3577.7631756757
epoch_time;  37.90607285499573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4204438626766205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1359028816223145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.28484344482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 967.4666137695312
6 4.2313418379 	 967.4666385135 	 967.4666385135
epoch_time;  37.27992224693298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5530275702476501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.620790958404541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 108.65181732177734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9680.1630859375
7 4.1832509283 	 9680.1635135135 	 9680.1635135135
epoch_time;  37.003567934036255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34719350934028625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8965249061584473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 835.55078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6102.86962890625
8 4.1740327601 	 6102.8695945946 	 6102.8695945946
epoch_time;  36.97599768638611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8336457014083862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2663161754608154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 156.07159423828125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1421.193115234375
9 4.0336110729 	 1421.1930743243 	 1421.1930743243
epoch_time;  37.01241493225098
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3219928443431854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4652856588363647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 389.8169250488281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11417.232421875
10 4.0620166776 	 11417.2324324324 	 11417.2324324324
epoch_time;  37.19882869720459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42489632964134216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.546262502670288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 148.35986328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15644.1083984375
11 3.9310782916 	 15644.1081081081 	 15644.1081081081
epoch_time;  36.958927154541016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32100921869277954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.411405086517334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.52214050292969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2306.97705078125
12 3.9158585948 	 2306.977027027 	 2306.977027027
epoch_time;  37.215792179107666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5561667680740356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.982405662536621
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.304656982421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8616.615234375
13 3.8133806914 	 8616.6148648649 	 8616.6148648649
epoch_time;  37.03200888633728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44484731554985046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3094077110290527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 211.8496551513672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1515.3397216796875
14 3.8770278383 	 1515.3396959459 	 1515.3396959459
epoch_time;  37.28394532203674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34784823656082153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4420169591903687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 201.4053192138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3729.960693359375
15 3.8170565203 	 3729.9608108108 	 3729.9608108108
epoch_time;  37.243293046951294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35820645093917847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.913541555404663
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 144.18759155273438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3188.73046875
16 3.7867711816 	 3188.7304054054 	 3188.7304054054
epoch_time;  36.91540479660034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3392208516597748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2943601608276367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 92.40774536132812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2336.813232421875
17 3.7753810379 	 2336.8133445946 	 2336.8133445946
epoch_time;  37.252556800842285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3385489881038666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6677440404891968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 582.1072387695312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3847.99267578125
18 3.751320376 	 3847.9925675676 	 3847.9925675676
epoch_time;  37.03703308105469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31698352098464966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9356133937835693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 302.85614013671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 316.2834167480469
19 3.734650712 	 316.2834248311 	 316.2834248311
epoch_time;  37.04612135887146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3170231580734253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9357901811599731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 302.4925842285156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 316.1226501464844
It took 797.7658009529114 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: - 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▁▂▂▂▂▂▂▂▂▃▆▁█▂▁▁▇▆▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▄▇▅▇▃█▃█▂▅▂▂▃▁▁▂▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▂▅▄▇▂▃▂█▃▆▃▁▆▁▂▂▄▄▆▆
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▆▄▄▃▄▂▅▃▆▂▃▃▂▁▂▁▂▁▁
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 206.06039
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.76696
wandb:    Test loss t(0, 0)_r(-5, 5)_none 48.41658
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2958
wandb:                         Train loss 3.76743
wandb: 
wandb: 🚀 View run crimson-pig-1269 at: https://wandb.ai/nreints/thesis/runs/exq0883o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150108-exq0883o/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151854-z5lk1lzn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-noodles-1280
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/z5lk1lzn
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7398571372032166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.599321722984314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.460811614990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 567.3484497070312
0 8.0820742267 	 567.3484375 	 567.3484375
epoch_time;  37.12961483001709
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5219473242759705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2739192247390747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 33.419761657714844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.32511901855469
1 5.2891712641 	 96.3251161318 	 96.3251161318
epoch_time;  37.39276719093323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6184188723564148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0644888877868652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 42.250022888183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 147.80288696289062
2 4.887562746 	 147.8028821791 	 147.8028821791
epoch_time;  37.38983678817749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48756158351898193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6701509952545166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.30648422241211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 141.1109161376953
3 4.7992701237 	 141.1109163851 	 141.1109163851
epoch_time;  37.18564438819885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48607075214385986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9732648134231567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 51.46957778930664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 132.95053100585938
4 4.5095388403 	 132.9505384291 	 132.9505384291
epoch_time;  37.27532458305359
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.416839063167572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.070326328277588
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 31.02841567993164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 146.45315551757812
5 4.3798877713 	 146.4531566723 	 146.4531566723
epoch_time;  37.775946855545044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5004891753196716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2166316509246826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 33.73417282104492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 127.0914077758789
6 4.3337833764 	 127.09140625 	 127.09140625
epoch_time;  37.520999908447266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3561490476131439
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0062898397445679
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 30.08946990966797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 165.89393615722656
7 4.2206036174 	 165.8939294764 	 165.8939294764
epoch_time;  38.24286341667175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5360515117645264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2768590450286865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.899314880371094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 189.73257446289062
8 4.1711219163 	 189.7325802365 	 189.7325802365
epoch_time;  37.698572635650635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3919615149497986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8591775298118591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 36.38893508911133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 164.153564453125
9 4.0908318831 	 164.1535684122 	 164.1535684122
epoch_time;  36.83421301841736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6219866275787354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6568268537521362
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.4662971496582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 235.7760772705078
10 4.0004753882 	 235.7760768581 	 235.7760768581
epoch_time;  36.82630729675293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36543381214141846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.881796658039093
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 35.65599060058594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 496.1817932128906
11 4.0189447836 	 496.1817989865 	 496.1817989865
epoch_time;  36.9699387550354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3947342038154602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8252830505371094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 27.218887329101562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 95.66816711425781
12 3.9366735253 	 95.6681693412 	 95.6681693412
epoch_time;  36.85625743865967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4229305684566498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9962265491485596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.191829681396484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 634.0963134765625
13 3.9501305169 	 634.0963260135 	 634.0963260135
epoch_time;  37.01214790344238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3289644718170166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6582358479499817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 28.3648681640625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 191.4125213623047
14 3.903827768 	 191.4125211149 	 191.4125211149
epoch_time;  36.788421869277954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26959532499313354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5978403091430664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 30.976430892944336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 110.8825454711914
15 3.8634353049 	 110.8825485642 	 110.8825485642
epoch_time;  37.323704957962036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34671324491500854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8318010568618774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 30.896434783935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.60176849365234
16 3.8421347379 	 80.6017683699 	 80.6017683699
epoch_time;  37.48089551925659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2976827919483185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7142503261566162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 39.647396087646484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 542.4547729492188
17 3.8037006892 	 542.4547719595 	 542.4547719595
epoch_time;  37.21688532829285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3356725871562958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7025154829025269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 39.98713684082031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 452.7122802734375
18 3.9093496107 	 452.7122888514 	 452.7122888514
epoch_time;  36.76906490325928
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29584231972694397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7671485543251038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.33431625366211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 205.9723663330078
19 3.7674316737 	 205.9723606419 	 205.9723606419
epoch_time;  37.00881838798523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29580217599868774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7669563889503479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.41658401489258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 206.06039428710938
It took 1066.452867269516 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▃▄▄▆▆▄▃▃▄▆▁▃█▂▂▃▅▇▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▃▅▅█▄▄▃▃▂▂▂▃▃▂▄▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▃▅▇▅▂▃▂▁▃▅▂▆▁▃▃▃▁█▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▆██▄▄▂▃▁▃▂▄▂▁▄▅▃▁▂▁▁
wandb:                         Train loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 718.52832
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.43709
wandb:    Test loss t(0, 0)_r(-5, 5)_none 69.80659
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.31826
wandb:                         Train loss 3.64335
wandb: 
wandb: 🚀 View run prosperous-noodles-1280 at: https://wandb.ai/nreints/thesis/runs/z5lk1lzn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151854-z5lk1lzn/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_153209-v4tsj7v7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-bao-1289
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/v4tsj7v7
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4197638928890228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.131390333175659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 129.45538330078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 916.0216064453125
0 7.4457178032 	 916.0216216216 	 916.0216216216
epoch_time;  37.31501078605652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5982893109321594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.461472272872925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 171.3087921142578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 545.4052734375
1 5.0115524633 	 545.4052787162 	 545.4052787162
epoch_time;  37.06722640991211
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7167975306510925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.381443977355957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 250.88760375976562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 611.253173828125
2 4.5033859015 	 611.2531672297 	 611.2531672297
epoch_time;  37.21067929267883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7021321058273315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.387929439544678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 337.1434020996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 671.120849609375
3 4.5060259923 	 671.1208192568 	 671.1208192568
epoch_time;  37.92366671562195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47250837087631226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.352316856384277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 256.8017883300781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1046.54541015625
4 4.3427889752 	 1046.5454391892 	 1046.5454391892
epoch_time;  36.965981245040894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4687354862689972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8585383892059326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 130.54473876953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 894.3272094726562
5 4.0732683041 	 894.3271959459 	 894.3271959459
epoch_time;  36.91254639625549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34285029768943787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.058892726898193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 134.43603515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 606.7476196289062
6 4.0168290083 	 606.7475929054 	 606.7475929054
epoch_time;  37.051491260528564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4463738203048706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.227628469467163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 101.31004333496094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 481.85186767578125
7 3.9509483962 	 481.8518581081 	 481.8518581081
epoch_time;  37.128045082092285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33265528082847595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.120558738708496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.77584457397461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 546.4575805664062
8 4.0324073952 	 546.4576013514 	 546.4576013514
epoch_time;  37.08289980888367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4002213776111603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7094860076904297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 138.3155975341797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 714.5673217773438
9 3.879761486 	 714.5673141892 	 714.5673141892
epoch_time;  36.76796865463257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33617737889289856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.516144037246704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 236.19296264648438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 931.9881591796875
10 3.8576149395 	 931.9881756757 	 931.9881756757
epoch_time;  36.81366491317749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4904084801673889
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.891355037689209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 124.63980865478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 188.83956909179688
11 3.769029084 	 188.8395692568 	 188.8395692568
epoch_time;  37.15048098564148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3768848776817322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3812196254730225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 288.269775390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 435.6175537109375
12 3.7601808459 	 435.6175675676 	 435.6175675676
epoch_time;  37.39246845245361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30355778336524963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0850565433502197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 74.5382308959961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1284.6080322265625
13 3.7772105223 	 1284.6080236486 	 1284.6080236486
epoch_time;  36.965193033218384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4921266436576843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.813446521759033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 156.25833129882812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 280.106689453125
14 3.7007361575 	 280.1066934122 	 280.1066934122
epoch_time;  37.07267999649048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.521320641040802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.92089581489563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.11769104003906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 403.4207763671875
15 3.7538586451 	 403.420777027 	 403.420777027
epoch_time;  37.161248207092285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4032577574253082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5382814407348633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 162.03880310058594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 427.76983642578125
16 3.7010789792 	 427.769847973 	 427.769847973
epoch_time;  36.70675492286682
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32000449299812317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.503596067428589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.75432586669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 867.6212768554688
17 3.6899133061 	 867.6212837838 	 867.6212837838
epoch_time;  36.87178874015808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3428158164024353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1783103942871094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 386.6314697265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1113.0692138671875
18 3.6359395497 	 1113.0692567568 	 1113.0692567568
epoch_time;  37.30512285232544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3182237148284912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.436701536178589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.79460144042969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 719.6212768554688
19 3.6433467465 	 719.6212837838 	 719.6212837838
epoch_time;  37.38400459289551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3182602822780609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4370899200439453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.80659484863281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 718.5283203125
It took 794.5240981578827 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▃▅▅▆█▃▁▃▂▂▂▃▁▆▂▇▆▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▅▇▃▃▁▂▂▂▆▂▂▄▂▂▁▃▅▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▄▅▅▇█▃▃▅▂▃▇▄▁▄▂▅█▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▄▇▅▄▃▂▂▂▂█▂▂▄▂▂▁▁▃▆▆
wandb:                         Train loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 320.27005
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.71298
wandb:    Test loss t(0, 0)_r(-5, 5)_none 197.33501
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.75655
wandb:                         Train loss 3.80398
wandb: 
wandb: 🚀 View run prosperous-bao-1289 at: https://wandb.ai/nreints/thesis/runs/v4tsj7v7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_153209-v4tsj7v7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154530-9g3gbrf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glittering-dog-1296
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9g3gbrf1
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5206424593925476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8201587200164795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 283.8399353027344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 388.4925537109375
0 7.7479774511 	 388.4925675676 	 388.4925675676
epoch_time;  37.56061673164368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5694656372070312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.000739812850952
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 244.69862365722656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 358.57867431640625
1 5.3049070117 	 358.5786739865 	 358.5786739865
epoch_time;  37.38072872161865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8553183674812317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.581634283065796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 318.05914306640625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 709.2008666992188
2 4.8509733888 	 709.2008445946 	 709.2008445946
epoch_time;  37.362271308898926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7218106389045715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.360562801361084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 316.7381896972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 674.1716918945312
3 4.4859056311 	 674.1717060811 	 674.1717060811
epoch_time;  37.17065238952637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6512549519538879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.850694179534912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 407.7943115234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 880.2060546875
4 4.4748083901 	 880.2060810811 	 880.2060810811
epoch_time;  37.18508982658386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5565470457077026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8113815784454346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 466.4786376953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1172.670654296875
5 4.3693677402 	 1172.6706081081 	 1172.6706081081
epoch_time;  37.50456666946411
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41308408975601196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2325730323791504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 230.3119354248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 425.5912170410156
6 4.3650614765 	 425.5912162162 	 425.5912162162
epoch_time;  38.14279103279114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41974958777427673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6242578029632568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 184.70787048339844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 181.8855743408203
7 4.155413277 	 181.8855785473 	 181.8855785473
epoch_time;  37.38122916221619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3821749687194824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3953677415847778
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 329.473876953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 468.5242004394531
8 4.1902700126 	 468.5241976351 	 468.5241976351
epoch_time;  37.095858335494995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37542712688446045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3302725553512573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 160.2884979248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 235.9679412841797
9 3.9767873011 	 235.9679476351 	 235.9679476351
epoch_time;  37.18617057800293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9996346831321716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9428014755249023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 190.1796875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 332.22674560546875
10 4.0509230731 	 332.2267525338 	 332.2267525338
epoch_time;  37.18294620513916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44754648208618164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4857454299926758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 438.5203552246094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 335.9580078125
11 4.0355281636 	 335.9580025338 	 335.9580025338
epoch_time;  37.49984931945801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37556853890419006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4539161920547485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 251.83506774902344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 495.4211120605469
12 3.9786594316 	 495.4211148649 	 495.4211148649
epoch_time;  37.6854305267334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5631170868873596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.061946392059326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 100.27632904052734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 119.64634704589844
13 3.8852046484 	 119.6463471284 	 119.6463471284
epoch_time;  37.86341190338135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3906576335430145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.405387282371521
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 277.6690368652344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 896.1765747070312
14 3.8176968729 	 896.1766047297 	 896.1766047297
epoch_time;  38.056729316711426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3757250905036926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3814915418624878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 162.13076782226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 198.79782104492188
15 3.8277167215 	 198.7978251689 	 198.7978251689
epoch_time;  37.24620223045349
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33242741227149963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0522860288619995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 319.8433837890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1007.0888671875
16 3.8240818858 	 1007.0888513514 	 1007.0888513514
epoch_time;  37.14190196990967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3133828341960907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6689995527267456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 466.0229797363281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 881.6683959960938
17 3.7953344512 	 881.6684121622 	 881.6684121622
epoch_time;  37.645973443984985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5193933248519897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5061373710632324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 158.13926696777344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 404.2527770996094
18 3.7842352266 	 404.2527871622 	 404.2527871622
epoch_time;  37.63059067726135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.756483256816864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7066421508789062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 197.34857177734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 320.91558837890625
19 3.8039840047 	 320.9155827703 	 320.9155827703
epoch_time;  37.52381467819214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7565457820892334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7129812240600586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 197.3350067138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 320.2700500488281
It took 801.7451055049896 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁█▆▆▂▁█▁▂▂▁▃▄▆▁▁▁▂▄▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▅█▄▅▂▃▄▃▆▁▂▄▄▅▂▅▅▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▂▆▄▂▂▁▂▃▄▇▂▃▂▇▁▃█▆▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▅█▄▅▂▄▂▃▃▂▃▃▃▂▁▂▁▁▃▃
wandb:                         Train loss █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 257.13678
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.41287
wandb:    Test loss t(0, 0)_r(-5, 5)_none 98.39751
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.38591
wandb:                         Train loss 3.5205
wandb: 
wandb: 🚀 View run glittering-dog-1296 at: https://wandb.ai/nreints/thesis/runs/9g3gbrf1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154530-9g3gbrf1/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155850-l5bge4j2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-horse-1304
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/l5bge4j2
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5113012194633484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.930651664733887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.45767211914062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 203.06756591796875
0 7.1196656017 	 203.0675675676 	 203.0675675676
epoch_time;  37.133599042892456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5211062431335449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.329280376434326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 78.44225311279297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1325.843017578125
1 4.5533145602 	 1325.8429898649 	 1325.8429898649
epoch_time;  37.411816358566284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6489537954330444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.129554748535156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 188.38365173339844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 918.9857177734375
2 4.2907772697 	 918.9857263514 	 918.9857263514
epoch_time;  36.939234495162964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45122724771499634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.9582695960998535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 150.50411987304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1079.9366455078125
3 4.1779945492 	 1079.9366554054 	 1079.9366554054
epoch_time;  37.81040573120117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4752745032310486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.571035861968994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 101.14857482910156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 299.5262145996094
4 4.0149548943 	 299.5262246622 	 299.5262246622
epoch_time;  37.1113338470459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32085925340652466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.515413284301758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 75.44597625732422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 260.4639587402344
5 3.8769951099 	 260.4639569257 	 260.4639569257
epoch_time;  37.43028712272644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4673144817352295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.843823432922363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.0931282043457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1278.844482421875
6 3.828020203 	 1278.8444256757 	 1278.8444256757
epoch_time;  37.25324892997742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33653828501701355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.16419792175293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.13566589355469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 182.0825653076172
7 3.789858891 	 182.0825591216 	 182.0825591216
epoch_time;  37.085997343063354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37269026041030884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.049405574798584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 119.7256851196289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 283.624755859375
8 3.7275089788 	 283.6247677365 	 283.6247677365
epoch_time;  37.97873544692993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3969931900501251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.131392478942871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 139.0113525390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 415.167236328125
9 3.7488029341 	 415.1672297297 	 415.1672297297
epoch_time;  37.73503375053406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36206334829330444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.712345600128174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 224.15647888183594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 244.1886749267578
10 3.7024872921 	 244.1886824324 	 244.1886824324
epoch_time;  36.89700150489807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4147709906101227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.454502582550049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 91.98983001708984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 441.666015625
11 3.6675393137 	 441.6660050676 	 441.6660050676
epoch_time;  36.911967039108276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38844653964042664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.621570587158203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 108.47821807861328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 725.48876953125
12 3.6473663888 	 725.4887668919 	 725.4887668919
epoch_time;  37.421120166778564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39360904693603516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.709441184997559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 97.4214859008789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 961.6231689453125
13 3.5799478726 	 961.6231418919 	 961.6231418919
epoch_time;  37.457509994506836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3607567846775055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.272462368011475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 242.33876037597656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 255.4180145263672
14 3.6095250878 	 255.4180109797 	 255.4180109797
epoch_time;  36.97506284713745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2925385534763336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.622491836547852
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 62.837379455566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 198.9977569580078
15 3.5725962149 	 198.9977618243 	 198.9977618243
epoch_time;  36.877922773361206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33787286281585693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.928503036499023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 105.877197265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 249.55804443359375
16 3.5428940367 	 249.5580447635 	 249.5580447635
epoch_time;  37.31345725059509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2914031744003296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.382726192474365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 257.76751708984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 401.6311950683594
17 3.5176544913 	 401.6312077703 	 401.6312077703
epoch_time;  37.35864472389221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29912325739860535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.104045391082764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 203.900146484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 698.0391845703125
18 3.5345168892 	 698.0391891892 	 698.0391891892
epoch_time;  37.26277136802673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38588422536849976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4125428199768066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.41840362548828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 257.11505126953125
19 3.5204965724 	 257.1150548986 	 257.1150548986
epoch_time;  37.27142667770386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3859061896800995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.412867784500122
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.39750671386719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 257.13677978515625
It took 800.0008783340454 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▅▂█▁▃▁▃▄▄▃▁▁▁▁▁▁▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▇▅▁▅▂▂▁▁▂▃▂▂▂▂▃▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃█▂▆▁▂▂▃▅▇█▂▃▂▂▄▂▄▃▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▆█▂▄▂▃▂▂▂▂▅▁▂▂▃▁▁▂▂
wandb:                         Train loss █▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 175.00296
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.17087
wandb:    Test loss t(0, 0)_r(-5, 5)_none 83.54722
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.40221
wandb:                         Train loss 3.79635
wandb: 
wandb: 🚀 View run dancing-horse-1304 at: https://wandb.ai/nreints/thesis/runs/l5bge4j2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155850-l5bge4j2/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_161207-rhicxft7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-festival-1312
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rhicxft7
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6058434844017029
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.442361831665039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 66.95809173583984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 453.37408447265625
0 8.0365784477 	 453.3740709459 	 453.3740709459
epoch_time;  37.0357460975647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7623578906059265
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3213765621185303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 156.4557647705078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 637.0946044921875
1 5.2488462413 	 637.0945945946 	 637.0945945946
epoch_time;  36.870802879333496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6734843850135803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.260801315307617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 60.31235122680664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 268.0837707519531
2 4.9904591761 	 268.0837837838 	 268.0837837838
epoch_time;  36.55846071243286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7696529626846313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8289463520050049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 119.9795913696289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 960.409912109375
3 4.676989556 	 960.4098817568 	 960.4098817568
epoch_time;  36.76057243347168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3823639452457428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9439947009086609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 38.95018768310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 116.30147552490234
4 4.5063235232 	 116.3014780405 	 116.3014780405
epoch_time;  36.72587442398071
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.514147937297821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.849433422088623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.89215087890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 324.8890075683594
5 4.3900785349 	 324.8890202703 	 324.8890202703
epoch_time;  36.816712617874146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3792237639427185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1239069700241089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 53.592411041259766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 107.53406524658203
6 4.3144594062 	 107.5340688345 	 107.5340688345
epoch_time;  37.0153706073761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4606829583644867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.199203372001648
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 64.55476379394531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 334.3398132324219
7 4.1556060109 	 334.3398015203 	 334.3398015203
epoch_time;  36.99346375465393
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3835458755493164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9909665584564209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 101.9809341430664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 486.23480224609375
8 4.1225384278 	 486.2347972973 	 486.2347972973
epoch_time;  37.28690195083618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3840703070163727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9585588574409485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 136.95362854003906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 440.2281188964844
9 4.0381302365 	 440.228125 	 440.228125
epoch_time;  37.6285445690155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4036308825016022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.070487141609192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 153.7528533935547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 359.2398986816406
10 4.0980030388 	 359.2399070946 	 359.2399070946
epoch_time;  37.57243299484253
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39320144057273865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3705525398254395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.87332534790039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 131.60133361816406
11 3.9680184046 	 131.6013302365 	 131.6013302365
epoch_time;  38.55313754081726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5644948482513428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1958036422729492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.18853759765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 102.03070068359375
12 3.9168559845 	 102.0307010135 	 102.0307010135
epoch_time;  37.71112060546875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3485783636569977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0504566431045532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.605995178222656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 96.42078399658203
13 3.9138426577 	 96.4207875845 	 96.4207875845
epoch_time;  37.1546151638031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3814927041530609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1534768342971802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 61.745704650878906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 83.70277404785156
14 3.8862846855 	 83.7027766047 	 83.7027766047
epoch_time;  36.91903066635132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3846621811389923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.219553828239441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 86.47663116455078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 123.28787994384766
15 3.8400659064 	 123.2878800676 	 123.2878800676
epoch_time;  37.076828956604004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4500707983970642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2746738195419312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.0423469543457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 85.90408325195312
16 3.8293997546 	 85.9040857264 	 85.9040857264
epoch_time;  37.121328353881836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3437821567058563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1037296056747437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 81.82238006591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 177.11647033691406
17 3.7984803444 	 177.1164695946 	 177.1164695946
epoch_time;  37.51312756538391
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3561166822910309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9215973615646362
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 72.17070007324219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 157.5359649658203
18 3.7678859042 	 157.5359586149 	 157.5359586149
epoch_time;  37.20451283454895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4023485779762268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.170674204826355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.54708862304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 175.14920043945312
19 3.7963516762 	 175.1491976351 	 175.1491976351
epoch_time;  37.33999681472778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40220651030540466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1708701848983765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.5472183227539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 175.00296020507812
It took 796.4488008022308 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▂▇▄▄▄█▄▁▃▃▁▂▇▁▄▃▂▁▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▂█▃▃▁▂▁▃▃▁▁▁▂▁▂▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▆▆▅▅▃█▄▂▂▄▁▁▆▁▂▁▂▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▂█▂▃▁▂▂▄▄▂▁▂▂▁▂▁▂▁▁▁
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 277009.375
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.02141
wandb:    Test loss t(0, 0)_r(-5, 5)_none 325.87744
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.30374
wandb:                         Train loss 3.74618
wandb: 
wandb: 🚀 View run festive-festival-1312 at: https://wandb.ai/nreints/thesis/runs/rhicxft7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_161207-rhicxft7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162525-kv8c09ih
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-tiger-1320
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kv8c09ih
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.801732063293457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.209956645965576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 327.19805908203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 560341.4375
0 8.0281702247 	 560341.4486486487 	 560341.4486486487
epoch_time;  37.191280126571655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4521436095237732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4013512134552002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 689.593505859375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 192435.78125
1 5.1844129517 	 192435.7837837838 	 192435.7837837838
epoch_time;  36.747276067733765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5817514657974243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.066473484039307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 781.7896118164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 827134.4375
2 5.012941602 	 827134.4432432433 	 827134.4432432433
epoch_time;  36.74776339530945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4846683740615845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3272016048431396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 617.0401000976562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 433918.1875
3 4.6665329982 	 433918.1837837838 	 433918.1837837838
epoch_time;  37.16414523124695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6878543496131897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.314168930053711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 621.7954711914062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 487246.84375
4 4.3985634547 	 487246.8324324324 	 487246.8324324324
epoch_time;  37.28497934341431
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3365151584148407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.962658166885376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 393.8964538574219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 469088.6875
5 4.3257645883 	 469088.6918918919 	 469088.6918918919
epoch_time;  37.57078409194946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.509218692779541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7670907974243164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 995.95947265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 964165.0
6 4.3014493721 	 964165.0162162163 	 964165.0162162163
epoch_time;  37.05933141708374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.405307799577713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1880279779434204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 452.8229675292969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 392055.625
7 4.0891405853 	 392055.6108108108 	 392055.6108108108
epoch_time;  36.95193290710449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8120989203453064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.299069404602051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 190.2159881591797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 28400.73046875
8 4.0514305285 	 28400.7297297297 	 28400.7297297297
epoch_time;  37.222148418426514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7666164636611938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.969170331954956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 217.03228759765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 334338.25
9 4.1080615664 	 334338.2486486487 	 334338.2486486487
epoch_time;  37.59804916381836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4544767141342163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0329018831253052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 535.77490234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 325358.6875
10 3.9296713011 	 325358.6810810811 	 325358.6810810811
epoch_time;  37.09399342536926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3481588065624237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0268572568893433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 141.3664093017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45188.60546875
11 3.9651624386 	 45188.6054054054 	 45188.6054054054
epoch_time;  37.55209445953369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40337932109832764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0560903549194336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 131.7305450439453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 162086.390625
12 3.9150337731 	 162086.3891891892 	 162086.3891891892
epoch_time;  37.09512138366699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5009474754333496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4979064464569092
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 770.388427734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 888545.1875
13 3.877311815 	 888545.2108108108 	 888545.2108108108
epoch_time;  37.13595175743103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3127623498439789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8150950074195862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.56826782226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 86883.046875
14 3.7997022046 	 86883.0432432432 	 86883.0432432432
epoch_time;  38.01123881340027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49813488125801086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3600993156433105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 176.50360107421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 479245.09375
15 3.8411363564 	 479245.1027027027 	 479245.1027027027
epoch_time;  37.63499689102173
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3321835696697235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9224045276641846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 96.64976501464844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 239219.5
16 3.7777302096 	 239219.5027027027 	 239219.5027027027
epoch_time;  37.80920672416687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4106353223323822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.156423568725586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 161.00820922851562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 173256.0
17 3.7740636372 	 173256.0 	 173256.0
epoch_time;  37.677629709243774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30108633637428284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9501259326934814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 410.0209045410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 67522.515625
18 3.7044219041 	 67522.5189189189 	 67522.5189189189
epoch_time;  37.49308204650879
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30384352803230286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0213249921798706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 325.8514709472656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 277009.3125
19 3.7461833399 	 277009.3189189189 	 277009.3189189189
epoch_time;  37.101258277893066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30373746156692505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0214054584503174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 325.87744140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 277009.375
It took 798.0236132144928 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▃▁▁▆▃▄▂▂▃▃▂▅▄▄▂▃▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▃▂▄▂▆▃▂▂▃▃▃▂▄▃▁▁▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▄▂▇▃▁▃▂▂▂▁▄▂▇▂▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▃▃▃▃▂▃▃▁▂▂▂▂▂▂▂▁▁
wandb:                         Train loss █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 120.32383
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.11975
wandb:    Test loss t(0, 0)_r(-5, 5)_none 103.72581
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25268
wandb:                         Train loss 3.6765
wandb: 
wandb: 🚀 View run luminous-tiger-1320 at: https://wandb.ai/nreints/thesis/runs/kv8c09ih
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162525-kv8c09ih/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0116803646087646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0631327629089355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 345.8975830078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 134.70277404785156
0 7.781349177 	 134.7027766047 	 134.7027766047
epoch_time;  36.8425817489624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6905554533004761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.718583822250366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 163.1702423095703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 111.77313232421875
1 5.3104582769 	 111.7731313345 	 111.7731313345
epoch_time;  37.32230305671692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47107845544815063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5183351039886475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 141.491455078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77.42168426513672
2 4.9323213096 	 77.4216849662 	 77.4216849662
epoch_time;  37.249021768569946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3876487612724304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3885232210159302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 131.59861755371094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 81.03382873535156
3 4.4833273607 	 81.0338312922 	 81.0338312922
epoch_time;  37.628955602645874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4555175006389618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8926959037780762
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 166.5724639892578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 54.76676559448242
4 4.4643928254 	 54.7667652027 	 54.7667652027
epoch_time;  37.38843631744385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4618748426437378
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4392447471618652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 97.29801177978516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 59.28787612915039
5 4.223857283 	 59.2878747889 	 59.2878747889
epoch_time;  37.31161570549011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47828370332717896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4072017669677734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 316.7533874511719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 112.36349487304688
6 4.2445303905 	 112.3634923986 	 112.3634923986
epoch_time;  36.95933675765991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4552905857563019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.520312786102295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 130.39471435546875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 72.05615997314453
7 4.1112504364 	 72.0561602618 	 72.0561602618
epoch_time;  37.331393003463745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3421323895454407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1823447942733765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 74.37751770019531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 89.44839477539062
8 3.9992740246 	 89.4483952703 	 89.4483952703
epoch_time;  37.15357828140259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4597090780735016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2796826362609863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 142.82591247558594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 70.8794937133789
9 4.0327455886 	 70.8794974662 	 70.8794974662
epoch_time;  37.04744911193848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41585585474967957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.487251877784729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.02491760253906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 70.98339080810547
10 3.8838816277 	 70.9833878801 	 70.9833878801
epoch_time;  37.076459646224976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2981569170951843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5010181665420532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 107.97444915771484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 76.53987121582031
11 3.9179221368 	 76.5398701436 	 76.5398701436
epoch_time;  36.9897403717041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38088470697402954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4744893312454224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 98.86019134521484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 76.45365142822266
12 3.8306440949 	 76.4536528716 	 76.4536528716
epoch_time;  37.16963028907776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40102988481521606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4225449562072754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 54.95271301269531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 67.45641326904297
13 3.8084522575 	 67.4564136402 	 67.4564136402
epoch_time;  37.29448199272156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3773297965526581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8664377927780151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 189.55377197265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 97.96418762207031
14 3.7517455146 	 97.9641891892 	 97.9641891892
epoch_time;  36.90327548980713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36982232332229614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5993056297302246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 87.72726440429688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 88.94754791259766
15 3.7976806475 	 88.9475506757 	 88.9475506757
epoch_time;  37.023709774017334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.327547550201416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0035929679870605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 293.59228515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 92.45741271972656
16 3.7564445191 	 92.4574113176 	 92.4574113176
epoch_time;  37.44959855079651
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33106938004493713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.033645510673523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 78.75691986083984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 66.0764389038086
17 3.7439872313 	 66.0764358108 	 66.0764358108
epoch_time;  37.97235894203186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39998751878738403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2962099313735962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 122.21222686767578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 80.12409210205078
18 3.736574745 	 80.1240920608 	 80.1240920608
epoch_time;  37.350860834121704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25272276997566223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.119365930557251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 103.6580581665039
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 120.16261291503906
19 3.6765025971 	 120.1626161318 	 120.1626161318
epoch_time;  37.42296624183655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2526835501194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1197476387023926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 103.72581481933594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 120.3238296508789
It took 800.7828550338745 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138596
Array Job ID: 2137927_13
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-17:27:54 core-walltime
Job Wall-clock time: 02:18:13
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
