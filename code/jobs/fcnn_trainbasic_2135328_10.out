wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165024-8whzakx4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-springroll-1139
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/8whzakx4
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–…â–‡â–†â–„â–ƒâ–ƒâ–â–„â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–…â–„â–ˆâ–†â–„â–ƒâ–„â–â–ƒâ–„â–„â–„â–‚â–â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.91614
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.18893
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.88838
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12172
wandb:                         Train loss 2.20668
wandb: 
wandb: ðŸš€ View run glistening-springroll-1139 at: https://wandb.ai/nreints/thesis/runs/8whzakx4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165024-8whzakx4/logs
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18787184357643127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3267306089401245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.199616432189941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.539375305175781
0 6.205600457 	 6.5393752639 	 6.5739772487
epoch_time;  35.571717977523804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15748342871665955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2525467276573181
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8432188034057617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9356284141540527
1 2.704940269 	 3.9356283652 	 3.9372964395
epoch_time;  34.650994539260864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1394987404346466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24098919332027435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.977583885192871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.149463653564453
2 2.5143284265 	 4.1494635505 	 4.1512794288
epoch_time;  34.489856004714966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19799749553203583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2929033637046814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.405343532562256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.719504356384277
3 2.427655364 	 4.7195041966 	 4.7215714738
epoch_time;  34.56281781196594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17491312325000763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26501864194869995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7778077125549316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.123747825622559
4 2.3770642032 	 4.1237479545 	 4.1253381678
epoch_time;  34.27482891082764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13308803737163544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21489597856998444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9475619792938232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.282005786895752
5 2.3447809445 	 4.2820058462 	 4.2836481868
epoch_time;  34.018598794937134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12426305562257767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20097661018371582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.222057342529297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.489106178283691
6 2.3182223455 	 4.4891063793 	 4.4907315641
epoch_time;  33.96040749549866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13383060693740845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19995175302028656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.115485668182373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.291022300720215
7 2.2964814328 	 4.2910222234 	 4.2925995698
epoch_time;  34.34602189064026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09316982328891754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1313612014055252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0015363693237305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.086936950683594
8 2.2690404893 	 4.086936827 	 4.0884224557
epoch_time;  35.844441652297974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12287073582410812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22632834315299988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.755866289138794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.921809673309326
9 2.2662595673 	 3.9218096759 	 3.9233655775
epoch_time;  35.36566925048828
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13658665120601654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2211681604385376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.951591968536377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.194692134857178
10 2.2577450597 	 4.1946919209 	 4.1959921347
epoch_time;  34.36469745635986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14094237983226776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22320057451725006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.008045196533203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1250319480896
11 2.2453495922 	 4.1250320022 	 4.1258954022
epoch_time;  34.44311571121216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1306951493024826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.203276127576828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.810803174972534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9184999465942383
12 2.2363798623 	 3.9184999208 	 3.9196213181
epoch_time;  34.23053002357483
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10310589522123337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1872757226228714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.89845871925354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.952326536178589
13 2.2368997149 	 3.9523265942 	 3.9532569679
epoch_time;  34.498719692230225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09530960023403168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15297144651412964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7406883239746094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7583727836608887
14 2.2255265966 	 3.7583727038 	 3.7591711096
epoch_time;  34.22449588775635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10328040271997452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16109119355678558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.810845136642456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.830735921859741
15 2.2223681409 	 3.830735985 	 3.8316551415
epoch_time;  34.014636754989624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11882862448692322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1753803789615631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.943721055984497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9581820964813232
16 2.2235600827 	 3.9581820101 	 3.9591087548
epoch_time;  33.762662410736084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1200418621301651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16883988678455353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8362789154052734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.835183620452881
17 2.2176273168 	 3.8351836333 	 3.8358075116
epoch_time;  34.13022422790527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13295124471187592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19794447720050812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7186272144317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7763216495513916
18 2.2078628488 	 3.7763216586 	 3.7768960489
epoch_time;  34.064653396606445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12168435752391815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18888090550899506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8868675231933594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.914757013320923
19 2.2066786759 	 3.9147569811 	 3.915349517
epoch_time;  34.016948223114014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12171904742717743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18892721831798553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.888376235961914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.916139602661133
It took 748.0066838264465 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn53: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135366.0

JOB STATISTICS
==============
Job ID: 2135366
Array Job ID: 2135328_10
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:02:02
CPU Efficiency: 27.17% of 03:48:18 core-walltime
Job Wall-clock time: 00:12:41
Memory Utilized: 3.89 GB
Memory Efficiency: 12.46% of 31.25 GB
