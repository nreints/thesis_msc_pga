wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165346-2bk9jy26
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-fish-1144
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/2bk9jy26
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–‡â–…â–„â–„â–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–‡â–…â–…â–„â–„â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–„â–…â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–ˆâ–„â–†â–ƒâ–†â–ƒâ–ƒâ–…â–„â–â–‚â–ƒâ–‚â–‚â–â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.46622
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3997
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.35695
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29895
wandb:                         Train loss 2.75329
wandb: 
wandb: ðŸš€ View run dancing-fish-1144 at: https://wandb.ai/nreints/thesis/runs/2bk9jy26
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165346-2bk9jy26/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4426819980144501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8930002450942993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5182339549064636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.995821475982666
0 5.5577619859 	 0.9958214837 	 0.9958214837
epoch_time;  33.426982164382935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40936923027038574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7984610795974731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4539856016635895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8676565885543823
1 3.1505450105 	 0.8676565634 	 0.8676565634
epoch_time;  32.772255420684814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4373105466365814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8202603459358215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4791223108768463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.881773829460144
2 3.0408182628 	 0.8817738301 	 0.8817738301
epoch_time;  32.33024311065674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33823350071907043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6395784020423889
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41316869854927063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7408909797668457
3 2.9850689082 	 0.7408909978 	 0.7408909978
epoch_time;  32.16838717460632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3882594406604767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6488509774208069
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4496244192123413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.705010175704956
4 2.9381829987 	 0.7050101615 	 0.7050101615
epoch_time;  32.131081342697144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3265060484409332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5891396403312683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4097340404987335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.680635392665863
5 2.906178262 	 0.6806354007 	 0.6806354007
epoch_time;  32.19269013404846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39671316742897034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6135854721069336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42729124426841736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6384877562522888
6 2.8847071106 	 0.6384877385 	 0.6384877385
epoch_time;  31.90678095817566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3201991021633148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.53298419713974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3886086046695709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.607440710067749
7 2.8609479466 	 0.6074406804 	 0.6074406804
epoch_time;  32.299668073654175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31716108322143555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4786131978034973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3756033778190613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5447683334350586
8 2.8401640916 	 0.5447683386 	 0.5447683386
epoch_time;  31.989962100982666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3630487322807312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48148226737976074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38765236735343933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5038822293281555
9 2.8213928677 	 0.5038822483 	 0.5038822483
epoch_time;  32.063377380371094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3396325707435608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48102182149887085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3950234055519104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5423130989074707
10 2.8069401394 	 0.5423131169 	 0.5423131169
epoch_time;  32.17421007156372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2869967818260193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39821869134902954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3470352590084076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4600776731967926
11 2.799265826 	 0.4600776879 	 0.4600776879
epoch_time;  32.29232382774353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3077378273010254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4322511851787567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36050844192504883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4909634292125702
12 2.789284987 	 0.4909634152 	 0.4909634152
epoch_time;  32.242595911026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3147961497306824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42387714982032776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3566429018974304
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46753329038619995
13 2.779753089 	 0.4675332972 	 0.4675332972
epoch_time;  31.997905015945435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3000982701778412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4241117835044861
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36237773299217224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48741117119789124
14 2.7751307814 	 0.4874111691 	 0.4874111691
epoch_time;  32.149057149887085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3101607859134674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4308936595916748
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3569372296333313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4886341094970703
15 2.7686697328 	 0.4886340992 	 0.4886340992
epoch_time;  32.1212203502655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.285467267036438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3687596917152405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33773526549339294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4241883158683777
16 2.7630500354 	 0.4241883149 	 0.4241883149
epoch_time;  32.9047486782074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2786385715007782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41037705540657043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35985878109931946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49862921237945557
17 2.7583974788 	 0.4986292246 	 0.4986292246
epoch_time;  32.92095756530762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29135388135910034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42518892884254456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34807607531547546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4849834144115448
18 2.7537845929 	 0.4849834133 	 0.4849834133
epoch_time;  32.27121591567993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29896387457847595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3996530771255493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35692572593688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46618372201919556
19 2.7532868899 	 0.4661837191 	 0.4661837191
epoch_time;  32.42703652381897
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29894575476646423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3996952772140503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35694819688796997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4662179946899414
It took 722.7309193611145 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn20: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135383.0

JOB STATISTICS
==============
Job ID: 2135383
Array Job ID: 2135328_15
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:41:42 core-walltime
Job Wall-clock time: 00:12:19
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
