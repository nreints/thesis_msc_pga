/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_111754-664ra5h9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-dumpling-1663
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/664ra5h9
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(-10,', '10)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b43a2e7f70>, <torch.utils.data.dataloader.DataLoader object at 0x14b4335f4430>, <torch.utils.data.dataloader.DataLoader object at 0x14b4335f43a0>, <torch.utils.data.dataloader.DataLoader object at 0x14b4335f46a0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20813941955566406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2556399703025818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7872737646102905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5859284400939941
0 6.9609278896 	 1.5859284646
epoch_time;  47.07991576194763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0804169625043869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12123660743236542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5796500444412231
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2332781553268433
1 1.1391900963 	 1.2332782111
epoch_time;  45.32664108276367
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07746399939060211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12692569196224213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5429922938346863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0553935766220093
2 0.9014319652 	 1.0553935532
epoch_time;  44.63312602043152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10860274732112885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16706906259059906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5498014092445374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1431859731674194
3 0.8062392992 	 1.1431859872
epoch_time;  44.46385598182678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06208202615380287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08667835593223572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5135267376899719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9301590919494629
4 0.7210883257 	 0.9301590934
epoch_time;  44.3650004863739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06286095827817917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08846714347600937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4708167016506195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9544193744659424
5 0.6867841191 	 0.9544193694
epoch_time;  44.27804946899414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03401309251785278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.056143227964639664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41068312525749207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8004127144813538
6 0.6792135881 	 0.8004127341
epoch_time;  44.51685285568237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05038774758577347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08550102263689041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49111855030059814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8394536375999451
7 0.6394820578 	 0.8394536597
epoch_time;  44.36396312713623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05666443333029747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08584730327129364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.465678334236145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.874896764755249
8 0.5975660603 	 0.8748967381
epoch_time;  44.30440354347229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07404043525457382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11187008768320084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4385271668434143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8762189745903015
9 0.5932216139 	 0.8762189514
epoch_time;  44.42715263366699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045813512057065964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06345826387405396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41394007205963135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7711699604988098
10 0.577048776 	 0.7711699333
epoch_time;  44.33650541305542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02371487021446228
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04054741561412811
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3402427136898041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6571936011314392
11 0.5683574344 	 0.6571936017
epoch_time;  48.42020106315613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042806338518857956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07023192197084427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4093773066997528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6498655080795288
12 0.5353910575 	 0.6498655106
epoch_time;  44.22609210014343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022538308054208755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0520254522562027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.440798282623291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8189800381660461
13 0.5543311305 	 0.8189800528
epoch_time;  44.29242277145386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09341393411159515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14142248034477234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5278289318084717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8092299103736877
14 0.5246843082 	 0.809229917
epoch_time;  44.06869435310364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023082546889781952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03946084901690483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35665175318717957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6540926694869995
15 0.5264225858 	 0.6540926562
epoch_time;  44.400516986846924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0566968210041523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0871010273694992
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3792635202407837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7663560509681702
16 0.5463715639 	 0.7663560389
epoch_time;  44.521334409713745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03752506524324417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05875426530838013
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47305870056152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7873477339744568
17 0.5105675835 	 0.7873477071
epoch_time;  44.07823181152344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0743134394288063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11229764670133591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4501481056213379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8918533325195312
18 0.4984647417 	 0.8918533556
epoch_time;  44.344061613082886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02056674286723137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03755580261349678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34282156825065613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6562768816947937
19 0.5078545454 	 0.6562768758
epoch_time;  44.35855722427368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026638183742761612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049712568521499634
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4061805307865143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7148046493530273
20 0.4802471568 	 0.714804658
epoch_time;  44.18793725967407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04650716856122017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06570285558700562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4101593494415283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6922488808631897
21 0.4698527449 	 0.6922488947
epoch_time;  44.51251697540283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02650519646704197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0451212003827095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3270741403102875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.643142819404602
22 0.4832608427 	 0.6431427924
epoch_time;  44.85478472709656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0260928887873888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04279029741883278
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–â–‚â–‚â–ƒâ–â–‚â–â–â–â–â–‚â–â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–„â–â–ƒâ–‚â–ƒâ–â–â–‚â–â–â–â–‚â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–â–‚â–ƒâ–„â–â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–„â–â–‚â–‚â–ƒâ–â–â–‚â–â–â–â–‚â–â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.70357
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05125
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.38134
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.033
wandb:                         Train loss 0.44663
wandb: 
wandb: ğŸš€ View run dazzling-dumpling-1663 at: https://wandb.ai/nreints/thesis/runs/664ra5h9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_111754-664ra5h9/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_114144-iicbvwys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-moon-1672
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/iicbvwys
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3636920750141144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6838749647140503
23 0.48297632 	 0.6838749543
epoch_time;  44.068846225738525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01946520432829857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03492214158177376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3685612082481384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6865004897117615
24 0.4621854633 	 0.6865004802
epoch_time;  44.36871910095215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036239732056856155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.059368398040533066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3659825921058655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7154668569564819
25 0.4478102286 	 0.715466871
epoch_time;  44.375643491744995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030307715758681297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044681206345558167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32644057273864746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6601744890213013
26 0.4519852921 	 0.6601745052
epoch_time;  45.81077170372009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03038156032562256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04788903519511223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37240466475486755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7203094959259033
27 0.4284979844 	 0.7203094851
epoch_time;  44.85423159599304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02557516284286976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.043656252324581146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37585923075675964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7366460561752319
28 0.4440361676 	 0.7366460702
epoch_time;  44.262956380844116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03299670293927193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051244106143713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3810606896877289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7025818824768066
29 0.4466337425 	 0.702581907
epoch_time;  44.313034772872925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.033002860844135284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051246218383312225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38133665919303894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7035746574401855
It took  1431.1693997383118  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b43b875480>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c156590>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c2000a0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c200250>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12514911592006683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1840161681175232
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7850524187088013
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7970753908157349
0 6.6445016796 	 1.7970754387
epoch_time;  47.38727831840515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.061231810599565506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11231788247823715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6076183319091797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3903684616088867
1 1.0397633185 	 1.3903684126
epoch_time;  44.27522587776184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.053089071065187454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07424315065145493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43794652819633484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9608995318412781
2 0.818033731 	 0.9608995144
epoch_time;  43.97245979309082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10912118852138519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15958158671855927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5312617421150208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0584182739257812
3 0.6886711727 	 1.058418297
epoch_time;  44.08639121055603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04569096863269806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06911861151456833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45192578434944153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.914165198802948
4 0.6695483639 	 0.9141652087
epoch_time;  44.45054292678833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05544055625796318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08898227661848068
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4011150598526001
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8961591720581055
5 0.6151045887 	 0.8961591922
epoch_time;  44.031256914138794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.055055808275938034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07868582755327225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.528319776058197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9759098291397095
6 0.603241683 	 0.9759098295
epoch_time;  44.459959506988525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.046414606273174286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.060510244220495224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4052182137966156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7653224468231201
7 0.5633437729 	 0.7653224519
epoch_time;  43.97077441215515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.033989518880844116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06258653849363327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.514572024345398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0844449996948242
8 0.5629800108 	 1.0844449968
epoch_time;  44.03069496154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019734084606170654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042566586285829544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42476820945739746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9108459949493408
9 0.5470277958 	 0.9108459841
epoch_time;  44.12870454788208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04074174910783768
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0638304054737091
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3559914529323578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.852457582950592
10 0.5333286144 	 0.8524575594
epoch_time;  44.268856048583984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03563018515706062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05140869319438934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3963306248188019
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7932351231575012
11 0.5023664176 	 0.7932351104
epoch_time;  43.882972240448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04188217967748642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06169538572430611
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4289337694644928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9436642527580261
12 0.5518561341 	 0.9436642742
epoch_time;  44.57050967216492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0741814523935318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10872123390436172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5247565507888794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.999988853931427
13 0.5031012882 	 0.999988844
epoch_time;  45.0997953414917
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022903354838490486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0383136160671711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3806140720844269
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8091503381729126
14 0.4705295837 	 0.8091503501
epoch_time;  44.47432351112366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05396408960223198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0738934651017189
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4116589426994324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8354165554046631
15 0.4732802317 	 0.8354165806
epoch_time;  43.92493748664856
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–‡â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–…â–â–ƒâ–ƒâ–„â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–ƒâ–„â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–‚â–‚â–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–â–‚â–â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–ƒâ–‡â–ƒâ–„â–„â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–…â–‚â–„â–„â–…â–‚â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.77649
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05355
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.42376
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.03469
wandb:                         Train loss 0.42037
wandb: 
wandb: ğŸš€ View run floating-moon-1672 at: https://wandb.ai/nreints/thesis/runs/iicbvwys
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_114144-iicbvwys/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_120509-mrf4anf2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-firecracker-1680
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/mrf4anf2
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05473572015762329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07319105416536331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37632298469543457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8061705827713013
16 0.4751744931 	 0.806170599
epoch_time;  44.09251546859741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06974438577890396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08952655643224716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3924573063850403
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8478700518608093
17 0.4414457542 	 0.8478700574
epoch_time;  44.12332773208618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032508693635463715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.048656072467565536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4093075692653656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8181226253509521
18 0.4534225984 	 0.8181226102
epoch_time;  44.017794132232666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018336230888962746
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.032148685306310654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3318403661251068
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7212048172950745
19 0.4257009941 	 0.7212048211
epoch_time;  45.63253426551819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029882745817303658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04417946934700012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38521748781204224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8703147768974304
20 0.4250020699 	 0.8703147681
epoch_time;  48.977139711380005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013528953306376934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027641376480460167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32529643177986145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6603689193725586
21 0.4054340417 	 0.660368905
epoch_time;  44.40836262702942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03660529479384422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05254020541906357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37599244713783264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7483605742454529
22 0.4296884241 	 0.7483605791
epoch_time;  44.53580594062805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0310666561126709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05176926776766777
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44825735688209534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0326114892959595
23 0.6988023618 	 1.0326114897
epoch_time;  43.84272265434265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0244035292416811
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04135506600141525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41582298278808594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8724488615989685
24 0.5074408832 	 0.872448878
epoch_time;  44.23118591308594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017083384096622467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02981039509177208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2981186509132385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6594550609588623
25 0.4411949602 	 0.6594550372
epoch_time;  43.69581699371338
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03782966732978821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.053410597145557404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39943116903305054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.823087751865387
26 0.4303992688 	 0.8230877556
epoch_time;  44.148587703704834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030640650540590286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.046696364879608154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.358929306268692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7777118682861328
27 0.4005410182 	 0.7777118971
epoch_time;  44.02048444747925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03654041886329651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05535159632563591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3267061114311218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7049439549446106
28 0.4257386039 	 0.7049439767
epoch_time;  44.24475646018982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03469311445951462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.053543008863925934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4239106774330139
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7777570486068726
29 0.4203691735 	 0.7777570742
epoch_time;  43.882994174957275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03469471633434296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0535452701151371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4237593114376068
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7764852643013
It took  1404.9573454856873  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b40c1563e0>, <torch.utils.data.dataloader.DataLoader object at 0x14b433db92a0>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dbac80>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dbae30>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12295625358819962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15645329654216766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.79042649269104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4940778017044067
0 7.5105765933 	 1.4940778381
epoch_time;  44.34761619567871
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12449682503938675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16077537834644318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5000712871551514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.018109679222107
1 1.1527226208 	 1.0181096472
epoch_time;  44.37461614608765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06272072345018387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09657962620258331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4515823423862457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9759388566017151
2 0.9129915894 	 0.9759388719
epoch_time;  44.57955837249756
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09606048464775085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12925665080547333
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3863178789615631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7973676919937134
3 0.7665550353 	 0.7973677068
epoch_time;  44.869492530822754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02863037772476673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04133627191185951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3477424681186676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7810420989990234
4 0.7359804418 	 0.7810420932
epoch_time;  44.6364905834198
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045725006610155106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0654689148068428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38946008682250977
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8543815612792969
5 0.6648405712 	 0.8543815498
epoch_time;  44.195849895477295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05102870985865593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07963205128908157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49817314743995667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9733296632766724
6 0.6711377267 	 0.9733296651
epoch_time;  44.37798810005188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0389147624373436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05990344658493996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5377362966537476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0510170459747314
7 0.6234637715 	 1.0510170928
epoch_time;  44.686249017715454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030181627720594406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04310697317123413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37318524718284607
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–…â–‚â–‚â–ƒâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ˆâ–…â–†â–‚â–ƒâ–„â–ƒâ–‚â–‚â–„â–ƒâ–‚â–â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–…â–‚â–ƒâ–‚â–ƒâ–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ˆâ–„â–†â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–ƒâ–‚â–â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.65805
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05119
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.30082
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.03872
wandb:                         Train loss 0.43303
wandb: 
wandb: ğŸš€ View run thriving-firecracker-1680 at: https://wandb.ai/nreints/thesis/runs/mrf4anf2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_120509-mrf4anf2/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_122839-36i2fiuy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-horse-1687
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/36i2fiuy
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7443830966949463
8 0.5802227889 	 0.7443831061
epoch_time;  47.38825249671936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02573096938431263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03847635164856911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38711297512054443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.702937126159668
9 0.559701485 	 0.7029371463
epoch_time;  46.90981674194336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0693654790520668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08931618183851242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37946242094039917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.837449848651886
10 0.5571906807 	 0.8374498258
epoch_time;  44.4351863861084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04254094138741493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05707958713173866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4300282597541809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.873701274394989
11 0.5190356686 	 0.8737012973
epoch_time;  44.90029263496399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025544388219714165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03572899475693703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28521081805229187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5813220739364624
12 0.5228102379 	 0.5813220621
epoch_time;  44.54430603981018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01669955439865589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02586236223578453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27571579813957214
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5945129990577698
13 0.5235223301 	 0.5945129856
epoch_time;  44.115333795547485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03546707704663277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05011948570609093
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3291361331939697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6959448456764221
14 0.5258950776 	 0.6959448408
epoch_time;  44.097970724105835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05158136785030365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06367706507444382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3759008049964905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.744462788105011
15 0.5208260639 	 0.7444628113
epoch_time;  44.315810680389404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03277131915092468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04594435542821884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3305998146533966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6888307929039001
16 0.4661783454 	 0.6888307877
epoch_time;  44.472689628601074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.048176027834415436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06321568787097931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4243307113647461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.691669225692749
17 0.4931538515 	 0.691669199
epoch_time;  44.9342405796051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01695193350315094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02574818953871727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.341093510389328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6311648488044739
18 0.5280407798 	 0.6311648735
epoch_time;  44.68468642234802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03088049218058586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042524661868810654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37776514887809753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7636345624923706
19 0.5440123541 	 0.7636345348
epoch_time;  44.482321977615356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03190341591835022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04283839464187622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29295045137405396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5797925591468811
20 0.4822001894 	 0.5797925413
epoch_time;  44.13864254951477
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04430706799030304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06542138010263443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35625553131103516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7000919580459595
21 0.4624339009 	 0.7000919584
epoch_time;  44.44761824607849
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04199652746319771
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05866879224777222
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3067967891693115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6229707598686218
22 0.4524291715 	 0.6229707655
epoch_time;  44.23309278488159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031534671783447266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.046455759555101395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32602161169052124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6759259104728699
23 0.4656397482 	 0.6759259089
epoch_time;  44.55897903442383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019160177558660507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026876535266637802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2701321542263031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6483731269836426
24 0.4607352662 	 0.6483730996
epoch_time;  44.027546882629395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03035893477499485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04288040101528168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29305434226989746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6183775067329407
25 0.4577766551 	 0.6183775012
epoch_time;  44.205570936203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02603563666343689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03742307424545288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3579954504966736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7713868021965027
26 0.4156265894 	 0.7713868294
epoch_time;  44.27366375923157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020321767777204514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029510578140616417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28561800718307495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5478596687316895
27 0.4498811732 	 0.5478596759
epoch_time;  44.41061329841614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027880268171429634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.039031509310007095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31382516026496887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5845901370048523
28 0.4351171801 	 0.5845901628
epoch_time;  46.69380712509155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03869752958416939
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051209867000579834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3009684383869171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6597675681114197
29 0.4330306649 	 0.6597675427
epoch_time;  47.185638666152954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03871883079409599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05119001492857933
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30082234740257263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6580525040626526
It took  1410.0760083198547  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b43a31e920>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dd2d10>, <torch.utils.data.dataloader.DataLoader object at 0x14b431922d10>, <torch.utils.data.dataloader.DataLoader object at 0x14b431922e90>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12091220170259476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20182129740715027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6840247511863708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6101640462875366
0 7.7961549607 	 1.6101640315
epoch_time;  44.64320659637451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09734269231557846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1612769067287445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5321692824363708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.168628454208374
1 1.1127641376 	 1.1686284276
epoch_time;  44.30401682853699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06670618057250977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1123015433549881
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4539121389389038
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.000927448272705
2 0.8703468201 	 1.0009274209
epoch_time;  44.5824179649353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030625086277723312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06816957890987396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44042667746543884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9119871258735657
3 0.8244942168 	 0.9119871203
epoch_time;  44.15779137611389
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022222714498639107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0729759931564331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3647584617137909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8339523077011108
4 0.7554366107 	 0.8339522901
epoch_time;  44.5822913646698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04258017614483833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08109582215547562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3517400622367859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7551594972610474
5 0.689559024 	 0.7551594991
epoch_time;  44.446861267089844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.050347112119197845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09187784045934677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35394811630249023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7981739044189453
6 0.6371308875 	 0.7981738871
epoch_time;  44.22680401802063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03204206004738808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06716231256723404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3367828130722046
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7494450211524963
7 0.6188807751 	 0.7494450134
epoch_time;  44.543089628219604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07768172025680542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11730681359767914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4283270239830017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8873648047447205
8 0.6102316176 	 0.8873647822
epoch_time;  44.499645471572876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01705171912908554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05041744187474251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3363550305366516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.78546142578125
9 0.6184986701 	 0.7854614258
epoch_time;  44.307920694351196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02807612344622612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07182463258504868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33366289734840393
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7877800464630127
10 0.5446411872 	 0.7877800241
epoch_time;  44.209179162979126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015173554420471191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04341583698987961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33971109986305237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.730617344379425
11 0.5451984969 	 0.7306173734
epoch_time;  44.147642612457275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.033326201140880585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0753551721572876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4448375403881073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.845805823802948
12 0.5796563309 	 0.8458058337
epoch_time;  45.11235237121582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02748125232756138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05976666137576103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3852684497833252
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8203126788139343
13 0.5149988468 	 0.8203126844
epoch_time;  44.44797658920288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032477688044309616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06963393837213516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3838038444519043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8039500117301941
14 0.5147072549 	 0.8039500072
epoch_time;  44.30081486701965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05157998949289322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10218701511621475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3380042016506195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7243155837059021
15 0.5053607441 	 0.7243155857
epoch_time;  44.27617907524109
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030849002301692963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06525196135044098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32095929980278015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7383241057395935
16 0.5070712532 	 0.7383241221
epoch_time;  44.325573205947876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03393352031707764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07335443794727325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4428291320800781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8935319185256958
17 0.4837447808 	 0.8935319146
epoch_time;  47.40594840049744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026878472417593002
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06390389800071716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3817160427570343
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8096539378166199
18 0.4958922554 	 0.8096539362
epoch_time;  45.79554581642151
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02641148306429386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06677407771348953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3602141737937927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7325037717819214
19 0.4843226511 	 0.732503793
epoch_time;  44.394771575927734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024359190836548805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0601620227098465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36353600025177
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8018394708633423
20 0.5041850896 	 0.8018395
epoch_time;  44.000900983810425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020535632967948914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05339658632874489
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3199326694011688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.670164167881012
21 0.5151124618 	 0.6701641717
epoch_time;  47.04442024230957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0354464016854763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07835941016674042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42672139406204224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8209716081619263
22 0.485476405 	 0.8209716244
epoch_time;  44.2628812789917
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05069052055478096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0988331139087677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37805435061454773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7702652812004089
23 0.4673390298 	 0.7702652853
epoch_time;  44.71450853347778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03363838791847229
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07074090093374252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37511858344078064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.708663284778595
24 0.4709675491 	 0.708663295
epoch_time;  44.822065353393555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04295123368501663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08701109886169434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34697425365448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7571703195571899
25 0.443081367 	 0.757170294
epoch_time;  44.13497352600098
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027888135984539986
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–‚â–‚â–ƒâ–ƒâ–‚â–„â–â–‚â–â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚â–„â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–„â–‚â–â–ƒâ–ƒâ–‚â–…â–â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.6272
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.06319
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33697
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02668
wandb:                         Train loss 0.42681
wandb: 
wandb: ğŸš€ View run vivid-horse-1687 at: https://wandb.ai/nreints/thesis/runs/36i2fiuy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_122839-36i2fiuy/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_125208-5yi9xne5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-laughter-1692
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/5yi9xne5
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06667296588420868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33165299892425537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6864007711410522
26 0.4494024099 	 0.6864007679
epoch_time;  44.47241711616516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02008236013352871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050264861434698105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25902223587036133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6134074926376343
27 0.4634813862 	 0.6134075153
epoch_time;  44.50380563735962
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030840519815683365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07282120734453201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3447498381137848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6691534519195557
28 0.4572801496 	 0.6691534498
epoch_time;  44.23541235923767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026668675243854523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06319087743759155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33698058128356934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6253699064254761
29 0.4268121019 	 0.6253698989
epoch_time;  44.219356298446655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026677701622247696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06318990886211395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3369702696800232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6272008419036865
It took  1409.1149685382843  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b433dd3880>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dd2d70>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dd3a00>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dd3dc0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12154458463191986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18233971297740936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6725821495056152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5132423639297485
0 5.3826339687 	 1.5132424162
epoch_time;  44.29463863372803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06687031686306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10269240289926529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4300660192966461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0666539669036865
1 1.0329651757 	 1.0666539864
epoch_time;  44.22574186325073
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03548537567257881
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07078766822814941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3587716817855835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9876551032066345
2 0.9190128624 	 0.9876551326
epoch_time;  44.2159686088562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021934671327471733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05036650970578194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41481348872184753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9941167235374451
3 0.809442975 	 0.9941167457
epoch_time;  44.222180128097534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02674562856554985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0588083416223526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3835914433002472
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8824616074562073
4 0.7662645523 	 0.882461594
epoch_time;  43.9598331451416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09217990189790726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12211107462644577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40781670808792114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0193620920181274
5 0.7425566218 	 1.0193620664
epoch_time;  44.64385509490967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05969981104135513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09558477997779846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4585437476634979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9269786477088928
6 0.6794622567 	 0.926978627
epoch_time;  50.48489570617676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04516115039587021
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.070701003074646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35949042439460754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8315780758857727
7 0.6385645437 	 0.8315780962
epoch_time;  43.90113663673401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030027836561203003
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054382894188165665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36476820707321167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9306671023368835
8 0.6108170653 	 0.930667105
epoch_time;  44.24319005012512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02554001472890377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054394591599702835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30249905586242676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7347490787506104
9 0.5613252234 	 0.7347490939
epoch_time;  44.410080909729004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03283277899026871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07027365267276764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33846113085746765
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8226612210273743
10 0.626443128 	 0.8226612471
epoch_time;  44.11362147331238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.043324537575244904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06495772302150726
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32765018939971924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7300304770469666
11 0.5685771795 	 0.7300304862
epoch_time;  44.23929762840271
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02573135681450367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04779178649187088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29230016469955444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.772585391998291
12 0.5283934992 	 0.772585405
epoch_time;  44.26953482627869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018642498180270195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03893662989139557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3171256482601166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7359604239463806
13 0.5164785681 	 0.7359604389
epoch_time;  44.016592025756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036770012229681015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06341083347797394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33023035526275635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7918153405189514
14 0.5699027651 	 0.7918153515
epoch_time;  44.25948429107666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026636460795998573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04964686185121536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2905764579772949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7529219388961792
15 0.5436109359 	 0.7529219429
epoch_time;  44.148324728012085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027064090594649315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04582536593079567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30149632692337036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.742507815361023
16 0.5195811038 	 0.7425077963
epoch_time;  44.16509675979614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.037713032215833664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06441247463226318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3505777418613434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7615548968315125
17 0.516011402 	 0.7615548678
epoch_time;  44.08898568153381
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03655126318335533
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0644034668803215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3079843521118164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6762587428092957
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–„â–ƒâ–„â–„â–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–‚â–‚â–…â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–„â–ƒâ–„â–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–‚â–â–‚â–†â–„â–ƒâ–‚â–â–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–ƒâ–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.65846
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.04338
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.29372
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02237
wandb:                         Train loss 0.48256
wandb: 
wandb: ğŸš€ View run luminous-laughter-1692 at: https://wandb.ai/nreints/thesis/runs/5yi9xne5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_125208-5yi9xne5/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_131535-e8aixx0c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-tiger-1698
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/e8aixx0c
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
18 0.5250612385 	 0.6762587441
epoch_time;  44.04856562614441
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03421587124466896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.055798523128032684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30492159724235535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7809149622917175
19 0.4855147079 	 0.780914952
epoch_time;  44.29753136634827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03061065450310707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049392424523830414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3659873604774475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8325390815734863
20 0.4569845842 	 0.8325390773
epoch_time;  43.920987606048584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02109861746430397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0381767675280571
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2866809666156769
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7333188652992249
21 0.4571279082 	 0.7333188705
epoch_time;  44.40065622329712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02230188623070717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.040271077305078506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2886611521244049
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6838808655738831
22 0.4765569093 	 0.6838808549
epoch_time;  44.286022424697876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03237786144018173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0565924271941185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30295440554618835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7227925062179565
23 0.4480950286 	 0.7227925188
epoch_time;  44.302618980407715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030354700982570648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05411795899271965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33449527621269226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7573873996734619
24 0.4804369618 	 0.7573874206
epoch_time;  44.22656226158142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030548542737960815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05788100138306618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2891775965690613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6900338530540466
25 0.4640319501 	 0.690033881
epoch_time;  44.19528269767761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019932428374886513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03689907491207123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2256959229707718
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5766252875328064
26 0.4832192594 	 0.5766252593
epoch_time;  49.27496099472046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02421441301703453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05068487301468849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3065814673900604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7017828822135925
27 0.426968132 	 0.7017828719
epoch_time;  44.902097940444946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04483611509203911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06564220041036606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3718397319316864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7669348120689392
28 0.4423008932 	 0.7669348126
epoch_time;  44.27309226989746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022369088605046272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0433884859085083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2938734292984009
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6583475470542908
29 0.482559516 	 0.6583475534
epoch_time;  44.133970499038696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0223709624260664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.043379805982112885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29372432827949524
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6584580540657043
It took  1406.4855654239655  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b40c2021a0>, <torch.utils.data.dataloader.DataLoader object at 0x14b433dbb970>, <torch.utils.data.dataloader.DataLoader object at 0x14b431923850>, <torch.utils.data.dataloader.DataLoader object at 0x14b431923280>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13371609151363373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18601520359516144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7270485758781433
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.560245394706726
0 6.218508262 	 1.5602453871
epoch_time;  44.15622591972351
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08570471405982971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12208923697471619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5078095197677612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.080750584602356
1 1.0718413822 	 1.0807506181
epoch_time;  43.877142667770386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042654767632484436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07716582715511322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4514951705932617
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9426517486572266
2 0.868674614 	 0.9426517544
epoch_time;  43.993709564208984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05013652145862579
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07333242148160934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4801797866821289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9745367169380188
3 0.7967633717 	 0.9745367229
epoch_time;  44.19077134132385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0605640634894371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1024288758635521
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5272759199142456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0348076820373535
4 0.7188379257 	 1.0348077411
epoch_time;  43.8413245677948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06640136241912842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10722266137599945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5132913589477539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1266635656356812
5 0.7359064234 	 1.1266636229
epoch_time;  44.23782444000244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027004068717360497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05015186965465546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4451565444469452
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.844411313533783
6 0.6587324814 	 0.8444113371
epoch_time;  43.64252948760986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026440240442752838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04516638070344925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3987692892551422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8627983331680298
7 0.6192009209 	 0.8627983162
epoch_time;  44.00865864753723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027756301686167717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.053757429122924805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38073888421058655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8234249353408813
8 0.6495858018 	 0.8234249242
epoch_time;  43.67862010002136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027618857100605965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04891415312886238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4919310510158539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8155601620674133
9 0.5901098836 	 0.8155601478
epoch_time;  43.61450242996216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021422648802399635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04293522611260414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3751537799835205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.748026430606842
10 0.5847415159 	 0.748026407
epoch_time;  43.66035771369934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04312051832675934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06921462714672089
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–‚â–„â–ƒâ–ƒâ–â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–ƒâ–„â–…â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–‚â–„â–‚â–ƒâ–„â–‚â–‚â–â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–…â–…â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ƒâ–ƒâ–„â–„â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.72393
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05091
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.37404
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02916
wandb:                         Train loss 0.51959
wandb: 
wandb: ğŸš€ View run alight-tiger-1698 at: https://wandb.ai/nreints/thesis/runs/e8aixx0c
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_131535-e8aixx0c/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_133845-rhpbak8q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-rooster-1703
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/rhpbak8q
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38607171177864075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8122071623802185
11 0.6065206708 	 0.8122071788
epoch_time;  43.92563796043396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03684607893228531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06467511504888535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46353834867477417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9260168075561523
12 0.6132123602 	 0.9260168162
epoch_time;  44.12080669403076
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025415582582354546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04913555830717087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4950887858867645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8816329836845398
13 0.6410038246 	 0.8816330094
epoch_time;  43.67733693122864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017583711072802544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.034158408641815186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39257481694221497
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8545871376991272
14 0.5480382719 	 0.8545871516
epoch_time;  44.00136733055115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029002351686358452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04905138909816742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3892427980899811
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8159322738647461
15 0.5642391092 	 0.8159322595
epoch_time;  49.95537090301514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02189619652926922
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04115556180477142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3246161937713623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6499601006507874
16 0.5223973611 	 0.6499601059
epoch_time;  44.11375570297241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026846133172512054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049562968313694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40070369839668274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7943681478500366
17 0.5044728004 	 0.7943681331
epoch_time;  43.66383671760559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02363206259906292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03903520107269287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33754250407218933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.726521909236908
18 0.5533206082 	 0.7265219328
epoch_time;  43.615535259246826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013494675979018211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026686126366257668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28381338715553284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.642671525478363
19 0.484480668 	 0.6426715217
epoch_time;  44.04249143600464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032443586736917496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049145523458719254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3295907974243164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6561266183853149
20 0.5448698958 	 0.6561266389
epoch_time;  44.05470156669617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06283042579889297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10222255438566208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5286716222763062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0470021963119507
21 0.5372070497 	 1.0470022334
epoch_time;  43.89076209068298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02830582670867443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04630481079220772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3801270127296448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6938478350639343
22 0.5556623992 	 0.6938478406
epoch_time;  43.68203043937683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04654460772871971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06815224140882492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.365204781293869
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7565338611602783
23 0.457382953 	 0.7565338504
epoch_time;  43.887802600860596
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05346506088972092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08557745069265366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4386224150657654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9366022944450378
24 0.5203157897 	 0.9366022669
epoch_time;  43.83861041069031
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036859601736068726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057806238532066345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41259732842445374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8253949880599976
25 0.523268117 	 0.8253950136
epoch_time;  44.1776237487793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026542257517576218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050514597445726395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41168341040611267
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8978453278541565
26 0.484606296 	 0.8978453115
epoch_time;  44.4296817779541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026153553277254105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03319364786148071
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3153682053089142
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5503072738647461
27 0.4857584327 	 0.5503072595
epoch_time;  43.56809616088867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02504972741007805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04307439923286438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3638792037963867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7654314637184143
28 0.4403205542 	 0.7654314761
epoch_time;  43.795666456222534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029171453788876534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05088776722550392
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3747996687889099
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7242458462715149
29 0.5195893281 	 0.7242458378
epoch_time;  43.993935108184814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029163865372538567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050908397883176804
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3740430176258087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7239255309104919
It took  1390.1789305210114  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b43a31efb0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1ba3e0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1baf80>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1ba350>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11964602023363113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18601052463054657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.874199628829956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7769899368286133
0 7.1788085198 	 1.7769898936
epoch_time;  43.9984290599823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07790756225585938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11266428232192993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7533818483352661
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3142259120941162
1 1.1850036113 	 1.3142258566
epoch_time;  43.59129476547241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11852703243494034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1575726568698883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.644547700881958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3257828950881958
2 1.0123170886 	 1.3257828911
epoch_time;  44.124894857406616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06603549420833588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12057187408208847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5702835917472839
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1107381582260132
3 0.8911672472 	 1.1107381492
epoch_time;  47.177425384521484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.059860873967409134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10822427272796631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5901258587837219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.140654444694519
4 0.8570015133 	 1.1406544112
epoch_time;  47.38189458847046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027502527460455894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07314494997262955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5087621212005615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9343498945236206
5 0.7429883958 	 0.9343498668
epoch_time;  44.256956815719604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05641866847872734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11922857165336609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5347722768783569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0869911909103394
6 0.7240310426 	 1.0869911401
epoch_time;  44.3480064868927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0296228788793087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0629010796546936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4419722557067871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8714832067489624
7 0.6710556689 	 0.8714831949
epoch_time;  43.82524657249451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.044499073177576065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07946018129587173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5063232779502869
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8690462112426758
8 0.6082934626 	 0.8690462141
epoch_time;  43.72977828979492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045941151678562164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08835949748754501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5110615491867065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9107446670532227
9 0.5850271475 	 0.9107446584
epoch_time;  43.86614418029785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07721152901649475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11636027693748474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5841532349586487
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9359995722770691
10 0.5730393068 	 0.9359995678
epoch_time;  43.79552340507507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03369805961847305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06641409546136856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44099223613739014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7888320088386536
11 0.5558667122 	 0.7888320047
epoch_time;  44.16988682746887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06997239589691162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11977219581604004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5354363322257996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0010459423065186
12 0.5552796473 	 1.0010459877
epoch_time;  43.76483654975891
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02038690820336342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.048312775790691376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39687082171440125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6542456150054932
13 0.5478585855 	 0.6542456128
epoch_time;  44.00318241119385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03797269985079765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08109042048454285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5959645509719849
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9419447779655457
14 0.5614100073 	 0.9419447792
epoch_time;  44.09306454658508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04846864938735962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08874931931495667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47223228216171265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8550075888633728
15 0.5561185688 	 0.855007575
epoch_time;  43.96973538398743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03303883224725723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06374598294496536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37645968794822693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6927823424339294
16 0.5732612234 	 0.6927823531
epoch_time;  44.10699915885925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04504747316241264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07602277398109436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5524578094482422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8258005976676941
17 0.4977080167 	 0.8258005932
epoch_time;  44.269471406936646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025903774425387383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04875694215297699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42331042885780334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7398396730422974
18 0.5291885734 	 0.7398396748
epoch_time;  43.9502055644989
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02250693365931511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04747576639056206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43465396761894226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7179582715034485
19 0.4889739366 	 0.7179582947
epoch_time;  44.01590013504028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023176580667495728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05579647049307823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5284175276756287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8710338473320007
20 0.4642649094 	 0.8710338212
epoch_time;  44.171889781951904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021614817902445793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04597238078713417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46294018626213074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7030134201049805
21 0.4572454968 	 0.7030134403
epoch_time;  44.17769956588745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05162481591105461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07911248505115509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37980931997299194
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6856909394264221
22 0.4403905067 	 0.6856909346
epoch_time;  44.009517431259155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036403726786375046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06583339720964432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4412330090999603
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7129659652709961
23 0.4491024552 	 0.7129659509
epoch_time;  44.03411602973938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02742566168308258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.055712081491947174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42122238874435425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6914147734642029
24 0.4460743537 	 0.6914147783
epoch_time;  49.604015588760376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024682551622390747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05152466520667076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4476045072078705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6852960586547852
25 0.4452691609 	 0.68529605
epoch_time;  44.309961557388306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01948428712785244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.046527959406375885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45565542578697205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5947902798652649
26 0.4505203559 	 0.5947902714
epoch_time;  44.154924631118774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022770388051867485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0529421791434288
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.501624345779419
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7183147072792053
27 0.412155856 	 0.7183147327
epoch_time;  44.21613693237305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036113616079092026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06377734243869781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5190696120262146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7452232837677002
28 0.4226078377 	 0.7452233075
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‡â–…â–„â–‚â–…â–‚â–ƒâ–ƒâ–…â–‚â–…â–â–ƒâ–ƒâ–‚â–ƒâ–â–â–â–â–ƒâ–‚â–â–â–â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–â–„â–‚â–â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ˆâ–„â–„â–‚â–„â–‚â–ƒâ–ƒâ–…â–‚â–…â–â–‚â–ƒâ–‚â–ƒâ–â–â–â–â–ƒâ–‚â–‚â–â–â–â–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.73699
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05589
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.42579
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02818
wandb:                         Train loss 0.42343
wandb: 
wandb: ğŸš€ View run chromatic-rooster-1703 at: https://wandb.ai/nreints/thesis/runs/rhpbak8q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_133845-rhpbak8q/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_140205-mgvxxov9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-tiger-1707
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/mgvxxov9
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  44.109920501708984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02818959578871727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05588988587260246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4300215244293213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7416861653327942
29 0.4234338222 	 0.7416861727
epoch_time;  44.27484607696533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028184467926621437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.055891092866659164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4257906675338745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7369857430458069
It took  1400.1071956157684  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b40c1ba0e0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c201ea0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c202e00>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c203400>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1502864807844162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19626426696777344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7711073756217957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6649713516235352
0 6.0334171408 	 1.6649713891
epoch_time;  44.23858976364136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06774601340293884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09994006901979446
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4778856933116913
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0490734577178955
1 1.0345499644 	 1.0490734642
epoch_time;  44.15966868400574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0715257078409195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1050012931227684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48961904644966125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.002416968345642
2 0.8431605325 	 1.0024169737
epoch_time;  43.87111043930054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07604864984750748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11398930847644806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4683595895767212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.923096239566803
3 0.7723315208 	 0.9230962563
epoch_time;  43.60954022407532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028097841888666153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04867875203490257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3897932469844818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7524358034133911
4 0.7218292028 	 0.7524357822
epoch_time;  44.00513195991516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04838431254029274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0752023234963417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4034422039985657
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8378678560256958
5 0.693438522 	 0.8378678521
epoch_time;  44.11532926559448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04595695063471794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07785098999738693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44045230746269226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8205474019050598
6 0.6509549114 	 0.8205474208
epoch_time;  44.090209007263184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032711390405893326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05427892506122589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43672338128089905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8090248703956604
7 0.6354833541 	 0.8090248684
epoch_time;  43.779422998428345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014247333630919456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.028063448145985603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31527385115623474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5738046169281006
8 0.634262882 	 0.573804596
epoch_time;  43.9051616191864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03150143474340439
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04961174726486206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3537505567073822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6579946279525757
9 0.5726474066 	 0.6579946189
epoch_time;  43.83328580856323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02766893059015274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04557653144001961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3602934777736664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6948890089988708
10 0.5855274879 	 0.694889034
epoch_time;  43.58957076072693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.036385051906108856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05742572247982025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38053765892982483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7100457549095154
11 0.5640606241 	 0.7100457598
epoch_time;  43.68415546417236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06547191739082336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09521126747131348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3465551435947418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7004801630973816
12 0.5575757427 	 0.7004801586
epoch_time;  44.037729024887085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04541078954935074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06942965090274811
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39560770988464355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7185543775558472
13 0.5543828325 	 0.7185543556
epoch_time;  50.33452486991882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03123556263744831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05100071057677269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34807080030441284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6459094285964966
14 0.560172486 	 0.6459094275
epoch_time;  43.682133197784424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03855006396770477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06435245275497437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38952794671058655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.677151083946228
15 0.5592287744 	 0.6771510836
epoch_time;  43.83534097671509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023227708414196968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044254690408706665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44998419284820557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.66404789686203
16 0.5369868426 	 0.6640478866
epoch_time;  44.170621156692505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0184580460190773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.028930500149726868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2913053333759308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5606775879859924
17 0.4947823489 	 0.5606776119
epoch_time;  44.23917269706726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017362792044878006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03175969049334526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33179017901420593
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5457541942596436
18 0.505447792 	 0.5457541935
epoch_time;  43.89186120033264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027075977995991707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04399402067065239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4162440001964569
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6628628373146057
19 0.5019570061 	 0.662862818
epoch_time;  43.965737104415894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02701147459447384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.038943663239479065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34716570377349854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5209487080574036
20 0.5001892968 	 0.5209487039
epoch_time;  44.12890338897705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0184406079351902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029327813535928726
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–…â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–„â–ƒâ–‚â–ƒâ–‚â–â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–„â–„â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–„â–ƒâ–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.58054
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08133
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.34932
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0582
wandb:                         Train loss 0.45754
wandb: 
wandb: ğŸš€ View run golden-tiger-1707 at: https://wandb.ai/nreints/thesis/runs/mgvxxov9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_140205-mgvxxov9/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_142517-vk4enz0d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-laughter-1710
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/vk4enz0d
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2844509184360504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4908154606819153
21 0.4832377871 	 0.4908154537
epoch_time;  44.053770542144775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022350069135427475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03758040815591812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38041815161705017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5807363390922546
22 0.471178423 	 0.5807363274
epoch_time;  43.743191957473755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03041505441069603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04414251446723938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3192824125289917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5444713234901428
23 0.4952078855 	 0.5444713489
epoch_time;  43.86270713806152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030066784471273422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05009966343641281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3368483781814575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6322630643844604
24 0.4842649889 	 0.6322630453
epoch_time;  43.981773376464844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02316107787191868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03725802153348923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39973509311676025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6019609570503235
25 0.4923901035 	 0.6019609342
epoch_time;  44.514753103256226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019624128937721252
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029962841421365738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3275901675224304
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5147793292999268
26 0.4389150361 	 0.5147793588
epoch_time;  43.72663855552673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030733387917280197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051953334361314774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40235698223114014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.645345151424408
27 0.4254246956 	 0.6453451289
epoch_time;  43.54101586341858
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03770625591278076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.055144596844911575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3781718313694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6433424949645996
28 0.5125541138 	 0.6433424935
epoch_time;  43.90929126739502
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.058203913271427155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08132704347372055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35024601221084595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.581161618232727
29 0.4575401843 	 0.5811616373
epoch_time;  44.08099985122681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.058201711624860764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08133496344089508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34932005405426025
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5805442929267883
It took  1391.5876734256744  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b40c2025c0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1ba0e0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1b83d0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1b81c0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15915636718273163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19318366050720215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.719813346862793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.617309331893921
0 7.2197199092 	 1.6173093859
epoch_time;  44.00978469848633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08893607556819916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12264905124902725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5756792426109314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2521650791168213
1 1.089777309 	 1.2521650885
epoch_time;  47.82199048995972
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21199363470077515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2698414921760559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5669082403182983
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1737680435180664
2 0.8948227033 	 1.1737680118
epoch_time;  46.995853662490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04469624534249306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06293738633394241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.387815922498703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8977701663970947
3 0.7884803327 	 0.89777017
epoch_time;  43.82377004623413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.040898919105529785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0724705308675766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4803054630756378
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9984444975852966
4 0.7418345495 	 0.9984445255
epoch_time;  44.36703419685364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.038933224976062775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05894360691308975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34869393706321716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8898512721061707
5 0.7602947504 	 0.8898512734
epoch_time;  43.79818940162659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042498596012592316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.059334538877010345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3529409170150757
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8735324740409851
6 0.6333977167 	 0.8735324825
epoch_time;  43.96481895446777
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022771766409277916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.038324832916259766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4236915409564972
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9364307522773743
7 0.6418632606 	 0.9364307784
epoch_time;  44.068608045578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06460154801607132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09082038700580597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41116365790367126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9016749262809753
8 0.6135037673 	 0.9016749448
epoch_time;  44.09773135185242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0793110579252243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09312743693590164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4005822539329529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8298531770706177
9 0.6086985309 	 0.8298531616
epoch_time;  43.97769904136658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02823966182768345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0393095463514328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35832810401916504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7416019439697266
10 0.5545140049 	 0.7416019497
epoch_time;  43.86004161834717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04835968837141991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06879263371229172
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4024601876735687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7619484663009644
11 0.5279861208 	 0.7619484616
epoch_time;  43.58335089683533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03653408959507942
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04967956244945526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3564819097518921
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7703640460968018
12 0.5185596313 	 0.7703640756
epoch_time;  43.657336950302124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024590451270341873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03579424321651459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37321966886520386
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8817813396453857
13 0.5222737661 	 0.8817813562
epoch_time;  44.23917055130005
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–„â–ˆâ–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–†â–ƒâ–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–†â–„â–ˆâ–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.68969
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.03325
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.31181
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02585
wandb:                         Train loss 0.46047
wandb: 
wandb: ğŸš€ View run thriving-laughter-1710 at: https://wandb.ai/nreints/thesis/runs/vk4enz0d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_142517-vk4enz0d/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_144836-nwybywxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-firecracker-1711
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/nwybywxp
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.040407974272966385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057373058050870895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35541069507598877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7592048048973083
14 0.5382810097 	 0.7592047838
epoch_time;  43.912373304367065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.056120287626981735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06977181136608124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34643831849098206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8407145738601685
15 0.5227298895 	 0.8407145613
epoch_time;  44.477190256118774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018433669582009315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029855063185095787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30654993653297424
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7171875238418579
16 0.4890728127 	 0.7171875184
epoch_time;  44.136558532714844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02354355715215206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.037907861173152924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3370380699634552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7136585712432861
17 0.465767132 	 0.7136585893
epoch_time;  43.91915726661682
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03410068526864052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052903398871421814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32699015736579895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6818891763687134
18 0.4598796189 	 0.6818891911
epoch_time;  43.92627453804016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020734701305627823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03204921633005142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.339699387550354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7934455275535583
19 0.5143162325 	 0.7934455065
epoch_time;  43.92460060119629
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03326911851763725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04954103007912636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3504432737827301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.819814145565033
20 0.4637443317 	 0.8198141692
epoch_time;  43.60864281654358
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04021875187754631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05523095652461052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3208024799823761
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7636448740959167
21 0.5146283389 	 0.763644861
epoch_time;  44.131080865859985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020480621606111526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.028112057596445084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3062431812286377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6390479803085327
22 0.4541068297 	 0.6390479972
epoch_time;  48.67874813079834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020751966163516045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029426641762256622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31640389561653137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6783204674720764
23 0.4312149498 	 0.6783204785
epoch_time;  44.50613236427307
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019489187747240067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027624735608696938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26727166771888733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6224132180213928
24 0.4201932296 	 0.6224131973
epoch_time;  43.818116664886475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.035661887377500534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04758921265602112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3084612488746643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6711764335632324
25 0.442132573 	 0.6711764609
epoch_time;  44.07924699783325
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021251244470477104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03423512727022171
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28233441710472107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6081398725509644
26 0.4112481833 	 0.6081398679
epoch_time;  44.27084565162659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026607396081089973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03331135958433151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30976077914237976
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6728499531745911
27 0.410168956 	 0.672849949
epoch_time;  44.010979652404785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05650141090154648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07050923258066177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34253230690956116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.690514862537384
28 0.4703377457 	 0.6905148786
epoch_time;  44.63966965675354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02584526315331459
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03326588124036789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31227046251296997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.674045741558075
29 0.4604660369 	 0.6740457126
epoch_time;  44.06825637817383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025850385427474976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03325493633747101
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3118082284927368
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6896883845329285
It took  1399.2258508205414  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14b4319230a0>, <torch.utils.data.dataloader.DataLoader object at 0x14b40c1bb220>, <torch.utils.data.dataloader.DataLoader object at 0x14b4319208b0>, <torch.utils.data.dataloader.DataLoader object at 0x14b431921c00>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13601823151111603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18244555592536926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8246473073959351
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7142459154129028
0 5.722229697 	 1.7142459374
epoch_time;  43.897918939590454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10990268737077713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1545267254114151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5287008881568909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2101802825927734
1 1.1047334635 	 1.2101802768
epoch_time;  43.595826148986816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05072461813688278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08961359411478043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.458726167678833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9309818744659424
2 0.9024196166 	 0.9309818694
epoch_time;  43.58754897117615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032577481120824814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06284020096063614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41875892877578735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9013595581054688
3 0.8146152078 	 0.9013595351
epoch_time;  44.18228077888489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04081828519701958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0690525472164154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.442898154258728
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9419390559196472
4 0.7571479052 	 0.9419390629
epoch_time;  44.00349259376526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05025804787874222
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07622545957565308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3943295180797577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7977869510650635
5 0.7771273316 	 0.7977869316
epoch_time;  43.71071171760559
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042590171098709106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06879699975252151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42810726165771484
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–„â–„
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‡â–‡
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–ƒâ–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–†â–†
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.006
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.15427
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.50659
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10927
wandb:                         Train loss 0.54879
wandb: 
wandb: ğŸš€ View run luminous-firecracker-1711 at: https://wandb.ai/nreints/thesis/runs/nwybywxp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_144836-nwybywxp/logs
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7461275458335876
6 0.6604611024 	 0.7461275406
epoch_time;  44.228246450424194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03378220647573471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05181330814957619
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3846476078033447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7758382558822632
7 0.6216358124 	 0.7758382469
epoch_time;  44.28114104270935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.053876034915447235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08620071411132812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38446491956710815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8064654469490051
8 0.6695071501 	 0.8064654486
epoch_time;  44.1943838596344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03824413940310478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.061455585062503815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3675593435764313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7654386162757874
9 0.6755853923 	 0.7654386215
epoch_time;  44.00264024734497
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03685625642538071
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0528019517660141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3747992515563965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.694964587688446
10 0.6283042151 	 0.6949645904
epoch_time;  44.762645959854126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.034574247896671295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05268026515841484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2987349033355713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7282058596611023
11 0.5706594902 	 0.7282058854
epoch_time;  47.56557083129883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031217964366078377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0496581606566906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3091461658477783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6637346148490906
12 0.5970290341 	 0.6637346435
epoch_time;  43.51359724998474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029746761545538902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04873743653297424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37695276737213135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7172786593437195
13 0.5605321822 	 0.7172786563
epoch_time;  43.84282827377319
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01877111755311489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.031331297010183334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30340540409088135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6477369666099548
14 0.5460113595 	 0.6477369787
epoch_time;  43.80036401748657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02890925481915474
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04299847409129143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3960135281085968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6847987174987793
15 0.5722697833 	 0.6847987333
epoch_time;  43.67166996002197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024031328037381172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03743236884474754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3219165503978729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6608120799064636
16 0.5139575066 	 0.6608120552
epoch_time;  43.737565755844116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01999489776790142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03493955731391907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3142874836921692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6071627736091614
17 0.5285016622 	 0.6071627983
epoch_time;  43.711148500442505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017841443419456482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027761263772845268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29688718914985657
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5422201156616211
18 0.5052663036 	 0.5422201013
epoch_time;  43.67052340507507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02846074104309082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0494411326944828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.310323029756546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6578837037086487
19 0.5187928216 	 0.6578837046
epoch_time;  43.708080768585205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03019207902252674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04672551527619362
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3455874025821686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.720615029335022
20 0.6039960054 	 0.7206150297
epoch_time;  44.49198293685913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01799127832055092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03308061882853508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30507156252861023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5793527364730835
21 0.6004281055 	 0.5793527563
epoch_time;  43.94939112663269
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03742162883281708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054326336830854416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2776482403278351
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5938702821731567
22 0.4965235373 	 0.5938702724
epoch_time;  43.73930597305298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.037104714661836624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057065192610025406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3309629261493683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6987676620483398
23 0.5633283871 	 0.6987676707
epoch_time;  43.553309202194214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02612895704805851
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.040782030671834946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30959755182266235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6390987634658813
24 0.5112410032 	 0.6390987523
epoch_time;  44.37049961090088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02761698141694069
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04338778927922249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2930608093738556
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.557598352432251
25 0.4557305682 	 0.5575983791
epoch_time;  44.086483001708984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021631397306919098
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0334140844643116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30125805735588074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5410376787185669
26 0.4532916107 	 0.5410376604
epoch_time;  44.437891483306885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020542293787002563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03995225206017494
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3142637610435486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7336195111274719
27 0.7238381882 	 0.7336194825
epoch_time;  43.93617582321167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029162781313061714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04668232426047325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29760441184043884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6659303903579712
28 0.5536505492 	 0.6659303878
epoch_time;  43.913370847702026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10926443338394165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15431448817253113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5104027390480042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9991940259933472
29 0.5487898683 	 0.999194004
epoch_time;  43.987210273742676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10926883667707443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1542683094739914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5065853595733643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0060045719146729
It took  1390.7945456504822  seconds.

JOB STATISTICS
==============
Job ID: 2142891
Array Job ID: 2141141_27
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-22:15:18 core-walltime
Job Wall-clock time: 03:54:11
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
