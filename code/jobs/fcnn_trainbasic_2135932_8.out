wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195411-9xhvtb7d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-kumquat-1165
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/9xhvtb7d
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▄▄▃▃▃▃▂▂▂▂▁▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▅▃▄▃▃▂▂▂▁▃▂▂▂▁▂▁▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▄▄▄▃▃▃▃▂▃▂▂▁▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▃▅▃▃▁▂▂▁▃▁▂▂▁▂▁▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.1714
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39072
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.06205
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29094
wandb:                         Train loss 2.91349
wandb: 
wandb: 🚀 View run twinkling-kumquat-1165 at: https://wandb.ai/nreints/thesis/runs/9xhvtb7d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195411-9xhvtb7d/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45312368869781494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6411022543907166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1162917613983154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3817033767700195
0 6.6062734058 	 3.3817033098 	 3.3817033098
epoch_time;  31.30249047279358
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3610377907752991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5120204091072083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.605454683303833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8309342861175537
1 3.3241906499 	 2.8309342668 	 2.8309342668
epoch_time;  30.764283895492554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36314210295677185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5142328143119812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.630119800567627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8474841117858887
2 3.2295665702 	 2.8474840319 	 2.8474840319
epoch_time;  30.37526297569275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3101469576358795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4363826811313629
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5127999782562256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.68277645111084
3 3.1734585255 	 2.6827763738 	 2.6827763738
epoch_time;  30.357017993927002
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3577684164047241
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47906267642974854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5709168910980225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7371082305908203
4 3.1320832187 	 2.7371082203 	 2.7371082203
epoch_time;  30.50752878189087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30272942781448364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41067108511924744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.495767831802368
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6462552547454834
5 3.0971028425 	 2.6462552457 	 2.6462552457
epoch_time;  30.323322296142578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3173384368419647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4437076449394226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4131295680999756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5818028450012207
6 3.0725158815 	 2.5818027806 	 2.5818027806
epoch_time;  30.69033145904541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2606358230113983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.373705118894577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3066227436065674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.451122760772705
7 3.0364538153 	 2.451122717 	 2.451122717
epoch_time;  30.34706401824951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27380654215812683
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37218043208122253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3162856101989746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4332473278045654
8 3.0228559924 	 2.4332473342 	 2.4332473342
epoch_time;  30.637459754943848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2811989188194275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3889845609664917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.281597375869751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4102747440338135
9 3.0053558866 	 2.4102746912 	 2.4102746912
epoch_time;  30.586873054504395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27004414796829224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3417750895023346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2288658618927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3116984367370605
10 2.989024416 	 2.3116984599 	 2.3116984599
epoch_time;  30.885307550430298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3139369487762451
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4327525794506073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.273617744445801
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4083380699157715
11 2.9733471828 	 2.4083380622 	 2.4083380622
epoch_time;  30.691075563430786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2706592381000519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37299519777297974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.205756664276123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.320101022720337
12 2.959495467 	 2.3201010214 	 2.3201010214
epoch_time;  30.700968265533447
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2846760153770447
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38680458068847656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2154228687286377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3260672092437744
13 2.9554791305 	 2.3260672904 	 2.3260672904
epoch_time;  30.430201053619385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27652260661125183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3751852810382843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1095404624938965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2101986408233643
14 2.9383803608 	 2.2101986447 	 2.2101986447
epoch_time;  30.611473321914673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2598434388637543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34086504578590393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1427786350250244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2324655055999756
15 2.9392468369 	 2.2324655894 	 2.2324655894
epoch_time;  30.566810369491577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2815057635307312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39648160338401794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0724945068359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.202516794204712
16 2.9358387859 	 2.2025167929 	 2.2025167929
epoch_time;  30.84245800971985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2616628408432007
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35007014870643616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0456087589263916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1433722972869873
17 2.9206713223 	 2.1433722419 	 2.1433722419
epoch_time;  30.299599170684814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.261384516954422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3672861158847809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0430288314819336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.159543037414551
18 2.9166953178 	 2.1595430941 	 2.1595430941
epoch_time;  30.660900354385376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2909550964832306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3905887007713318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.061933994293213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1712965965270996
19 2.9134876042 	 2.1712966507 	 2.1712966507
epoch_time;  30.555452585220337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2909395396709442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3907150328159332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0620534420013428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.171402931213379
It took 670.6704618930817 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn38: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135940.0

JOB STATISTICS
==============
Job ID: 2135940
Array Job ID: 2135932_8
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:25:30 core-walltime
Job Wall-clock time: 00:11:25
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
