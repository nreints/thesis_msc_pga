wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_165551-vcs6mk6y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-pig-1145
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/vcs6mk6y
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–ƒâ–„â–ƒâ–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–‚â–â–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ƒâ–ƒâ–„â–ƒâ–„â–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.40975
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.31647
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27153
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19596
wandb:                         Train loss 2.46558
wandb: 
wandb: ðŸš€ View run crimson-pig-1145 at: https://wandb.ai/nreints/thesis/runs/vcs6mk6y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_165551-vcs6mk6y/logs
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.334497332572937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8077747225761414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41438767313957214
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9152035117149353
0 4.5561774686 	 0.9152035275 	 0.9152035275
epoch_time;  34.510154247283936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2937418520450592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.588782787322998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3358234763145447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6307032108306885
1 2.8206764795 	 0.6307031992 	 0.6307031992
epoch_time;  33.17225384712219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22214770317077637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44268107414245605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2872112989425659
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5103612542152405
2 2.6945579286 	 0.5103612539 	 0.5103612539
epoch_time;  32.79259014129639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23467347025871277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43615344166755676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31346529722213745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5331407785415649
3 2.6307044936 	 0.5331407702 	 0.5331407702
epoch_time;  32.95887207984924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24910412728786469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4339524209499359
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2936720848083496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48008471727371216
4 2.5961881004 	 0.4800847234 	 0.4800847234
epoch_time;  32.90474081039429
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21847271919250488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38890400528907776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31239718198776245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5013033747673035
5 2.5717087464 	 0.501303348 	 0.501303348
epoch_time;  33.15176606178284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24534380435943604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38574978709220886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3067159652709961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46020209789276123
6 2.5540568998 	 0.4602021088 	 0.4602021088
epoch_time;  33.45105481147766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18956312537193298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3617633283138275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26421380043029785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4486992359161377
7 2.5407779309 	 0.4486992501 	 0.4486992501
epoch_time;  34.39710974693298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19649727642536163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3452807068824768
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25401386618614197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40966343879699707
8 2.5296211837 	 0.4096634324 	 0.4096634324
epoch_time;  33.075973987579346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22483590245246887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39749202132225037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28479528427124023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4673314690589905
9 2.5164954431 	 0.4673314687 	 0.4673314687
epoch_time;  33.34523367881775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23235268890857697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38459038734436035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2795657515525818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4362950921058655
10 2.5066420465 	 0.4362950918 	 0.4362950918
epoch_time;  33.13040566444397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2149980664253235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3859710395336151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28522607684135437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4646211266517639
11 2.5040419571 	 0.4646211366 	 0.4646211366
epoch_time;  32.98894953727722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20295466482639313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33808863162994385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27153241634368896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41697850823402405
12 2.4957170428 	 0.4169784958 	 0.4169784958
epoch_time;  33.11645221710205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21290594339370728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3664066195487976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2507040202617645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41122084856033325
13 2.4877709845 	 0.4112208598 	 0.4112208598
epoch_time;  33.11143445968628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2000732272863388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36976945400238037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2622177004814148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4440934360027313
14 2.4846827792 	 0.4440934465 	 0.4440934465
epoch_time;  32.75326466560364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1973423957824707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3237672448158264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24711188673973083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38179683685302734
15 2.4809824867 	 0.381796842 	 0.381796842
epoch_time;  32.96977400779724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2040913701057434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3465974032878876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26076018810272217
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40881627798080444
16 2.4770048293 	 0.4088162809 	 0.4088162809
epoch_time;  33.1274356842041
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18510185182094574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30417490005493164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2389642596244812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36696505546569824
17 2.4761702551 	 0.3669650516 	 0.3669650516
epoch_time;  33.02145767211914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24480512738227844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4050334095954895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29175230860710144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45575380325317383
18 2.4709571833 	 0.4557538007 	 0.4557538007
epoch_time;  33.06508278846741
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19600756466388702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3163825273513794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2714981436729431
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40976208448410034
19 2.4655798084 	 0.4097620784 	 0.4097620784
epoch_time;  32.85443925857544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19595953822135925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3164670467376709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2715322971343994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40974605083465576
It took 729.0539166927338 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn36: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135390.0

JOB STATISTICS
==============
Job ID: 2135390
Array Job ID: 2135328_16
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:43:12 core-walltime
Job Wall-clock time: 00:12:24
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
