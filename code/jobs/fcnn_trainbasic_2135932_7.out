wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_195345-scaicjwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-festival-1160
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/scaicjwp
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–‡â–ˆâ–…â–„â–…â–ƒâ–…â–‚â–ˆâ–…â–ƒâ–„â–â–‚â–„â–…â–„â–…â–…
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‚â–‚â–â–‚â–„â–‚â–ƒâ–‡â–ˆâ–„â–„â–‡â–„â–‡â–…â–‡â–„â–‡â–‡â–‡
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 20.35561
wandb:  Test loss t(-10, 10)_r(0, 0)_none 7.13961
wandb:    Test loss t(0, 0)_r(-5, 5)_none 10.32397
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.46322
wandb:                         Train loss 0.5722
wandb: 
wandb: ðŸš€ View run lambent-festival-1160 at: https://wandb.ai/nreints/thesis/runs/scaicjwp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_195345-scaicjwp/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21771612763404846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.218659400939941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.06643009185791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 24.830949783325195
0 2.7439822724 	 24.8309491132 	 24.8309491132
epoch_time;  33.53121566772461
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18648448586463928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.537018775939941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.78447151184082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.96572494506836
1 0.7766405853 	 22.9657252956 	 22.9657252956
epoch_time;  31.876919984817505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2010916769504547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.724872589111328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.763083457946777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.097864151000977
2 0.7176364906 	 22.0978647593 	 22.0978647593
epoch_time;  32.291584730148315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13966374099254608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.645997047424316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.070611000061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.426494598388672
3 0.6818848069 	 22.4264938767 	 22.4264938767
epoch_time;  31.637189388275146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18023918569087982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.454893112182617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.392826080322266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.728212356567383
4 0.65292939 	 21.7282120988 	 21.7282120988
epoch_time;  31.493010759353638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3151707947254181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.075545310974121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.082584381103516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.077951431274414
5 0.6373352588 	 21.077952122 	 21.077952122
epoch_time;  31.82411289215088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20977668464183807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.914303779602051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.420941352844238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.5168399810791
6 0.6203709152 	 21.5168391047 	 21.5168391047
epoch_time;  32.00731921195984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26220613718032837
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.05518913269043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.825596809387207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.014509201049805
7 0.6089865569 	 21.0145098712 	 21.0145098712
epoch_time;  31.73009943962097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.47190678119659424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.534788131713867
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.283966064453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.764490127563477
8 0.6004917194 	 20.764490076 	 20.764490076
epoch_time;  31.64193606376648
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5131170153617859
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.4180731773376465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.776107788085938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.935564041137695
9 0.5950766572 	 19.9355640308 	 19.9355640308
epoch_time;  31.585771322250366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3087104856967926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.979531288146973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.049422264099121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 22.179227828979492
10 0.589638743 	 22.1792282517 	 22.1792282517
epoch_time;  31.57870364189148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28023940324783325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.943543434143066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.30911636352539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.353487014770508
11 0.5877781831 	 21.3534879117 	 21.3534879117
epoch_time;  31.347781896591187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4790668785572052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.380234718322754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.834710121154785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.11284828186035
12 0.5828310709 	 20.1128483953 	 20.1128483953
epoch_time;  31.69864249229431
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3154814541339874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.502109527587891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.080350875854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.689130783081055
13 0.5804820645 	 20.6891311233 	 20.6891311233
epoch_time;  31.5384738445282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45805466175079346
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.1410322189331055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.439943313598633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.456666946411133
14 0.5781856632 	 19.4566670186 	 19.4566670186
epoch_time;  31.686960458755493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3494679927825928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.251481533050537
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.767210960388184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.014219284057617
15 0.5758474837 	 20.0142195418 	 20.0142195418
epoch_time;  31.41001033782959
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4724327623844147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.496497631072998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.06103801727295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.71653175354004
16 0.5736935253 	 20.7165316195 	 20.7165316195
epoch_time;  31.487621784210205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3077161908149719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.605839729309082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.476103782653809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.092817306518555
17 0.5742635863 	 21.0928169869 	 21.0928169869
epoch_time;  31.59003448486328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4606418013572693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.209865093231201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.103487968444824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.139453887939453
18 0.5723829018 	 20.1394544447 	 20.1394544447
epoch_time;  31.53892183303833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4633565843105316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.144045829772949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.321048736572266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.354631423950195
19 0.5722022757 	 20.3546307538 	 20.3546307538
epoch_time;  31.44251012802124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4632211923599243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.139609336853027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.323967933654785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 20.355606079101562
It took 697.7381105422974 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 440, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn44: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135939.0

JOB STATISTICS
==============
Job ID: 2135939
Array Job ID: 2135932_7
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:33:36 core-walltime
Job Wall-clock time: 00:11:52
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
