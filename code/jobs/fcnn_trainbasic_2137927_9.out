wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140928-wnm2w9ab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-lantern-1241
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/wnm2w9ab
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▃▂▂▂▂▂▁▂▂▂▁▂▁▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▆▃▃▃▄▂▃▂▃▃▁▃▂▃▃▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▃▂▂▂▂▂▁▂▂▂▁▂▁▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▆▃▃▃▄▂▂▂▃▃▁▃▁▃▄▂▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.39017
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29332
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.28635
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19436
wandb:                         Train loss 2.46669
wandb: 
wandb: 🚀 View run vermilion-lantern-1241 at: https://wandb.ai/nreints/thesis/runs/wnm2w9ab
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140928-wnm2w9ab/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142224-rs2o7lro
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-laughter-1249
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/rs2o7lro
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2792307138442993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42766696214675903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.423900604248047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.517544269561768
0 5.1505555893 	 4.5175441433 	 4.5175441433
epoch_time;  42.62340068817139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24053260684013367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3593471348285675
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9677629470825195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9989824295043945
1 2.8613677958 	 3.9989825274 	 3.9989825274
epoch_time;  40.9590060710907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2257319688796997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3677167296409607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.835522413253784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.900768280029297
2 2.7312858029 	 3.9007683831 	 3.9007683831
epoch_time;  41.38306212425232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2448408454656601
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3596416711807251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4792816638946533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.539726734161377
3 2.6747210483 	 3.5397266285 	 3.5397266285
epoch_time;  41.09694242477417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19785261154174805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28451862931251526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.410792589187622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4506850242614746
4 2.6286330175 	 3.4506849134 	 3.4506849134
epoch_time;  35.892688512802124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1943189948797226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27856937050819397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3230299949645996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3756601810455322
5 2.5967527479 	 3.3756601694 	 3.3756601694
epoch_time;  33.39727807044983
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19114050269126892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29030293226242065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3697683811187744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4388444423675537
6 2.5763079127 	 3.438844423 	 3.438844423
epoch_time;  34.652589082717896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20918764173984528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3185245096683502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4196434020996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4916749000549316
7 2.5540193779 	 3.4916748047 	 3.4916748047
epoch_time;  33.63912224769592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17937782406806946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.272347092628479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3167099952697754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.382443428039551
8 2.5413131559 	 3.3824433198 	 3.3824433198
epoch_time;  34.587883710861206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1838047057390213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2894173264503479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.201021194458008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.262638807296753
9 2.5298729279 	 3.2626388962 	 3.2626388962
epoch_time;  33.10504603385925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18372982740402222
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2633902132511139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.398606777191162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4358367919921875
10 2.5137829532 	 3.4358368745 	 3.4358368745
epoch_time;  33.01288723945618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19926781952381134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2889496386051178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.332232713699341
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4033873081207275
11 2.5088784871 	 3.4033872862 	 3.4033872862
epoch_time;  34.98170757293701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20092307031154633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2947223484516144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.357863664627075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4280216693878174
12 2.5040507178 	 3.4280217351 	 3.4280217351
epoch_time;  33.32500743865967
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16495747864246368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23130947351455688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2135672569274902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.243086814880371
13 2.4912585424 	 3.2430868613 	 3.2430868613
epoch_time;  34.36830520629883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1930294781923294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2863228917121887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.408352851867676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5021376609802246
14 2.4846968292 	 3.5021375501 	 3.5021375501
epoch_time;  34.13706994056702
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16662874817848206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24977567791938782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.261296510696411
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3298866748809814
15 2.4836663806 	 3.3298867715 	 3.3298867715
epoch_time;  33.23460674285889
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.195254385471344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2809034585952759
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.434788465499878
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4826576709747314
16 2.4804107315 	 3.4826577676 	 3.4826577676
epoch_time;  33.0521399974823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20695333182811737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2760946750640869
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3874640464782715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.44820499420166
17 2.4788105112 	 3.4482049066 	 3.4482049066
epoch_time;  34.48803186416626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17534127831459045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26148751378059387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2887890338897705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3693349361419678
18 2.4741878729 	 3.3693349477 	 3.3693349477
epoch_time;  32.8913414478302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19428221881389618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2934338450431824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.285383939743042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3890678882598877
19 2.4666859621 	 3.3890677787 	 3.3890677787
epoch_time;  33.07083821296692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1943632811307907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29331594705581665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.286348342895508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.390169143676758
It took 775.5214023590088 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▃▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▇▆▆▃▄▄▆▄▅▁▂▇▁▄▂▂▂▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▃▃▂▂▁▂▂▂▂▂▁▂▁▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▇▅▇▄▄▅█▄▅▁▂█▁▆▂▂▂▄▄
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.423
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28139
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.32235
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19588
wandb:                         Train loss 2.46091
wandb: 
wandb: 🚀 View run prosperous-laughter-1249 at: https://wandb.ai/nreints/thesis/runs/rs2o7lro
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142224-rs2o7lro/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_143359-5nlw5br2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-mandu-1255
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/5nlw5br2
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24050214886665344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3549565374851227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5442609786987305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.690953731536865
0 5.1943491611 	 4.6909536001 	 4.6909536001
epoch_time;  32.49802613258362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2325184941291809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3732687532901764
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.144068241119385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.289684295654297
1 2.8555623069 	 4.2896843988 	 4.2896843988
epoch_time;  32.80989360809326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2253032624721527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3473051190376282
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.711912155151367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8294005393981934
2 2.7255284149 	 3.8294004698 	 3.8294004698
epoch_time;  32.47238039970398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21165429055690765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3273034393787384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.648042917251587
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7743873596191406
3 2.6546205225 	 3.774387339 	 3.774387339
epoch_time;  32.76220893859863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23428472876548767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33649834990501404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.531851053237915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6378586292266846
4 2.62003735 	 3.6378586228 	 3.6378586228
epoch_time;  32.561371088027954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20161543786525726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27714163064956665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.53881573677063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.631566286087036
5 2.5883104741 	 3.6315663931 	 3.6315663931
epoch_time;  31.202645301818848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19839955866336823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29755470156669617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.335721969604492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4187800884246826
6 2.565799287 	 3.4187800227 	 3.4187800227
epoch_time;  32.07045030593872
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21195510029792786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28976988792419434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5284767150878906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.605456829071045
7 2.5483785228 	 3.6054568729 	 3.6054568729
epoch_time;  31.79899001121521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23665452003479004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33941635489463806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.372589588165283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4651713371276855
8 2.5329092306 	 3.4651713603 	 3.4651713603
epoch_time;  31.82518696784973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20000259578227997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3014925420284271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.491562843322754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.610987424850464
9 2.5243610028 	 3.6109873179 	 3.6109873179
epoch_time;  31.68254017829895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20692990720272064
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3170739412307739
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4853901863098145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.628981828689575
10 2.5116846992 	 3.6289818016 	 3.6289818016
epoch_time;  31.521158933639526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17004084587097168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23024357855319977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.379472494125366
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4609196186065674
11 2.509159819 	 3.4609196843 	 3.4609196843
epoch_time;  31.544492483139038
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.176578089594841
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2509768307209015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2881412506103516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3873047828674316
12 2.4979757982 	 3.3873046875 	 3.3873046875
epoch_time;  32.44003462791443
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23935432732105255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3472554683685303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.498776435852051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6280157566070557
13 2.4886364688 	 3.6280157966 	 3.6280157966
epoch_time;  31.91468071937561
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16463465988636017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2304907739162445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3328304290771484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.412274122238159
14 2.4864307732 	 3.412274005 	 3.412274005
epoch_time;  31.441168546676636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21792757511138916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30166107416152954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4282588958740234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.533625364303589
15 2.4778301469 	 3.5336254223 	 3.5336254223
epoch_time;  31.644193172454834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17025397717952728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24273191392421722
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.345046281814575
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.43774151802063
16 2.4727479887 	 3.4377415013 	 3.4377415013
epoch_time;  31.80176615715027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17177273333072662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2532433271408081
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2900452613830566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4031460285186768
17 2.4704551826 	 3.4031461149 	 3.4031461149
epoch_time;  31.4515323638916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17883047461509705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25859618186950684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2711827754974365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.367403268814087
18 2.4637635476 	 3.3674032675 	 3.3674032675
epoch_time;  31.625498056411743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1959001123905182
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2815546691417694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.32073974609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.421434164047241
19 2.4609143587 	 3.4214342272 	 3.4214342272
epoch_time;  31.50227189064026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19587846100330353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2813868224620819
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.322350025177002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4229977130889893
It took 695.2199490070343 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▃▃▂▃▂▂▃▂▁▂▂▂▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▃▄▅▂▄▃▅▄▃▁▂▄▁▃▁▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▃▂▃▂▂▃▂▁▂▂▂▂▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▃▂▄▅▂▅▃▆▄▄▁▂▃▂▄▁▃▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.13398
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26279
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.10736
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18452
wandb:                         Train loss 2.48084
wandb: 
wandb: 🚀 View run lunar-mandu-1255 at: https://wandb.ai/nreints/thesis/runs/5nlw5br2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_143359-5nlw5br2/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144535-mrnoo0d9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-envelope-1261
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/mrnoo0d9
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25551456212997437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3849020004272461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.449553966522217
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5562944412231445
0 5.1091366447 	 4.5562942092 	 4.5562942092
epoch_time;  31.92795968055725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2251063585281372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33862200379371643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8477180004119873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8826184272766113
1 2.8647123736 	 3.8826185072 	 3.8826185072
epoch_time;  31.64739680290222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19202126562595367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2816385328769684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6835341453552246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7076656818389893
2 2.7313408461 	 3.7076656857 	 3.7076656857
epoch_time;  31.8772189617157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1832677125930786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2674553394317627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4494268894195557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4592301845550537
3 2.6652235317 	 3.4592301652 	 3.4592301652
epoch_time;  31.824959993362427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20409953594207764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.300902783870697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.455909252166748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4735805988311768
4 2.6235097008 	 3.4735806852 	 3.4735806852
epoch_time;  31.590815782546997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21976631879806519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30780696868896484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4853856563568115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.482219696044922
5 2.595063103 	 3.4822196342 	 3.4822196342
epoch_time;  31.949771404266357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17752890288829803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24774576723575592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.363267660140991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.340296983718872
6 2.572943169 	 3.3402970598 	 3.3402970598
epoch_time;  31.67719292640686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21667560935020447
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2932688593864441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.475465774536133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.471169948577881
7 2.5635454905 	 3.4711699615 	 3.4711699615
epoch_time;  31.61589217185974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19796571135520935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2793385982513428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.286238431930542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2704572677612305
8 2.5494825277 	 3.2704573348 	 3.2704573348
epoch_time;  31.737913131713867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2271457016468048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3078618049621582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3276798725128174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3049230575561523
9 2.5320896983 	 3.3049230627 	 3.3049230627
epoch_time;  31.5700261592865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2049674391746521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2964458167552948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3870298862457275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3963863849639893
10 2.5314205844 	 3.3963863888 	 3.3963863888
epoch_time;  31.939629077911377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20377221703529358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26560020446777344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.253906488418579
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2263636589050293
11 2.5172519981 	 3.2263635584 	 3.2263635584
epoch_time;  31.665533304214478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1677333265542984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22735187411308289
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.159274101257324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.127882957458496
12 2.5070212331 	 3.1278828389 	 3.1278828389
epoch_time;  31.73759150505066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18508197367191315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25291702151298523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.362602949142456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3428635597229004
13 2.5071316501 	 3.3428635056 	 3.3428635056
epoch_time;  31.877158641815186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19840700924396515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2981257736682892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.212791681289673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2472593784332275
14 2.4967524503 	 3.2472593565 	 3.2472593565
epoch_time;  31.87251877784729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17847120761871338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22426842153072357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2018847465515137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1786458492279053
15 2.4989488895 	 3.1786459433 	 3.1786459433
epoch_time;  31.61045813560486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2033076137304306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28003716468811035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.273313045501709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2724850177764893
16 2.4948282519 	 3.2724850216 	 3.2724850216
epoch_time;  31.72792387008667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16995838284492493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23529231548309326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1767373085021973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.173266649246216
17 2.4894854236 	 3.1732666016 	 3.1732666016
epoch_time;  31.871697425842285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19362898170948029
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24830466508865356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0682640075683594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.045598268508911
18 2.4848761743 	 3.0455982105 	 3.0455982105
epoch_time;  31.976935863494873
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18458585441112518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26272353529930115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.107576847076416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.13323974609375
19 2.4808373577 	 3.1332397461 	 3.1332397461
epoch_time;  31.987177848815918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1845228523015976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26279136538505554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1073555946350098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1339774131774902
It took 695.8303487300873 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▆▄▃▄▄▃▄▂▃▃▃▂▃▃▁▃▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▇▄▃▄▅▂▄▂▃▄▂▂▂▂▁▂▄▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.16282
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23348
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.16848
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.1807
wandb:                         Train loss 2.45825
wandb: 
wandb: 🚀 View run vibrant-envelope-1261 at: https://wandb.ai/nreints/thesis/runs/mrnoo0d9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144535-mrnoo0d9/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_145701-7hll3vhb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-lamp-1268
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7hll3vhb
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.263564795255661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43099310994148254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.534226894378662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.766045093536377
0 5.1529811273 	 4.7660453178 	 4.7660453178
epoch_time;  31.945992708206177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21127651631832123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3219859004020691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8916540145874023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.016165256500244
1 2.846740516 	 4.0161654086 	 4.0161654086
epoch_time;  32.05299258232117
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24541392922401428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35818374156951904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.589695692062378
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.680708885192871
2 2.715875521 	 3.6807089316 	 3.6807089316
epoch_time;  31.653169631958008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1999349296092987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29750964045524597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2472424507141113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2762768268585205
3 2.6438770126 	 3.2762767895 	 3.2762767895
epoch_time;  31.53817129135132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1876051425933838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2693862318992615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2273929119110107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2360072135925293
4 2.6057755374 	 3.2360071131 	 3.2360071131
epoch_time;  31.7677059173584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2011459916830063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.297960489988327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2591552734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.290926933288574
5 2.5799048959 	 3.2909268766 	 3.2909268766
epoch_time;  31.924259901046753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21385949850082397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31340292096138
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3470733165740967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3769490718841553
6 2.5548670103 	 3.376949166 	 3.376949166
epoch_time;  31.461971521377563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1832381933927536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2645336389541626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.203845500946045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2002177238464355
7 2.5452473407 	 3.200217747 	 3.200217747
epoch_time;  31.75765299797058
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20250418782234192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3035944700241089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2884154319763184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3334875106811523
8 2.5263222273 	 3.3334875158 	 3.3334875158
epoch_time;  31.667185068130493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1802019476890564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2581862211227417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1538989543914795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1601014137268066
9 2.5124078646 	 3.1601014833 	 3.1601014833
epoch_time;  31.880950689315796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19415976107120514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2904890775680542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.186676502227783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.203073263168335
10 2.5047867761 	 3.2030732026 	 3.2030732026
epoch_time;  31.565484523773193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21116040647029877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29129162430763245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.191450834274292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2173588275909424
11 2.4965949243 	 3.2173587284 	 3.2173587284
epoch_time;  31.910109996795654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18044248223304749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2692086398601532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1020543575286865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1284196376800537
12 2.488818128 	 3.1284196183 	 3.1284196183
epoch_time;  31.551217079162598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17764721810817719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24457813799381256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.135118246078491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.134664297103882
13 2.4791219268 	 3.1346643396 	 3.1346643396
epoch_time;  31.76267647743225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1769362837076187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26430463790893555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.200561285018921
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2424116134643555
14 2.4744075499 	 3.2424115155 	 3.2424115155
epoch_time;  31.431822061538696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18136736750602722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2775030732154846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0443313121795654
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0858395099639893
15 2.4703627827 	 3.0858395138 	 3.0858395138
epoch_time;  31.74240517616272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1618572175502777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21844688057899475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0575215816497803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0477991104125977
16 2.4642225716 	 3.0477991053 	 3.0477991053
epoch_time;  31.58588171005249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17615045607089996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2642422616481781
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1015443801879883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1344268321990967
17 2.4654141828 	 3.1344267974 	 3.1344267974
epoch_time;  31.34337067604065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19869495928287506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2765580415725708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0374977588653564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.050464153289795
18 2.4575180436 	 3.0504641971 	 3.0504641971
epoch_time;  31.32506513595581
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18069757521152496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2335151731967926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.168952226638794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1611878871917725
19 2.4582517878 	 3.1611879091 	 3.1611879091
epoch_time;  31.141329050064087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1807042956352234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23348422348499298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1684751510620117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.162822961807251
It took 686.2149486541748 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▃▂▂▂▁▂▁▁▂▁▁▂▂▂▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▆▄▄▃▂▃▃▃▃▂▁▃▂▁▂▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▂▂▂▁▂▁▁▂▁▁▂▂▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▇▅▅▃▂▃▃▃▃▂▂▃▂▁▃▁▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.34788
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28957
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.28952
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19148
wandb:                         Train loss 2.46232
wandb: 
wandb: 🚀 View run vermilion-lamp-1268 at: https://wandb.ai/nreints/thesis/runs/7hll3vhb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_145701-7hll3vhb/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150818-ptjr789s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run incandescent-envelope-1275
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ptjr789s
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2751711905002594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46431055665016174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.232626914978027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.55019998550415
0 5.1393829738 	 5.5501999314 	 5.5501999314
epoch_time;  31.294048309326172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22153334319591522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32661864161491394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.246305465698242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.353370666503906
1 2.869309887 	 4.3533707902 	 4.3533707902
epoch_time;  31.2609224319458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.199842631816864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2908281087875366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8632311820983887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8900539875030518
2 2.73460195 	 3.8900539089 	 3.8900539089
epoch_time;  31.45124840736389
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2529192268848419
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37714824080467224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6360983848571777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.7020962238311768
3 2.660082859 	 3.7020963102 	 3.7020963102
epoch_time;  31.48038601875305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22825773060321808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32964086532592773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3478472232818604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.432455539703369
4 2.6192288925 	 3.4324555268 	 3.4324555268
epoch_time;  31.294214248657227
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22614069283008575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31581220030784607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2113184928894043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2591614723205566
5 2.5871223341 	 3.2591615419 	 3.2591615419
epoch_time;  31.630314111709595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1930430829524994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2891302704811096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1816940307617188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2562901973724365
6 2.5670123865 	 3.2562902502 	 3.2562902502
epoch_time;  31.65462350845337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1770877093076706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24246951937675476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1662299633026123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.18365740776062
7 2.547484773 	 3.1836574245 	 3.1836574245
epoch_time;  30.954307556152344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19438010454177856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27234524488449097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2411208152770996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2640182971954346
8 2.5283103948 	 3.2640182908 	 3.2640182908
epoch_time;  31.005615711212158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18682178854942322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2719779908657074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0174732208251953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0614240169525146
9 2.5203054915 	 3.0614241317 	 3.0614241317
epoch_time;  31.34499716758728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1973470151424408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30047088861465454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.107311964035034
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1851766109466553
10 2.510959385 	 3.185176705 	 3.185176705
epoch_time;  31.072479009628296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19132263958454132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26799383759498596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.263575792312622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2959938049316406
11 2.502630669 	 3.2959937843 	 3.2959937843
epoch_time;  31.0847647190094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1672460287809372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24230286478996277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0976500511169434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1192564964294434
12 2.4927597511 	 3.1192564268 	 3.1192564268
epoch_time;  31.009737730026245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.167861670255661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22588568925857544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.145127296447754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1570677757263184
13 2.4880934749 	 3.1570678711 	 3.1570678711
epoch_time;  31.148942470550537
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18541911244392395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26949983835220337
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2510390281677246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3072757720947266
14 2.4808264482 	 3.3072757205 	 3.3072757205
epoch_time;  31.03220558166504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1745404601097107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23153504729270935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2378547191619873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.257091999053955
15 2.4756089135 	 3.2570919552 	 3.2570919552
epoch_time;  31.2047438621521
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15711866319179535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2128840535879135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2750346660614014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.284857988357544
16 2.471766302 	 3.2848580025 	 3.2848580025
epoch_time;  30.949925422668457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19050061702728271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2665235102176666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.284959316253662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.318207263946533
17 2.4677451721 	 3.318207282 	 3.318207282
epoch_time;  31.064935445785522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16150790452957153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22408218681812286
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2956743240356445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.310436964035034
18 2.465192357 	 3.3104370117 	 3.3104370117
epoch_time;  31.215224504470825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19149921834468842
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2894635498523712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2882237434387207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3491904735565186
19 2.4623236634 	 3.3491903769 	 3.3491903769
epoch_time;  31.055288314819336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19148437678813934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2895677387714386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2895233631134033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.347883462905884
It took 676.7516493797302 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▆▃▅▃▃▃▃▄▂▃▃▂▃▁▃▃▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▂▂▂▁▂▂▂▂▁▂▂▁▁▁▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▅▄▄▃▅▃▄▃▂▅▃▂▄▁▂▃▃▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.14165
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24936
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.16393
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17494
wandb:                         Train loss 2.44759
wandb: 
wandb: 🚀 View run incandescent-envelope-1275 at: https://wandb.ai/nreints/thesis/runs/ptjr789s
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150818-ptjr789s/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151927-e2o1o0jz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-springroll-1282
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/e2o1o0jz
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24720177054405212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3969912528991699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.631982326507568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.813636779785156
0 5.0611958229 	 4.8136365736 	 4.8136365736
epoch_time;  31.047702074050903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23020638525485992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35508692264556885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9222421646118164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.031445503234863
1 2.8256359888 	 4.0314453125 	 4.0314453125
epoch_time;  30.704979419708252
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20700332522392273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33486127853393555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7519614696502686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.863586187362671
2 2.6997913317 	 3.8635860959 	 3.8635860959
epoch_time;  30.780378580093384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19669467210769653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27746284008026123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.5376534461975098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.571542739868164
3 2.6384087255 	 3.5715427708 	 3.5715427708
epoch_time;  31.034589767456055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19342577457427979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32204747200012207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3229429721832275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.38394832611084
4 2.5987230635 	 3.3839484137 	 3.3839484137
epoch_time;  30.790250778198242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18422721326351166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26278719305992126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3933680057525635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.402329206466675
5 2.5719599947 	 3.4023292335 	 3.4023292335
epoch_time;  30.90793538093567
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20322905480861664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2765815258026123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4355032444000244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4277737140655518
6 2.5538691248 	 3.4277736355 	 3.4277736355
epoch_time;  31.01757550239563
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18135172128677368
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27448126673698425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.260215997695923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2612152099609375
7 2.5358415628 	 3.2612152924 	 3.2612152924
epoch_time;  30.526912450790405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1887521743774414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26503241062164307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.450972557067871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.440983533859253
8 2.516272295 	 3.4409836228 	 3.4409836228
epoch_time;  30.735779523849487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18653422594070435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2813776731491089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.423597812652588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.434237003326416
9 2.5160813793 	 3.4342370935 	 3.4342370935
epoch_time;  30.698623418807983
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16919459402561188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25368863344192505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2762012481689453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2903943061828613
10 2.5000158683 	 3.2903943861 	 3.2903943861
epoch_time;  30.84617853164673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.207464337348938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27978986501693726
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.40255069732666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3683886528015137
11 2.489430711 	 3.3683887379 	 3.3683887379
epoch_time;  31.052191019058228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1858801394701004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2787134647369385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.234266757965088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.236635208129883
12 2.4831030554 	 3.2366352803 	 3.2366352803
epoch_time;  31.09337091445923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16079911589622498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24413743615150452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3028082847595215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2848598957061768
13 2.4801619626 	 3.2848599821 	 3.2848599821
epoch_time;  31.0355703830719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1889670491218567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2743988037109375
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.325185775756836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3156328201293945
14 2.4749386425 	 3.3156329181 	 3.3156329181
epoch_time;  30.825615644454956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15399028360843658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21698009967803955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2369368076324463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.195047616958618
15 2.4721859859 	 3.1950475744 	 3.1950475744
epoch_time;  30.875375986099243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1702815145254135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2584491968154907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2580509185791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.244715690612793
16 2.4659950293 	 3.2447156751 	 3.2447156751
epoch_time;  30.748709201812744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1805684119462967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27473708987236023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1930768489837646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2043087482452393
17 2.4581312878 	 3.2043087521 	 3.2043087521
epoch_time;  31.012267351150513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17589911818504333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24391135573387146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.341583013534546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3168015480041504
18 2.4553913589 	 3.3168014939 	 3.3168014939
epoch_time;  31.249656677246094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17488929629325867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24935095012187958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1641674041748047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.141953468322754
19 2.4475915866 	 3.1419535869 	 3.1419535869
epoch_time;  31.015421867370605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17493557929992676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2493559718132019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1639318466186523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.141650438308716
It took 669.7111546993256 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▂▃▂▂▃▃▂▂▃▂▂▂▁▃▂▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▃▂▂▂▂▃▃▂▂▂▂▂▂▁▂▂▂▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.09275
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23754
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.93546
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.17307
wandb:                         Train loss 2.45989
wandb: 
wandb: 🚀 View run lucky-springroll-1282 at: https://wandb.ai/nreints/thesis/runs/e2o1o0jz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151927-e2o1o0jz/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_153041-uwfa3tmx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-wonton-1287
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/uwfa3tmx
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4200597107410431
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5975005030632019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.818728923797607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.149848937988281
0 5.1338981572 	 5.1498488967 	 5.1498488967
epoch_time;  30.983291625976562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22192813456058502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3527936041355133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.320939540863037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.603156566619873
1 2.8473246684 	 4.6031563424 	 4.6031563424
epoch_time;  30.754631280899048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23127950727939606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3363353908061981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6053197383880615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8156635761260986
2 2.7290381223 	 3.8156636006 	 3.8156636006
epoch_time;  31.001288175582886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18576152622699738
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26935648918151855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4102025032043457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5907535552978516
3 2.6608670182 	 3.5907536687 	 3.5907536687
epoch_time;  31.06099271774292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20448701083660126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2918187975883484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2966697216033936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4933950901031494
4 2.624567919 	 3.4933950063 	 3.4933950063
epoch_time;  30.990870237350464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18793460726737976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2635047435760498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2388694286346436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3977410793304443
5 2.5891455442 	 3.3977410394 	 3.3977410394
epoch_time;  30.802955150604248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20458753407001495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2773524224758148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1121432781219482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.243807077407837
6 2.5673795066 	 3.2438070761 	 3.2438070761
epoch_time;  30.868335723876953
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22321732342243195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33655670285224915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.252281665802002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4545090198516846
7 2.5540560733 	 3.4545090134 	 3.4545090134
epoch_time;  31.291759490966797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20993691682815552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30035844445228577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.267871141433716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.430788516998291
8 2.5308227126 	 3.4307884423 	 3.4307884423
epoch_time;  30.640196561813354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17159582674503326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26085057854652405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.185004711151123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.353419065475464
9 2.5215461953 	 3.3534189585 	 3.3534189585
epoch_time;  31.013490200042725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18519601225852966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26773130893707275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1749205589294434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3309311866760254
10 2.514254705 	 3.3309312975 	 3.3309312975
epoch_time;  30.84026050567627
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1962602287530899
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2846793234348297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2069218158721924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.369601249694824
11 2.5038240984 	 3.369601193 	 3.369601193
epoch_time;  31.00063943862915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1974177211523056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27633729577064514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1133768558502197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2419495582580566
12 2.5015031922 	 3.2419496279 	 3.2419496279
epoch_time;  30.933378219604492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20516324043273926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27194103598594666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1959853172302246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3250699043273926
13 2.4905117399 	 3.325069943 	 3.325069943
epoch_time;  31.120035409927368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19394497573375702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28194016218185425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0385775566101074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2104430198669434
14 2.4768882963 	 3.2104429503 	 3.2104429503
epoch_time;  31.065352201461792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15090471506118774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1967197060585022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.013007164001465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1322572231292725
15 2.4741599381 	 3.132257245 	 3.132257245
epoch_time;  31.21027898788452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18346527218818665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2845373749732971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.984715700149536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1891677379608154
16 2.4691589397 	 3.1891677444 	 3.1891677444
epoch_time;  30.769983530044556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1946491301059723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2670843303203583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.064553737640381
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2132983207702637
17 2.4613694157 	 3.2132984058 	 3.2132984058
epoch_time;  31.0891330242157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1955515444278717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27531149983406067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9018049240112305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0618975162506104
18 2.4636420558 	 3.0618975665 	 3.0618975665
epoch_time;  30.935078382492065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17308533191680908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2375372052192688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.937422752380371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.09201717376709
19 2.4598893648 	 3.0920172614 	 3.0920172614
epoch_time;  31.15339469909668
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17307034134864807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23754264414310455
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9354617595672607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0927469730377197
It took 673.286167383194 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▂▂▁▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▄▃▂▂▂▃▄▂▂▂▂▂▁▁▂▂▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▃▂▁▁▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▄▄▂▂▃▄▄▃▃▂▁▂▁▁▂▂▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.96937
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29117
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.8705
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18666
wandb:                         Train loss 2.45121
wandb: 
wandb: 🚀 View run festive-wonton-1287 at: https://wandb.ai/nreints/thesis/runs/uwfa3tmx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_153041-uwfa3tmx/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154151-ezd45vi7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-lantern-1293
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/ezd45vi7
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26397186517715454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44313642382621765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.687407493591309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.98583984375
0 5.0713145179 	 4.9858398438 	 4.9858398438
epoch_time;  31.212298154830933
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20880283415317535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3054872751235962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8315672874450684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.950981855392456
1 2.8368264562 	 3.9509818412 	 3.9509818412
epoch_time;  31.01422429084778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2060415893793106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31032952666282654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2788784503936768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.401211738586426
2 2.7095265424 	 3.4012117953 	 3.4012117953
epoch_time;  31.19705367088318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20390449464321136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3016149401664734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.046595573425293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1485095024108887
3 2.6521363049 	 3.1485094225 	 3.1485094225
epoch_time;  31.07356905937195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20428740978240967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28699925541877747
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.930142879486084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9964442253112793
4 2.6088465521 	 2.9964441248 	 2.9964441248
epoch_time;  30.76077103614807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18013998866081238
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23718172311782837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9129421710968018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9482364654541016
5 2.5904145134 	 2.9482365789 	 2.9482365789
epoch_time;  31.101696729660034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17813841998577118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2557738125324249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.031317710876465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.086322784423828
6 2.5643931229 	 3.0863228463 	 3.0863228463
epoch_time;  30.85820722579956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18461845815181732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2678781747817993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8487160205841064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.902438163757324
7 2.5471240995 	 2.9024381071 	 2.9024381071
epoch_time;  31.021116971969604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20590057969093323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2965548634529114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9446895122528076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.028634786605835
8 2.5301362427 	 3.028634726 	 3.028634726
epoch_time;  31.069451332092285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20471304655075073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31151726841926575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.871403217315674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.978210687637329
9 2.5176817649 	 2.9782107791 	 2.9782107791
epoch_time;  30.51792550086975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18333478271961212
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24762676656246185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.911463975906372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.961977005004883
10 2.5079652325 	 2.9619770772 	 2.9619770772
epoch_time;  30.7446768283844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18506205081939697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2606688141822815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.970796585083008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.046781539916992
11 2.5001474431 	 3.0467816327 	 3.0467816327
epoch_time;  31.07820987701416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.181169331073761
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23708510398864746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.863051414489746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9114956855773926
12 2.4953340545 	 2.9114957242 	 2.9114957242
epoch_time;  30.927128314971924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16719473898410797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24459345638751984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9362852573394775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0038578510284424
13 2.4865797039 	 3.0038577518 	 3.0038577518
epoch_time;  30.975531816482544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16798925399780273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24598711729049683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.84810471534729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9081315994262695
14 2.4825651857 	 2.9081315324 	 2.9081315324
epoch_time;  31.068026065826416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16018898785114288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22073808312416077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8383798599243164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.877927780151367
15 2.4699675625 	 2.877927708 	 2.877927708
epoch_time;  30.905430555343628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1671040803194046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2254551500082016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7965850830078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8271074295043945
16 2.46637653 	 2.8271075274 	 2.8271075274
epoch_time;  30.786726713180542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1736980527639389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24164119362831116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8845577239990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.953866958618164
17 2.4637093308 	 2.9538669895 	 2.9538669895
epoch_time;  30.91726016998291
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17467814683914185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2534579634666443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.846123218536377
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9220921993255615
18 2.4609041104 	 2.9220920872 	 2.9220920872
epoch_time;  30.711625814437866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1866258829832077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2912127375602722
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.87109637260437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.967771053314209
19 2.4512098928 	 2.9677711281 	 2.9677711281
epoch_time;  30.84633445739746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18666285276412964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2911660671234131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8705029487609863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.969367265701294
It took 669.8719718456268 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▄▄▂▂▂▂▃▁▂▁▁▁▁▁▂▂▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▃▅▂▂▂▂▂▂▃▁▃▂▂▃▃▃▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▄▃▃▂▂▂▃▁▂▁▁▁▁▁▂▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▃▄▂▂▂▂▂▂▃▁▃▂▂▃▃▃▁▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.99951
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23709
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.99635
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16498
wandb:                         Train loss 2.44975
wandb: 
wandb: 🚀 View run twinkling-lantern-1293 at: https://wandb.ai/nreints/thesis/runs/ezd45vi7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154151-ezd45vi7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155302-f7kwm1ix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-fish-1300
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/f7kwm1ix
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29868459701538086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48706528544425964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7550482749938965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.9104905128479
0 5.1461237014 	 4.9104904587 	 4.9104904587
epoch_time;  30.855594396591187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22202560305595398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3431224226951599
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.027981758117676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.068244457244873
1 2.83238432 	 4.0682445629 	 4.0682445629
epoch_time;  30.875348806381226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2003515511751175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29515451192855835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7983810901641846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.830996513366699
2 2.702872264 	 3.8309966216 	 3.8309966216
epoch_time;  30.838356733322144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22068287432193756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37067899107933044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.54009747505188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6692538261413574
3 2.6404289602 	 3.6692537875 	 3.6692537875
epoch_time;  31.222366333007812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18435567617416382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27438703179359436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3467795848846436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3698954582214355
4 2.5992085348 	 3.3698954814 	 3.3698954814
epoch_time;  30.95888590812683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17951582372188568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27590927481651306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2888028621673584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3511900901794434
5 2.5724891038 	 3.3511900206 	 3.3511900206
epoch_time;  31.09575581550598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17888228595256805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27945762872695923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2272183895111084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2708122730255127
6 2.5493064886 	 3.2708123284 	 3.2708123284
epoch_time;  30.90310835838318
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18169188499450684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2779216468334198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1495025157928467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1783673763275146
7 2.5367358084 	 3.178367491 	 3.178367491
epoch_time;  30.783524751663208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1793166846036911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2657358646392822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3489878177642822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.383418321609497
8 2.5208985697 	 3.3834182327 	 3.3834182327
epoch_time;  30.818896055221558
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18204529583454132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27522775530815125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.964994430541992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9930195808410645
9 2.5044636245 	 2.9930195576 	 2.9930195576
epoch_time;  30.663782358169556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1886652410030365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.298235148191452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.111741781234741
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1742873191833496
10 2.4986592927 	 3.1742873733 	 3.1742873733
epoch_time;  30.713377952575684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1579631119966507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22678357362747192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.96175479888916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.976609230041504
11 2.4924923039 	 2.9766093486 	 2.9766093486
epoch_time;  30.77497434616089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19838611781597137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31150496006011963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.972909927368164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.045945167541504
12 2.4824917948 	 3.0459452861 	 3.0459452861
epoch_time;  30.69235634803772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17821599543094635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26280343532562256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.945441484451294
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9707374572753906
13 2.4754548586 	 2.9707374367 	 2.9707374367
epoch_time;  30.882519483566284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17327497899532318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24766583740711212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9445886611938477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9599783420562744
14 2.4713583541 	 2.9599784232 	 2.9599784232
epoch_time;  30.805680990219116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20226185023784637
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2966485917568207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0326948165893555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0781033039093018
15 2.4659462672 	 3.0781032253 	 3.0781032253
epoch_time;  30.83326506614685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1958671510219574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2886940538883209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0952858924865723
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1262638568878174
16 2.4572086115 	 3.1262639226 	 3.1262639226
epoch_time;  31.089114665985107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19585397839546204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2860864996910095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0861289501190186
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.114161729812622
17 2.4557472389 	 3.1141618058 	 3.1141618058
epoch_time;  31.034566640853882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1660701483488083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25513455271720886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9889132976531982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.032778263092041
18 2.4507409511 	 3.0327781883 	 3.0327781883
epoch_time;  30.83881163597107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16493800282478333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23719535768032074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.997370958328247
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.998594284057617
19 2.4497475285 	 2.9985942119 	 2.9985942119
epoch_time;  30.89022183418274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16497662663459778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23708924651145935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.996347427368164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9995076656341553
It took 671.5230555534363 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▁▁▂▂▁▂▂▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▃█▄▃▃▁▂▁▂▂▃▃▁▁▃▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▃▃▂▂▂▂▂▂▁▁▂▂▁▂▂▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇▇▃█▄▃▃▁▃▁▂▂▂▂▂▁▂▂▂▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.2598
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27694
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.16274
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18196
wandb:                         Train loss 2.47165
wandb: 
wandb: 🚀 View run glistening-fish-1300 at: https://wandb.ai/nreints/thesis/runs/f7kwm1ix
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155302-f7kwm1ix/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26964446902275085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4146168529987335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.55162239074707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.837733745574951
0 5.2697289427 	 4.8377339131 	 4.8377339131
epoch_time;  31.0837082862854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26344117522239685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39919108152389526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7424352169036865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9012863636016846
1 2.8584182729 	 3.9012863572 	 3.9012863572
epoch_time;  30.96121859550476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19532909989356995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2962108850479126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.521927833557129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6503217220306396
2 2.7339728374 	 3.6503216718 	 3.6503216718
epoch_time;  30.640320539474487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2898789644241333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4136981964111328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.456519603729248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6156601905822754
3 2.6583227985 	 3.6156603014 	 3.6156603014
epoch_time;  30.70225954055786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22357594966888428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3213493525981903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2855870723724365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3587841987609863
4 2.6222037055 	 3.3587841137 	 3.3587841137
epoch_time;  30.888152360916138
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20619653165340424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28501802682876587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.172457218170166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2513086795806885
5 2.5925891336 	 3.2513087917 	 3.2513087917
epoch_time;  31.194703340530396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20255079865455627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2942853569984436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2887630462646484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3763387203216553
6 2.5663976547 	 3.3763388144 	 3.3763388144
epoch_time;  31.123918533325195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1711065024137497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24773778021335602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2833964824676514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.348301649093628
7 2.559469152 	 3.3483015731 	 3.3483015731
epoch_time;  30.909738540649414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1896396279335022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2753799259662628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.303072929382324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.404773712158203
8 2.5439390313 	 3.4047736091 	 3.4047736091
epoch_time;  30.835914611816406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16214990615844727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24407702684402466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.177443742752075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.271134376525879
9 2.5345516011 	 3.2711343301 	 3.2711343301
epoch_time;  30.839698553085327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1730979084968567
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2557724714279175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0770840644836426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1695404052734375
10 2.5197186945 	 3.1695404878 	 3.1695404878
epoch_time;  30.96631932258606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17732883989810944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2666657269001007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.064865827560425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.137284994125366
11 2.5190770195 	 3.1372848923 	 3.1372848923
epoch_time;  30.937286853790283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18497803807258606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2869676649570465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2203736305236816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.329887866973877
12 2.5078817586 	 3.3298877613 	 3.3298877613
epoch_time;  31.2538001537323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1874908208847046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28490784764289856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1636576652526855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2581934928894043
13 2.5000269431 	 3.2581935573 	 3.2581935573
epoch_time;  31.012063026428223
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17671999335289001
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25180625915527344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0148892402648926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.077575922012329
14 2.4862919254 	 3.0775760135 	 3.0775760135
epoch_time;  30.916505813598633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1677839159965515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24284148216247559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.195354461669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.271909713745117
15 2.4941474021 	 3.2719096416 	 3.2719096416
epoch_time;  30.84540629386902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1856551468372345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28881940245628357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1614511013031006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2699062824249268
16 2.480507429 	 3.2699063688 	 3.2699063688
epoch_time;  30.914726734161377
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17521585524082184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2564122676849365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.14263916015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2204790115356445
17 2.4848226187 	 3.2204791095 	 3.2204791095
epoch_time;  31.071043968200684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.171390101313591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24678252637386322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.116206407546997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.187375545501709
18 2.4753317139 	 3.1873756202 	 3.1873756202
epoch_time;  30.980228662490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18195810914039612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27679121494293213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1639790534973145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2600088119506836
19 2.4716550586 	 3.2600087759 	 3.2600087759
epoch_time;  30.89277672767639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1819584220647812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27693507075309753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1627418994903564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.2598044872283936
It took 668.8172471523285 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138504
Array Job ID: 2137927_9
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 10:06:57
CPU Efficiency: 29.32% of 1-10:30:00 core-walltime
Job Wall-clock time: 01:55:00
Memory Utilized: 6.06 GB
Memory Efficiency: 19.39% of 31.25 GB
