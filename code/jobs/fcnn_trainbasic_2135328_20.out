wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170356-eb0ast2f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-envelope-1149
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/eb0ast2f
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–†â–ƒâ–ƒâ–…â–ƒâ–ˆâ–„â–ƒâ–„â–‚â–‚â–ƒâ–â–‚â–â–„â–„â–„â–„
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–…â–…â–…â–ƒâ–ƒâ–…â–‚â–ˆâ–„â–ƒâ–„â–‚â–ƒâ–‚â–â–‚â–‚â–…â–„â–„â–„
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–…â–…â–ƒâ–‚â–…â–‚â–ˆâ–„â–ƒâ–„â–‚â–‚â–ƒâ–â–‚â–‚â–„â–„â–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–„â–„â–‚â–‚â–„â–‚â–ˆâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–â–â–â–„â–ƒâ–ƒâ–ƒ
wandb:                         Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 14.55771
wandb:  Test loss t(-10, 10)_r(0, 0)_none 8.37248
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.84223
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.03361
wandb:                         Train loss 10.89761
wandb: 
wandb: ðŸš€ View run beaming-envelope-1149 at: https://wandb.ai/nreints/thesis/runs/eb0ast2f
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170356-eb0ast2f/logs
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.3302884101867676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.927887916564941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.915565490722656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 18.797386169433594
0 20.9494052027 	 18.7973870355 	 18.7973870355
epoch_time;  39.783169984817505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.233870029449463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.61677360534668
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.787497520446777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.51802635192871
1 13.8037644931 	 16.5180268159 	 16.5180268159
epoch_time;  37.827632904052734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.1403896808624268
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.632820129394531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.074025630950928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 17.13436508178711
2 12.8840449391 	 17.1343657622 	 17.1343657622
epoch_time;  38.51096153259277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5500438213348389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.303705215454102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.412237644195557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.213878631591797
3 12.4155091613 	 14.2138790646 	 14.2138790646
epoch_time;  38.72221851348877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4008209705352783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.170186996459961
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8454766273498535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.766851425170898
4 12.1291046039 	 13.7668509818 	 13.7668509818
epoch_time;  38.20965003967285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.208862066268921
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.896472930908203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.725568771362305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.064279556274414
5 11.9195053318 	 16.0642789274 	 16.0642789274
epoch_time;  38.05331301689148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.397452473640442
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.997381210327148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.089027404785156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.568982124328613
6 11.6576926942 	 13.5689822635 	 13.5689822635
epoch_time;  37.914549350738525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.7182376384735107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 11.320022583007812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.855573654174805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 19.261117935180664
7 11.4824655129 	 19.261118296 	 19.261118296
epoch_time;  38.0941047668457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8888335227966309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.39645767211914
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.051559925079346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.242033958435059
8 11.7098359745 	 15.2420344172 	 15.2420344172
epoch_time;  38.38133645057678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8178216218948364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.808353424072266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.501150608062744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.171101570129395
9 11.2919875592 	 14.1711016681 	 14.1711016681
epoch_time;  38.1257746219635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.8168411254882812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.171911239624023
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.180085182189941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.194418907165527
10 11.3807125931 	 15.1944190773 	 15.1944190773
epoch_time;  38.16397547721863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3184428215026855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.770354747772217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.710050106048584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.677541732788086
11 11.0880936072 	 12.6775417019 	 12.6775417019
epoch_time;  37.82025694847107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.5051252841949463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 7.137810230255127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.863207817077637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.913783073425293
12 11.2097024162 	 12.913782728 	 12.913782728
epoch_time;  37.7526421546936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4148356914520264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.979054927825928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.418434143066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.769071578979492
13 11.1939385685 	 13.7690720017 	 13.7690720017
epoch_time;  38.073975563049316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2149035930633545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.8849616050720215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.069884300231934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.779451370239258
14 11.0959953665 	 11.7794512774 	 11.7794512774
epoch_time;  37.83394241333008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2632783651351929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.405890941619873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.675549030303955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.734678268432617
15 11.0567153383 	 12.7346785262 	 12.7346785262
epoch_time;  38.08830189704895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.3446768522262573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.406570911407471
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.451643466949463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.168133735656738
16 10.8363097442 	 12.1681337099 	 12.1681337099
epoch_time;  38.11514949798584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.400033950805664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 9.13127613067627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.440188407897949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.351899147033691
17 10.8296543088 	 15.3518990182 	 15.3518990182
epoch_time;  38.07057213783264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.7947962284088135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.001143455505371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.148687362670898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.632915496826172
18 10.8782973034 	 14.632915435 	 14.632915435
epoch_time;  37.65100455284119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.0336880683898926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.372725486755371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.84414529800415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.488945007324219
19 10.8976077194 	 14.4889450486 	 14.4889450486
epoch_time;  37.978249311447144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.033612012863159
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.372481346130371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.842234134674072
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.55771255493164
It took 825.168110370636 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn9: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135411.0

JOB STATISTICS
==============
Job ID: 2135411
Array Job ID: 2135328_20
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:04:15
CPU Efficiency: 25.29% of 04:14:06 core-walltime
Job Wall-clock time: 00:14:07
Memory Utilized: 3.87 GB
Memory Efficiency: 12.39% of 31.25 GB
