/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_073249-soqseiqj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-monkey-1585
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/soqseiqj
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153065e13f70>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1184f0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f118760>, <torch.utils.data.dataloader.DataLoader object at 0x15305f118d60>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09009891003370285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19787150621414185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.630391538143158
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.148566484451294
0 4.5290957963 	 2.1485665774
epoch_time;  48.69975256919861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08730580657720566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15660880506038666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46530815958976746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6657835245132446
1 0.5165399267 	 1.6657834701
epoch_time;  48.6421115398407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05062033608555794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1140947937965393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4027671813964844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4538692235946655
2 0.4048486402 	 1.4538692232
epoch_time;  47.45249819755554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031919751316308975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08127854764461517
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3249022662639618
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.288317322731018
3 0.3584993003 	 1.2883173548
epoch_time;  48.46126461029053
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04895063489675522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1055358275771141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4420115351676941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4578843116760254
4 0.3164469093 	 1.4578843592
epoch_time;  47.77580904960632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.056834299117326736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12217221409082413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5073407888412476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7345595359802246
5 0.285490511 	 1.7345595806
epoch_time;  47.46267557144165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02599499747157097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05659140273928642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.318444162607193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0938702821731567
6 0.2778853504 	 1.0938703185
epoch_time;  48.57419753074646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026276133954524994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06275304406881332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2816247045993805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0983774662017822
7 0.2599280081 	 1.0983774237
epoch_time;  50.08929085731506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017865611240267754
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05060284584760666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31314754486083984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1147514581680298
8 0.2601508488 	 1.1147514412
epoch_time;  50.72488856315613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027649136260151863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06618905067443848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32966339588165283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1366395950317383
9 0.2400295538 	 1.1366395518
epoch_time;  47.69731688499451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03456656262278557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0726827085018158
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29145798087120056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9699086546897888
10 0.2525764136 	 0.9699086538
epoch_time;  48.48676800727844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015420326963067055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04729965701699257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24568210542201996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.005064845085144
11 0.232047255 	 1.0050648116
epoch_time;  47.61850190162659
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04751725122332573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10265130549669266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2952948808670044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1393245458602905
12 0.2275010137 	 1.1393245455
epoch_time;  47.70519495010376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019406633451581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05385599285364151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22511431574821472
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9392024278640747
13 0.2385309351 	 0.9392024383
epoch_time;  47.542304277420044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02060949057340622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051298584789037704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2626029849052429
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9791183471679688
14 0.2031877216 	 0.9791183241
epoch_time;  47.00963568687439
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0300743505358696
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06776659935712814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.255105584859848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9786753058433533
15 0.2250536472 	 0.9786753121
epoch_time;  42.84781765937805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04237652197480202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08871672302484512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2821870446205139
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0040090084075928
16 0.2208930917 	 1.0040090509
epoch_time;  43.01613974571228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05877064913511276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10215192288160324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3086546063423157
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9873356819152832
17 0.2056830707 	 0.9873356661
epoch_time;  43.05141639709473
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021255606785416603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05926598981022835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2460016906261444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0398979187011719
18 0.2079450678 	 1.0398979072
epoch_time;  43.019819021224976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009296532720327377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03622895106673241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23570553958415985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0144333839416504
19 0.2515867523 	 1.0144333393
epoch_time;  42.97319960594177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030872957780957222
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0673060268163681
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2647993266582489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0788365602493286
20 0.2112305935 	 1.0788365851
epoch_time;  43.13688611984253
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024204997345805168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05915750563144684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23779356479644775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9834685921669006
21 0.1903705929 	 0.9834686003
epoch_time;  43.32045269012451
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01170046441257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.036962710320949554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22170469164848328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9043087959289551
22 0.19542928 	 0.9043087686
epoch_time;  43.0271897315979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01977420039474964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04603607952594757
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–„â–†â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–„â–…â–‚â–‚â–‚â–‚â–ƒâ–â–„â–‚â–‚â–‚â–ƒâ–„â–‚â–â–‚â–‚â–â–â–â–ƒâ–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–…â–†â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ˆâ–…â–ƒâ–„â–…â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–„â–‚â–‚â–ƒâ–„â–…â–‚â–â–ƒâ–‚â–â–‚â–‚â–„â–â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.81528
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05457
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.208
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02495
wandb:                         Train loss 0.17481
wandb: 
wandb: ğŸš€ View run festive-monkey-1585 at: https://wandb.ai/nreints/thesis/runs/soqseiqj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_073249-soqseiqj/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_075729-e1h1hjfp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-cake-1595
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/e1h1hjfp
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22204352915287018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8465259075164795
23 0.1951937564 	 0.846525901
epoch_time;  43.26768612861633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01579648070037365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0398288369178772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21171586215496063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8435440063476562
24 0.187084686 	 0.8435440294
epoch_time;  43.02582335472107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.047634005546569824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08302748203277588
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2970215976238251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9942203760147095
25 0.1730043583 	 0.9942203764
epoch_time;  43.07634234428406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014801375567913055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04227769002318382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24404309689998627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8854454159736633
26 0.1862748151 	 0.8854454017
epoch_time;  43.343202352523804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011670384556055069
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03685547411441803
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23430386185646057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8598635792732239
27 0.1943109644 	 0.8598635578
epoch_time;  45.967698097229004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014259801246225834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04376442730426788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20901615917682648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8190448880195618
28 0.1746711702 	 0.819044868
epoch_time;  45.411293029785156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02494647726416588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05400605872273445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20889772474765778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8116925358772278
29 0.174812727 	 0.8116925289
epoch_time;  43.264933347702026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024950718507170677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054573409259319305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2080039381980896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8152846693992615
It took  1481.1644260883331  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f1184c0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18e4a0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f01f0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f02b0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08606438338756561
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17570431530475616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6553502678871155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.01383113861084
0 4.4312081579 	 2.0138311934
epoch_time;  42.9068341255188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06991301476955414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13183189928531647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.473736435174942
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4877114295959473
1 0.5010996955 	 1.4877113734
epoch_time;  43.084877252578735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05011517181992531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10167359560728073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3900161683559418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1787981986999512
2 0.3819082233 	 1.1787982491
epoch_time;  42.718467712402344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.041947636753320694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0939982682466507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5084649324417114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4050955772399902
3 0.3275502484 	 1.4050955873
epoch_time;  43.155083656311035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.037434160709381104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07915522158145905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4523007571697235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2264611721038818
4 0.2995631112 	 1.2264611743
epoch_time;  42.804497480392456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03250104933977127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06898628175258636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32262521982192993
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.071616768836975
5 0.2821745602 	 1.0716167346
epoch_time;  42.86256194114685
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027166493237018585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054847147315740585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2584550380706787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.835145890712738
6 0.2625181211 	 0.8351458869
epoch_time;  43.046647787094116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02278156392276287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058363351970911026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30477121472358704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9766263961791992
7 0.2487282879 	 0.9766263933
epoch_time;  42.98663592338562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01771240495145321
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04352348670363426
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2794686555862427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8885467648506165
8 0.2361312739 	 0.8885467621
epoch_time;  43.08789873123169
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0270729660987854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05196254327893257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2419653981924057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.762142539024353
9 0.2186809485 	 0.7621425387
epoch_time;  43.24368762969971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01223023608326912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.036001961678266525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21905307471752167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7738198637962341
10 0.22466311 	 0.7738198456
epoch_time;  43.209351539611816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025294771417975426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0493612214922905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.268850713968277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8383029103279114
11 0.2132837265 	 0.838302935
epoch_time;  42.96760010719299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03355478122830391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07162376493215561
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29616835713386536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8859347105026245
12 0.2183772281 	 0.8859346972
epoch_time;  43.04820108413696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03235584869980812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.060315683484077454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2610390782356262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.817762553691864
13 0.2127524296 	 0.8177625766
epoch_time;  42.99024939537048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017334412783384323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04352995753288269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23102983832359314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7767151594161987
14 0.2104680699 	 0.7767151432
epoch_time;  42.941187381744385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012417173944413662
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030544523149728775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21836179494857788
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7341787815093994
15 0.1902205924 	 0.7341788024
epoch_time;  43.13941502571106
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–…â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–„â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–†â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–â–â–…â–â–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.75516
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05298
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.25098
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02293
wandb:                         Train loss 0.16308
wandb: 
wandb: ğŸš€ View run lunar-cake-1595 at: https://wandb.ai/nreints/thesis/runs/e1h1hjfp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_075729-e1h1hjfp/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_082026-gd4lzhiw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-laughter-1602
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/gd4lzhiw
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010990826413035393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.031290605664253235
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23507896065711975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7720063924789429
16 0.1915070319 	 0.7720064008
epoch_time;  44.75755214691162
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024760732427239418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04975272715091705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22595970332622528
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7194214463233948
17 0.2059497357 	 0.7194214328
epoch_time;  45.05811548233032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019160587340593338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04160069301724434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20294851064682007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6799478530883789
18 0.210393936 	 0.6799478675
epoch_time;  43.20327186584473
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0298920888453722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.048369668424129486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21004991233348846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6798768639564514
19 0.1794850005 	 0.6798768749
epoch_time;  42.72706580162048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02958611212670803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0527888685464859
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22649817168712616
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7162046432495117
20 0.1875259319 	 0.7162046404
epoch_time;  42.73503661155701
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023853126913309097
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.054457493126392365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28372690081596375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8484786748886108
21 0.1766324733 	 0.8484786572
epoch_time;  43.08537316322327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02028113417327404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05081036314368248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24927562475204468
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8480256795883179
22 0.1672968666 	 0.8480256879
epoch_time;  43.180906772613525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015543939545750618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03122064284980297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.187315434217453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6754657030105591
23 0.1810441692 	 0.6754657019
epoch_time;  42.896565198898315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015245423652231693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03315560147166252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24531859159469604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6891492605209351
24 0.1600694563 	 0.68914924
epoch_time;  42.757591247558594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.049895189702510834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08974925428628922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24697758257389069
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7060733437538147
25 0.158495433 	 0.7060733576
epoch_time;  42.7821090221405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011644484475255013
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02904682792723179
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.18105237185955048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6192135214805603
26 0.1767883301 	 0.6192135076
epoch_time;  43.14989113807678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01729576475918293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.035967737436294556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17591862380504608
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6151770353317261
27 0.1633327695 	 0.6151770278
epoch_time;  42.86593699455261
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012368524447083473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.032788150012493134
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1818327158689499
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6464114785194397
28 0.1612874911 	 0.6464114924
epoch_time;  43.27529525756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022919490933418274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052967917174100876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25089237093925476
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7535568475723267
29 0.1630842626 	 0.7535568652
epoch_time;  43.25953221321106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022933263331651688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0529811829328537
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25097569823265076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7551604509353638
It took  1376.5050616264343  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f836740>, <torch.utils.data.dataloader.DataLoader object at 0x15305f119120>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18e740>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18e890>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13111574947834015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.264283686876297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9684215188026428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9014339447021484
0 5.2301072518 	 2.9014339389
epoch_time;  43.034361362457275
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13484586775302887
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23172257840633392
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6709205508232117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.170250415802002
1 0.548387757 	 2.1702504691
epoch_time;  43.1666476726532
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05247969552874565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12573064863681793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5279086232185364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6762359142303467
2 0.4042244275 	 1.6762359711
epoch_time;  43.06402254104614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045130155980587006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11254950612783432
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4769081473350525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5118987560272217
3 0.3526152063 	 1.5118988129
epoch_time;  43.138182640075684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.033861905336380005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09202778339385986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4027501940727234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3162868022918701
4 0.3186088753 	 1.3162867612
epoch_time;  43.14431834220886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04856251925230026
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10144447535276413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4260505139827728
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.261296272277832
5 0.3110613538 	 1.2612962982
epoch_time;  44.19462728500366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.055558301508426666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11393745988607407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35622990131378174
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.208405613899231
6 0.2599786705 	 1.2084056474
epoch_time;  44.46117115020752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03026541694998741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07691193372011185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3914516568183899
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0748621225357056
7 0.2433338844 	 1.0748621085
epoch_time;  42.69710946083069
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025176508352160454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07303401827812195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.359001487493515
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–â–â–‚â–â–‚â–‚â–â–ƒâ–â–â–â–â–ƒâ–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ˆâ–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–â–â–â–â–‚â–‚â–â–ƒâ–â–â–â–â–„â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.9133
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.06975
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.29821
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0323
wandb:                         Train loss 0.16345
wandb: 
wandb: ğŸš€ View run lambent-laughter-1602 at: https://wandb.ai/nreints/thesis/runs/gd4lzhiw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_082026-gd4lzhiw/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_084310-7gmv6iop
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-rocket-1609
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/7gmv6iop
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0677311420440674
8 0.230830664 	 1.067731137
epoch_time;  42.79434156417847
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04642904922366142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.092292420566082
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39571765065193176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.071990728378296
9 0.229305663 	 1.0719906902
epoch_time;  43.050942182540894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01654759794473648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0609920434653759
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3703453242778778
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0919171571731567
10 0.226169745 	 1.0919171014
epoch_time;  43.38211917877197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.035695336759090424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08072419464588165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3360027074813843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9591261744499207
11 0.2371015296 	 0.9591261757
epoch_time;  43.15946841239929
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022742001339793205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06551067531108856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3316674530506134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9108637571334839
12 0.2155831948 	 0.9108637784
epoch_time;  42.92790102958679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04696961119771004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1054694652557373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4030807316303253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1133583784103394
13 0.2267132397 	 1.1133584198
epoch_time;  42.65957427024841
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05004103109240532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09472600370645523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3716810643672943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9769876003265381
14 0.2074149953 	 0.9769876255
epoch_time;  42.707950830459595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0145057812333107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.048210978507995605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3314618468284607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9509152173995972
15 0.1950433169 	 0.9509151954
epoch_time;  43.087769508361816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01573093794286251
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04928980767726898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3612442910671234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9795345067977905
16 0.1921807173 	 0.9795345064
epoch_time;  43.079567193984985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01679539680480957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06119849160313606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37743377685546875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9731935858726501
17 0.2097502715 	 0.9731935807
epoch_time;  42.92401695251465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0162727702409029
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05355134978890419
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3112722337245941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9277918338775635
18 0.1926675778 	 0.9277918144
epoch_time;  42.84683704376221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02359628863632679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.061259761452674866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3309231400489807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8547006249427795
19 0.1893295877 	 0.8547006475
epoch_time;  43.0312077999115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02446148358285427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06301380693912506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3557192385196686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9094757437705994
20 0.1888057642 	 0.9094757357
epoch_time;  42.73572087287903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016714468598365784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05388491973280907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32366883754730225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8994319438934326
21 0.1903832065 	 0.8994319489
epoch_time;  43.00255823135376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.053518276661634445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10358945280313492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32883670926094055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9491689801216125
22 0.1836389257 	 0.949168963
epoch_time;  42.895474433898926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01948460377752781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05128439888358116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29012662172317505
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8524569869041443
23 0.1805622872 	 0.8524570062
epoch_time;  43.17607092857361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012278673239052296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04579265043139458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29457810521125793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8749299049377441
24 0.1869363027 	 0.8749299294
epoch_time;  43.21250557899475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016841894015669823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05459555611014366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3032423257827759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8183130621910095
25 0.1827472511 	 0.8183130915
epoch_time;  43.250895977020264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012842054478824139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04341147840023041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3110746443271637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9120298027992249
26 0.1959305445 	 0.912029808
epoch_time;  44.232287883758545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06173133850097656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10944461077451706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38085171580314636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0186383724212646
27 0.1746790942 	 1.0186384034
epoch_time;  44.04313921928406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010946905240416527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04120989143848419
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3279430568218231
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.945371150970459
28 0.1661026844 	 0.945371138
epoch_time;  42.796475648880005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.032310813665390015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06972865760326385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2979266941547394
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.915196418762207
29 0.1634479208 	 0.9151964447
epoch_time;  42.98261833190918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03230318799614906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06975256651639938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29820770025253296
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9132988452911377
It took  1363.9455275535583  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f18eb30>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f0fa0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f0eb0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f0490>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06240900605916977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1614874303340912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6637077927589417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.124584436416626
0 3.4000594365 	 2.1245843709
epoch_time;  42.89839196205139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.049271706491708755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12721772491931915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5659211874008179
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6602058410644531
1 0.479646046 	 1.6602058526
epoch_time;  43.08265137672424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030286284163594246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08533231168985367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42833569645881653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.39419424533844
2 0.3781723034 	 1.3941942659
epoch_time;  43.13500738143921
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023593377321958542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0664285346865654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4169369637966156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1680631637573242
3 0.3280945517 	 1.1680631609
epoch_time;  43.21243214607239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04547176510095596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10019953548908234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.527538001537323
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3457529544830322
4 0.2914020963 	 1.345752912
epoch_time;  42.903191566467285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02265355736017227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06216451898217201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3995073139667511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1052813529968262
5 0.3009633621 	 1.1052814034
epoch_time;  42.91067051887512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02507190778851509
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06625555455684662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3939497172832489
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.188698172569275
6 0.2607913483 	 1.1886981146
epoch_time;  42.84028720855713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029247194528579712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06259334087371826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34096604585647583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0497032403945923
7 0.2622875875 	 1.0497032696
epoch_time;  42.834688663482666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023939544335007668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05839147791266441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36265963315963745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0092682838439941
8 0.2395622259 	 1.0092683083
epoch_time;  42.77515912055969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03613099455833435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0684552937746048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33227962255477905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9761448502540588
9 0.2409874269 	 0.9761448425
epoch_time;  43.25203561782837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04494735226035118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08781711757183075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36236944794654846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.011337161064148
10 0.222803201 	 1.011337142
epoch_time;  43.07819128036499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06621068716049194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12310367822647095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3726150095462799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0026518106460571
11 0.218346803 	 1.0026518024
epoch_time;  42.822911977767944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027127454057335854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06185730919241905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32288289070129395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9188406467437744
12 0.2131615407 	 0.9188406676
epoch_time;  43.16204023361206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02299058996140957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.053498346358537674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2904626727104187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8949937224388123
13 0.2029870818 	 0.8949937158
epoch_time;  42.85174250602722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04285392537713051
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07593999058008194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32978588342666626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8729197382926941
14 0.1997473209 	 0.8729197338
epoch_time;  42.9126718044281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02843668684363365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05685326084494591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3654453456401825
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.920423150062561
15 0.2030307674 	 0.9204231562
epoch_time;  44.204018354415894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013029844500124454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03863288089632988
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29618316888809204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8619393706321716
16 0.1914365828 	 0.8619393985
epoch_time;  44.31716966629028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013503246009349823
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04230042174458504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3329150974750519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8490484356880188
17 0.1899231211 	 0.8490484416
epoch_time;  42.92723226547241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01901158131659031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044278211891651154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32116204500198364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8617587089538574
18 0.198126193 	 0.8617586902
epoch_time;  42.778536319732666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017123974859714508
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04115218669176102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.291710764169693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7987724542617798
19 0.1795852742 	 0.7987724373
epoch_time;  42.74138784408569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01642598770558834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04061968997120857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2652865946292877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7848861217498779
20 0.1763319063 	 0.7848861095
epoch_time;  43.03875684738159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018049655482172966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04649246484041214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29338720440864563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8056944012641907
21 0.193254146 	 0.8056943957
epoch_time;  43.05990958213806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018648909404873848
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.047104038298130035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2894761860370636
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8642195463180542
22 0.1744310147 	 0.8642195503
epoch_time;  42.760589599609375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015564732253551483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0396554060280323
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2777373492717743
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7643252611160278
23 0.1667969189 	 0.764325237
epoch_time;  42.92830538749695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012449333444237709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029842155054211617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24009718000888824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6939548254013062
24 0.1700441301 	 0.6939548366
epoch_time;  42.69073462486267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018940230831503868
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04831263795495033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2966780364513397
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8682786822319031
25 0.1725240126 	 0.8682786648
epoch_time;  43.080350160598755
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–…â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–†â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–…â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–„â–„â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–…â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ƒâ–‚â–…â–‚â–ƒâ–ƒâ–‚â–„â–…â–ˆâ–ƒâ–‚â–…â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–„â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.04233
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.06221
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.33216
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02218
wandb:                         Train loss 0.24593
wandb: 
wandb: ğŸš€ View run auspicious-rocket-1609 at: https://wandb.ai/nreints/thesis/runs/7gmv6iop
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_084310-7gmv6iop/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_090547-9yc2jruw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-orchid-1616
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/9yc2jruw
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01642964407801628
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0430423803627491
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3289962112903595
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9190459847450256
26 0.2822341168 	 0.9190459928
epoch_time;  43.33233976364136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015027093701064587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03997715562582016
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2921076714992523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7737091183662415
27 0.1770670542 	 0.7737091157
epoch_time;  42.70589232444763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03224018216133118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11131783574819565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4591165781021118
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.472109317779541
28 0.3087917793 	 1.4721093307
epoch_time;  43.17619562149048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02217693068087101
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06252508610486984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3318715989589691
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0415353775024414
29 0.245929126 	 1.0415353458
epoch_time;  42.958173990249634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02217835746705532
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06220637261867523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3321618437767029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0423295497894287
It took  1357.463633775711  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153065d65870>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f2950>, <torch.utils.data.dataloader.DataLoader object at 0x153034231000>, <torch.utils.data.dataloader.DataLoader object at 0x153034233370>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0941772535443306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17315472662448883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6379778385162354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.144472360610962
0 4.1610799554 	 2.1444724276
epoch_time;  43.469404220581055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.060624055564403534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11627629399299622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5154762864112854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.749429702758789
1 0.4815189613 	 1.7494296624
epoch_time;  42.69169807434082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03454780951142311
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07396399229764938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35025495290756226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2470766305923462
2 0.3977088206 	 1.2470766742
epoch_time;  42.72018837928772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022078536450862885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05590946972370148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3383488357067108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.182245135307312
3 0.3296237111 	 1.1822451681
epoch_time;  42.6617214679718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03890762850642204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07037261873483658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39866918325424194
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1850512027740479
4 0.301991257 	 1.1850512179
epoch_time;  44.53284478187561
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02485249936580658
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05478599667549133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34740376472473145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0573004484176636
5 0.2737213759 	 1.057300487
epoch_time;  43.79020380973816
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024059444665908813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0553802065551281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24213199317455292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8714147806167603
6 0.2475337361 	 0.8714147839
epoch_time;  43.343974351882935
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03244338184595108
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06782439351081848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3692018687725067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0575777292251587
7 0.2623717707 	 1.0575777267
epoch_time;  42.77820110321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026469089090824127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05729498341679573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28866541385650635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9098310470581055
8 0.2521733321 	 0.9098310672
epoch_time;  42.809136629104614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01709849387407303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03876224532723427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31081023812294006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9534118175506592
9 0.2346594765 	 0.9534118284
epoch_time;  42.946916341781616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015754394233226776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.037595391273498535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2654154896736145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7854238748550415
10 0.2206679639 	 0.7854239011
epoch_time;  42.99159646034241
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03293333947658539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06018946319818497
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3137265741825104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8865771293640137
11 0.2211124209 	 0.8865771337
epoch_time;  42.986740589141846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024915996938943863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04838801547884941
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31058821082115173
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9099423289299011
12 0.2084792294 	 0.9099423504
epoch_time;  42.74768424034119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02561001107096672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05675353854894638
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3332233726978302
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0473583936691284
13 0.2193228341 	 1.0473583947
epoch_time;  43.03677225112915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014747079461812973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.039756856858730316
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32974398136138916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9134666323661804
14 0.201303163 	 0.9134666235
epoch_time;  43.12316679954529
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02422802895307541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05013105273246765
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34790295362472534
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9736147522926331
15 0.1941807622 	 0.9736147417
epoch_time;  42.835817098617554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020274527370929718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.046187371015548706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3195328414440155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8333649635314941
16 0.1956252763 	 0.833364988
epoch_time;  42.399524211883545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01093868725001812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03104536049067974
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24713893234729767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7054858207702637
17 0.1939944225 	 0.7054858251
epoch_time;  42.88246035575867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009859413839876652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.024954654276371002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21349000930786133
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–„â–‚â–‚â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–‚â–â–ƒâ–â–„â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–ƒâ–ƒâ–„â–ƒâ–â–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–„â–„
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–â–ƒâ–â–ƒâ–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.00615
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05104
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.38249
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02112
wandb:                         Train loss 0.19615
wandb: 
wandb: ğŸš€ View run crimson-orchid-1616 at: https://wandb.ai/nreints/thesis/runs/9yc2jruw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_090547-9yc2jruw/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_092828-378r5ugb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-kumquat-1626
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/378r5ugb
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6808518767356873
18 0.1774601673 	 0.6808518701
epoch_time;  42.87037682533264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013773641549050808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03172615170478821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23984743654727936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7162057161331177
19 0.1823031868 	 0.7162057006
epoch_time;  43.21436619758606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014722040854394436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0351770743727684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2550712525844574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7611179351806641
20 0.1897206794 	 0.7611179409
epoch_time;  43.05018734931946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017899662256240845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03526824712753296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2596518397331238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.760540246963501
21 0.1822703359 	 0.7605402275
epoch_time;  42.897002935409546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025199448689818382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0452740304172039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2717575430870056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7027378678321838
22 0.1820659421 	 0.7027378601
epoch_time;  42.75320315361023
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011036301031708717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029155321419239044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24583645164966583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7472130060195923
23 0.1842853047 	 0.7472130352
epoch_time;  42.76415801048279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.034970302134752274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06402818113565445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3434463143348694
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8926069140434265
24 0.1690444889 	 0.8926068908
epoch_time;  43.01855993270874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013384471647441387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03151871636509895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27275198698043823
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7665119171142578
25 0.1749536022 	 0.7665119459
epoch_time;  45.092366456985474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0370868518948555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08038517832756042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3998315930366516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2772506475448608
26 0.300107068 	 1.2772506299
epoch_time;  43.6655957698822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01960461027920246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.045172788202762604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30998092889785767
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8680603504180908
27 0.2415889766 	 0.8680603396
epoch_time;  42.88225769996643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015080728568136692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.041836950927972794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35356441140174866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8434242010116577
28 0.2096199769 	 0.8434241718
epoch_time;  42.953893184661865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021126724779605865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05105151608586311
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3826350271701813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0098260641098022
29 0.1961533621 	 1.0098260148
epoch_time;  42.98162364959717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021119287237524986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05103951320052147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.382491797208786
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.006148099899292
It took  1360.8258256912231  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153065d650c0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f1150>, <torch.utils.data.dataloader.DataLoader object at 0x15305f1f1030>, <torch.utils.data.dataloader.DataLoader object at 0x153065ddee30>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09131132811307907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15725810825824738
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5739650130271912
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2387137413024902
0 4.3521646273 	 2.2387136592
epoch_time;  42.89389777183533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08319603651762009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12409651279449463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41897615790367126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.636411190032959
1 0.530316407 	 1.6364111771
epoch_time;  42.58582925796509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04207240790128708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07969269156455994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3615083396434784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4827152490615845
2 0.4077960237 	 1.4827152494
epoch_time;  42.77827501296997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05013739690184593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09227192401885986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33195531368255615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3231757879257202
3 0.3448883449 	 1.3231758049
epoch_time;  42.81336998939514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06079060584306717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09327490627765656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37384334206581116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3467237949371338
4 0.3075881484 	 1.3467238504
epoch_time;  42.917768478393555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028505166992545128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058825116604566574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27676641941070557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1192665100097656
5 0.2882680034 	 1.1192664754
epoch_time;  42.9189031124115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023639988154172897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.045821480453014374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24730944633483887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0506447553634644
6 0.2808476808 	 1.0506447968
epoch_time;  42.93578338623047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023794058710336685
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04404433071613312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24565622210502625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.005737543106079
7 0.2582407395 	 1.0057375813
epoch_time;  42.68285131454468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04914867505431175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07068099081516266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3209095001220703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1723759174346924
8 0.2481742125 	 1.1723759124
epoch_time;  42.69788980484009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01512546930462122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03039702959358692
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21027618646621704
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8915386199951172
9 0.2298028384 	 0.8915385912
epoch_time;  42.58910799026489
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03656627982854843
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07424447685480118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32623428106307983
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1754534244537354
10 0.246915091 	 1.1754533935
epoch_time;  42.267889738082886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018983112648129463
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–„â–…â–…â–ƒâ–‚â–‚â–„â–‚â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–ƒâ–â–â–ƒâ–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–…â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–â–‚â–‚â–‚â–ƒâ–â–â–â–‚â–‚â–‚â–â–â–â–â–ƒâ–ƒâ–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–„â–„â–…â–ƒâ–‚â–‚â–„â–â–ƒâ–‚â–‚â–„â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–‚â–‚â–â–ƒâ–â–â–ƒâ–‚â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.77467
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.02039
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.15072
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01011
wandb:                         Train loss 0.16826
wandb: 
wandb: ğŸš€ View run auspicious-kumquat-1626 at: https://wandb.ai/nreints/thesis/runs/378r5ugb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_092828-378r5ugb/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_095059-rjhaxhz2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-monkey-1633
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/rjhaxhz2
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03448951616883278
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16144680976867676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8119620084762573
11 0.2194506106 	 0.811962024
epoch_time;  42.45272183418274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023756342008709908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.040472257882356644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20363958179950714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.925430178642273
12 0.2086638596 	 0.9254301596
epoch_time;  42.76872754096985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04064223915338516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057274021208286285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23785223066806793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0059691667556763
13 0.2183630898 	 1.005969183
epoch_time;  42.898072481155396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027677394449710846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049476779997348785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22358126938343048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0252257585525513
14 0.2074315824 	 1.0252257748
epoch_time;  43.0759539604187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02540130354464054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04611355811357498
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2626344561576843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0430629253387451
15 0.2165269527 	 1.0430628843
epoch_time;  45.4196982383728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03170975670218468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04669104889035225
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17528818547725677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8938917517662048
16 0.1902930977 	 0.8938917638
epoch_time;  43.39289569854736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013773957267403603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027755727991461754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16070586442947388
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8359518051147461
17 0.2032222589 	 0.8359517907
epoch_time;  42.628621339797974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022730931639671326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03763624280691147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.17113357782363892
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8211032152175903
18 0.1943716068 	 0.8211031911
epoch_time;  43.05388259887695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029767926782369614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05787435173988342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2342006117105484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0520578622817993
19 0.2563031851 	 1.0520578252
epoch_time;  42.882267475128174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024266263470053673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04402166232466698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19428227841854095
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.921746015548706
20 0.2070674968 	 0.9217460148
epoch_time;  42.677878856658936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02397548221051693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04044336453080177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.20211084187030792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9798255562782288
21 0.1840535813 	 0.9798255759
epoch_time;  42.50204348564148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0113808149471879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.022536728531122208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15730082988739014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8178673982620239
22 0.1959355576 	 0.8178674058
epoch_time;  42.52269434928894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02829398214817047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.051473744213581085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1753208339214325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8087776899337769
23 0.1781532813 	 0.8087776853
epoch_time;  42.84527564048767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01440502144396305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02272183820605278
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.16068696975708008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7844359278678894
24 0.1794163764 	 0.7844359061
epoch_time;  42.63884997367859
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011914671398699284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.021855611354112625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.1521771103143692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7537222504615784
25 0.1692347328 	 0.7537222686
epoch_time;  42.9011549949646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.034608811140060425
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05510904639959335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24899423122406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9042531847953796
26 0.1715702747 	 0.9042531731
epoch_time;  42.437551736831665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024588149040937424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03991355746984482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24746456742286682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9911617040634155
27 0.1838567649 	 0.9911617037
epoch_time;  42.700257301330566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018145104870200157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.033740073442459106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.19544941186904907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8105146288871765
28 0.1722977181 	 0.8105146057
epoch_time;  43.041215658187866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010110568255186081
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02037801966071129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15066109597682953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7751595973968506
29 0.1682572116 	 0.7751595765
epoch_time;  43.13645386695862
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010113570839166641
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.020387785509228706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.15071777999401093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7746676206588745
It took  1350.8904948234558  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f1f1060>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18f970>, <torch.utils.data.dataloader.DataLoader object at 0x153034231e70>, <torch.utils.data.dataloader.DataLoader object at 0x153034231c00>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08202307671308517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13916617631912231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6464700102806091
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.990517020225525
0 4.2271868051 	 1.9905170544
epoch_time;  42.94927096366882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045659974217414856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.078421451151371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43454575538635254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.416236400604248
1 0.4970524316 	 1.4162363473
epoch_time;  42.4573700428009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031454939395189285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05930281803011894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4380997121334076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.263027548789978
2 0.3933149824 	 1.2630275023
epoch_time;  42.538695335388184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09899047017097473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1819528490304947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4632920026779175
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3457057476043701
3 0.3328106611 	 1.3457057987
epoch_time;  42.81308078765869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02674269862473011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0478990375995636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34369075298309326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0644704103469849
4 0.3175110048 	 1.0644704582
epoch_time;  43.66216039657593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0130413006991148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029414476826786995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3122262954711914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.953772783279419
5 0.2866669279 	 0.953772784
epoch_time;  44.28078055381775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01692485436797142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03627835959196091
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32011979818344116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9541438817977905
6 0.2656203796 	 0.9541438814
epoch_time;  42.8723247051239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022838063538074493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.032938290387392044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3073504567146301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8846754431724548
7 0.2583322922 	 0.8846754552
epoch_time;  42.73272967338562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015432492829859257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029780620709061623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30420392751693726
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8404195308685303
8 0.230544039 	 0.8404195273
epoch_time;  42.79230570793152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015036540105938911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0289370808750391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27236035466194153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8391481041908264
9 0.2310902367 	 0.8391481152
epoch_time;  42.88810753822327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019164355471730232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03568444028496742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2894341051578522
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8849146366119385
10 0.2221542462 	 0.8849146172
epoch_time;  42.961491107940674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02811099775135517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04850974306464195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.331133633852005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8863970041275024
11 0.2110603558 	 0.8863969786
epoch_time;  42.74808216094971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031303323805332184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04825717210769653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29207107424736023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8070443868637085
12 0.2043171954 	 0.8070443606
epoch_time;  42.8736937046051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017220640555024147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026063725352287292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24951036274433136
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7492566704750061
13 0.2113586472 	 0.7492566526
epoch_time;  42.92981934547424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023633042350411415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03669789433479309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.291226863861084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7984546422958374
14 0.2017576924 	 0.7984546304
epoch_time;  42.58309459686279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028093568980693817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04699929803609848
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27043184638023376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7468805909156799
15 0.2013677327 	 0.7468806149
epoch_time;  42.821038246154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01975523866713047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03651608154177666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.298932820558548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7705787420272827
16 0.1816059375 	 0.7705787129
epoch_time;  42.89263868331909
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014285222627222538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02868313528597355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2728511393070221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7390314936637878
17 0.1871273667 	 0.7390314661
epoch_time;  42.83422088623047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017243821173906326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.027683768421411514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24911174178123474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7078598737716675
18 0.1743196636 	 0.7078598806
epoch_time;  42.84928011894226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019910523667931557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.031231483444571495
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30469194054603577
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7865438461303711
19 0.1900833904 	 0.7865438317
epoch_time;  43.086281538009644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018475238233804703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03167510777711868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3678899109363556
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7428000569343567
20 0.183914843 	 0.7428000643
epoch_time;  42.65612316131592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01276993378996849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.020823117345571518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24288520216941833
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6411479711532593
21 0.1924403362 	 0.6411479477
epoch_time;  43.14094829559326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.034786079078912735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05047552287578583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29701414704322815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6825252771377563
22 0.1699320629 	 0.682525266
epoch_time;  42.871901750564575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017016669735312462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029617667198181152
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2830318212509155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7093006372451782
23 0.1760099124 	 0.7093006607
epoch_time;  42.970768451690674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012081691063940525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02120637334883213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.21431785821914673
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6408078074455261
24 0.1777230666 	 0.6408078289
epoch_time;  42.64419960975647
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02655600570142269
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.041883546859025955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3177471458911896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6958926916122437
25 0.1705999613 	 0.6958927028
epoch_time;  43.59404015541077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012198526412248611
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02110963501036167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25604909658432007
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6456342935562134
26 0.1682007275 	 0.6456343083
epoch_time;  44.25228691101074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030082909390330315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05081260949373245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3567933440208435
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.845132052898407
27 0.1775508884 	 0.8451320498
epoch_time;  42.68198871612549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.049653954803943634
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07104373723268509
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–ƒâ–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–„â–ƒâ–ˆâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–ƒâ–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–â–ƒâ–‚â–ƒâ–„â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–„â–ƒâ–ˆâ–‚â–â–‚â–‚â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–â–‚â–â–ƒâ–„â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.58188
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.01734
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.24104
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00982
wandb:                         Train loss 0.16167
wandb: 
wandb: ğŸš€ View run vibrant-monkey-1633 at: https://wandb.ai/nreints/thesis/runs/rjhaxhz2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_095059-rjhaxhz2/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_101333-rwu33r41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-rocket-1640
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/rwu33r41
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42726102471351624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0581313371658325
28 0.1767199947 	 1.0581313764
epoch_time;  42.61552596092224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009825444780290127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0173435490578413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24093690514564514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5788260102272034
29 0.1616705549 	 0.5788260284
epoch_time;  42.74923062324524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009824462234973907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.017342882230877876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24104337394237518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5818835496902466
It took  1353.7085223197937  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f18faf0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f8faec0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f8fab90>, <torch.utils.data.dataloader.DataLoader object at 0x15305f8fba90>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09883452951908112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19232416152954102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7543821930885315
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6199002265930176
0 5.2049247285 	 2.6199003375
epoch_time;  43.16006565093994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1511099934577942
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2435496747493744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5449431538581848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7962876558303833
1 0.5539352567 	 1.796287698
epoch_time;  43.07131552696228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06610984355211258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12134018540382385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3908574879169464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.380893349647522
2 0.4230923765 	 1.3808933961
epoch_time;  42.901610374450684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03082931973040104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06780454516410828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35590848326683044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2018754482269287
3 0.3754687833 	 1.2018754389
epoch_time;  43.12934398651123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03884836286306381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06993437558412552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36474281549453735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.068502426147461
4 0.3314204595 	 1.0685024665
epoch_time;  43.15958905220032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.048547450453042984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09128940850496292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3764442205429077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1853727102279663
5 0.3079753821 	 1.1853727127
epoch_time;  42.72167682647705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04723541811108589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07447117567062378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3370291590690613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.012413501739502
6 0.288332088 	 1.012413555
epoch_time;  42.61218023300171
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031082239001989365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0555947870016098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29048964381217957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8182915449142456
7 0.2553771926 	 0.8182915172
epoch_time;  42.693180322647095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026841897517442703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058716949075460434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33830565214157104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9683687686920166
8 0.2480816895 	 0.9683687608
epoch_time;  42.66892147064209
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05317721888422966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07980429381132126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3870776891708374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0115158557891846
9 0.2309279691 	 1.0115159141
epoch_time;  43.279990911483765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02947205677628517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05078762397170067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2909182608127594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8598368167877197
10 0.2364752839 	 0.8598368204
epoch_time;  42.649227142333984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022050704807043076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04336977377533913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3587917983531952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.861469566822052
11 0.2128692327 	 0.8614695569
epoch_time;  42.97639513015747
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05207684636116028
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08591791987419128
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35891908407211304
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9721337556838989
12 0.2271514361 	 0.9721337632
epoch_time;  42.546945571899414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.046724624931812286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08147161453962326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3438383936882019
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8337080478668213
13 0.2153093452 	 0.8337080572
epoch_time;  42.638396978378296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.031816016882658005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05118516832590103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3139042556285858
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7996448278427124
14 0.2173027599 	 0.799644816
epoch_time;  42.823601484298706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020915409550070763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.037528783082962036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26415950059890747
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6977441310882568
15 0.2185850295 	 0.6977441332
epoch_time;  44.30290222167969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01599096693098545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03184628114104271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29463130235671997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7605694532394409
16 0.1985245155 	 0.7605694543
epoch_time;  44.21691107749939
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01293098833411932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026997607201337814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.25813454389572144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7065362334251404
17 0.2036580065 	 0.7065362383
epoch_time;  42.871872425079346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029544997960329056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04365851357579231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2275843322277069
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6350403428077698
18 0.1896947013 	 0.6350403293
epoch_time;  43.009164333343506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03298616036772728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06588858366012573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3094853162765503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7748480439186096
19 0.1946100413 	 0.7748480391
epoch_time;  43.08967924118042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017661934718489647
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03798346221446991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3473968207836151
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7972068190574646
20 0.1998961894 	 0.797206821
epoch_time;  42.74936819076538
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–†â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ˆâ–„â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.74039
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.03778
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27621
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.02417
wandb:                         Train loss 0.17934
wandb: 
wandb: ğŸš€ View run glistening-rocket-1640 at: https://wandb.ai/nreints/thesis/runs/rwu33r41
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_101333-rwu33r41/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_103606-s8jxs2gj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-snake-1648
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/s8jxs2gj
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020518623292446136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03795789182186127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27562180161476135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7175882458686829
21 0.1912112124 	 0.7175882576
epoch_time;  42.69994044303894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01618146523833275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03288349136710167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24508295953273773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6686776280403137
22 0.1885588398 	 0.6686776153
epoch_time;  42.81440496444702
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030073257163167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04876836761832237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.24522556364536285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6989865303039551
23 0.1783936563 	 0.698986549
epoch_time;  42.69721221923828
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028555937111377716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04282906651496887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30843859910964966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7240526080131531
24 0.2011554958 	 0.7240526366
epoch_time;  42.895763635635376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02272627502679825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04172658175230026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3145332336425781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7219481468200684
25 0.18007493 	 0.7219481684
epoch_time;  42.73181915283203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.058830998837947845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09057040512561798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2835189998149872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7242929339408875
26 0.1888915762 	 0.724292951
epoch_time;  43.02374744415283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01995592564344406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04204364866018295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2551717162132263
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.681397557258606
27 0.1607153154 	 0.6813975447
epoch_time;  42.969749212265015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008727597072720528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.019604159519076347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2749994099140167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6097445487976074
28 0.1803654477 	 0.6097445762
epoch_time;  42.93763208389282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0241612046957016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03779516741633415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27639904618263245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7415709495544434
29 0.1793387425 	 0.7415709251
epoch_time;  42.824429512023926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024165602400898933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.037778183817863464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27621349692344666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7403854131698608
It took  1353.7402482032776  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f8fafb0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18d9c0>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18ee30>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18eda0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.07855664938688278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12603482604026794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6383017897605896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.834671974182129
0 3.5822689221 	 1.8346719886
epoch_time;  43.27607035636902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.045120373368263245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07507432252168655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3986579477787018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3482941389083862
1 0.4787874879 	 1.3482941688
epoch_time;  42.865431785583496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03517145290970802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06659882515668869
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.47761064767837524
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2716001272201538
2 0.3851500864 	 1.2716000836
epoch_time;  42.95483136177063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02972048707306385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06845322996377945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48203805088996887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2045949697494507
3 0.3461635685 	 1.2045950068
epoch_time;  42.76541042327881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027269041165709496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04681835323572159
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34009718894958496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9901174902915955
4 0.3126025954 	 0.9901174678
epoch_time;  44.116883516311646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02510807290673256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.043573230504989624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37743741273880005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.011484146118164
5 0.2846362037 	 1.0114841058
epoch_time;  43.20781922340393
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01431263331323862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.029102405533194542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31901174783706665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8790714740753174
6 0.3012738744 	 0.879071469
epoch_time;  43.48737049102783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.024346794933080673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.044889047741889954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3285770118236542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8829521536827087
7 0.2679113465 	 0.8829521802
epoch_time;  42.597461462020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012700436636805534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02717139758169651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28981491923332214
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8646916747093201
8 0.265713665 	 0.8646916969
epoch_time;  42.88942050933838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01990409940481186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.035183317959308624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33783605694770813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9007536768913269
9 0.2495099399 	 0.9007537012
epoch_time;  43.174731492996216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02465934492647648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.038409799337387085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3036353588104248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8269382119178772
10 0.2330052667 	 0.8269382258
epoch_time;  42.91115379333496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02081400901079178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03675640746951103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.293167382478714
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8070683479309082
11 0.2339423019 	 0.8070683321
epoch_time;  43.000975608825684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01100535411387682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023684514686465263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2762734293937683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.774176299571991
12 0.2364783103 	 0.7741762835
epoch_time;  42.93855428695679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019428817555308342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03714669123291969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3024959862232208
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–ƒâ–‚â–‚â–ƒâ–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–„â–…â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–ƒâ–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.79731
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.03542
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.31964
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01911
wandb:                         Train loss 0.19004
wandb: 
wandb: ğŸš€ View run beaming-snake-1648 at: https://wandb.ai/nreints/thesis/runs/s8jxs2gj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_103606-s8jxs2gj/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_105840-7rvfoizn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-paper-1657
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/7rvfoizn
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8593392968177795
13 0.2306095097 	 0.8593393193
epoch_time;  42.55852770805359
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017314374446868896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.028814295306801796
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31632882356643677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8580411672592163
14 0.2249927275 	 0.8580411698
epoch_time;  42.69800019264221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010313404724001884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02444942109286785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2865484356880188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7170294523239136
15 0.2109191571 	 0.7170294448
epoch_time;  42.75910258293152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023462720215320587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.036964043974876404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3133939206600189
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8464688062667847
16 0.2228318012 	 0.8464688304
epoch_time;  42.91913104057312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01741664670407772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02596343122422695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2906229794025421
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.756542444229126
17 0.2137617505 	 0.7565424248
epoch_time;  42.657360553741455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011622312478721142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.022349873557686806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2704976201057434
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8046137690544128
18 0.2190836373 	 0.8046137415
epoch_time;  42.42607235908508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026689808815717697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03690465912222862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3243824541568756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8202808499336243
19 0.2134348359 	 0.820280876
epoch_time;  42.91369676589966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012770676985383034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.020525027066469193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26410800218582153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7428315877914429
20 0.2004656276 	 0.7428315961
epoch_time;  43.055373191833496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01587093621492386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.026600543409585953
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2958904802799225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8148829340934753
21 0.2030992876 	 0.8148829526
epoch_time;  43.03988790512085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015482929535210133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02539590373635292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2625909745693207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7666258215904236
22 0.1933272518 	 0.7666258106
epoch_time;  42.77326846122742
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03236353024840355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050994765013456345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2799473702907562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7720323801040649
23 0.1895274899 	 0.7720324006
epoch_time;  43.27977156639099
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018140314146876335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.030631227418780327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3290097415447235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7729232907295227
24 0.2008161754 	 0.772923265
epoch_time;  42.749008655548096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019053872674703598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.02840588055551052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3079943358898163
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7749947905540466
25 0.192349823 	 0.7749948185
epoch_time;  43.778454065322876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025909122079610825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04386493191123009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3469093441963196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8904615044593811
26 0.190791821 	 0.8904615327
epoch_time;  43.577707052230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013967256061732769
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.024236837401986122
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2803672254085541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7188984155654907
27 0.1939399251 	 0.7188983929
epoch_time;  43.12204122543335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014082729816436768
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.023278813809156418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23503129184246063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7081007361412048
28 0.1755673497 	 0.7081007482
epoch_time;  42.927884340286255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01911809667944908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03543691709637642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3161535859107971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7971869111061096
29 0.1900431007 	 0.7971869062
epoch_time;  43.21290445327759
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01911064423620701
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03541862964630127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31964111328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7973098158836365
It took  1353.8343687057495  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15305f8fbb20>, <torch.utils.data.dataloader.DataLoader object at 0x15305f8fb850>, <torch.utils.data.dataloader.DataLoader object at 0x15305f8fb880>, <torch.utils.data.dataloader.DataLoader object at 0x15305f18d8d0>]
LSTM(
  (lstm): LSTM(6, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14390058815479279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3155744671821594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2839893102645874
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.034976482391357
0 6.0750507872 	 4.0349766481
epoch_time;  43.076088666915894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09188643097877502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17005404829978943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7574408650398254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4381892681121826
1 0.7677457522 	 2.4381892732
epoch_time;  42.883519411087036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08500148355960846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1662994623184204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6319345235824585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0558533668518066
2 0.4992136583 	 2.0558534374
epoch_time;  42.48811721801758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.08327507972717285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15495069324970245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5229930281639099
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6849472522735596
3 0.4411345719 	 1.6849472184
epoch_time;  43.00818586349487
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13385286927223206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23564033210277557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5615915060043335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8877687454223633
4 0.3603979511 	 1.8877687022
epoch_time;  43.269299030303955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.05353390425443649
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14644120633602142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6066644787788391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0176286697387695
5 0.3534241416 	 2.0176286496
epoch_time;  42.85299730300903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0639885812997818
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–„â–„â–ƒâ–ƒâ–„â–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–„â–†â–„â–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–ƒâ–„â–‚â–…â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–…â–…â–‡â–ƒâ–„â–…â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.93593
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.03847
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27657
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01307
wandb:                         Train loss 0.18778
wandb: 
wandb: ğŸš€ View run festive-paper-1657 at: https://wandb.ai/nreints/thesis/runs/7rvfoizn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_105840-7rvfoizn/logs
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11454267054796219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4222154915332794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3981159925460815
6 0.3218551509 	 1.3981160052
epoch_time;  42.89002013206482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09196355938911438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19092071056365967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7726697325706482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1565351486206055
7 0.321613667 	 2.156535261
epoch_time;  42.59942388534546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0379384309053421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07584819942712784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37077590823173523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2558115720748901
8 0.2819195848 	 1.2558115242
epoch_time;  42.7476122379303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03664457052946091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07586565613746643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3928329646587372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2611175775527954
9 0.2571695748 	 1.2611176182
epoch_time;  42.67148447036743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03293655812740326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07759998738765717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3802083134651184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2278085947036743
10 0.2451092636 	 1.2278085576
epoch_time;  42.73512101173401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03845646232366562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07861407846212387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3255346417427063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2608660459518433
11 0.2594475122 	 1.2608660096
epoch_time;  42.53433966636658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.037563011050224304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07795575261116028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3806631863117218
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.16722571849823
12 0.2283863238 	 1.1672257253
epoch_time;  42.83091950416565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015863237902522087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04821724072098732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2960134446620941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.171935796737671
13 0.2157936298 	 1.1719357586
epoch_time;  42.70584011077881
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018532348796725273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05210229381918907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28523188829421997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.036374807357788
14 0.2182194885 	 1.0363748326
epoch_time;  44.94554591178894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019394665956497192
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05223384127020836
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33425506949424744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1207131147384644
15 0.2063292533 	 1.1207131562
epoch_time;  43.25773596763611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.029860643669962883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06334558129310608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33861082792282104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0820022821426392
16 0.2014472032 	 1.0820022998
epoch_time;  44.447673320770264
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016189951449632645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050464026629924774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30709773302078247
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.069230318069458
17 0.2241659576 	 1.0692302785
epoch_time;  42.640920877456665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02207544445991516
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05097976326942444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2985130250453949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0717560052871704
18 0.2048395981 	 1.0717559538
epoch_time;  42.460312604904175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03519250079989433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07164514809846878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3388838768005371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1822398900985718
19 0.2037020618 	 1.1822399128
epoch_time;  42.84751653671265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00863337516784668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03299075737595558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.22772620618343353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8920497298240662
20 0.1921705871 	 0.8920497376
epoch_time;  42.61816048622131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01254233531653881
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03363850340247154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.23384346067905426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8503857851028442
21 0.1858102133 	 0.8503857754
epoch_time;  42.81130051612854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012546900659799576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.037864651530981064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.26654013991355896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9741450548171997
22 0.180123444 	 0.9741450653
epoch_time;  42.819739818573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013891017064452171
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042547062039375305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30037087202072144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9032081365585327
23 0.1767421379 	 0.9032081074
epoch_time;  42.49728035926819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010331927798688412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03807072341442108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2772141695022583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9505400657653809
24 0.1851075466 	 0.9505400413
epoch_time;  42.69505000114441
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015592815354466438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03986162319779396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3098425567150116
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0383551120758057
25 0.215484134 	 1.0383550638
epoch_time;  42.978853940963745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015507002361118793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03632878139615059
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2465595155954361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.856229841709137
26 0.1792334557 	 0.8562298455
epoch_time;  42.73725914955139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025199126452207565
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06689963489770889
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.394933819770813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2446624040603638
27 0.1890869141 	 1.2446623742
epoch_time;  42.54502773284912
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02288171462714672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05565784499049187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29205891489982605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9441922903060913
28 0.1714744597 	 0.9441922928
epoch_time;  42.83112931251526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01306808553636074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.038471147418022156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27670931816101074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9387660622596741
29 0.1877834254 	 0.9387660646
epoch_time;  42.649526596069336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013065380044281483
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.03846771642565727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2765735387802124
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9359316229820251
It took  1351.989934682846  seconds.

JOB STATISTICS
==============
Job ID: 2142311
Array Job ID: 2141141_20
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-20:37:48 core-walltime
Job Wall-clock time: 03:48:46
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
