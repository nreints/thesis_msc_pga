/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_000818-20jc6x45
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-springroll-1433
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/20jc6x45
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(0,', '0)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149130ddff40>, <torch.utils.data.dataloader.DataLoader object at 0x14912a0e8a90>, <torch.utils.data.dataloader.DataLoader object at 0x14912a0e83d0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a0e8550>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03129791095852852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5252825021743774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7383649349212646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5457000732421875
0 1.0645483971 	 2.5457000271
epoch_time;  38.647260665893555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009557986631989479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25178247690200806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7537133097648621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0662055015563965
1 0.0196875775 	 2.0662054425
epoch_time;  37.202221155166626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00424391170963645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16296687722206116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7927737236022949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8734357357025146
2 0.0082241574 	 1.8734357667
epoch_time;  36.828376054763794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005378423724323511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13422390818595886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.742394745349884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6833711862564087
3 0.00498415 	 1.6833711837
epoch_time;  37.67700910568237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037126513198018074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10601954162120819
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7126813530921936
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5187633037567139
4 0.0037142203 	 1.5187633318
epoch_time;  37.170676708221436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013375238049775362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0836932510137558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6578431129455566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3704543113708496
5 0.0030228765 	 1.3704542638
epoch_time;  37.20786809921265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012756168143823743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06740233302116394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6136675477027893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2457016706466675
6 0.0026185078 	 1.2457016314
epoch_time;  37.93230986595154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004433769267052412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3452049195766449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.325538396835327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.41645622253418
7 0.0213846976 	 5.416456332
epoch_time;  37.61540937423706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001765515306033194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1793963462114334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9244574308395386
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.292119026184082
8 0.002492609 	 3.2921190521
epoch_time;  36.881542921066284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001097007654607296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1427840143442154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.490440011024475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6578245162963867
9 0.0022305379 	 2.6578245595
epoch_time;  37.274739265441895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012525396887212992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11859899759292603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2845749855041504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.309061050415039
10 0.0018882616 	 2.3090610101
epoch_time;  39.58649730682373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017071511829271913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09853930026292801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0696746110916138
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9805965423583984
11 0.0016880144 	 1.9805965366
epoch_time;  37.71055722236633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012158923782408237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08978766202926636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9696601033210754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8459151983261108
12 0.0015728986 	 1.8459151807
epoch_time;  37.69254970550537
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007688907906413078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0865473821759224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8708128333091736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6914868354797363
13 0.0014340522 	 1.6914868312
epoch_time;  37.956946849823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015965413767844439
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07397148758172989
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8351653218269348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.616929531097412
14 0.0012952872 	 1.6169295297
epoch_time;  37.885767459869385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011248370865359902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06473758816719055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7995359301567078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5221954584121704
15 0.0012328796 	 1.5221954991
epoch_time;  37.568228244781494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020595912355929613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06472888588905334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7753420472145081
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4727230072021484
16 0.0011574665 	 1.4727230014
epoch_time;  37.09225869178772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000739800336305052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10977523028850555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8879318833351135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8934602737426758
17 0.0026681037 	 1.8934602766
epoch_time;  36.977269649505615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008433652110397816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09462016820907593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7452907562255859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5978707075119019
18 0.0009551828 	 1.5978707028
epoch_time;  36.87095308303833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010099854553118348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07238013297319412
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7023961544036865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4781992435455322
19 0.0009532832 	 1.478199201
epoch_time;  37.541096687316895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017879675142467022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07229205965995789
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6606536507606506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4223538637161255
20 0.0009370231 	 1.422353877
epoch_time;  37.25432062149048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000607741647399962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06834902614355087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6403687000274658
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3889046907424927
21 0.0009304359 	 1.3889046753
epoch_time;  36.989832639694214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000500130990985781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0705774575471878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6511813998222351
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.41606867313385
22 0.0009618024 	 1.4160687311
epoch_time;  37.25211024284363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015365451108664274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0709461197257042
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–‚â–â–â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–â–â–‚â–â–â–â–â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–ƒâ–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.33763
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.07419
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.66525
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00049
wandb:                         Train loss 0.00073
wandb: 
wandb: ğŸš€ View run dancing-springroll-1433 at: https://wandb.ai/nreints/thesis/runs/20jc6x45
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_000818-20jc6x45/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_002805-986dzpja
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run abundant-laughter-1437
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/986dzpja
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6266814470291138
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3772112131118774
23 0.0008620479 	 1.3772111875
epoch_time;  37.376535177230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00039587970240972936
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0681164488196373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6138764023780823
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3307772874832153
24 0.0008310082 	 1.3307772634
epoch_time;  37.754515647888184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008072813507169485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0740114226937294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6225147247314453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3502154350280762
25 0.0008872344 	 1.3502154854
epoch_time;  37.89990544319153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005554085364565253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06890644878149033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5960250496864319
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2914308309555054
26 0.000760965 	 1.2914308853
epoch_time;  37.049949645996094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005812646122649312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11020323634147644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8727320432662964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7502620220184326
27 0.0036284604 	 1.7502620271
epoch_time;  37.05222749710083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005056315567344427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08256199210882187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7037176489830017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4173566102981567
28 0.0006124153 	 1.4173566467
epoch_time;  37.20870041847229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00048793473979458213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07441865652799606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6651917099952698
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3389079570770264
29 0.0007305461 	 1.3389079391
epoch_time;  37.163743019104004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000487497600261122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07419230788946152
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6652498245239258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3376344442367554
It took  1187.310851097107  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1490fdd62a70>, <torch.utils.data.dataloader.DataLoader object at 0x14912a157eb0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a157fd0>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfc160>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03648403659462929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4956701993942261
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8239001035690308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.474834442138672
0 0.9984981008 	 2.4748343384
epoch_time;  38.402780532836914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009942833334207535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2138132005929947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8736861944198608
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9592616558074951
1 0.0188767406 	 1.9592617069
epoch_time;  38.30343580245972
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034357612021267414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12180328369140625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8639781475067139
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7128063440322876
2 0.0080549289 	 1.7128063559
epoch_time;  38.27104473114014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019015338039025664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0844612792134285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8000826835632324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.501465082168579
3 0.0049644719 	 1.5014650281
epoch_time;  39.51812481880188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011846116743981838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08695653080940247
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7237212657928467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3581209182739258
4 0.0036571775 	 1.3581209212
epoch_time;  37.74335241317749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011027636006474495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0682537704706192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.644317090511322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2001858949661255
5 0.0030127893 	 1.2001859083
epoch_time;  37.945403814315796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021116596180945635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.047772035002708435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5188887119293213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9983901977539062
6 0.0022997654 	 0.9983902208
epoch_time;  37.88380432128906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022705402225255966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1697673350572586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.067575216293335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4208106994628906
7 0.0188936397 	 3.4208106649
epoch_time;  37.14338302612305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019698163960129023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10420186817646027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.307627558708191
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.247042655944824
8 0.0018906478 	 2.2470426531
epoch_time;  37.41104316711426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009116596193052828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07822220772504807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0278546810150146
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7723883390426636
9 0.0017530826 	 1.7723882854
epoch_time;  38.019543409347534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015680526848882437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06412363052368164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9049173593521118
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.544613242149353
10 0.001744836 	 1.5446132879
epoch_time;  37.042243003845215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018440807471051812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05704447999596596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8302165865898132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4108268022537231
11 0.0015373147 	 1.4108268069
epoch_time;  36.99757266044617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0023351721465587616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05075383186340332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7966842651367188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3354384899139404
12 0.0014393309 	 1.3354385238
epoch_time;  37.495959520339966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008416043710894883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04520562291145325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7371917366981506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2371978759765625
13 0.001237368 	 1.2371978299
epoch_time;  37.10491609573364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00185482669621706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0451938696205616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7085635662078857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.178444266319275
14 0.0011932562 	 1.1784443005
epoch_time;  37.88398313522339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001542509999126196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04476515203714371
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6770402789115906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1404012441635132
15 0.0011387945 	 1.1404012352
epoch_time;  37.8984317779541
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–„â–ƒâ–‚â–‚â–‚â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.10298
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.0415
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.6876
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00053
wandb:                         Train loss 0.00074
wandb: 
wandb: ğŸš€ View run abundant-laughter-1437 at: https://wandb.ai/nreints/thesis/runs/986dzpja
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_002805-986dzpja/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_004800-c5nu3ewo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-snake-1445
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/c5nu3ewo
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008283331408165395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0437973327934742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6736854314804077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.128246784210205
16 0.0011052679 	 1.1282467568
epoch_time;  38.178542613983154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006974190473556519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11319749802350998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.6312209367752075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.523364782333374
17 0.0051164179 	 2.5233648479
epoch_time;  38.392372131347656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005600935546681285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08176123350858688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1219513416290283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8091888427734375
18 0.000803595 	 1.8091887967
epoch_time;  37.99182605743408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004645870067179203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06731827557086945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9347218871116638
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5418092012405396
19 0.0009741554 	 1.5418091742
epoch_time;  37.65672183036804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005133759113959968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06154028698801994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8064004778862
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3491489887237549
20 0.0009128935 	 1.3491490298
epoch_time;  37.17624521255493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006059846491552889
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.057471081614494324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.741959273815155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2539108991622925
21 0.0009161778 	 1.2539108599
epoch_time;  37.3018684387207
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009963858174160123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05618646740913391
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7014010548591614
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2045924663543701
22 0.0008942725 	 1.2045925175
epoch_time;  37.09822392463684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004174480272922665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050727952271699905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6836003065109253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1670137643814087
23 0.0008523022 	 1.1670137619
epoch_time;  37.25468873977661
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00040946018998511136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0491771399974823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6950079202651978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.174560546875
24 0.0008675029 	 1.1745605469
epoch_time;  37.49588346481323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000498976674862206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04787568375468254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6830626130104065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1458656787872314
25 0.0008066514 	 1.1458656334
epoch_time;  36.94474720954895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012730270391330123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04923634231090546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7119603753089905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1723283529281616
26 0.0007999906 	 1.1723283382
epoch_time;  39.396162033081055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00043144688243046403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04396408051252365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6871335506439209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.126289963722229
27 0.0007744107 	 1.1262899439
epoch_time;  38.94921827316284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004741801240015775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04213891178369522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6748728156089783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0985546112060547
28 0.0007370646 	 1.0985546285
epoch_time;  37.88731503486633
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005325052770785987
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04149778187274933
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6881263256072998
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1006040573120117
29 0.0007448622 	 1.1006040083
epoch_time;  37.864850759506226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005323875811882317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.041501786559820175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6875981092453003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1029802560806274
It took  1195.3156094551086  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14912a1562f0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a0ea740>, <torch.utils.data.dataloader.DataLoader object at 0x14912a156500>, <torch.utils.data.dataloader.DataLoader object at 0x14912a156380>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027419215068221092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5082809925079346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7879077196121216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2973179817199707
0 1.0219454884 	 2.2973179198
epoch_time;  37.84617614746094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00797437783330679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21379488706588745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6971548199653625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.669335126876831
1 0.0184654854 	 1.6693351262
epoch_time;  37.222002029418945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0039529502391815186
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1346963793039322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6807032823562622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4594179391860962
2 0.0078119268 	 1.4594178906
epoch_time;  36.79765725135803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0050485036335885525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0994655191898346
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6772139072418213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3235727548599243
3 0.0046564083 	 1.3235727178
epoch_time;  37.18821144104004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001932745915837586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07230652123689651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6581275463104248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2388461828231812
4 0.003482502 	 1.2388461479
epoch_time;  37.318336963653564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022773300297558308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05960674211382866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5798591375350952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1094436645507812
5 0.002748648 	 1.1094436876
epoch_time;  37.742279291152954
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00197562831453979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1631874144077301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.4905623197555542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.593846321105957
6 0.0104780428 	 2.5938464392
epoch_time;  38.31489372253418
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012561040930449963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09221232682466507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9350529909133911
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6797789335250854
7 0.0019386073 	 1.6797789605
epoch_time;  37.48276925086975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025856569409370422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06949081271886826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7845826148986816
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–‚â–‚â–‚â–‚â–â–„â–‚â–‚â–‚â–â–â–â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–â–ƒâ–‚â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–â–â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.99951
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.04906
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.59264
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00112
wandb:                         Train loss 0.00077
wandb: 
wandb: ğŸš€ View run prosperous-snake-1445 at: https://wandb.ai/nreints/thesis/runs/c5nu3ewo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_004800-c5nu3ewo/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_010749-wclpdesv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-dragon-1452
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/wclpdesv
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4081119298934937
8 0.0018307517 	 1.4081119411
epoch_time;  37.82476305961609
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002993467031046748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05319340154528618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6639462113380432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2090692520141602
9 0.0016696067 	 1.2090692895
epoch_time;  37.92623782157898
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011291282717138529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04448520019650459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6174460649490356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1323151588439941
10 0.0016380883 	 1.1323151833
epoch_time;  37.5347843170166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011897224467247725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04185057058930397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5577583909034729
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0433106422424316
11 0.0013769443 	 1.0433106206
epoch_time;  37.44838261604309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018240492790937424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.042721908539533615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5648564696311951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.057926058769226
12 0.0013783219 	 1.0579260512
epoch_time;  37.87394094467163
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036661685444414616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30468451976776123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.268148899078369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.881840229034424
13 0.0116690543 	 4.881840363
epoch_time;  37.56599712371826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012938794679939747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14144904911518097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3320362567901611
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2592711448669434
14 0.001546608 	 2.2592712587
epoch_time;  37.50601887702942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014942934503778815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10709857195615768
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.974060595035553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7099292278289795
15 0.0012202606 	 1.7099292213
epoch_time;  37.60365080833435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011726815719157457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08494492620229721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7810617089271545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4102205038070679
16 0.0011686523 	 1.4102205121
epoch_time;  37.098559617996216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009285295382142067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06398316472768784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6361083388328552
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1724733114242554
17 0.0010872918 	 1.1724733658
epoch_time;  36.98199653625488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005334910820238292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05555487796664238
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5711007714271545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0575255155563354
18 0.0010904737 	 1.0575255426
epoch_time;  37.81674838066101
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005222677136771381
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05067364498972893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.544577956199646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0002622604370117
19 0.0009927397 	 1.0002623037
epoch_time;  39.95989799499512
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005581395816989243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04825291410088539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5277600288391113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9625989198684692
20 0.0009743416 	 0.9625989101
epoch_time;  37.85876679420471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005118569824844599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04839117452502251
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5381259322166443
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9674134254455566
21 0.0009885758 	 0.9674134038
epoch_time;  37.49318504333496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007458836189471185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04536125808954239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5132157206535339
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9184525012969971
22 0.000861263 	 0.9184525135
epoch_time;  37.67337107658386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009585072402842343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04572049900889397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5384295582771301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9389960765838623
23 0.0008750446 	 0.9389960989
epoch_time;  38.01515817642212
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005431314930319786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0430854856967926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5253649353981018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9076138138771057
24 0.0008335747 	 0.9076137946
epoch_time;  38.26471400260925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005452856421470642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06904109567403793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8454643487930298
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3956904411315918
25 0.0033835621 	 1.395690457
epoch_time;  37.69696664810181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005912109045311809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0562344454228878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.655429482460022
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0917787551879883
26 0.000665123 	 1.0917788042
epoch_time;  37.79017519950867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005344889359548688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.050797540694475174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5954445004463196
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0018198490142822
27 0.0007535051 	 1.0018198065
epoch_time;  37.38541269302368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010419797617942095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05018097162246704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6218722462654114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0323928594589233
28 0.0007974281 	 1.0323928879
epoch_time;  37.553581953048706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011174192186444998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04888039454817772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5920221209526062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9998900294303894
29 0.0007742115 	 0.9998900076
epoch_time;  37.88167881965637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011174113024026155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04906288534402847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5926427245140076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9995062947273254
It took  1189.2553939819336  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14912a1569e0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a12d210>, <torch.utils.data.dataloader.DataLoader object at 0x14912a12ebf0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a12eda0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030815202742815018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5205540657043457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8120059370994568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6303532123565674
0 1.0296222418 	 2.6303532073
epoch_time;  37.45279669761658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.037557825446128845
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25957992672920227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8361819982528687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0268714427948
1 0.0189645682 	 2.0268713268
epoch_time;  37.59533619880676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004550021607428789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1323552280664444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.680134117603302
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6058812141418457
2 0.0075522307 	 1.6058812444
epoch_time;  37.92577934265137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003538854420185089
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1061713770031929
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6487276554107666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4503238201141357
3 0.0047234247 	 1.4503238367
epoch_time;  37.925533294677734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013335180701687932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07674939185380936
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5931907892227173
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2733087539672852
4 0.0035279524 	 1.2733086992
epoch_time;  37.93746781349182
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001082592993043363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06610693037509918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5771204829216003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2151145935058594
5 0.0028580397 	 1.2151146281
epoch_time;  37.54192566871643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011210264638066292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07046301662921906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6418938040733337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3251575231552124
6 0.004384923 	 1.3251575113
epoch_time;  37.657902002334595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035076583735644817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.061635326594114304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5519832372665405
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1552598476409912
7 0.001837634 	 1.1552597922
epoch_time;  37.69206094741821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008034228812903166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049295373260974884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5559715628623962
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1221601963043213
8 0.0018042255 	 1.1221602057
epoch_time;  37.97572064399719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006683848332613707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.045680705457925797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5593960285186768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0976344347000122
9 0.0016324343 	 1.097634399
epoch_time;  37.5988655090332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020573525689542294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24084161221981049
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4538865089416504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9195685386657715
10 0.0167403658 	 3.9195685718
epoch_time;  37.42291879653931
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012950972886756063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15173211693763733
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.6120729446411133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6978633403778076
11 0.0014795467 	 2.6978634376
epoch_time;  40.145867586135864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009101922041736543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11015214025974274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3048397302627563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1872801780700684
12 0.0014199262 	 2.1872801997
epoch_time;  37.80264449119568
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007324498146772385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08983854949474335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2150176763534546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9863466024398804
13 0.0014353368 	 1.9863465646
epoch_time;  37.777770042419434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008262281771749258
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07066400349140167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0852549076080322
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7548723220825195
14 0.0012031516 	 1.7548723019
epoch_time;  37.41301679611206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005798695492558181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06504473090171814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0325452089309692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6707273721694946
15 0.0012914994 	 1.6707273178
epoch_time;  37.56007623672485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000507593562360853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05789608508348465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9384959936141968
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5302915573120117
16 0.0010505246 	 1.5302916005
epoch_time;  37.71956181526184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006195718888193369
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05479175224900246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8791500329971313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4570589065551758
17 0.0010580545 	 1.4570589094
epoch_time;  37.787782192230225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007019388140179217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05501308664679527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8535341620445251
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.431593656539917
18 0.0010135338 	 1.4315936962
epoch_time;  38.1487500667572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005530885537154973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09402298927307129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2679462432861328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0488178730010986
19 0.0042091665 	 2.0488179832
epoch_time;  38.048877000808716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001430858625099063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0811602920293808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9665148258209229
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.647037148475647
20 0.0007527799 	 1.6470371949
epoch_time;  37.73498010635376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005666438955813646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07120559364557266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8718379735946655
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4847666025161743
21 0.0008558837 	 1.4847665654
epoch_time;  37.732006549835205
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008787863189354539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.069877028465271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8278204798698425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4347690343856812
22 0.0009057778 	 1.4347689995
epoch_time;  37.29257154464722
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005071261548437178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0642879456281662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7877510786056519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3477095365524292
23 0.0008198305 	 1.3477095405
epoch_time;  37.53301024436951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007422430207952857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.062008146196603775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7688314914703369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3044390678405762
24 0.0008394817 	 1.3044391183
epoch_time;  38.422783613204956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005316643510013819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05929926782846451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7677300572395325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.283529281616211
25 0.0007845862 	 1.283529322
epoch_time;  38.17419457435608
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–„â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–ˆâ–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.37614
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.05257
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.8836
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00041
wandb:                         Train loss 0.00064
wandb: 
wandb: ğŸš€ View run glistening-dragon-1452 at: https://wandb.ai/nreints/thesis/runs/wclpdesv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_010749-wclpdesv/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_012745-bc3qtzgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-rooster-1459
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/bc3qtzgx
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013848814414814115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058188632130622864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7939741015434265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3001433610916138
26 0.0008040334 	 1.3001433312
epoch_time;  37.772040128707886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003400792134925723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0608430951833725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8866060972213745
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.406776785850525
27 0.0008177145 	 1.4067767279
epoch_time;  37.5752956867218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009129369864240289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07811174541711807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2239991426467896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8159834146499634
28 0.0009075695 	 1.8159834294
epoch_time;  37.57123303413391
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004118912329431623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05262018367648125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8832449316978455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.374972939491272
29 0.0006377174 	 1.374972986
epoch_time;  38.10859274864197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00041197988321073353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.052574533969163895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8836044073104858
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3761411905288696
It took  1195.3571453094482  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14912a12cf70>, <torch.utils.data.dataloader.DataLoader object at 0x14912a871210>, <torch.utils.data.dataloader.DataLoader object at 0x14912a872bf0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a872da0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03309597074985504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5793315768241882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7539474368095398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3842506408691406
0 1.0507396666 	 2.3842506063
epoch_time;  38.083224296569824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008348423056304455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26344677805900574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7125107645988464
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7799732685089111
1 0.0188523234 	 1.7799732404
epoch_time;  37.405131101608276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004308187868446112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16547970473766327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6931167840957642
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5228956937789917
2 0.0075243951 	 1.5228956516
epoch_time;  37.40275859832764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00877210684120655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13022303581237793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6950200200080872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4090676307678223
3 0.004816077 	 1.4090676668
epoch_time;  40.53538632392883
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004409625194966793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09818505495786667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6601693630218506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2823538780212402
4 0.0034062998 	 1.2823538881
epoch_time;  38.844603061676025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010454977164044976
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07669777423143387
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5866237282752991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.121044635772705
5 0.0027870663 	 1.1210446084
epoch_time;  37.478596448898315
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021960341837257147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21037299931049347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3838510513305664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3392117023468018
6 0.0159474026 	 2.3392116397
epoch_time;  36.987300157547
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011204722104594111
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14328640699386597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9748339653015137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6959340572357178
7 0.0020827119 	 1.6959340997
epoch_time;  37.92446994781494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010828220983967185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1181304007768631
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8616145849227905
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4801685810089111
8 0.0020451336 	 1.4801685529
epoch_time;  37.250473499298096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004245874937623739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10704880952835083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7554649710655212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3096836805343628
9 0.0017346237 	 1.3096837162
epoch_time;  37.465458154678345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026905580889433622
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0961855798959732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7532538771629333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2528724670410156
10 0.0016237085 	 1.2528724325
epoch_time;  37.27426242828369
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009282506071031094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09309317171573639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6919599175453186
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1527191400527954
11 0.001432465 	 1.1527190886
epoch_time;  37.185950756073
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005587603664025664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0801996961236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6922882795333862
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.14639151096344
12 0.0013727663 	 1.1463915315
epoch_time;  37.526912212371826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004964795429259539
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41710278391838074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.200617551803589
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.763504981994629
13 0.0088743553 	 4.763504812
epoch_time;  37.5303008556366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008897018851712346
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16664506494998932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2589629888534546
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9970574378967285
14 0.001344933 	 1.9970574048
epoch_time;  37.43094515800476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007242499850690365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13804267346858978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9005290865898132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5071239471435547
15 0.0010074337 	 1.5071239644
epoch_time;  37.30927872657776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007426432566717267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1188216507434845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8177953958511353
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3526068925857544
16 0.0010757799 	 1.3526069203
epoch_time;  37.91095805168152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008971029310487211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10432912409305573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7848626971244812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2610597610473633
17 0.0010415467 	 1.26105981
epoch_time;  37.11059832572937
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009477853309363127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09859784692525864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7832261919975281
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–â–â–â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–â–â–â–†â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–â–â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒ
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–ƒâ–‚â–â–â–â–â–‚â–â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.60738
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.12114
wandb:    Test loss t(0, 0)_r(-5, 5)_none 1.17435
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0004
wandb:                         Train loss 0.00184
wandb: 
wandb: ğŸš€ View run fortuitous-rooster-1459 at: https://wandb.ai/nreints/thesis/runs/bc3qtzgx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_012745-bc3qtzgx/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_014735-xsq10wdq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-rabbit-1466
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/xsq10wdq
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2153067588806152
18 0.0010144951 	 1.215306769
epoch_time;  37.320106744766235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007712114602327347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09572935849428177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7917060256004333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2149263620376587
19 0.0009329561 	 1.2149263595
epoch_time;  37.28627324104309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005655023851431906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09518738836050034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7926742434501648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2043386697769165
20 0.0009566128 	 1.2043386961
epoch_time;  37.256598711013794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018367164302617311
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09203658252954483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8086063861846924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2059401273727417
21 0.0008690376 	 1.2059401774
epoch_time;  37.17685914039612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000495396729093045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09042000025510788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8193503022193909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2091209888458252
22 0.0008709513 	 1.2091210126
epoch_time;  37.30424451828003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006943055777810514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09031186997890472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7998173832893372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.179019570350647
23 0.0007906778 	 1.1790196168
epoch_time;  36.735893964767456
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017807955155149102
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09035372734069824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.840543806552887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2010117769241333
24 0.0008483991 	 1.2010117269
epoch_time;  37.20111393928528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006372038624249399
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08724945038557053
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8448375463485718
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2021448612213135
25 0.0007825416 	 1.2021448418
epoch_time;  37.850398778915405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012383863795548677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08480483293533325
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.821544885635376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1622952222824097
26 0.0007249098 	 1.1622952464
epoch_time;  38.21966910362244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004366278008092195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08312694728374481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8845503330230713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.217560887336731
27 0.0007904903 	 1.2175609208
epoch_time;  39.98530054092407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006546529475599527
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08340558409690857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8887472748756409
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.201599359512329
28 0.0006864515 	 1.2015993055
epoch_time;  38.71068716049194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0003978468303103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12112097442150116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.171288013458252
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6069610118865967
29 0.0018443984 	 1.6069610688
epoch_time;  38.16749691963196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00039803487015888095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12114142626523972
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1743478775024414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6073780059814453
It took  1190.4988720417023  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14912a12d810>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfcd00>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfcd30>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfc6d0>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030033769086003304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5389084815979004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.78211510181427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3073580265045166
0 1.0895358482 	 2.3073579264
epoch_time;  38.086681604385376
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009372827596962452
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24736246466636658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.750444233417511
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.755946397781372
1 0.0197801511 	 1.75594641
epoch_time;  37.032689571380615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004147899337112904
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1622048169374466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7201394438743591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4941667318344116
2 0.0075348739 	 1.4941667171
epoch_time;  37.20973348617554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024990818928927183
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12188929319381714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6627854704856873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3124574422836304
3 0.0047310685 	 1.3124574045
epoch_time;  37.92581105232239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018933865940198302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0976267084479332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6742070913314819
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.271296739578247
4 0.0037203048 	 1.2712967518
epoch_time;  37.62454533576965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00398839870467782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08490215986967087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6358178853988647
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1748234033584595
5 0.0030317151 	 1.1748234037
epoch_time;  37.21972465515137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001187097397632897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06768783926963806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5726711750030518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.048677921295166
6 0.0024471059 	 1.0486779343
epoch_time;  37.16549229621887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001091987593099475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058195337653160095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5355644822120667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9653235673904419
7 0.0021602436 	 0.965323549
epoch_time;  37.550049781799316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003840239020064473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06347156316041946
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5719782114028931
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9996439218521118
8 0.002092014 	 0.9996439309
epoch_time;  38.09884333610535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030967602506279945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31612828373908997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.365220069885254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9675166606903076
9 0.0180913361 	 3.9675167579
epoch_time;  37.445316791534424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004229082725942135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1808711588382721
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3602370023727417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.433985710144043
10 0.0020752412 	 2.4339857764
epoch_time;  37.111055850982666
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019443213241174817
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–‚â–â–‚â–â–â–â–â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–â–â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–â–ƒâ–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.02043
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.08
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.60001
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00072
wandb:                         Train loss 0.00071
wandb: 
wandb: ğŸš€ View run beaming-rabbit-1466 at: https://wandb.ai/nreints/thesis/runs/xsq10wdq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_014735-xsq10wdq/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_020720-04c991i5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-wonton-1472
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/04c991i5
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13416674733161926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0662329196929932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9417189359664917
11 0.0018115847 	 1.941718986
epoch_time;  37.56110858917236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007932682055979967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10672888904809952
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9375894069671631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6795344352722168
12 0.0016792546 	 1.6795344511
epoch_time;  37.35402274131775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012136560399085283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09416596591472626
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8615572452545166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.546738862991333
13 0.0015424606 	 1.5467389156
epoch_time;  36.95872402191162
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008256884757429361
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08562636375427246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.803286075592041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4408315420150757
14 0.0013605784 	 1.4408315791
epoch_time;  37.10297417640686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008969351183623075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07734589278697968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.74057936668396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3449705839157104
15 0.0012525518 	 1.3449706109
epoch_time;  37.09530711174011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008530071936547756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07915042340755463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7196819186210632
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2998098134994507
16 0.0011674102 	 1.2998098506
epoch_time;  36.785770654678345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005080575938336551
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0720326155424118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6806926727294922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.261758804321289
17 0.0011778852 	 1.261758764
epoch_time;  37.21721887588501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011135232634842396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07080385833978653
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6564752459526062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2102442979812622
18 0.0010176631 	 1.2102442623
epoch_time;  37.28303623199463
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022644526325166225
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07799720764160156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6825364828109741
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2500156164169312
19 0.0011144606 	 1.2500155815
epoch_time;  38.60863280296326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005818142089992762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09258245676755905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6260749101638794
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1850032806396484
20 0.0012457219 	 1.1850032749
epoch_time;  39.86758589744568
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011190957156941295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07591856271028519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5995153784751892
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.125617504119873
21 0.0009022683 	 1.125617543
epoch_time;  37.3503212928772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005705063813365996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07491402328014374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.57938152551651
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0853222608566284
22 0.0008909387 	 1.0853222619
epoch_time;  37.012426137924194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003057698719203472
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08138536661863327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5971159934997559
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1060658693313599
23 0.0008857978 	 1.1060659172
epoch_time;  37.72204875946045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005517820827662945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07665189355611801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6513079404830933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1642415523529053
24 0.0009355263 	 1.1642415488
epoch_time;  37.5502233505249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005417683278210461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07196660339832306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.597270667552948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0596457719802856
25 0.0007666528 	 1.0596457306
epoch_time;  37.324464559555054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013932330766692758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07513049989938736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.623564600944519
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0782865285873413
26 0.0008114571 	 1.0782865311
epoch_time;  37.068509101867676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004252821090631187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08031784743070602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6138545870780945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0615403652191162
27 0.0007861089 	 1.0615403098
epoch_time;  37.82675576210022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006225689430721104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08165246248245239
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6287156939506531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0649890899658203
28 0.0008131794 	 1.0649890727
epoch_time;  37.15872073173523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007156122010201216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07878078520298004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6000697016716003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0201404094696045
29 0.0007092053 	 1.020140403
epoch_time;  37.265501260757446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007157846703194082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08000311255455017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6000059843063354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0204275846481323
It took  1184.3049840927124  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149132834a60>, <torch.utils.data.dataloader.DataLoader object at 0x14912a0e92d0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a872ce0>, <torch.utils.data.dataloader.DataLoader object at 0x14912a873910>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027409836649894714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4546675682067871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8779507279396057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.490170955657959
0 1.0438322057 	 2.4901709427
epoch_time;  37.37434244155884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007874232716858387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20563822984695435
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8138393759727478
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.9203599691390991
1 0.0182205131 	 1.9203600005
epoch_time;  38.64620637893677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0053567285649478436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1260669082403183
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7920754551887512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6615749597549438
2 0.0078169392 	 1.6615749947
epoch_time;  37.808467626571655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038000186905264854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09839814156293869
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7836418151855469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5287775993347168
3 0.0047291623 	 1.5287776152
epoch_time;  37.14400029182434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013845288194715977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07950291782617569
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7426553964614868
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.406893253326416
4 0.0036183797 	 1.4068932663
epoch_time;  37.71973204612732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003916982561349869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09773579239845276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7705140113830566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4978508949279785
5 0.0051893325 	 1.497850954
epoch_time;  37.61489534378052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003935771528631449
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0745077133178711
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6769705414772034
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.276201844215393
6 0.0022470854 	 1.2762018763
epoch_time;  37.62647223472595
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008138217381201684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0606033131480217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6466732025146484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1927725076675415
7 0.0020923929 	 1.192772534
epoch_time;  37.22526407241821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031420195009559393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26326432824134827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3570761680603027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.952280044555664
8 0.0120604693 	 3.9522800964
epoch_time;  37.225497007369995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011580000864341855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12066905200481415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.2417083978652954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0823280811309814
9 0.0017100294 	 2.082328128
epoch_time;  37.27468991279602
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008390992297790945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09862309694290161
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.016836404800415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7035735845565796
10 0.0015951365 	 1.7035736361
epoch_time;  37.387590408325195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007886174134910107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08832459896802902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8988164067268372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5370025634765625
11 0.0015692787 	 1.5370025174
epoch_time;  37.68337154388428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006452624220401049
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07932140678167343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.790293276309967
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3846535682678223
12 0.0014407272 	 1.3846535121
epoch_time;  40.04953360557556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001230622990988195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07934635132551193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7352439761161804
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3093174695968628
13 0.0013165606 	 1.3093174131
epoch_time;  38.14030694961548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015154078137129545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07721881568431854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6830390095710754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2309741973876953
14 0.0012035798 	 1.2309741801
epoch_time;  37.187084436416626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00047315770643763244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07428709417581558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6483155488967896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1916872262954712
15 0.0013023747 	 1.1916872699
epoch_time;  37.40357947349548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002254852093756199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07365020364522934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6349586844444275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1530920267105103
16 0.0010241532 	 1.15309203
epoch_time;  37.314204931259155
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000926918291952461
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07152815163135529
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6234427690505981
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1339151859283447
17 0.001036131 	 1.1339151895
epoch_time;  37.242698192596436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012923026224598289
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0723256841301918
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6065505743026733
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1139403581619263
18 0.0009888871 	 1.1139403744
epoch_time;  37.77612638473511
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005227701622061431
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06564933061599731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6066455841064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0990427732467651
19 0.0009439512 	 1.0990427253
epoch_time;  37.78903675079346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010259024566039443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06704873591661453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5961938500404358
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0946667194366455
20 0.0009589028 	 1.0946667259
epoch_time;  37.35180115699768
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008948982576839626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06458406150341034
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5907838940620422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0729610919952393
21 0.0008541975 	 1.0729610754
epoch_time;  37.64142346382141
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001616278663277626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0671558529138565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6175634860992432
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1114321947097778
22 0.0009047452 	 1.1114322167
epoch_time;  38.080387115478516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005912588094361126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06363631039857864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6303101181983948
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1186174154281616
23 0.0008331344 	 1.1186174007
epoch_time;  37.08104586601257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021683527156710625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07525024563074112
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7248225212097168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2763525247573853
24 0.000861621 	 1.276352528
epoch_time;  37.54618549346924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004555455525405705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10719794780015945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8949041962623596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5984410047531128
25 0.0015576534 	 1.5984410404
epoch_time;  37.3134126663208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006042956374585629
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08389122039079666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7216757535934448
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2807223796844482
26 0.0005347807 	 1.2807223502
epoch_time;  37.57660627365112
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005438272492028773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07829520106315613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6945615410804749
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.235466480255127
27 0.0007556208 	 1.2354665336
epoch_time;  37.519710540771484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004328398790676147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08202612400054932
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7297676801681519
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–‚â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.1896
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.0777
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.66827
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00042
wandb:                         Train loss 0.00065
wandb: 
wandb: ğŸš€ View run dancing-wonton-1472 at: https://wandb.ai/nreints/thesis/runs/04c991i5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_020720-04c991i5/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_022708-zhjwjfhs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-snake-1479
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/zhjwjfhs
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2852662801742554
28 0.0008637663 	 1.2852663346
epoch_time;  37.9298152923584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004169316089246422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07760009169578552
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6680178642272949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1877888441085815
29 0.0006539434 	 1.1877888567
epoch_time;  37.368773460388184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00041706368210725486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07769738882780075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6682736277580261
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1896040439605713
It took  1188.5252242088318  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14912a899f30>, <torch.utils.data.dataloader.DataLoader object at 0x14912a802e00>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfdb10>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfd780>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03510921075940132
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48730552196502686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8658860325813293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6959388256073
0 1.0464817137 	 2.6959387096
epoch_time;  37.47142767906189
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008695604279637337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22418853640556335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7775558233261108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0304694175720215
1 0.0194512063 	 2.0304694507
epoch_time;  37.028764963150024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004972738213837147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1440199464559555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8050543665885925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8135288953781128
2 0.0082043403 	 1.813528931
epoch_time;  38.1148521900177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020712180994451046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1090875118970871
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7652456760406494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6458362340927124
3 0.0049137589 	 1.6458362222
epoch_time;  36.92079281806946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018086951458826661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08874909579753876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.747567892074585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5626287460327148
4 0.0038788537 	 1.5626287086
epoch_time;  40.44936990737915
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011389883002266288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0791364386677742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7208001613616943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4766610860824585
5 0.0032172347 	 1.4766610598
epoch_time;  37.2337532043457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017054140334948897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06895224004983902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7008905410766602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4095350503921509
6 0.0025732626 	 1.4095350191
epoch_time;  36.872440814971924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012918327702209353
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06257124990224838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6735411882400513
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3359746932983398
7 0.0022754967 	 1.335974748
epoch_time;  37.053455114364624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016978373751044273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1673174500465393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.7624932527542114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.024697780609131
8 0.0091972351 	 3.0246976639
epoch_time;  36.82961082458496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016484629595652223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11484017968177795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.141350269317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1283812522888184
9 0.0015339344 	 2.1283812739
epoch_time;  37.270312547683716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009461310692131519
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08684518188238144
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9330981969833374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.789662480354309
10 0.0015571576 	 1.7896625254
epoch_time;  37.15652513504028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006963438354432583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08120286464691162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8840082883834839
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7101267576217651
11 0.0016061971 	 1.7101267097
epoch_time;  37.24505639076233
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014591412618756294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08402067422866821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.840696394443512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6563807725906372
12 0.0013826739 	 1.6563807369
epoch_time;  37.34077525138855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019977616611868143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08322121202945709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7788735032081604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.55977201461792
13 0.0012456925 	 1.559772042
epoch_time;  37.781941413879395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028786903712898493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08825617283582687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7409996390342712
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5075526237487793
14 0.001207914 	 1.5075525935
epoch_time;  37.74725317955017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008927228045649827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09840072691440582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.9390636682510376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7895457744598389
15 0.0032241036 	 1.7895458026
epoch_time;  37.19822144508362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009544246131554246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09424859285354614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7500489950180054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5004751682281494
16 0.0009547414 	 1.5004751891
epoch_time;  37.00312519073486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012203480582684278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09187563508749008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6889894008636475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4000989198684692
17 0.0010362889 	 1.4000989101
epoch_time;  37.11004137992859
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005065851146355271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08025406301021576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.641416072845459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2916717529296875
18 0.0010001682 	 1.2916717068
epoch_time;  36.93700385093689
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000575278711039573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07981652766466141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5977871417999268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2207541465759277
19 0.0009537565 	 1.2207542028
epoch_time;  37.017093896865845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008269025129266083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07856512069702148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6075904369354248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2082387208938599
20 0.0009493778 	 1.2082387688
epoch_time;  37.28027820587158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005269862595014274
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‡â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–â–â–â–ƒâ–‚â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.07739
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.0737
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.57654
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00047
wandb:                         Train loss 0.00077
wandb: 
wandb: ğŸš€ View run enchanting-snake-1479 at: https://wandb.ai/nreints/thesis/runs/zhjwjfhs
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_022708-zhjwjfhs/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_024655-ngh86hnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-fish-1486
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ngh86hnp
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07108564674854279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.559977114200592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1181845664978027
21 0.0008690469 	 1.1181846227
epoch_time;  38.232850313186646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007589343003928661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07327385991811752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5602902173995972
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.115981936454773
22 0.0008592131 	 1.1159819174
epoch_time;  37.37211799621582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000529158569406718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07366887480020523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.552315890789032
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.108032464981079
23 0.001274525 	 1.108032411
epoch_time;  38.57582974433899
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004859972686972469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.067049041390419
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5411468744277954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0596507787704468
24 0.0008113734 	 1.0596508015
epoch_time;  37.583922147750854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011351550929248333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06978178769350052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5340060591697693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0420928001403809
25 0.0008157723 	 1.0420927757
epoch_time;  36.847907066345215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007243068539537489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07442305237054825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5564777851104736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0875821113586426
26 0.0007952026 	 1.0875821301
epoch_time;  37.84813570976257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005588415660895407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07422423362731934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5640206336975098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.079401969909668
27 0.0007751421 	 1.079401944
epoch_time;  37.869468450546265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000515389780048281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0770634263753891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5438011288642883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0633571147918701
28 0.0007434898 	 1.0633570737
epoch_time;  41.02839183807373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00047238991828635335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07373957335948944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5763322710990906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0771620273590088
29 0.0007651433 	 1.0771620828
epoch_time;  37.30052971839905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004727387859020382
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07369881868362427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5765398144721985
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0773946046829224
It took  1187.2634274959564  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149130d6cfa0>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcffbe0>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfd1e0>, <torch.utils.data.dataloader.DataLoader object at 0x1490fbcfc220>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.027884721755981445
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5336208343505859
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6856808662414551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3436989784240723
0 1.0147671811 	 2.3436989222
epoch_time;  37.476027488708496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015926048159599304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24520093202590942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6385799646377563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.7478326559066772
1 0.0179388752 	 1.7478326066
epoch_time;  37.78587985038757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008016079664230347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15739692747592926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6233680248260498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.521502137184143
2 0.0073326979 	 1.5215021692
epoch_time;  37.623926401138306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004510624334216118
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11205630749464035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6122129559516907
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.392920970916748
3 0.0047465132 	 1.3929210098
epoch_time;  36.81878709793091
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004982294049113989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31994038820266724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.618591785430908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.507602214813232
4 0.0169742102 	 4.5076022883
epoch_time;  36.68378806114197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018511523958295584
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14463815093040466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.298787236213684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.356574296951294
5 0.0028960241 	 2.3565742055
epoch_time;  37.22481346130371
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015319118974730372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10975714027881622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0101710557937622
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8430509567260742
6 0.0024018165 	 1.8430509538
epoch_time;  37.37465739250183
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022891536355018616
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09674609452486038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8915255665779114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.649685263633728
7 0.0023100941 	 1.6496853094
epoch_time;  37.03498554229736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009988159872591496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08285585045814514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7994226813316345
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.50467848777771
8 0.0020462813 	 1.5046785015
epoch_time;  36.934372663497925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009231063304468989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08189455419778824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.749949038028717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4360769987106323
9 0.0018794275 	 1.4360770142
epoch_time;  37.0379855632782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0012206240789964795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07276924699544907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6847897171974182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3202009201049805
10 0.0016167648 	 1.3202009403
epoch_time;  37.30698347091675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001797528937458992
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07071653753519058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6480288505554199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.256724238395691
11 0.0014452866 	 1.2567241934
epoch_time;  37.80309057235718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17335999011993408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5042374730110168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2045435905456543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.8264050483703613
12 0.0027359534 	 3.8264049519
epoch_time;  37.04419159889221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006666298140771687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09135560691356659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8519529700279236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6013773679733276
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‡â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–‚â–…â–‚â–‚â–‚â–‚â–‚â–â–â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–‚â–‚â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‡â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.84583
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.04024
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.44721
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00097
wandb:                         Train loss 0.00076
wandb: 
wandb: ğŸš€ View run beaming-fish-1486 at: https://wandb.ai/nreints/thesis/runs/ngh86hnp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_024655-ngh86hnp/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_030637-v1klaqgv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lambent-festival-1493
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/v1klaqgv
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
13 0.0019487299 	 1.6013773662
epoch_time;  37.15619397163391
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006891530938446522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07465687394142151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6833482980728149
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3184454441070557
14 0.001132496 	 1.318445488
epoch_time;  36.617063999176025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006728540174663067
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07080339640378952
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6470626592636108
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2586661577224731
15 0.0011961075 	 1.2586661624
epoch_time;  37.01585245132446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007899711490608752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.06501059234142303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5993854403495789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1836835145950317
16 0.0010834095 	 1.1836834588
epoch_time;  36.99356412887573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007551998132839799
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.058654382824897766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5434359908103943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0786017179489136
17 0.0010245394 	 1.0786016643
epoch_time;  37.71069836616516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005989403580315411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.05484955012798309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4988442063331604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0053919553756714
18 0.0010224211 	 1.0053919305
epoch_time;  37.13819241523743
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000805020157713443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0520341657102108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.49481698870658875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9882403016090393
19 0.001014771 	 0.988240314
epoch_time;  36.97450399398804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004969527362845838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.049248166382312775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4747910499572754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9490025639533997
20 0.0009551918 	 0.9490025454
epoch_time;  38.43744206428528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006331708864308894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04726254567503929
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4613782465457916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9110603332519531
21 0.0009110796 	 0.9110603448
epoch_time;  38.943658113479614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0014146949397400022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04594345763325691
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4352080523967743
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8703319430351257
22 0.0008749723 	 0.8703319169
epoch_time;  37.3021924495697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016095539322122931
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04365283623337746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4629608690738678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8945192694664001
23 0.0009327177 	 0.8945192642
epoch_time;  36.91972589492798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010772458044812083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04274710267782211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4391956031322479
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.855316698551178
24 0.0008400989 	 0.8553167153
epoch_time;  37.39638042449951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0004046546237077564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.040279321372509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4388604760169983
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8412455320358276
25 0.0008240583 	 0.8412455302
epoch_time;  37.56702733039856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008510160259902477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.041677605360746384
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.438849538564682
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8526418805122375
26 0.000825639 	 0.8526418634
epoch_time;  37.65234351158142
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005700259935110807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04142102599143982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4526950418949127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8646261692047119
27 0.0008224168 	 0.864626144
epoch_time;  37.77004647254944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00042979305726476014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.039281390607357025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4442374110221863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8422405123710632
28 0.0007579717 	 0.8422405324
epoch_time;  37.280669927597046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009748522425070405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04084063693881035
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44756069779396057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8464641571044922
29 0.0007594222 	 0.8464641283
epoch_time;  37.300214529037476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0009746961295604706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.04024013504385948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4472077786922455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8458348512649536
It took  1181.3553771972656  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x149130d331c0>, <torch.utils.data.dataloader.DataLoader object at 0x149130d6e200>, <torch.utils.data.dataloader.DataLoader object at 0x149130d6d0c0>, <torch.utils.data.dataloader.DataLoader object at 0x149130d6d180>]
LSTM(
  (lstm): LSTM(7, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030500955879688263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5346683263778687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7843024730682373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4087698459625244
0 1.0380685649 	 2.4087697747
epoch_time;  37.00710654258728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014404365792870522
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23574425280094147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7288278341293335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.769734501838684
1 0.019158712 	 1.7697344547
epoch_time;  37.382500648498535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003637497080489993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1365676075220108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7438582181930542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5710951089859009
2 0.0080312906 	 1.5710950777
epoch_time;  37.526456356048584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002695190953090787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10245492309331894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7624551653862
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.489372968673706
3 0.005259947 	 1.489372968
epoch_time;  37.680548429489136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015152107225731015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08441208302974701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6952001452445984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.331516981124878
4 0.003403564 	 1.3315169689
epoch_time;  37.77584195137024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015021230792626739
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.07295078039169312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6737614870071411
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2595422267913818
5 0.0027493884 	 1.259542229
epoch_time;  37.39646768569946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005969127174466848
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4309122562408447
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–‚â–‚â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–ƒâ–‚â–â–â–â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–‚â–‚â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–‚â–‚â–â–â–‚â–‚â–â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 1.01055
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.10449
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.55009
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00075
wandb:                         Train loss 0.00073
wandb: 
wandb: ğŸš€ View run lambent-festival-1493 at: https://wandb.ai/nreints/thesis/runs/v1klaqgv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_030637-v1klaqgv/logs
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.79693603515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5323166847229
6 0.0328944117 	 4.5323169167
epoch_time;  36.7690212726593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027474104426801205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27542948722839355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.850831151008606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1072275638580322
7 0.0037851633 	 3.1072275214
epoch_time;  37.01009440422058
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016481142956763506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21583935618400574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.506398320198059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.591801643371582
8 0.0029965369 	 2.5918016693
epoch_time;  37.10998511314392
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00809318944811821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18375463783740997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.3155303001403809
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.286630392074585
9 0.0024735435 	 2.286630498
epoch_time;  37.39848184585571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001786999637261033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1567031294107437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.204743504524231
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.097101926803589
10 0.0020613835 	 2.0971019549
epoch_time;  37.72174644470215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025278013199567795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14423859119415283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.0228317975997925
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.8442832231521606
11 0.0017326089 	 1.8442832739
epoch_time;  36.55589318275452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001354325097054243
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12425442785024643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8878551721572876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.639339804649353
12 0.0015578961 	 1.6393397582
epoch_time;  37.02017092704773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003289594780653715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11382434517145157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7806475758552551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.4704740047454834
13 0.0013603611 	 1.4704740127
epoch_time;  40.3899359703064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000717535731382668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10211426764726639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7088331580162048
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3514900207519531
14 0.0012974772 	 1.3514900323
epoch_time;  37.2342483997345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008558742702007294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20639438927173615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.1783177852630615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.092395305633545
15 0.0057230761 	 2.0923952408
epoch_time;  37.45549154281616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006524015334434807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16524219512939453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.8557091355323792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.6141479015350342
16 0.0010798238 	 1.6141479123
epoch_time;  37.181102991104126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0005463731940835714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1432357132434845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.7175462245941162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.3939460515975952
17 0.0011058947 	 1.3939460685
epoch_time;  37.63996148109436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008408126304857433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12691086530685425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6273757815361023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.2344639301300049
18 0.0010416527 	 1.234463879
epoch_time;  37.75703501701355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007179381791502237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.119652658700943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5856071710586548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.153414249420166
19 0.0009914204 	 1.1534142624
epoch_time;  37.09632682800293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007085922989062965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11286131292581558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6141185760498047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1596543788909912
20 0.0009603249 	 1.1596544156
epoch_time;  37.28105187416077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.000465244404040277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1018679291009903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6160805225372314
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1208324432373047
21 0.0008663457 	 1.1208324605
epoch_time;  38.1215603351593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0010084378300234675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1235213354229927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6417089700698853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1729955673217773
22 0.0015846781 	 1.172995576
epoch_time;  37.8328115940094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00069069629535079
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11181921511888504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5982652306556702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0909547805786133
23 0.0007521287 	 1.0909548296
epoch_time;  37.24803066253662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0008372007869184017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10965786129236221
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5892769694328308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.070422887802124
24 0.0008006848 	 1.0704228612
epoch_time;  37.41476321220398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007663253927603364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10774854570627213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6000301241874695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0689492225646973
25 0.0007964684 	 1.0689492586
epoch_time;  37.14718198776245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0011629650834947824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10701814293861389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.6186593770980835
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0882279872894287
26 0.0007817766 	 1.0882279779
epoch_time;  37.12177586555481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00998980738222599
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11580651998519897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5709558725357056
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0234931707382202
27 0.0007552569 	 1.0234931877
epoch_time;  37.068416595458984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0006139036850072443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09813985228538513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5476729869842529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9907912611961365
28 0.0007665195 	 0.9907912517
epoch_time;  37.681050300598145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007498663035221398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.103901207447052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5497428774833679
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.0112627744674683
29 0.0007274017 	 1.0112628303
epoch_time;  37.446818113327026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0007501064683310688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10449479520320892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5500878691673279
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.010554552078247
It took  1185.2941317558289  seconds.

JOB STATISTICS
==============
Job ID: 2141351
Array Job ID: 2141141_4
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-11:29:06 core-walltime
Job Wall-clock time: 03:18:17
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
