wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_170459-6thfsdnu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-firecracker-1151
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6thfsdnu
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–„â–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–…â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–‡â–…â–…â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–†â–„â–ƒâ–„â–„â–„â–ƒâ–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–„â–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–†â–ˆâ–„â–…â–…â–ƒâ–ƒâ–‚â–ƒâ–â–…â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.46425
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.39069
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.40075
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.32372
wandb:                         Train loss 3.90873
wandb: 
wandb: ðŸš€ View run glowing-firecracker-1151 at: https://wandb.ai/nreints/thesis/runs/6thfsdnu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_170459-6thfsdnu/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43276384472846985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5967186093330383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5903508067131042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7661012411117554
0 8.2843158905 	 0.7661012392 	 0.7661012392
epoch_time;  31.012707710266113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41668471693992615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5429497957229614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5008043050765991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6337324976921082
1 4.4383383759 	 0.6337324813 	 0.6337324813
epoch_time;  30.267525911331177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46081358194351196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5708832144737244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.5139933824539185
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.629509687423706
2 4.2979735502 	 0.6295097145 	 0.6295097145
epoch_time;  30.16201114654541
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37696078419685364
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5064232349395752
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.48769575357437134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6135256290435791
3 4.2083708002 	 0.6135256381 	 0.6135256381
epoch_time;  30.08991026878357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.396892249584198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4993511438369751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.46608173847198486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5717213153839111
4 4.1533874222 	 0.5717212986 	 0.5717212986
epoch_time;  30.05177116394043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3947003483772278
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5176233649253845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4740273058414459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5986498594284058
5 4.1046022739 	 0.5986498446 	 0.5986498446
epoch_time;  31.230680465698242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3596310317516327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4575231075286865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4384112060070038
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.526401937007904
6 4.075432676 	 0.5264019116 	 0.5264019116
epoch_time;  31.24925684928894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37058043479919434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4466938376426697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.45647990703582764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5297229886054993
7 4.0438725827 	 0.5297229664 	 0.5297229664
epoch_time;  30.389826774597168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3346328139305115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42069268226623535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4332616925239563
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5163175463676453
8 4.0213977605 	 0.5163175428 	 0.5163175428
epoch_time;  29.904513835906982
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3575793206691742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45098015666007996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.41682833433151245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5066564083099365
9 4.0091907292 	 0.5066564199 	 0.5066564199
epoch_time;  30.20766282081604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33098602294921875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4161190986633301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4309026896953583
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5165804624557495
10 3.9910456426 	 0.5165804889 	 0.5165804889
epoch_time;  30.37940001487732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41042137145996094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5388559103012085
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4879082441329956
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.616982638835907
11 3.9740771253 	 0.6169826611 	 0.6169826611
epoch_time;  30.236993074417114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38366249203681946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4651702046394348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.451406866312027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.524415135383606
12 3.9668616706 	 0.5244151347 	 0.5244151347
epoch_time;  30.4398136138916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3533216714859009
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4365641474723816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4299812614917755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5075827836990356
13 3.947044791 	 0.5075827934 	 0.5075827934
epoch_time;  30.015496969223022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37084585428237915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46819037199020386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43233928084373474
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5258009433746338
14 3.9434654944 	 0.5258009627 	 0.5258009627
epoch_time;  30.08973526954651
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3825736939907074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4717106521129608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43796440958976746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.52756267786026
15 3.93253454 	 0.5275626518 	 0.5275626518
epoch_time;  30.15891432762146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3611241579055786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4682319462299347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.44115275144577026
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5442770719528198
16 3.9327289269 	 0.5442770881 	 0.5442770881
epoch_time;  30.337058782577515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3474770486354828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43576693534851074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43653303384780884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5254954099655151
17 3.923219509 	 0.5254954158 	 0.5254954158
epoch_time;  30.3136203289032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3567902445793152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43461787700653076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.407488077878952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48110029101371765
18 3.9198636912 	 0.4811002989 	 0.4811002989
epoch_time;  30.339638471603394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32379350066185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3906688690185547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40070608258247375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46419474482536316
19 3.9087267794 	 0.4641947566 	 0.4641947566
epoch_time;  30.44495964050293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32371777296066284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39068540930747986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4007458984851837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4642457365989685
It took 675.5853610038757 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn30: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135417.0

JOB STATISTICS
==============
Job ID: 2135417
Array Job ID: 2135328_22
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:26:42 core-walltime
Job Wall-clock time: 00:11:29
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
