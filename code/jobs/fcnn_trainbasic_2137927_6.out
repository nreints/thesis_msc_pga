wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121535-kixkgis4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-goat-1177
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kixkgis4
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▅▂▂▁█▄█▄▄▄▅▄▄▅▄▂▁▁▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▂▂▁▅▄▃█▆▂▂▄▃▅▆▇▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▅▂▄▁▅▄▄▂▃▆▂▄▅▇▄▄█▄▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▇▇▄▄▄▂▅▃▂▁▂▁▃▅▄▂▂▂▂
wandb:                         Train loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 689.10315
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.02789
wandb:    Test loss t(0, 0)_r(-5, 5)_none 136.30446
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.23293
wandb:                         Train loss 1.73665
wandb: 
wandb: 🚀 View run dancing-goat-1177 at: https://wandb.ai/nreints/thesis/runs/kixkgis4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121535-kixkgis4/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122923-zpd3wnk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-fireworks-1188
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/zpd3wnk1
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4055996835231781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.712065696716309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 163.6057891845703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 895.060302734375
0 3.2916394481 	 895.0603040541 	 895.0603040541
epoch_time;  39.59753775596619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2541455030441284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.588479042053223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 140.24285888671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 784.7429809570312
1 2.081527431 	 784.7429898649 	 784.7429898649
epoch_time;  38.146218061447144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3645549416542053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7204811573028564
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 109.85785675048828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 358.6221618652344
2 1.9878268542 	 358.6221706081 	 358.6221706081
epoch_time;  38.0759060382843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3637946844100952
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.94010066986084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 130.89730834960938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 336.602783203125
3 1.9292301289 	 336.6027871622 	 336.6027871622
epoch_time;  38.152814626693726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29898908734321594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.629967212677002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 93.07186126708984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 257.8879089355469
4 1.8877653976 	 257.8879011824 	 257.8879011824
epoch_time;  37.98706912994385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.271916002035141
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3021950721740723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 144.80343627929688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1240.7740478515625
5 1.8646558427 	 1240.7740709459 	 1240.7740709459
epoch_time;  37.96799850463867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28016355633735657
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.143398284912109
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 135.17628479003906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 765.2420043945312
6 1.844994175 	 765.2419763514 	 765.2419763514
epoch_time;  37.9839084148407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2229519784450531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.947673797607422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 135.73898315429688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1279.821044921875
7 1.8297596017 	 1279.8210304054 	 1279.8210304054
epoch_time;  38.20367932319641
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3204762935638428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2464823722839355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 111.47149658203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 729.0712280273438
8 1.8198508312 	 729.0711993243 	 729.0711993243
epoch_time;  38.09914183616638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24580200016498566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.731725215911865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 114.66874694824219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 670.1886596679688
9 1.8053660357 	 670.1886402027 	 670.1886402027
epoch_time;  38.69443321228027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21036019921302795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.81143045425415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 153.18115234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 701.0858154296875
10 1.7916632506 	 701.0858108108 	 701.0858108108
epoch_time;  38.31155037879944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19317610561847687
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6925101280212402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 107.91351318359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 818.352783203125
11 1.7846291244 	 818.3527871622 	 818.3527871622
epoch_time;  38.68721389770508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22541595995426178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9570233821868896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 129.13475036621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 719.6316528320312
12 1.7711984148 	 719.6316722973 	 719.6316722973
epoch_time;  38.694623708724976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2023567408323288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9145917892456055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 141.74017333984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 681.8601684570312
13 1.7679070619 	 681.8601773649 	 681.8601773649
epoch_time;  38.77717161178589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2562727630138397
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.396576404571533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 168.66273498535156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 873.146728515625
14 1.7645234745 	 873.1467060811 	 873.1467060811
epoch_time;  38.31397724151611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31685781478881836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.377071380615234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 133.11553955078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 753.194091796875
15 1.7577164636 	 753.1940878378 	 753.1940878378
epoch_time;  38.433406829833984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29343485832214355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.6004509925842285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 127.24290466308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 373.687744140625
16 1.7509489582 	 373.6877533784 	 373.6877533784
epoch_time;  38.02225923538208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22214271128177643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.374024868011475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 184.6094207763672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 328.1353759765625
17 1.7429004171 	 328.1353673986 	 328.1353673986
epoch_time;  39.160688638687134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23810221254825592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.5083625316619873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 130.9438018798828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 316.29766845703125
18 1.7404824827 	 316.29765625 	 316.29765625
epoch_time;  38.91914105415344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2329004853963852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0096678733825684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 136.2860565185547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 689.1786499023438
19 1.7366486319 	 689.1786317568 	 689.1786317568
epoch_time;  38.40730690956116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23292860388755798
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.027893304824829
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 136.30445861816406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 689.1031494140625
It took 827.9925785064697 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▃▃▆▂▃▃▂▃█▁▅▁▄▃▃▃▅▂▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅█▃▃▄▅▄▃▅▁▅▃▃▄▃▅▄▃▅▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▄█▃▇█▅▂▁▄▂▃▄▃▆▄▁▄▆▃██
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄█▂▃▃▃▃▂▃▁▃▂▂▃▂▄▃▃▄▂▂
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 544.94171
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.08076
wandb:    Test loss t(0, 0)_r(-5, 5)_none 94.74106
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2524
wandb:                         Train loss 1.74479
wandb: 
wandb: 🚀 View run glistening-fireworks-1188 at: https://wandb.ai/nreints/thesis/runs/zpd3wnk1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122923-zpd3wnk1/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124255-whtep45o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-rocket-1194
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/whtep45o
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3487914502620697
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.29737389087677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 81.39698791503906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 252.5486297607422
0 3.6882395296 	 252.5486275338 	 252.5486275338
epoch_time;  38.09479808807373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5090676546096802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5858629941940308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 92.8939437866211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 391.001953125
1 2.0951874048 	 391.0019425676 	 391.0019425676
epoch_time;  38.448625564575195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2653959095478058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1386923789978027
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.47476959228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 327.08258056640625
2 1.9841204458 	 327.0825802365 	 327.0825802365
epoch_time;  37.78659772872925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2763615846633911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1693686246871948
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 90.5755386352539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 606.99462890625
3 1.9314419405 	 606.9946368243 	 606.9946368243
epoch_time;  38.030797243118286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27022743225097656
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2149378061294556
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 94.25536346435547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 312.42950439453125
4 1.8932131882 	 312.4294974662 	 312.4294974662
epoch_time;  37.92798399925232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28596359491348267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.34348726272583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 85.01193237304688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 378.42584228515625
5 1.8672045254 	 378.4258445946 	 378.4258445946
epoch_time;  37.89846396446228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2973499596118927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2514349222183228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 70.67754364013672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 363.1335754394531
6 1.8470053185 	 363.1335726351 	 363.1335726351
epoch_time;  38.14887022972107
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2608638405799866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.140262246131897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 69.71418762207031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 247.6737823486328
7 1.8318560374 	 247.6737753378 	 247.6737753378
epoch_time;  37.87938952445984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2994089424610138
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3508321046829224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 80.36448669433594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 338.25653076171875
8 1.8109954855 	 338.2565456081 	 338.2565456081
epoch_time;  37.79898190498352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20392519235610962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.962441623210907
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 73.16475677490234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 772.5825805664062
9 1.7986897731 	 772.5826013514 	 772.5826013514
epoch_time;  37.89942979812622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28984126448631287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2767610549926758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 75.65532684326172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 242.15870666503906
10 1.7877705548 	 242.1586993243 	 242.1586993243
epoch_time;  37.77725958824158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2648787498474121
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1748639345169067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.96068572998047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 531.7579345703125
11 1.7807559334 	 531.7579391892 	 531.7579391892
epoch_time;  37.948891162872314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26631876826286316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1273789405822754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.29566955566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 203.8435821533203
12 1.7712480032 	 203.8435810811 	 203.8435810811
epoch_time;  37.744104862213135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2708158791065216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2293648719787598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 85.37107849121094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 424.6900634765625
13 1.7631054092 	 424.6900760135 	 424.6900760135
epoch_time;  37.70670461654663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2522181272506714
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0997775793075562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 77.90668487548828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 337.6377868652344
14 1.7564860081 	 337.6377956081 	 337.6377956081
epoch_time;  38.08483004570007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3317245543003082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.324815273284912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 68.32954406738281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 329.84613037109375
15 1.7495436207 	 329.8461359797 	 329.8461359797
epoch_time;  38.46813130378723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27737393975257874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2064777612686157
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 79.15369415283203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 350.03094482421875
16 1.7509562312 	 350.030933277 	 350.030933277
epoch_time;  38.522730112075806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2822226881980896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1439628601074219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 87.45854949951172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 517.4640502929688
17 1.7470132857 	 517.4640202703 	 517.4640202703
epoch_time;  38.17597508430481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3379102051258087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3589789867401123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 75.66837310791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 262.7711486816406
18 1.7359135653 	 262.7711359797 	 262.7711359797
epoch_time;  37.985146045684814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2524876594543457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.080651879310608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 94.68563842773438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 545.9971923828125
19 1.7447894076 	 545.9972128378 	 545.9972128378
epoch_time;  37.842973470687866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2524004876613617
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.08075749874115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 94.74105834960938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 544.9417114257812
It took 812.0097539424896 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃█▃▅▅▁▁▂▂▁▁▃▃▁▆▃▄▅▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄█▅▄▇▃▃▃▂▃▃▃▄▄▁▆▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▆▅▆█▃▂▃▄▁▁▃▃▂▄▃▂▂▅▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▆█▇▃▆▆▄▆▃▆▄▄▇▂▇▆▁▁▂▂
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 389.35904
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.39573
wandb:    Test loss t(0, 0)_r(-5, 5)_none 249.66191
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.18163
wandb:                         Train loss 1.77573
wandb: 
wandb: 🚀 View run beaming-rocket-1194 at: https://wandb.ai/nreints/thesis/runs/whtep45o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124255-whtep45o/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125633-b2b24gc6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-laughter-1201
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/b2b24gc6
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.254083514213562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.988169193267822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 353.3766784667969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 531.5603637695312
0 3.3722590793 	 531.5603462838 	 531.5603462838
epoch_time;  38.26970601081848
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2433706372976303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.625701904296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 394.4098815917969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1372.09228515625
1 2.0933703175 	 1372.0923141892 	 1372.0923141892
epoch_time;  38.42801523208618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2873854339122772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.103579521179199
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 375.4844055175781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 582.14697265625
2 1.9860458012 	 582.1469594595 	 582.1469594595
epoch_time;  37.909706592559814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27463045716285706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.733948230743408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 399.4541015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 918.1071166992188
3 1.9278971905 	 918.1070945946 	 918.1070945946
epoch_time;  38.38299202919006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19738227128982544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.32222318649292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 514.582275390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 961.1393432617188
4 1.898959832 	 961.1393581081 	 961.1393581081
epoch_time;  37.96550726890564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24442000687122345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7973358631134033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 271.729248046875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 360.4220886230469
5 1.8679914946 	 360.4220861486 	 360.4220861486
epoch_time;  38.23132801055908
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25502681732177734
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.356514930725098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 219.04364013671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 299.0607604980469
6 1.8495156528 	 299.0607474662 	 299.0607474662
epoch_time;  38.245018005371094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2155909538269043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.421365976333618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 246.16587829589844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 402.6466979980469
7 1.8387628864 	 402.6467060811 	 402.6467060811
epoch_time;  38.21629858016968
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2526879906654358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.241668462753296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 332.17388916015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 444.6772766113281
8 1.8255911939 	 444.6772804054 	 444.6772804054
epoch_time;  38.3106849193573
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19480885565280914
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3511626720428467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 154.74844360351562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 315.5540771484375
9 1.8169522192 	 315.5540751689 	 315.5540751689
epoch_time;  37.619791984558105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24664419889450073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0348920822143555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 177.49635314941406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 306.56396484375
10 1.8076147081 	 306.5639569257 	 306.5639569257
epoch_time;  38.03491401672363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22216013073921204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3971121311187744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 269.69305419921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 672.5191650390625
11 1.7993316134 	 672.5191722973 	 672.5191722973
epoch_time;  38.39434194564819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2162618339061737
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.232367992401123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 232.7891845703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 632.5079345703125
12 1.7990367272 	 632.5079391892 	 632.5079391892
epoch_time;  38.41961407661438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2721516489982605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3254027366638184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 190.69480895996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 313.044677734375
13 1.7912676998 	 313.0446790541 	 313.0446790541
epoch_time;  38.611085176467896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17237715423107147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.976604461669922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 284.79095458984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1086.8427734375
14 1.7866605992 	 1086.8427364865 	 1086.8427364865
epoch_time;  38.09199118614197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.275630384683609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.060305595397949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 249.94512939453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 628.9605712890625
15 1.7836406607 	 628.9605996622 	 628.9605996622
epoch_time;  38.53487515449524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24267293512821198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.520206928253174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 185.9008026123047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 773.5064086914062
16 1.7780882384 	 773.5064189189 	 773.5064189189
epoch_time;  38.29581594467163
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16513828933238983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.131876468658447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 204.65846252441406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 867.8252563476562
17 1.7720191041 	 867.8252533784 	 867.8252533784
epoch_time;  38.078097343444824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15992362797260284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7092881202697754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 371.65753173828125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 597.4974365234375
18 1.7712744504 	 597.4974662162 	 597.4974662162
epoch_time;  38.80909037590027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18166932463645935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.396267890930176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 249.65538024902344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 390.6846008300781
19 1.7757334467 	 390.6845861486 	 390.6845861486
epoch_time;  38.1568865776062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18162648379802704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3957347869873047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 249.6619110107422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 389.3590393066406
It took 818.711555480957 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▅▄▅▅▃▅▁▁▁▂▂▇▁▁▂▅▁█▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▂▁▃▁▃▃▂▁▃▁▂▄▄▂▃▂▂▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▂▃▄▅▃▄▂▄▂▂▂▃▃▄▂▁▃▁▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆█▂▂▂▂▃▁▂▂▂▁▁▁▂▁▂▃▁▁▁
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 478.89722
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.2245
wandb:    Test loss t(0, 0)_r(-5, 5)_none 107.41595
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20038
wandb:                         Train loss 1.76447
wandb: 
wandb: 🚀 View run crimson-laughter-1201 at: https://wandb.ai/nreints/thesis/runs/b2b24gc6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125633-b2b24gc6/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_131007-v9zpe2n1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-rat-1208
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/v9zpe2n1
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5492004156112671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9478914737701416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 142.69491577148438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 601.7075805664062
0 3.4490498186 	 601.7075591216 	 601.7075591216
epoch_time;  37.69817876815796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.656076967716217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.618149757385254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 90.44326782226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 791.098876953125
1 2.0948060694 	 791.098902027 	 791.098902027
epoch_time;  38.187987089157104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2659836709499359
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8616914749145508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 92.77831268310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 585.2758178710938
2 1.985946459 	 585.2758023649 	 585.2758023649
epoch_time;  38.21523451805115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28863513469696045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.832561731338501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 108.27855682373047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 818.1326904296875
3 1.934689986 	 818.1326858108 	 818.1326858108
epoch_time;  37.72771620750427
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27680251002311707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.131910800933838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 113.07130432128906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 747.3961791992188
4 1.8966301651 	 747.3961993243 	 747.3961993243
epoch_time;  38.04457998275757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2538530230522156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7486714124679565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 99.94105529785156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 540.9937744140625
5 1.8663970596 	 540.9937922297 	 540.9937922297
epoch_time;  38.21626877784729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.311542809009552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.136603593826294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 102.4015121459961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 799.9920043945312
6 1.8537217478 	 799.9919763514 	 799.9919763514
epoch_time;  38.163989782333374
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2092682272195816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0964572429656982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 83.00221252441406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 292.60009765625
7 1.8364961969 	 292.6001055743 	 292.6001055743
epoch_time;  38.453550577163696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2562544047832489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9667445421218872
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 104.97205352783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 333.6528625488281
8 1.8243096622 	 333.6528505068 	 333.6528505068
epoch_time;  38.19297933578491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28677377104759216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.822789192199707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 84.06802368164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 277.8232116699219
9 1.8140435241 	 277.8232052365 	 277.8232052365
epoch_time;  37.96629762649536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24161411821842194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.033726692199707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 87.29849243164062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 361.51690673828125
10 1.8067275701 	 361.5168918919 	 361.5168918919
epoch_time;  38.20982813835144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19087204337120056
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8301327228546143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 82.60770416259766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 341.7383728027344
11 1.7975017984 	 341.7383657095 	 341.7383657095
epoch_time;  38.205700159072876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19986492395401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.942330241203308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 90.76679992675781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 962.9559936523438
12 1.7921174811 	 962.9559966216 	 962.9559966216
epoch_time;  37.95392608642578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2023925632238388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1936702728271484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 95.06452941894531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 312.30706787109375
13 1.7861043818 	 312.3070734797 	 312.3070734797
epoch_time;  38.1730797290802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23722727596759796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.187988042831421
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 101.29959869384766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 295.79339599609375
14 1.7821412703 	 295.7933910473 	 295.7933910473
epoch_time;  38.253873348236084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20471717417240143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8409888744354248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 81.61294555664062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 427.37518310546875
15 1.7791372001 	 427.3751689189 	 427.3751689189
epoch_time;  38.068174600601196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26402366161346436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.0896897315979004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 76.38261413574219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 753.7027587890625
16 1.7785421383 	 753.7027871622 	 753.7027871622
epoch_time;  38.06185245513916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3298599123954773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9452911615371704
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 91.50117492675781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 314.0264587402344
17 1.7804246824 	 314.0264569257 	 314.0264569257
epoch_time;  38.482348680496216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20302098989486694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9461708068847656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 78.14983367919922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1132.7154541015625
18 1.7714321418 	 1132.7154560811 	 1132.7154560811
epoch_time;  38.30876636505127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20036950707435608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2247719764709473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 106.46604919433594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 479.3287048339844
19 1.7644715719 	 479.3287162162 	 479.3287162162
epoch_time;  38.375404834747314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20038336515426636
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.224503993988037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 107.41595458984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 478.897216796875
It took 813.4972217082977 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▂▅█▂▂▄▄▃▂▂▂▂▂▂▂▁▃▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▅▇▇▃▅▄▃▅█▅▅▅▅▅▁▃▄▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▂▂▂▄▂▂▁▂▂▂▂▃▂▂▄▆▁▄▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▄▇█▁▃▃▂▄▇▄▃▆▆▄▂▃▄▂▄▄
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5105.40234
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.2616
wandb:    Test loss t(0, 0)_r(-5, 5)_none 405.36057
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27034
wandb:                         Train loss 1.74132
wandb: 
wandb: 🚀 View run sweet-rat-1208 at: https://wandb.ai/nreints/thesis/runs/v9zpe2n1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_131007-v9zpe2n1/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132338-h9agw4in
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-rabbit-1216
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/h9agw4in
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27575498819351196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2172783613204956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 886.7100219726562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2209.544677734375
0 3.5230364945 	 2209.5445945946 	 2209.5445945946
epoch_time;  37.81769871711731
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26000815629959106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4942821264266968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 289.32763671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1308.7652587890625
1 2.0812697362 	 1308.7652871622 	 1308.7652871622
epoch_time;  38.23187446594238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3342173397541046
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6776866912841797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 231.86795043945312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2111.772216796875
2 1.9766200226 	 2111.7721283784 	 2111.7721283784
epoch_time;  37.8744592666626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34618261456489563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.651663899421692
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 293.30145263671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6984.9599609375
3 1.9218883889 	 6984.9601351351 	 6984.9601351351
epoch_time;  38.01484823226929
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21165239810943604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.318955898284912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 485.3036193847656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11805.44140625
4 1.8894735555 	 11805.4418918919 	 11805.4418918919
epoch_time;  38.157118797302246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25029557943344116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5196259021759033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 301.00030517578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2096.648193359375
5 1.8632159585 	 2096.6483108108 	 2096.6483108108
epoch_time;  38.4304141998291
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2532508969306946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3914811611175537
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 206.4423370361328
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1831.3297119140625
6 1.8531220578 	 1831.3297297297 	 1831.3297297297
epoch_time;  38.26763653755188
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2296275794506073
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.349588394165039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 193.2412567138672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5363.376953125
7 1.836574712 	 5363.377027027 	 5363.377027027
epoch_time;  38.48047590255737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2771349251270294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4723429679870605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 237.13226318359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5547.89306640625
8 1.8179590307 	 5547.8929054054 	 5547.8929054054
epoch_time;  38.13508319854736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32740962505340576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7466598749160767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 301.17877197265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4512.95166015625
9 1.8063882196 	 4512.9516891892 	 4512.9516891892
epoch_time;  37.952532052993774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2723214626312256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5134122371673584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 203.66323852539062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2101.8212890625
10 1.7890005144 	 2101.8212837838 	 2101.8212837838
epoch_time;  38.16867756843567
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.255817711353302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4759396314620972
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 221.4768829345703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1885.69140625
11 1.7826307086 	 1885.6913851351 	 1885.6913851351
epoch_time;  37.944868326187134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3025639057159424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.50668466091156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 364.5743713378906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1559.720947265625
12 1.7739112353 	 1559.7209459459 	 1559.7209459459
epoch_time;  38.03066921234131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3021141588687897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.499966025352478
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 306.12274169921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2932.7392578125
13 1.7654284643 	 2932.7391891892 	 2932.7391891892
epoch_time;  38.28921151161194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2715262472629547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.490127444267273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 220.58609008789062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2404.366455078125
14 1.7598398424 	 2404.3665540541 	 2404.3665540541
epoch_time;  38.04377365112305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22654841840267181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1880930662155151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 461.1274108886719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2570.4716796875
15 1.7564359237 	 2570.4717905405 	 2570.4717905405
epoch_time;  38.0124077796936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25657156109809875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3517987728118896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 649.2445678710938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1391.4459228515625
16 1.7498241262 	 1391.4459459459 	 1391.4459459459
epoch_time;  37.75139665603638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27482229471206665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4186934232711792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 147.84140014648438
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 561.00390625
17 1.7453770311 	 561.0039273649 	 561.0039273649
epoch_time;  37.98815870285034
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22130081057548523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1828417778015137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 417.5006408691406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3214.17333984375
18 1.7474653674 	 3214.1733108108 	 3214.1733108108
epoch_time;  38.02008390426636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27016496658325195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2614892721176147
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 405.3572692871094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5105.935546875
19 1.7413160651 	 5105.935472973 	 5105.935472973
epoch_time;  37.932753801345825
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2703356444835663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2616032361984253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 405.3605651855469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5105.40234375
It took 810.8126311302185 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▄▂▄█▂▄▃▁▄▁▁▂▃▃▃▄▁▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▁▃▂▄▅▂▃▇▃▆▄▃▂▄▄▃▄▅▆▆
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▅▅▁▅█▅█▄▁▆▄▁▄▂▂▄▇▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄▄▅▃▃▅▂▂█▂▅▂▃▂▁▁▁▂▂▃▃
wandb:                         Train loss █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 252836.45312
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.21548
wandb:    Test loss t(0, 0)_r(-5, 5)_none 385.7038
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25195
wandb:                         Train loss 1.74996
wandb: 
wandb: 🚀 View run beaming-rabbit-1216 at: https://wandb.ai/nreints/thesis/runs/h9agw4in
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132338-h9agw4in/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_133714-nq9tw55m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-goat-1223
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/nq9tw55m
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29516950249671936
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.413484811782837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 697.4955444335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 107769.4453125
0 3.320474489 	 107769.4486486486 	 107769.4486486486
epoch_time;  38.33386421203613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29787322878837585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.65395987033844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 651.7196044921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 95164.8984375
1 2.0807039316 	 95164.8972972973 	 95164.8972972973
epoch_time;  38.72401189804077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29927462339401245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9053118228912354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 688.374267578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 404380.59375
2 1.9834557948 	 404380.5837837838 	 404380.5837837838
epoch_time;  38.536219358444214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25696203112602234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7192202806472778
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 276.3550109863281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 116147.7421875
3 1.9253559461 	 116147.7405405405 	 116147.7405405405
epoch_time;  38.465651750564575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25782811641693115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9711154699325562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 736.7378540039062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 315947.15625
4 1.8980016172 	 315947.1567567568 	 315947.1567567568
epoch_time;  37.83261847496033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3001488745212555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.096127986907959
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 978.4505004882812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 761782.75
5 1.8744120459 	 761782.745945946 	 761782.745945946
epoch_time;  38.024582386016846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2346687763929367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.815004587173462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 675.6334228515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 134477.65625
6 1.8494468901 	 134477.6540540541 	 134477.6540540541
epoch_time;  37.837316036224365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21425092220306396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8973342180252075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1031.0458984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 388216.0
7 1.8405145169 	 388216.0 	 388216.0
epoch_time;  38.48097586631775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39669445157051086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3283889293670654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 615.0939331054688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 214890.75
8 1.8258044244 	 214890.7459459459 	 214890.7459459459
epoch_time;  38.44729733467102
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23376436531543732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9092265367507935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 265.87799072265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 55297.93359375
9 1.81617368 	 55297.9351351351 	 55297.9351351351
epoch_time;  38.4746413230896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30669593811035156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2084827423095703
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 763.2142944335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 401598.84375
10 1.8066993047 	 401598.8324324324 	 401598.8324324324
epoch_time;  38.01856875419617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2417445182800293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9407869577407837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 558.6627197265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 86373.3671875
11 1.8006298398 	 86373.3675675676 	 86373.3675675676
epoch_time;  38.93831419944763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24191395938396454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8973205089569092
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 241.30506896972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 79025.90625
12 1.794264497 	 79025.9081081081 	 79025.9081081081
epoch_time;  39.043317556381226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23759765923023224
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7629793882369995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 620.4265747070312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 141679.0625
13 1.7791702591 	 141679.0594594595 	 141679.0594594595
epoch_time;  38.77007555961609
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21326301991939545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9493088722229004
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 410.1235656738281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 245903.8125
14 1.7798010246 	 245903.8054054054 	 245903.8054054054
epoch_time;  38.553879499435425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19955934584140778
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9568594694137573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 361.6437072753906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 213336.96875
15 1.7749539158 	 213336.972972973 	 213336.972972973
epoch_time;  38.73934054374695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2021775096654892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9147404432296753
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 523.5306396484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 231021.90625
16 1.7720029052 	 231021.9027027027 	 231021.9027027027
epoch_time;  38.313833236694336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22057661414146423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9656884670257568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 952.4642944335938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 370413.0625
17 1.7604135812 	 370413.0594594594 	 370413.0594594594
epoch_time;  38.734049558639526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21942037343978882
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1269209384918213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 371.2420959472656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48855.984375
18 1.757635965 	 48855.9837837838 	 48855.9837837838
epoch_time;  38.597522258758545
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25193366408348083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.215916633605957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 385.6361083984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 252836.328125
19 1.749964131 	 252836.3243243243 	 252836.3243243243
epoch_time;  38.26253533363342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25195321440696716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.215484380722046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 385.70379638671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 252836.453125
It took 816.3617744445801 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆▂▄▃▄▃▄▂▁▆▂▂▃▁▁▄█▄▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▂▃█▇▇▁▂▄▃▆▃▅▄█▅▄▄▆▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▁▃▃▄▂▆▂▂█▃▄▆▇▁▇▇▅▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▂▅█▂▃▂▁▅▂▆▄▅▂▄▇▃▂▆▄▄
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 235.93513
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.74195
wandb:    Test loss t(0, 0)_r(-5, 5)_none 124.36281
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26995
wandb:                         Train loss 1.72924
wandb: 
wandb: 🚀 View run sparkling-goat-1223 at: https://wandb.ai/nreints/thesis/runs/nq9tw55m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_133714-nq9tw55m/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135046-b4ievw2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-dog-1231
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/b4ievw2o
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28809279203414917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3867318630218506
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 368.0506896972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 669.1050415039062
0 3.7262500926 	 669.1050253378 	 669.1050253378
epoch_time;  38.0950391292572
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23054371774196625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.3369078636169434
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 102.76469421386719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 273.1141357421875
1 2.090783617 	 273.1141469595 	 273.1141469595
epoch_time;  38.11444139480591
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2848268747329712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.407392978668213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 220.34060668945312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 445.8515930175781
2 1.9803018021 	 445.8516047297 	 445.8516047297
epoch_time;  38.239972829818726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35098618268966675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2486066818237305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 228.41726684570312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 391.31103515625
3 1.929011609 	 391.3110219595 	 391.3110219595
epoch_time;  38.427037954330444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.234903946518898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1884560585021973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 257.63916015625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 498.235595703125
4 1.8911751016 	 498.2355996622 	 498.2355996622
epoch_time;  38.03933525085449
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2554706335067749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1833152770996094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 148.86012268066406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 425.8401184082031
5 1.8716856712 	 425.8401182432 	 425.8401182432
epoch_time;  38.0739483833313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22168855369091034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.1095833778381348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 381.19696044921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 495.3025817871094
6 1.846168265 	 495.3025760135 	 495.3025760135
epoch_time;  37.84929537773132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20733433961868286
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.2575912475585938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 138.46658325195312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 323.232421875
7 1.8326747432 	 323.2324324324 	 323.2324324324
epoch_time;  37.949153900146484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29066696763038635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.602513074874878
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 138.1867218017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 214.6454620361328
8 1.8184106164 	 214.6454603041 	 214.6454603041
epoch_time;  38.09101366996765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23239295184612274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4604945182800293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 457.14996337890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 677.11865234375
9 1.8095435347 	 677.1186655405 	 677.1186655405
epoch_time;  38.265740394592285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31767430901527405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.9171648025512695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 207.47271728515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 271.383544921875
10 1.8003562767 	 271.3835304054 	 271.3835304054
epoch_time;  38.343748331069946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2678109407424927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4767699241638184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 253.7443389892578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 335.3729553222656
11 1.7828993129 	 335.3729518581 	 335.3729518581
epoch_time;  38.50803184509277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2870621681213379
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7064812183380127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 362.3292236328125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 409.1742858886719
12 1.7713979257 	 409.1742820946 	 409.1742820946
epoch_time;  37.70067095756531
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22531268000602722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6953887939453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 413.8675231933594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 244.09259033203125
13 1.7659622017 	 244.0925886824 	 244.0925886824
epoch_time;  37.807124614715576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2625124454498291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3007102012634277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 118.24105072021484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 247.14088439941406
14 1.7593879261 	 247.1408783784 	 247.1408783784
epoch_time;  38.128775119781494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33970075845718384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.8172924518585205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 424.53643798828125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 460.6182861328125
15 1.7502621578 	 460.618285473 	 460.618285473
epoch_time;  38.413294076919556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25475040078163147
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.662677764892578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 429.7407531738281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 817.9818115234375
16 1.7437268918 	 817.9818412162 	 817.9818412162
epoch_time;  38.3415265083313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23642472922801971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.545106887817383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 287.18780517578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 481.7717590332031
17 1.7372355942 	 481.7717483108 	 481.7717483108
epoch_time;  37.87217855453491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3082927167415619
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0233869552612305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 225.5929718017578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 309.28216552734375
18 1.7338710157 	 309.2821579392 	 309.2821579392
epoch_time;  38.01340651512146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27005934715270996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7409305572509766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 124.3893814086914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 237.60873413085938
19 1.729244741 	 237.6087415541 	 237.6087415541
epoch_time;  37.85482740402222
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26994654536247253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7419488430023193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 124.36280822753906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 235.9351348876953
It took 811.84139585495 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▄▂▄▁▄▄▂▅▂▄█▁▁▁▁▃▁▁▃▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅█▄▅▂▄▄▁▆▂▃▃▂▂▁▁▂▃▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▃▇▄▃▃▂▄▁▄▂▂▂▃▁▂▂▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄█▄▃▁▂▂▁▃▂▁▃▂▂▂▁▂▄▁▃▃
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 276.81369
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.33424
wandb:    Test loss t(0, 0)_r(-5, 5)_none 43.7571
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27298
wandb:                         Train loss 1.73476
wandb: 
wandb: 🚀 View run bright-dog-1231 at: https://wandb.ai/nreints/thesis/runs/b4ievw2o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135046-b4ievw2o/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_140420-du0pev2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run abundant-goat-1238
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/du0pev2k
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3394657075405121
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6541279554367065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 86.91279602050781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 693.0724487304688
0 3.5062666617 	 693.0724239865 	 693.0724239865
epoch_time;  38.12863063812256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5266925096511841
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8930221796035767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 99.82320404052734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 372.344482421875
1 2.0794832285 	 372.3444679054 	 372.3444679054
epoch_time;  38.185842514038086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3563545048236847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5333788394927979
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 57.250919342041016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 688.3447875976562
2 1.9775907998 	 688.3448057432 	 688.3448057432
epoch_time;  37.96798276901245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2983921766281128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6347308158874512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 87.90612030029297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 314.4668884277344
3 1.9264214373 	 314.4668918919 	 314.4668918919
epoch_time;  37.96262001991272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22096440196037292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.359231948852539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.79826354980469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 625.44580078125
4 1.8841394878 	 625.445777027 	 625.445777027
epoch_time;  38.31088042259216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26191118359565735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5443271398544312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 50.77310562133789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 679.2986450195312
5 1.8573964196 	 679.2986486486 	 679.2986486486
epoch_time;  38.294991970062256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25463491678237915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4893441200256348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.69569396972656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 367.95257568359375
6 1.8390038864 	 367.9525760135 	 367.9525760135
epoch_time;  38.71975064277649
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2177039086818695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.255185604095459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 48.84255599975586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 735.53125
7 1.8167700642 	 735.53125 	 735.53125
epoch_time;  38.38805079460144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28613045811653137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7113639116287231
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 65.65118408203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 381.1042785644531
8 1.8017262412 	 381.1042652027 	 381.1042652027
epoch_time;  38.12101173400879
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23706622421741486
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.328264832496643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 36.87802505493164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 591.1376342773438
9 1.7940277947 	 591.1376266892 	 591.1376266892
epoch_time;  38.126123666763306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19953377544879913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4781261682510376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 59.55255889892578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1153.4388427734375
10 1.7815926564 	 1153.4388513514 	 1153.4388513514
epoch_time;  38.267179012298584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27566391229629517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4211031198501587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.07027053833008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 272.9911804199219
11 1.7727290459 	 272.9911951014 	 272.9911951014
epoch_time;  38.28398871421814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22825853526592255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.322633147239685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 42.756927490234375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 284.85498046875
12 1.7707954257 	 284.8549831081 	 284.8549831081
epoch_time;  38.17951416969299
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22296689450740814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3874706029891968
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.645015716552734
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 334.76983642578125
13 1.7673615886 	 334.7698268581 	 334.7698268581
epoch_time;  38.03270602226257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24787992238998413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2670824527740479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 51.963191986083984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 309.0339660644531
14 1.7569099896 	 309.0339738176 	 309.0339738176
epoch_time;  37.96075916290283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22182081639766693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2649765014648438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 36.23859405517578
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 501.51708984375
15 1.7512573986 	 501.5171030405 	 501.5171030405
epoch_time;  38.010836601257324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2324531376361847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3394979238510132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 47.82878875732422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 309.12396240234375
16 1.7499502462 	 309.1239653716 	 309.1239653716
epoch_time;  37.98254132270813
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33220937848091125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4193092584609985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 46.23595428466797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 283.71319580078125
17 1.7456902651 	 283.7131967905 	 283.7131967905
epoch_time;  38.42165565490723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21703098714351654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2883626222610474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 49.82428741455078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 561.2540283203125
18 1.7400570135 	 561.2540540541 	 561.2540540541
epoch_time;  38.29196524620056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2730599343776703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.334317922592163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.76344299316406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 276.87371826171875
19 1.7347626166 	 276.8737119932 	 276.8737119932
epoch_time;  38.56772589683533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2729799449443817
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3342384099960327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 43.75709533691406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 276.8136901855469
It took 813.6182773113251 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▆▄▃▇█▁▄▃▇▁▃▂▅▄▃▃▂▁▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▁▃▂▃▃▄▃▃▃▇▃▄▃▂▆▄▆▄██
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▂▃▇█▅▂▁▁▆▇▃▂▁▇▅▆▃▇▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▂▄▁▂▃▅▂▄▂▆▂▃▃▃▄▃▆▃██
wandb:                         Train loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 10838.72656
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.68495
wandb:    Test loss t(0, 0)_r(-5, 5)_none 196.58054
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.39659
wandb:                         Train loss 1.72996
wandb: 
wandb: 🚀 View run abundant-goat-1238 at: https://wandb.ai/nreints/thesis/runs/du0pev2k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_140420-du0pev2k/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_141750-vxds48eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-springroll-1245
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/vxds48eu
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3349859118461609
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1903655529022217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 275.32275390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1173.2813720703125
0 3.38400064 	 1173.2814189189 	 1173.2814189189
epoch_time;  38.27509641647339
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22663673758506775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0161556005477905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 164.014892578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9316.61328125
1 2.1021165686 	 9316.6128378378 	 9316.6128378378
epoch_time;  38.50160312652588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29115062952041626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2196675539016724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 215.0020751953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6201.3701171875
2 1.9846067435 	 6201.3702702703 	 6201.3702702703
epoch_time;  37.968332052230835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21345816552639008
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0800288915634155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 361.6689147949219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4177.03857421875
3 1.9337714421 	 4177.0385135135 	 4177.0385135135
epoch_time;  37.86249780654907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2277470976114273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2472254037857056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 428.4168395996094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12567.1279296875
4 1.9021544873 	 12567.1283783784 	 12567.1283783784
epoch_time;  38.44393491744995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26187482476234436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.219186544418335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 273.73809814453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13612.6591796875
5 1.8746427976 	 13612.6594594595 	 13612.6594594595
epoch_time;  37.43336033821106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3118302524089813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.255815029144287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 167.36297607421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 412.3221740722656
6 1.8402716986 	 412.3221706081 	 412.3221706081
epoch_time;  36.85931730270386
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24964949488639832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1913083791732788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 104.56939697265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5362.45068359375
7 1.8260923681 	 5362.4506756757 	 5362.4506756757
epoch_time;  37.99117136001587
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28008776903152466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2407820224761963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 115.07113647460938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4167.5439453125
8 1.8062959851 	 4167.5439189189 	 4167.5439189189
epoch_time;  38.05602836608887
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24703241884708405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2351000308990479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 321.62908935546875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12330.7314453125
9 1.7909918225 	 12330.7310810811 	 12330.7310810811
epoch_time;  38.38188171386719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35478952527046204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.569380521774292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 365.04327392578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 413.6606140136719
10 1.7799795431 	 413.6605996622 	 413.6605996622
epoch_time;  37.746943950653076
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22740110754966736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.235404372215271
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 199.79727172851562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4262.56396484375
11 1.7681811209 	 4262.5638513514 	 4262.5638513514
epoch_time;  37.986348390579224
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2679005265235901
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2553826570510864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 162.96337890625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2506.050537109375
12 1.7608621916 	 2506.0505067568 	 2506.0505067568
epoch_time;  38.04893350601196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25482186675071716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1875396966934204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 121.75439453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7526.57373046875
13 1.7501201694 	 7526.5736486486 	 7526.5736486486
epoch_time;  38.40734791755676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2739564776420593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1521918773651123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 396.4398193359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6840.40087890625
14 1.7495680843 	 6840.4006756757 	 6840.4006756757
epoch_time;  38.14355826377869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30343663692474365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5248757600784302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 269.70391845703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3269.447021484375
15 1.7394741836 	 3269.4469594595 	 3269.4469594595
epoch_time;  37.93994951248169
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25740641355514526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.272408127784729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 340.06707763671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3694.75
16 1.741498716 	 3694.75 	 3694.75
epoch_time;  38.18003702163696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33823513984680176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4791125059127808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 217.6381378173828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2844.51416015625
17 1.7373119605 	 2844.5141891892 	 2844.5141891892
epoch_time;  38.098105669021606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2746121287345886
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2594431638717651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 370.9740295410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1329.0361328125
18 1.7430538108 	 1329.0361486486 	 1329.0361486486
epoch_time;  38.10969948768616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39662083983421326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6841216087341309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 196.59222412109375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10838.767578125
19 1.7299556745 	 10838.7675675676 	 10838.7675675676
epoch_time;  37.91112160682678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3965908885002136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6849507093429565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 196.58053588867188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10838.7265625
It took 810.4121074676514 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
log_dualQ
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.121 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.121 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▂▂█▃▁▁▄▁▅▃▃▅▄▂▇▆▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▄▅▄▃█▄▃▅▆▅▃▅▆█▃▃▆▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▃▄▂▃▅▂█▄▁▅▃▁▁▂▃▁▂▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▄█▅▃█▂▆▂▆▅▁▄▆▇▃▄▅▂▃▃
wandb:                         Train loss █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 260774.9375
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.22145
wandb:    Test loss t(0, 0)_r(-5, 5)_none 75.27223
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26167
wandb:                         Train loss 1.74272
wandb: 
wandb: 🚀 View run filigreed-springroll-1245 at: https://wandb.ai/nreints/thesis/runs/vxds48eu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_141750-vxds48eu/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3130817115306854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.358046054840088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 91.34862518310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 124748.703125
0 3.4658047748 	 124748.7027027027 	 124748.7027027027
epoch_time;  36.55728197097778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.270265132188797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3545914888381958
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 125.7260971069336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 218273.296875
1 2.0968051464 	 218273.2972972973 	 218273.2972972973
epoch_time;  36.80087661743164
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36555933952331543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.424591064453125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 193.87322998046875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 153761.984375
2 1.9946234515 	 153761.9891891892 	 153761.9891891892
epoch_time;  37.396719455718994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3100745379924774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.328028678894043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 114.82958984375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1137673.125
3 1.938580533 	 1137673.1675675677 	 1137673.1675675677
epoch_time;  37.78440237045288
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26243993639945984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2816658020019531
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 124.46714782714844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 285015.875
4 1.8951554037 	 285015.8702702703 	 285015.8702702703
epoch_time;  37.401055335998535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3579522669315338
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6386138200759888
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 207.2854461669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 27680.091796875
5 1.8745138676 	 27680.0918918919 	 27680.0918918919
epoch_time;  37.61210036277771
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2372749298810959
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3143399953842163
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 92.96536254882812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 84300.453125
6 1.8606676064 	 84300.454054054 	 84300.454054054
epoch_time;  37.314783573150635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31806668639183044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2695749998092651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 335.166748046875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 443315.625
7 1.843970834 	 443315.6324324324 	 443315.6324324324
epoch_time;  37.57902979850769
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23730972409248352
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.416206955909729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 190.1387939453125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77295.46875
8 1.8319856286 	 77295.4648648649 	 77295.4648648649
epoch_time;  37.572680711746216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31736505031585693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4669899940490723
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 73.14144134521484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 705952.9375
9 1.8154916731 	 705952.9081081081 	 705952.9081081081
epoch_time;  38.36265420913696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3046709895133972
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.453054428100586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 232.8846435546875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 305009.125
10 1.7999314687 	 305009.1243243243 	 305009.1243243243
epoch_time;  37.92654085159302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21321465075016022
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.238716959953308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 121.25894165039062
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 403215.96875
11 1.7850696355 	 403215.9567567568 	 403215.9567567568
epoch_time;  38.37687921524048
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2840580344200134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4130316972732544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 63.21800994873047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 721460.0
12 1.7717055396 	 721460.0216216217 	 721460.0216216217
epoch_time;  38.34013915061951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31973087787628174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4697051048278809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 71.22711944580078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 550183.5
13 1.7652692853 	 550183.4810810811 	 550183.4810810811
epoch_time;  37.75779175758362
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34739622473716736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.610457181930542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 95.71479797363281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 215622.421875
14 1.7596166943 	 215622.4216216216 	 215622.4216216216
epoch_time;  37.294955015182495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25375401973724365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2859960794448853
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 125.20989227294922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 906385.625
15 1.7564440232 	 906385.6432432432 	 906385.6432432432
epoch_time;  37.867682218551636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2855329215526581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3025217056274414
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 58.87202453613281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 830273.0625
16 1.7512549192 	 830273.0378378378 	 830273.0378378378
epoch_time;  37.87008094787598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3079487085342407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4842206239700317
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 82.41394805908203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 168707.03125
17 1.7464258275 	 168707.027027027 	 168707.027027027
epoch_time;  37.50399708747864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2410624474287033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1264243125915527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 56.39807891845703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 88245.703125
18 1.7441611216 	 88245.7027027027 	 88245.7027027027
epoch_time;  37.45300006866455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2617305517196655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2273553609848022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 75.27338409423828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 260775.140625
19 1.7427161132 	 260775.1351351351 	 260775.1351351351
epoch_time;  37.280765533447266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2616730034351349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2214466333389282
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 75.27223205566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 260774.9375
It took 806.5170228481293 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137936
Array Job ID: 2137927_6
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-17:01:30 core-walltime
Job Wall-clock time: 02:16:45
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
