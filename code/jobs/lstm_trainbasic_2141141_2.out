/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_235629-4am3jsty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-rabbit-1430
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/4am3jsty
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(0,', '0)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15304a497f40>, <torch.utils.data.dataloader.DataLoader object at 0x1530437a0a60>, <torch.utils.data.dataloader.DataLoader object at 0x1530437a02e0>, <torch.utils.data.dataloader.DataLoader object at 0x1530437a01f0>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7815051078796387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.501389265060425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.304265975952148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.697185039520264
0 3.7655495955 	 5.6971852282
epoch_time;  31.692156553268433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.748400926589966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3215653896331787
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.382822036743164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.654627799987793
1 2.7010561808 	 5.6546276819
epoch_time;  31.249103546142578
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7356374263763428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.316899299621582
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.319295883178711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.571043491363525
2 2.66230417 	 5.5710434467
epoch_time;  31.142235040664673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.713016986846924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.313809633255005
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.369167327880859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.622113227844238
3 2.6356738059 	 5.6221130924
epoch_time;  31.130030393600464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7280006408691406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3158087730407715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.330209732055664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.508326530456543
4 2.6128489677 	 5.5083265967
epoch_time;  30.799049377441406
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7442281246185303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2962236404418945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4750895500183105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.589910507202148
5 2.5901958226 	 5.5899105014
epoch_time;  31.121846914291382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7641420364379883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4076013565063477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.554856300354004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.818386077880859
6 2.5682861883 	 5.8183862969
epoch_time;  32.06777381896973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7749056816101074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4569380283355713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.599119663238525
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.944948673248291
7 2.5455125438 	 5.9449488706
epoch_time;  31.728552103042603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8130717277526855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.497918128967285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6636271476745605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.040449142456055
8 2.5319990552 	 6.0404491597
epoch_time;  30.927168607711792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8234519958496094
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5382349491119385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.718716621398926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.163380146026611
9 2.5115065824 	 6.1633802339
epoch_time;  31.07120156288147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8471131324768066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5946593284606934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.819450855255127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.363730430603027
10 2.489886738 	 6.3637304392
epoch_time;  30.95588254928589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.840473175048828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5721800327301025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.798482418060303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.349811553955078
11 2.4941581824 	 6.3498114733
epoch_time;  31.129735231399536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.866044521331787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.664543867111206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.821200370788574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.486648082733154
12 2.4615582936 	 6.4866482369
epoch_time;  31.10362982749939
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.863372564315796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.639636993408203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.864307403564453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.422106742858887
13 2.4519070691 	 6.4221066017
epoch_time;  30.87501549720764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8674428462982178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7021236419677734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.863194942474365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.566431999206543
14 2.4332559082 	 6.5664320655
epoch_time;  30.90871787071228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8794314861297607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.701364040374756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.826826095581055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.574714183807373
15 2.4191552623 	 6.5747144071
epoch_time;  31.202778577804565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8856043815612793
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.716369152069092
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.825128078460693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.529055595397949
16 2.4371392411 	 6.5290556847
epoch_time;  31.18154740333557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9034054279327393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.749155044555664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.844263076782227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.617246627807617
17 2.4071805436 	 6.6172465068
epoch_time;  31.04265594482422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9363694190979004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.759660482406616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.89823579788208
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.602474689483643
18 2.3934948074 	 6.6024748926
epoch_time;  31.254440546035767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.926283597946167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7332613468170166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.902425289154053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.542422294616699
19 2.3819891491 	 6.5424221995
epoch_time;  31.06781816482544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.935168743133545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8238937854766846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.927157402038574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.686800956726074
20 2.3759353488 	 6.6868007694
epoch_time;  31.855868577957153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.947336196899414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8012397289276123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.987995624542236
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.642668724060059
21 2.3667733296 	 6.642668848
epoch_time;  31.828109741210938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9442946910858154
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7888545989990234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.933394908905029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.591192722320557
22 2.3677175494 	 6.5911927929
epoch_time;  31.30289125442505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9642832279205322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.805260181427002
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.018608093261719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.628859996795654
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▂▁▂▁▁▃▄▄▅▆▆▇▆▇▇▇█▇▇██▇██▇▇▇▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▁▁▁▁▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇████████▇██
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▂▁▂▁▃▃▄▅▅▆▆▆▆▆▆▆▆▇▇▇█▇███▇████
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▂▂▁▁▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.50241
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.79133
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.96857
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.99924
wandb:                         Train loss 2.33719
wandb: 
wandb: 🚀 View run crimson-rabbit-1430 at: https://wandb.ai/nreints/thesis/runs/4am3jsty
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_235629-4am3jsty/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_001308-h87h2zgs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prosperous-laughter-1434
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/h87h2zgs
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
23 2.3681272843 	 6.6288597821
epoch_time;  31.33185887336731
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.971014976501465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.808816909790039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.986805438995361
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.62166690826416
24 2.3474930583 	 6.6216668535
epoch_time;  31.132051467895508
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9625720977783203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8136487007141113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.014822959899902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.597527980804443
25 2.3696080222 	 6.5975279102
epoch_time;  30.75507640838623
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9577794075012207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.791156768798828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.948649883270264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.549886703491211
26 2.3403035697 	 6.5498865594
epoch_time;  31.026322603225708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9835331439971924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.795916795730591
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9927778244018555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.536742687225342
27 2.3326448358 	 6.5367427953
epoch_time;  30.96440625190735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9804232120513916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.747089385986328
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.006697177886963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.4898905754089355
28 2.3302097133 	 6.4898906604
epoch_time;  30.726699829101562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.999821424484253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7918925285339355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.96775484085083
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.496522903442383
29 2.3371905458 	 6.4965230245
epoch_time;  30.953085899353027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9992411136627197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.791334629058838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.968570709228516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.502411842346191
It took  999.9791235923767  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153043f82ec0>, <torch.utils.data.dataloader.DataLoader object at 0x1530438162c0>, <torch.utils.data.dataloader.DataLoader object at 0x153043f80130>, <torch.utils.data.dataloader.DataLoader object at 0x1530165b80a0>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.756101131439209
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3416600227355957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.114035606384277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.356998920440674
0 3.7344241608 	 5.35699887
epoch_time;  31.174818992614746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7316462993621826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.20108962059021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.198802471160889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.302880764007568
1 2.7006712111 	 5.3028807856
epoch_time;  31.22586941719055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7157132625579834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.192593812942505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.311909198760986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.394629955291748
2 2.651412652 	 5.3946300864
epoch_time;  31.17663025856018
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.700718402862549
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.184680223464966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.322131156921387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.427883625030518
3 2.6174998374 	 5.427883736
epoch_time;  32.19225811958313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7119646072387695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.218493700027466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.329147815704346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.436781406402588
4 2.5880137118 	 5.4367812234
epoch_time;  31.654022216796875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.733315944671631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2987780570983887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.332845211029053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.563055515289307
5 2.5602735336 	 5.5630554015
epoch_time;  30.821695804595947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.75154972076416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3760986328125
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.389378070831299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.721414089202881
6 2.5337233792 	 5.7214141569
epoch_time;  31.18818688392639
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7747437953948975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.435593605041504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.423776149749756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.841880798339844
7 2.5151084421 	 5.8418805909
epoch_time;  31.224245309829712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.788226366043091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5465123653411865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.42962646484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.987534999847412
8 2.488308233 	 5.9875351828
epoch_time;  31.363572120666504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7852909564971924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.545341968536377
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.339362144470215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.769937515258789
9 2.5278004279 	 5.7699376593
epoch_time;  31.421545267105103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7851927280426025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.548229455947876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.367214202880859
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.920100212097168
10 2.4853737403 	 5.9201000018
epoch_time;  30.943366050720215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8256654739379883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6039910316467285
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.40977144241333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.927218914031982
11 2.4566858308 	 5.9272188031
epoch_time;  30.84723973274231
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.80967116355896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5947823524475098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.363502025604248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.860937118530273
12 2.4427341458 	 5.860937205
epoch_time;  31.266575574874878
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8397974967956543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6304707527160645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.352848052978516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.917534351348877
13 2.4149257709 	 5.9175343125
epoch_time;  31.180073976516724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7809174060821533
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.633847951889038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.403700351715088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.999303817749023
14 2.412035449 	 5.9993037198
epoch_time;  30.721056699752808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8751232624053955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.714104652404785
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.41691780090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.967538356781006
15 2.3922650484 	 5.9675381479
epoch_time;  30.78148365020752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8769888877868652
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.704660177230835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.365089416503906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.909834861755371
16 2.3802354395 	 5.9098350317
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▁▂▂▂▃▄▅▆▄▅▅▅▅▆▆▅▆▆▆▆▇▇▇█▇▇▇▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▂▁▁▁▁▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇██████▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▂▄▄▄▄▅▆▆▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇███████
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▂▁▁▁▂▂▃▃▃▃▄▄▅▃▆▆▆▆▆▇▇▇██████▆▆
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.17778
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.77107
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.58685
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.88857
wandb:                         Train loss 2.40055
wandb: 
wandb: 🚀 View run prosperous-laughter-1434 at: https://wandb.ai/nreints/thesis/runs/h87h2zgs
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_001308-h87h2zgs/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_002942-zx8xaq1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chromatic-dumpling-1438
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/zx8xaq1j
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  30.74981117248535
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8830814361572266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.730947732925415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.431865215301514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.956790924072266
17 2.3603262428 	 5.9567907973
epoch_time;  30.784064531326294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.884464979171753
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7650949954986572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.429574012756348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.0471882820129395
18 2.3594005045 	 6.0471884736
epoch_time;  30.640509366989136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9024956226348877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7567875385284424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.466209411621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.020190238952637
19 2.357195107 	 6.0201902822
epoch_time;  30.871750593185425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.906358003616333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7932634353637695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.473623275756836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.051613807678223
20 2.334764664 	 6.0516139834
epoch_time;  31.123913764953613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.921860694885254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8212945461273193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5067830085754395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.162132263183594
21 2.3436951099 	 6.1621322401
epoch_time;  31.346150159835815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.911935329437256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8003995418548584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.489604473114014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.0807623863220215
22 2.3248469362 	 6.0807626039
epoch_time;  30.915546894073486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9443857669830322
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8882694244384766
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.514901638031006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.184942245483398
23 2.3215387476 	 6.1849424241
epoch_time;  31.69569206237793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.94868540763855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9243500232696533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.582067489624023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.286318778991699
24 2.3068953001 	 6.2863186839
epoch_time;  31.097513675689697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9487435817718506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9096086025238037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5697855949401855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.203701972961426
25 2.3115143816 	 6.2037017914
epoch_time;  31.024235010147095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9434709548950195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.879973888397217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.561633586883545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.153506755828857
26 2.3055245272 	 6.1535069215
epoch_time;  31.176036596298218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.960866928100586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.925405502319336
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.564542293548584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.2031145095825195
27 2.2951787679 	 6.2031146738
epoch_time;  31.204345226287842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9555797576904297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.924865245819092
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.588213920593262
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.196163654327393
28 2.3077521025 	 6.1961636731
epoch_time;  31.17829704284668
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.887636423110962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7724897861480713
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.587367057800293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.180419445037842
29 2.400548457 	 6.1804195531
epoch_time;  30.99154019355774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8885722160339355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.771066188812256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.586850643157959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.177783966064453
It took  993.9167056083679  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153043816290>, <torch.utils.data.dataloader.DataLoader object at 0x15304a461150>, <torch.utils.data.dataloader.DataLoader object at 0x15304a462b60>, <torch.utils.data.dataloader.DataLoader object at 0x15304a4629e0>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7663016319274902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.494065761566162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.308480739593506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.606099605560303
0 3.6850305019 	 5.6060997539
epoch_time;  32.04897952079773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7253522872924805
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2801129817962646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.293315887451172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.419778823852539
1 2.7017126437 	 5.4197787835
epoch_time;  31.879825115203857
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.716973304748535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2629730701446533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.349395751953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.4195685386657715
2 2.6586708918 	 5.4195685718
epoch_time;  30.92240810394287
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.718167304992676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2773330211639404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.34611177444458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.410149574279785
3 2.6300637464 	 5.4101496117
epoch_time;  31.051686763763428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.731238603591919
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3321309089660645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.414271831512451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.504857063293457
4 2.605193006 	 5.504856997
epoch_time;  31.261881589889526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7389113903045654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4305050373077393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600350856781006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.792022228240967
5 2.5808112861 	 5.7920224285
epoch_time;  31.075162649154663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7567973136901855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5034191608428955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.655966281890869
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.927861213684082
6 2.5559471991 	 5.9278612396
epoch_time;  31.3392915725708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7887401580810547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5814034938812256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.854946136474609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.2440595626831055
7 2.5352907953 	 6.2440594907
epoch_time;  30.92637276649475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8083035945892334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.619387626647949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.848875045776367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.268026351928711
8 2.5125764765 	 6.2680265767
epoch_time;  30.92000675201416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8138206005096436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6476516723632812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7951812744140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.299558162689209
9 2.4919315312 	 6.2995583341
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▁▁▂▃▄▆▇▇███▇▇▇▇▇▇▆▆▇▇▆▆▆▇▆▇▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▁▁▁▂▃▃▄▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▁▂▂▂▄▅▇▆▆▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▁▁▁▁▂▂▃▃▄▄▅▅▅▆▆▆▆▇▆▇▇▇█▇▇███▇▇
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.21885
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.83919
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.9284
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.93829
wandb:                         Train loss 2.32048
wandb: 
wandb: 🚀 View run chromatic-dumpling-1438 at: https://wandb.ai/nreints/thesis/runs/zx8xaq1j
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_002942-zx8xaq1j/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_004614-96txfptd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-snake-1444
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/96txfptd
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  30.97687792778015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.840142250061035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6901509761810303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.949464321136475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.435347557067871
10 2.4765320055 	 6.4353477271
epoch_time;  31.13573384284973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8543124198913574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.748150110244751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.938143253326416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.491006851196289
11 2.457189082 	 6.4910066265
epoch_time;  31.123224020004272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8647282123565674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7277908325195312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.001521587371826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.446108341217041
12 2.4436192249 	 6.4461083542
epoch_time;  30.9157612323761
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8687071800231934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.702925443649292
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.872824668884277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.284886360168457
13 2.4414225137 	 6.2848862939
epoch_time;  30.921112537384033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8972504138946533
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7758755683898926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.905475616455078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.33944034576416
14 2.4139444032 	 6.339440291
epoch_time;  31.00846004486084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9159996509552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.772679328918457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9679999351501465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.349610805511475
15 2.4047953211 	 6.3496108502
epoch_time;  30.95242166519165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.90330171585083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7680180072784424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9291462898254395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.271788120269775
16 2.3986269354 	 6.27178826
epoch_time;  31.106312036514282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.921868324279785
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7942378520965576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.942154407501221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.307584762573242
17 2.3850171582 	 6.3075847338
epoch_time;  30.86304759979248
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.925569772720337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8300280570983887
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.927349090576172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.265824317932129
18 2.3744196809 	 6.2658241479
epoch_time;  30.85355281829834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.92046856880188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7950258255004883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.875601291656494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.228214740753174
19 2.3655019871 	 6.2282146903
epoch_time;  31.073895692825317
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.937513828277588
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8233225345611572
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.883997917175293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.218081951141357
20 2.3555585701 	 6.218081748
epoch_time;  31.170165061950684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9558372497558594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.872224807739258
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.926340579986572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.287398338317871
21 2.3508603878 	 6.2873985083
epoch_time;  30.80453896522522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9520699977874756
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.861955165863037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.929551601409912
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.286439418792725
22 2.3518269703 	 6.2864396478
epoch_time;  30.896807193756104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.978788375854492
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.876216173171997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.900884628295898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.243083477020264
23 2.3449220968 	 6.2430836657
epoch_time;  31.224026679992676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9537787437438965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8552463054656982
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8930816650390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.224194049835205
24 2.3480779978 	 6.2241941147
epoch_time;  31.000493049621582
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.952322244644165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8918368816375732
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.876998424530029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.2508158683776855
25 2.3335136512 	 6.250815769
epoch_time;  30.697885036468506
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9649062156677246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.881511688232422
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.9413557052612305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.27601957321167
26 2.3303342786 	 6.276019785
epoch_time;  31.128597736358643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9821360111236572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.884840965270996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8657965660095215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.185723304748535
27 2.3305959767 	 6.1857235266
epoch_time;  31.025556564331055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.971107244491577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9465701580047607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.892587184906006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.266822814941406
28 2.3156466603 	 6.266822838
epoch_time;  31.05535316467285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.939147472381592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8411171436309814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.927609920501709
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.217170238494873
29 2.320480312 	 6.217170093
epoch_time;  31.923490524291992
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9382941722869873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8391942977905273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.928402423858643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.218846797943115
It took  992.057044506073  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15304a460eb0>, <torch.utils.data.dataloader.DataLoader object at 0x1530437a29b0>, <torch.utils.data.dataloader.DataLoader object at 0x1530437e2bc0>, <torch.utils.data.dataloader.DataLoader object at 0x1530437e2d40>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7220876216888428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3617441654205322
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.297551155090332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.453432559967041
0 3.7089156238 	 5.4534325729
epoch_time;  31.323397636413574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.702101707458496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.208871841430664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.335912704467773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.371219158172607
1 2.7054991336 	 5.3712191394
epoch_time;  31.42996883392334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.673248529434204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.189211845397949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.477578163146973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.508145332336426
2 2.6635361469 	 5.5081451508
epoch_time;  31.42773962020874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6828389167785645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1699705123901367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.478331565856934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.495367050170898
3 2.6309589903 	 5.4953672288
epoch_time;  31.474146127700806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6947286128997803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.248861074447632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.512714862823486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.6381144523620605
4 2.6025250166 	 5.6381142608
epoch_time;  31.590439081192017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7035746574401855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.355428457260132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4762983322143555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.729844093322754
5 2.5751219409 	 5.7298440155
epoch_time;  31.023106336593628
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7212307453155518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.428980827331543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.50908088684082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.837127208709717
6 2.548953984 	 5.8371272246
epoch_time;  31.0809907913208
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.73551869392395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5026614665985107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6761155128479
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.158073425292969
7 2.5249991868 	 6.15807331
epoch_time;  31.306658506393433
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.752863645553589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.544165849685669
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.858850002288818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.407064437866211
8 2.5017901182 	 6.4070642938
epoch_time;  31.273353576660156
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.759941816329956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5589957237243652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.932670593261719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.48364782333374
9 2.480895303 	 6.4836477412
epoch_time;  31.378933668136597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.779378652572632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5497026443481445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.040607929229736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.593374729156494
10 2.4604163217 	 6.5933745693
epoch_time;  31.063919067382812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.802712917327881
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.561335325241089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0842671394348145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.593542575836182
11 2.4488323018 	 6.5935427386
epoch_time;  31.190803289413452
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.832341432571411
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6267826557159424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.119724273681641
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.694864749908447
12 2.4285627159 	 6.6948647859
epoch_time;  31.014299392700195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.778749704360962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.49503755569458
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.961268901824951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.413875102996826
13 2.4622910851 	 6.4138751534
epoch_time;  31.151957035064697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8004589080810547
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5789146423339844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.93493127822876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.510743618011475
14 2.4167120079 	 6.5107436627
epoch_time;  31.19967031478882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.844054937362671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.625833511352539
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.044864177703857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.582008361816406
15 2.3974533601 	 6.5820083849
epoch_time;  31.238754987716675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8455166816711426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.627279043197632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0666351318359375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.594834804534912
16 2.3821959571 	 6.5948349875
epoch_time;  31.12319540977478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.848522901535034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.599231481552124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.987619400024414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.423255920410156
17 2.3748968732 	 6.4232557591
epoch_time;  31.332377672195435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8173775672912598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.561723232269287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.955073833465576
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.347040176391602
18 2.4630732222 	 6.3470403666
epoch_time;  31.201074838638306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.880791664123535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.596320390701294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.065321922302246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.441678524017334
19 2.3661041144 	 6.4416784189
epoch_time;  31.203774213790894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8749923706054688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.594327211380005
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.035961627960205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.389561176300049
20 2.3513133694 	 6.3895614025
epoch_time;  31.1194064617157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.882500648498535
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5748727321624756
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.049437999725342
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.326233863830566
21 2.3593884915 	 6.3262338321
epoch_time;  31.096460342407227
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.882340669631958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5822067260742188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.044721603393555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.341256141662598
22 2.3577159157 	 6.3412562252
epoch_time;  30.991180896759033
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8896467685699463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5654783248901367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.073375701904297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.306560039520264
23 2.3319236877 	 6.3065602282
epoch_time;  31.35433053970337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.898214817047119
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.549853801727295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.137821674346924
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.3280558586120605
24 2.3316091325 	 6.328055667
epoch_time;  31.304359912872314
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.734522819519043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3986480236053467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5379862785339355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.7057695388793945
25 2.4638322575 	 5.7057696109
epoch_time;  31.335355281829834
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8452298641204834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4203951358795166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.746228218078613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.783779621124268
26 2.4348979895 	 5.7837795477
epoch_time;  32.48514938354492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8069584369659424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.381059169769287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.874861717224121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.88643741607666
27 2.3729701768 	 5.8864373613
epoch_time;  31.80478310585022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9189977645874023
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▁▂▂▂▃▃▅▆▇▇▇█▇▇▇▇▇▆▇▆▆▆▆▆▃▃▄▅▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▂▁▁▂▄▅▆▇▇▇▇█▆▇███▇█▇▇▇▇▇▅▅▄▅▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▁▂▃▃▂▃▄▆▆▇██▇▆▇▇▇▆▇▇▇▇▇█▃▅▆█▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▂▁▁▂▂▂▃▃▃▄▅▆▄▅▆▆▆▅▇▇▇▇▇▇▃▆▅█▅▅
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.90059
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.3303
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.97767
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.8109
wandb:                         Train loss 2.32401
wandb: 
wandb: 🚀 View run vibrant-snake-1444 at: https://wandb.ai/nreints/thesis/runs/96txfptd
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_004614-96txfptd/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_010252-p82nhhmf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-rooster-1451
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/p82nhhmf
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.444491147994995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.11385440826416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.040787220001221
28 2.3346970217 	 6.0407873425
epoch_time;  31.44046664237976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.810741901397705
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3272387981414795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.978679180145264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.896913528442383
29 2.3240104635 	 5.8969136495
epoch_time;  31.006644010543823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.810903310775757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3302953243255615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.97767448425293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.900585174560547
It took  997.3890538215637  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1530437e2c80>, <torch.utils.data.dataloader.DataLoader object at 0x153043f291b0>, <torch.utils.data.dataloader.DataLoader object at 0x153043f2abc0>, <torch.utils.data.dataloader.DataLoader object at 0x153043f2aa40>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7384941577911377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.52539324760437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.075924396514893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.287522315979004
0 3.7178436672 	 5.2875224226
epoch_time;  30.95090889930725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7054502964019775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3337502479553223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1528472900390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.163246154785156
1 2.7039343049 	 5.1632459934
epoch_time;  31.36673069000244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6820805072784424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.268418312072754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.250707149505615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.186323642730713
2 2.6588594031 	 5.186323552
epoch_time;  31.366231441497803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6888365745544434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2671749591827393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.174227237701416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1045145988464355
3 2.6306470226 	 5.1045146838
epoch_time;  31.652538776397705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6876349449157715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2448432445526123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2387800216674805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.10334587097168
4 2.6049161531 	 5.1033459805
epoch_time;  31.32195019721985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7039506435394287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.333040714263916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.299521446228027
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.248704433441162
5 2.579789444 	 5.248704432
epoch_time;  30.89571237564087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7268152236938477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.419265031814575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.334824562072754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.391584873199463
6 2.5544575901 	 5.3915849668
epoch_time;  31.316277027130127
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7429039478302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.534484624862671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.434749603271484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.561763763427734
7 2.5309709293 	 5.5617638902
epoch_time;  31.465906858444214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7720959186553955
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.592768907546997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.552872180938721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.689104080200195
8 2.5081669752 	 5.6891042473
epoch_time;  31.445164918899536
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.773273468017578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.613532066345215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.579474925994873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.749182224273682
9 2.4915622708 	 5.7491823871
epoch_time;  31.316713094711304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7960269451141357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6545495986938477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6010308265686035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.787559509277344
10 2.4670631934 	 5.7875593018
epoch_time;  31.246626615524292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8225061893463135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.678135871887207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6284894943237305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.82956075668335
11 2.4513869997 	 5.8295607091
epoch_time;  31.148492336273193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8211867809295654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6818833351135254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6469807624816895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.845531940460205
12 2.4301123897 	 5.8455320053
epoch_time;  30.958003282546997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.848724126815796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.708817720413208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.712459564208984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.919081687927246
13 2.4146831088 	 5.9190817657
epoch_time;  30.82414150238037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.858015537261963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7285208702087402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.67722225189209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.893282890319824
14 2.4014854691 	 5.8932828874
epoch_time;  30.828789949417114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.872041940689087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6722447872161865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.696913242340088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.8520050048828125
15 2.3927281948 	 5.852005051
epoch_time;  30.70268678665161
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8787715435028076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.765420436859131
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.684882640838623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.954236030578613
16 2.3758274168 	 5.954235803
epoch_time;  31.042309522628784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.823634624481201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.660954236984253
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.564416408538818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.766645908355713
17 2.4380293102 	 5.7666458176
epoch_time;  31.228805780410767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8566854000091553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6975631713867188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.645892143249512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.86796760559082
18 2.3807113382 	 5.8679674961
epoch_time;  31.199665546417236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8818583488464355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6945650577545166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.642254829406738
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.802921772003174
19 2.3559221012 	 5.8029217216
epoch_time;  31.112595796585083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.901841640472412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7136175632476807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.614852428436279
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.769450664520264
20 2.3424605457 	 5.7694504844
epoch_time;  31.043289184570312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9177098274230957
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▁▂▁▁▂▃▅▆▆▇▇▇█▇▇█▆▇▇▆▇▆▇▆▆▆▆▆▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▂▁▁▁▂▃▅▆▆▇▇▇▇█▇█▇▇▇▇██████▇▇▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▂▃▂▃▃▄▅▆▆▇▇▇█▇██▆▇▇▇█▇██▇█████
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▂▁▁▁▂▂▃▃▃▄▅▅▅▆▆▆▅▆▆▇▇▇████████
wandb:                         Train loss █▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 5.7052
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.68989
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.71065
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.9376
wandb:                         Train loss 2.2953
wandb: 
wandb: 🚀 View run lunar-rooster-1451 at: https://wandb.ai/nreints/thesis/runs/p82nhhmf
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_010252-p82nhhmf/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_011924-92hdckxm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run virtuous-dragon-1455
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/92hdckxm
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7347803115844727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.682136058807373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.810581207275391
21 2.3371506257 	 5.8105811727
epoch_time;  30.905916929244995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.926192045211792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.755117654800415
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.635835647583008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.762873649597168
22 2.3355612166 	 5.7628734393
epoch_time;  30.974371433258057
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.932607889175415
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.736802816390991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.685002326965332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.802957534790039
23 2.3195479203 	 5.8029574945
epoch_time;  32.17325186729431
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9370200634002686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.730405569076538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.682149410247803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.7640604972839355
24 2.3133902539 	 5.7640605823
epoch_time;  31.46326446533203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9514145851135254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.737287998199463
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.676352024078369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.718273162841797
25 2.3125055445 	 5.7182731513
epoch_time;  30.714938640594482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.938209295272827
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7420315742492676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.699761390686035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.767554759979248
26 2.3144499831 	 5.7675545223
epoch_time;  30.85378885269165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9408369064331055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7098171710968018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.727231979370117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.749963760375977
27 2.3028691051 	 5.7499638583
epoch_time;  30.89643359184265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9426257610321045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6937997341156006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7261834144592285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.708901405334473
28 2.3045864062 	 5.7089013967
epoch_time;  30.77346158027649
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.937429428100586
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.690819501876831
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.712342262268066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.715376853942871
29 2.2952970495 	 5.7153770239
epoch_time;  30.969229698181152
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9375991821289062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6898860931396484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7106475830078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.705201625823975
It took  992.1466796398163  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x1530437a2080>, <torch.utils.data.dataloader.DataLoader object at 0x1530437a1b40>, <torch.utils.data.dataloader.DataLoader object at 0x153043f4eb60>, <torch.utils.data.dataloader.DataLoader object at 0x153043f4ed40>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.767751932144165
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.427685260772705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.24383544921875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.407077312469482
0 3.7288996706 	 5.4070772015
epoch_time;  31.004496574401855
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.722093105316162
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.238664388656616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.311566352844238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.293351173400879
1 2.7061105016 	 5.2933511878
epoch_time;  30.865402460098267
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7129454612731934
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.161764144897461
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.402346134185791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.30994176864624
2 2.6656982976 	 5.3099416865
epoch_time;  30.822421312332153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7175700664520264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1720356941223145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.517583847045898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.443979740142822
3 2.6345379329 	 5.443979684
epoch_time;  30.878742694854736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7376322746276855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1850295066833496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.501272201538086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.427440166473389
4 2.6105406283 	 5.4274400786
epoch_time;  30.83363914489746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.740447998046875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.248964548110962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.569174766540527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.586592197418213
5 2.5898871815 	 5.5865921067
epoch_time;  30.9113712310791
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7488420009613037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3151309490203857
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.636776447296143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.711449146270752
6 2.5710626641 	 5.7114490152
epoch_time;  30.85763645172119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.751058340072632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3566818237304688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5675153732299805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.725715160369873
7 2.5527829813 	 5.7257153837
epoch_time;  31.129835605621338
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7766242027282715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4457085132598877
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.643683433532715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.916449546813965
8 2.535974426 	 5.916449325
epoch_time;  31.2839834690094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8023808002471924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4528658390045166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.712030410766602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.008855819702148
9 2.5212524322 	 6.0088558139
epoch_time;  31.361113786697388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.797680139541626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.499558925628662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.640120506286621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.979053974151611
10 2.50643378 	 5.979054062
epoch_time;  31.360276222229004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8159689903259277
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.608973741531372
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.688453674316406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.192667007446289
11 2.4902523021 	 6.1926667827
epoch_time;  31.184136629104614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8080506324768066
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6373417377471924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.653199672698975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.195774078369141
12 2.495372417 	 6.1957742282
epoch_time;  31.213497161865234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8222758769989014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.616641044616699
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.668250560760498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.134315013885498
13 2.4740545603 	 6.1343149606
epoch_time;  31.246676921844482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.834714651107788
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▁▂▂▂▃▃▄▅▄▅▆▅▅▆▆▆▇▆▆▆▇▇▇▇▇▇█▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▃▂▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▆▆▇▇▇███
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▂▃▄▄▄▅▄▅▆▅▆▅▅▆▆▇▆▇▆▇▇▇▇██▇▇███
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▁▁▁▂▂▂▂▃▄▃▄▄▄▅▅▆▅▆▆▇▇▇▇▇██▇███
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.57826
wandb:  Test loss t(-10, 10)_r(0, 0)_none 4.06693
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.92358
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.9504
wandb:                         Train loss 2.34003
wandb: 
wandb: 🚀 View run virtuous-dragon-1455 at: https://wandb.ai/nreints/thesis/runs/92hdckxm
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_011924-92hdckxm/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_013555-7end3hmb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-pig-1461
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/7end3hmb
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.677232027053833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.712242126464844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.189891815185547
14 2.4598097588 	 6.1898919881
epoch_time;  31.269147634506226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8561956882476807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.725198984146118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.726658344268799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.250749588012695
15 2.4470436622 	 6.2507493863
epoch_time;  31.423689365386963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.884152412414551
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.762842893600464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.826873779296875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.332365989685059
16 2.4359852562 	 6.3323661136
epoch_time;  31.190276622772217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8528242111206055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7631497383117676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.709081172943115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.2515363693237305
17 2.4319328175 	 6.2515363895
epoch_time;  31.50012445449829
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8795740604400635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8256871700286865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.789243221282959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.433264255523682
18 2.4222191255 	 6.4332640495
epoch_time;  31.08991312980652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8994455337524414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.82597017288208
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.768612384796143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.331510543823242
19 2.4089320354 	 6.331510515
epoch_time;  31.11156415939331
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.91363525390625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8564465045928955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8048553466796875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.362853527069092
20 2.410629746 	 6.3628534507
epoch_time;  32.08613395690918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9160232543945312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.900576114654541
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.782902717590332
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.391439914703369
21 2.3915117424 	 6.3914400314
epoch_time;  31.691284894943237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9162073135375977
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9538629055023193
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.860551357269287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.486505031585693
22 2.3862077365 	 6.4865051454
epoch_time;  31.309072256088257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.914032220840454
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8992178440093994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.843660831451416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.434889793395996
23 2.3768607175 	 6.4348896868
epoch_time;  31.35974621772766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9153997898101807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.901609182357788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.887967586517334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.423855304718018
24 2.4042953965 	 6.4238554156
epoch_time;  31.38656449317932
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9349076747894287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9840190410614014
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.88119649887085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.493692398071289
25 2.3719520309 	 6.4936921733
epoch_time;  31.0767343044281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.946751117706299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.04703426361084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.844378471374512
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.527077674865723
26 2.3577621194 	 6.5270774818
epoch_time;  31.15406632423401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.922381639480591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.960695266723633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.871585369110107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.4572062492370605
27 2.3606432006 	 6.4572060576
epoch_time;  31.062413454055786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.938509464263916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.119900226593018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.916086196899414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6939544677734375
28 2.3481077531 	 6.6939546061
epoch_time;  30.950681447982788
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9500715732574463
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.065121173858643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.925185680389404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.575995445251465
29 2.3400311524 	 6.5759955922
epoch_time;  31.025899648666382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9504013061523438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.066926002502441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.923577785491943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.5782575607299805
It took  990.9018287658691  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x153043f297b0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a4602e0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a4639a0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a460430>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.772052764892578
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.677260398864746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7927937507629395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.9995808601379395
0 3.7356173265 	 5.9995810517
epoch_time;  31.06191921234131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7463972568511963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4540834426879883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.681827545166016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.710492134094238
1 2.6997669113 	 5.7104923675
epoch_time;  31.24470615386963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.716224193572998
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.385266065597534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.515437126159668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.539412498474121
2 2.65478645 	 5.5394124841
epoch_time;  31.234842538833618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7188167572021484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.416486978530884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.635522842407227
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.6996331214904785
3 2.6266269265 	 5.6996332728
epoch_time;  30.969376802444458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7117011547088623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4395198822021484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6267547607421875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.667099952697754
4 2.6115271708 	 5.6670998749
epoch_time;  31.176554679870605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7264678478240967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.490292549133301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.736154556274414
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.869556903839111
5 2.5845201537 	 5.8695569917
epoch_time;  30.959809064865112
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7508580684661865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6086666584014893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.905630588531494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.142219066619873
6 2.555902289 	 6.1422192899
epoch_time;  31.089922189712524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.775189161300659
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▃▂▁▂▂▃▄▅▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▂▁▁▂▂▃▄▆▆▆▇▇▇▇▇██▇█▇▇██▇▇▇█▇▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▃▂▁▂▂▃▄▆▇▆▆▆▇▆▆▇▇▇▆▇█▇██▇██▇███
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▂▁▁▁▁▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇██▅▇█▄▇██
wandb:                         Train loss █▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.6858
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.92909
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.29622
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.96211
wandb:                         Train loss 2.36673
wandb: 
wandb: 🚀 View run dazzling-pig-1461 at: https://wandb.ai/nreints/thesis/runs/7end3hmb
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_013555-7end3hmb/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_015227-4i0xkqc1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-dragon-1467
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/4i0xkqc1
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6995935440063477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.046896457672119
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.414063930511475
7 2.5326028459 	 6.4140639752
epoch_time;  31.21016240119934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7943003177642822
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8045780658721924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.193358898162842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.7671799659729
8 2.5113395096 	 6.7671798291
epoch_time;  30.94776678085327
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.808148145675659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8193771839141846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.103546142578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6530327796936035
9 2.491130543 	 6.6530326544
epoch_time;  30.92902636528015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.813149929046631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.880063056945801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.091715335845947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6280293464660645
10 2.4761616362 	 6.6280292615
epoch_time;  30.95718550682068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.849911689758301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9280028343200684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.102431774139404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6835103034973145
11 2.4574603904 	 6.6835104029
epoch_time;  31.08928871154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.851393461227417
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9489974975585938
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.214486598968506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.820519924163818
12 2.4447732098 	 6.8205197614
epoch_time;  30.95986247062683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8663382530212402
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.939117431640625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.152949810028076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.750303268432617
13 2.4367160147 	 6.7503031474
epoch_time;  30.810054779052734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8546950817108154
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9453563690185547
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.131058692932129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.767839431762695
14 2.4397137142 	 6.7678392301
epoch_time;  30.900919675827026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.878894090652466
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.979541540145874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.177517414093018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.788525581359863
15 2.4155554356 	 6.7885255381
epoch_time;  31.038808584213257
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.891831874847412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.008603572845459
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.217896938323975
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.871870517730713
16 2.4049381831 	 6.871870427
epoch_time;  31.259063959121704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.891002655029297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.001216411590576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.191608428955078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.766669273376465
17 2.4300111111 	 6.7666694203
epoch_time;  32.53075933456421
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.895242929458618
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9598166942596436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.113089561462402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.689886093139648
18 2.3943286928 	 6.6898860874
epoch_time;  31.657779932022095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.925083875656128
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.994490623474121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.202929496765137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.780258655548096
19 2.3733422094 	 6.7802586858
epoch_time;  30.95882296562195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9215128421783447
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9745380878448486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.295050144195557
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.8524274826049805
20 2.3651696897 	 6.8524276872
epoch_time;  30.723753452301025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9185571670532227
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.976665735244751
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.192420959472656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.7803850173950195
21 2.3666084746 	 6.7803848128
epoch_time;  30.93787670135498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.964804172515869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.032514572143555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.321035385131836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.869295597076416
22 2.351703329 	 6.8692955178
epoch_time;  30.862165212631226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.956193208694458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.992518663406372
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.327004432678223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.868710517883301
23 2.3461561066 	 6.868710613
epoch_time;  30.86950993537903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8692409992218018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8942461013793945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.2077956199646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.73569917678833
24 2.4148270797 	 6.735698965
epoch_time;  30.804877281188965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9306447505950928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9402811527252197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.298363208770752
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.767181396484375
25 2.4319864139 	 6.7671813043
epoch_time;  30.968153715133667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9635534286499023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9411048889160156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.292793273925781
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.787570953369141
26 2.3342693597 	 6.7875711032
epoch_time;  31.357418537139893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8088674545288086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.0019426345825195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.21881103515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.990660190582275
27 2.3750630219 	 6.9906599615
epoch_time;  31.190088748931885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.946357011795044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.895780086517334
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.287653923034668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.716867446899414
28 2.4083125355 	 6.7168676832
epoch_time;  30.751826286315918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.960214376449585
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9261932373046875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.293511390686035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.6911725997924805
29 2.3667347032 	 6.6911724356
epoch_time;  30.800843477249146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9621119499206543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9290852546691895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.296216011047363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.685801982879639
It took  991.6519107818604  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15304a463fa0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a460670>, <torch.utils.data.dataloader.DataLoader object at 0x153043f4eb60>, <torch.utils.data.dataloader.DataLoader object at 0x153043f4f910>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7416188716888428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.340970039367676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2286481857299805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.4077630043029785
0 3.7732075901 	 5.4077627868
epoch_time;  31.081809520721436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7033257484436035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1783297061920166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.226799488067627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.207170009613037
1 2.711518928 	 5.207169916
epoch_time;  31.105931043624878
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.680474281311035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.143541097640991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.219181060791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.1909613609313965
2 2.6716406546 	 5.1909614863
epoch_time;  31.312265157699585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.688570737838745
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.135701894760132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.316249847412109
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.249435901641846
3 2.6432358816 	 5.2494357475
epoch_time;  31.195631742477417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6882247924804688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1461243629455566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.338590145111084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.267670631408691
4 2.6196207744 	 5.2676706919
epoch_time;  31.002867937088013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6937386989593506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.145573854446411
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.37410306930542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2785964012146
5 2.5968273547 	 5.2785965381
epoch_time;  31.070799350738525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.702794075012207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.1716954708099365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.459415435791016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.409088611602783
6 2.5757115008 	 5.4090885958
epoch_time;  31.364560842514038
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7247719764709473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.225937843322754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.587609767913818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.620179176330566
7 2.5564569189 	 5.6201791446
epoch_time;  31.28628706932068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7378859519958496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2774817943573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.600081920623779
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.714226245880127
8 2.5366220178 	 5.7142263914
epoch_time;  31.301217079162598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7541697025299072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.322227716445923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.596374988555908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.783277988433838
9 2.5202577578 	 5.7832779899
epoch_time;  31.04056429862976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.760270833969116
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3352766036987305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.715588092803955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.911013126373291
10 2.505221948 	 5.9110133237
epoch_time;  30.54086995124817
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7720301151275635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3558666706085205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.677754878997803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.868629455566406
11 2.493192524 	 5.8686294786
epoch_time;  30.95636558532715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.798825979232788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.373716115951538
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.751260280609131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.927873611450195
12 2.4732848058 	 5.9278737786
epoch_time;  30.831364631652832
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7976675033569336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.37349534034729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.666602611541748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.795047283172607
13 2.4598082803 	 5.7950472644
epoch_time;  30.800479888916016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.818922519683838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4316020011901855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.714882850646973
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.923675060272217
14 2.4422233171 	 5.9236750761
epoch_time;  32.295212745666504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.83788800239563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.437303066253662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.83099889755249
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.001971244812012
15 2.4388750237 	 6.0019711958
epoch_time;  32.07635045051575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8222239017486572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4249563217163086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.743992328643799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.9034528732299805
16 2.417400259 	 5.9034530778
epoch_time;  31.122758626937866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8495969772338867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4388668537139893
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.828166961669922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.990832328796387
17 2.4065891355 	 5.9908321876
epoch_time;  30.81908369064331
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.845318555831909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4257071018218994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.761138439178467
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.958397388458252
18 2.4059319334 	 5.9583976262
epoch_time;  30.928329467773438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.849426746368408
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4676804542541504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.86750602722168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.099916458129883
19 2.4004399706 	 6.0999165791
epoch_time;  31.426180362701416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8814451694488525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5911312103271484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.896349906921387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.283575057983398
20 2.3901600054 	 6.2835748678
epoch_time;  30.83951711654663
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8752198219299316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.519629716873169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.965444564819336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.297976970672607
21 2.3748588013 	 6.2979769519
epoch_time;  31.221990823745728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8233232498168945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.477200984954834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.865424156188965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.170101642608643
22 2.3649591855 	 6.1701014769
epoch_time;  31.029837131500244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.882019519805908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5754878520965576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.964655876159668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.326172828674316
23 2.3862846195 	 6.3261726126
epoch_time;  31.26685380935669
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8774571418762207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6073498725891113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.983147144317627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.38743782043457
24 2.3608801334 	 6.3874378953
epoch_time;  31.076746702194214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8871870040893555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.621433734893799
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▁▁▁▁▂▃▄▄▅▅▅▄▅▅▅▅▅▆▇▇▆▇███▇█▆▆
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▄▂▁▁▁▁▁▂▃▃▄▄▄▄▅▅▅▅▅▅▇▆▅▇▇▇▇██▇▇
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▁▁▂▂▂▃▄▄▄▅▅▆▅▅▇▆▇▆▇▇█▇████▇▇▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▃▂▁▁▁▁▂▂▃▃▃▄▄▄▅▆▅▆▆▆▇▇▅▇▇▇▇██▆▆
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.036
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.58488
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.606
wandb:     Test loss t(0, 0)_r(0, 0)_none 2.845
wandb:                         Train loss 2.42188
wandb: 
wandb: 🚀 View run festive-dragon-1467 at: https://wandb.ai/nreints/thesis/runs/4i0xkqc1
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_015227-4i0xkqc1/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_020900-917y1xwj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-kumquat-1473
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/917y1xwj
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.991755962371826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.462526798248291
25 2.3473616549 	 6.4625269956
epoch_time;  31.10226082801819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8966310024261475
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6411056518554688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.970837116241455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.40541934967041
26 2.3613966914 	 6.4054194793
epoch_time;  31.291396856307983
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9239273071289062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.673057794570923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.933096408843994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.37086820602417
27 2.3317815649 	 6.370868049
epoch_time;  31.051801919937134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9114859104156494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.6830854415893555
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.923842430114746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.416544437408447
28 2.3481833424 	 6.4165444734
epoch_time;  31.054298400878906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.844308614730835
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5856542587280273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.607249736785889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.034224510192871
29 2.4218781419 	 6.0342243114
epoch_time;  31.469966411590576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8449957370758057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.58487868309021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.605999946594238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.0359954833984375
It took  993.5684623718262  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15304a4ceef0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a460280>, <torch.utils.data.dataloader.DataLoader object at 0x15304a460dc0>, <torch.utils.data.dataloader.DataLoader object at 0x15304a460130>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.76489520072937
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.650118350982666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4060187339782715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.641709327697754
0 3.7272655363 	 5.6417092499
epoch_time;  31.219661712646484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.718036413192749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.417538642883301
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.322011947631836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.418558120727539
1 2.7034850196 	 5.4185580804
epoch_time;  30.994182348251343
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.692878246307373
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3978588581085205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.329618453979492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.414444923400879
2 2.6603423587 	 5.4144449378
epoch_time;  31.057074069976807
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6936748027801514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.426637887954712
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.471602439880371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.590351581573486
3 2.6291871688 	 5.5903515773
epoch_time;  30.88198494911194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.694878339767456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4256765842437744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.410969257354736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.534856796264648
4 2.6004994441 	 5.5348567905
epoch_time;  30.955199480056763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7107067108154297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.5102357864379883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.494876861572266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.657365322113037
5 2.5773149558 	 5.6573652285
epoch_time;  30.993186473846436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.714632749557495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.591482639312744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.568519115447998
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.843573093414307
6 2.5554788779 	 5.8435729796
epoch_time;  31.112844228744507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7378921508789062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.656707763671875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.733658790588379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.04690408706665
7 2.5361923229 	 6.0469041346
epoch_time;  31.2459614276886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.738515853881836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7382888793945312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.690819263458252
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.161081790924072
8 2.5175868113 	 6.1610819191
epoch_time;  31.266932487487793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.755063533782959
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8125195503234863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.847560882568359
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.432703495025635
9 2.4999388262 	 6.4327034849
epoch_time;  30.915411710739136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7602105140686035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.907365322113037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.8686203956604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.488192081451416
10 2.4842092209 	 6.4881920022
epoch_time;  31.074825763702393
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.767028331756592
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.024623394012451
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.985501289367676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.706090927124023
11 2.4715915307 	 6.7060908292
epoch_time;  32.13937020301819
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7941787242889404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.068656921386719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.945656776428223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.633675575256348
12 2.4568850613 	 6.6336754744
epoch_time;  31.76868200302124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7825489044189453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.064868450164795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.958749771118164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.630545139312744
13 2.4424314188 	 6.6305451638
epoch_time;  31.283735275268555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8209753036499023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.064887046813965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.939518928527832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.5428080558776855
14 2.4266661443 	 6.5428079565
epoch_time;  30.94236731529236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.817401647567749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.070811748504639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.014657497406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.551212787628174
15 2.4072967922 	 6.5512127372
epoch_time;  30.567709922790527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8193633556365967
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.071555137634277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.018130779266357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.595760822296143
16 2.4067207238 	 6.5957606566
epoch_time;  30.887022495269775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.836625576019287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9843876361846924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.079048156738281
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.4912004470825195
17 2.3885349271 	 6.4912006113
epoch_time;  30.766127109527588
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.840358018875122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9219226837158203
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▆▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▆▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▆▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂█▅▃▃
wandb:                         Train loss ▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▃
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 10.19368
wandb:  Test loss t(-10, 10)_r(0, 0)_none 5.83277
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.31729
wandb:     Test loss t(0, 0)_r(0, 0)_none 3.18191
wandb:                         Train loss 3.30586
wandb: 
wandb: 🚀 View run floating-kumquat-1473 at: https://wandb.ai/nreints/thesis/runs/917y1xwj
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_020900-917y1xwj/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_022532-6dbs7e7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-rat-1478
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/6dbs7e7k
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.025112628936768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.380501747131348
18 2.4082731699 	 6.3805016463
epoch_time;  31.150518894195557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.811325788497925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.949620008468628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.013736248016357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.438340187072754
19 2.3797406898 	 6.4383401093
epoch_time;  31.264437198638916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.850412607192993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.007444858551025
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.984665870666504
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.366817951202393
20 2.3955789663 	 6.3668179699
epoch_time;  30.71145725250244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.849438428878784
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9611966609954834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.062211513519287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.39323616027832
21 2.3581197365 	 6.3932360508
epoch_time;  30.797781944274902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8207411766052246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.993875741958618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.03404426574707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.428929328918457
22 2.3684385128 	 6.4289292627
epoch_time;  30.957480430603027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8826076984405518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.938406229019165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.045567989349365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.31013298034668
23 2.3464525499 	 6.3101330898
epoch_time;  31.250618934631348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.880155086517334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9647703170776367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.990738868713379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.253531455993652
24 2.3397487551 	 6.2535315568
epoch_time;  30.799102783203125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8442189693450928
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9098401069641113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.929588317871094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.22605562210083
25 2.3771203826 	 6.2260557791
epoch_time;  30.981609106063843
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8904919624328613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9370245933532715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.912123203277588
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.123456001281738
26 2.428152981 	 6.1234562347
epoch_time;  31.39574933052063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 4.075129985809326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 8.414702415466309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.121994018554688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.122726440429688
27 5.14343234 	 14.1227267631
epoch_time;  31.19936966896057
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.4695804119110107
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 6.947759628295898
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.66401195526123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.095969200134277
28 3.7129460695 	 12.0959693932
epoch_time;  31.19374966621399
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.182976484298706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.831542015075684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.31576681137085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.186556816101074
29 3.3058595968 	 10.1865566288
epoch_time;  31.071605920791626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.181913137435913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.832766056060791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.317286014556885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.193681716918945
It took  991.5151119232178  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x15304a438f70>, <torch.utils.data.dataloader.DataLoader object at 0x153043f4d060>, <torch.utils.data.dataloader.DataLoader object at 0x153043817c10>, <torch.utils.data.dataloader.DataLoader object at 0x1530438166e0>]
LSTM(
  (lstm): LSTM(12, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8537960052490234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.4944467544555664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.269514560699463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.522061347961426
0 3.6870930374 	 5.5220615352
epoch_time;  31.206993341445923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7823522090911865
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3141515254974365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.128352165222168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2624382972717285
1 2.6936020371 	 5.2624382641
epoch_time;  31.284069061279297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.761791706085205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2995853424072266
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.142297267913818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.274372577667236
2 2.655330915 	 5.2743727577
epoch_time;  30.897531747817993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.758117198944092
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3109946250915527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.144430160522461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.229865550994873
3 2.6260841248 	 5.2298654055
epoch_time;  30.958579778671265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7687184810638428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3807899951934814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.274881839752197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.424976348876953
4 2.6004029706 	 5.4249765448
epoch_time;  31.079468488693237
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.776271343231201
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.435539960861206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.369853496551514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.546480655670166
5 2.5764703512 	 5.5464807608
epoch_time;  30.846837282180786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.7919816970825195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.556382179260254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.428797721862793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.715336322784424
6 2.5522383315 	 5.7153364568
epoch_time;  31.04386615753174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8059611320495605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.624918222427368
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.460493087768555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.869609355926514
7 2.5333807322 	 5.8696093602
epoch_time;  31.381373405456543
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.791322946548462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.653916358947754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.404555320739746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.830183982849121
8 2.5191751115 	 5.8301839684
epoch_time;  32.93233299255371
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8308846950531006
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.664757013320923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.456579208374023
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.858941078186035
9 2.5043457401 	 5.8589413
epoch_time;  31.99777364730835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.848607063293457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.699490547180176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.504740238189697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.926112651824951
10 2.4886858101 	 5.9261124257
epoch_time;  30.687976837158203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8742640018463135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.722487688064575
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▁▁▁▂▂▃▄▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▅▆█▅▅▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▂▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▄▅▄▄▅▅▅▅▅▅█▄▅▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▁▁▁▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▄▅▅▄▅▅▅█▄▅▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▂▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▄▄
wandb:                         Train loss █▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 6.24551
wandb:  Test loss t(-10, 10)_r(0, 0)_none 3.92373
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.66409
wandb:     Test loss t(0, 0)_r(0, 0)_none 3.01273
wandb:                         Train loss 2.32633
wandb: 
wandb: 🚀 View run vibrant-rat-1478 at: https://wandb.ai/nreints/thesis/runs/6dbs7e7k
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_022532-6dbs7e7k/logs
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.543619155883789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.975729942321777
11 2.4678375683 	 5.9757297666
epoch_time;  31.11922335624695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.8680148124694824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.772721290588379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.593958377838135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.1054229736328125
12 2.4496499234 	 6.1054230197
epoch_time;  30.881956577301025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.905174732208252
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8492558002471924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6339616775512695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.165728569030762
13 2.4317167688 	 6.1657283357
epoch_time;  30.866796493530273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9157392978668213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8226096630096436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.601906776428223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.111198425292969
14 2.4171237758 	 6.11119831
epoch_time;  31.012501001358032
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.926487922668457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.829427480697632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.678431987762451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.23689603805542
15 2.4035121505 	 6.2368960654
epoch_time;  30.821165561676025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.943591833114624
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.87304425239563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.661067485809326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.21661376953125
16 2.3921281005 	 6.2166139539
epoch_time;  31.184046268463135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9281506538391113
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8945791721343994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.680996894836426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.323664665222168
17 2.3827866259 	 6.3236648237
epoch_time;  31.132147073745728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.921605110168457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8213613033294678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6680121421813965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.259374141693115
18 2.3875652027 	 6.2593739674
epoch_time;  31.196114778518677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9307422637939453
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8523612022399902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.619126796722412
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.205080509185791
19 2.4504019357 	 6.2050803378
epoch_time;  31.229645013809204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9752023220062256
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9024221897125244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.686713218688965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.282713413238525
20 2.3683385279 	 6.2827133686
epoch_time;  30.99242663383484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.970623254776001
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8913309574127197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.714823246002197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.311288356781006
21 2.3475749314 	 6.3112881479
epoch_time;  31.063195943832397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.974682092666626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.9116170406341553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.640498638153076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.258090496063232
22 2.3483293463 	 6.2580905695
epoch_time;  31.029725074768066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9954824447631836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.934643268585205
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7076568603515625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.368446350097656
23 2.3505776208 	 6.3684465575
epoch_time;  30.933837413787842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.9842514991760254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.923107862472534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.685393333435059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.317570686340332
24 2.3297253132 	 6.3175708967
epoch_time;  31.05233144760132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.99763822555542
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8922293186187744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.767605304718018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.379181385040283
25 2.3487326126 	 6.3791813692
epoch_time;  30.84737753868103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.4644830226898193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.469027519226074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.178226470947266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.9542341232299805
26 2.3206475696 	 6.9542343278
epoch_time;  30.996906042099
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.968202829360962
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.834822654724121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.6434855461120605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.138984203338623
27 2.4682009146 	 6.1389842422
epoch_time;  31.442291259765625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.001777410507202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.905959129333496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.668954849243164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.193122386932373
28 2.3233833862 	 6.1931226102
epoch_time;  30.91088366508484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.0130157470703125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.920921564102173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.663944244384766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.250044345855713
29 2.3263263803 	 6.2500442551
epoch_time;  31.571811199188232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 3.012727737426758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.923725128173828
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.664093017578125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.245510101318359
It took  995.081465959549  seconds.

JOB STATISTICS
==============
Job ID: 2141143
Array Job ID: 2141141_2
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 13:47:28
CPU Efficiency: 27.72% of 2-01:45:36 core-walltime
Job Wall-clock time: 02:45:52
Memory Utilized: 18.20 GB
Memory Efficiency: 58.25% of 31.25 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
