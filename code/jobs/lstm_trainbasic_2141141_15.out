/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_054609-1zyjo2u9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-dumpling-1547
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/1zyjo2u9
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
BEFORE ['data_t(0,', '0)_r(-5,', '5)_none']
----- ITERATION 0/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbd3ff7f70>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd304b20>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd3043a0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd304610>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042206138372421265
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37631168961524963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.050826091319322586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43086007237434387
0 1.6437186184 	 0.4308600757
epoch_time;  34.94691586494446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03464668616652489
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5145208835601807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0441209152340889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.570731520652771
1 0.1133487652 	 0.5707315405
epoch_time;  35.027077436447144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01485449355095625
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2657826840877533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01967557519674301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3047664761543274
2 0.0295267242 	 0.3047664677
epoch_time;  35.2869234085083
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.020550483837723732
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20358091592788696
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024199584499001503
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23454047739505768
3 0.0179761068 	 0.2345404726
epoch_time;  34.78653407096863
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016752954572439194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.410874605178833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02231852151453495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4291030466556549
4 0.1074562044 	 0.4291030561
epoch_time;  35.154683351516724
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007521347608417273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26633697748184204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010912390425801277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2838374376296997
5 0.0172454551 	 0.283837425
epoch_time;  34.813329458236694
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.015315311029553413
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39392057061195374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02056797407567501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4312404990196228
6 0.0551957413 	 0.4312404852
epoch_time;  34.9828314781189
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010250840336084366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27938663959503174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01351875253021717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3016931414604187
7 0.0154786727 	 0.3016931355
epoch_time;  35.021392583847046
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005659879185259342
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2081034928560257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007419687230139971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2310856282711029
8 0.010692325 	 0.2310856246
epoch_time;  34.88340520858765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010733306407928467
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32260286808013916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014590776525437832
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33962810039520264
9 0.0129877952 	 0.3396280986
epoch_time;  34.89040517807007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003479353152215481
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18820792436599731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005128873977810144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20485982298851013
10 0.0072005753 	 0.2048598229
epoch_time;  34.80742430686951
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002501704962924123
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17898303270339966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003880872391164303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19525741040706635
11 0.0080373686 	 0.1952574116
epoch_time;  34.95594143867493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003984565380960703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17281490564346313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005148570518940687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18358108401298523
12 0.0059875406 	 0.1835810843
epoch_time;  34.9918646812439
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019392763497307897
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17037075757980347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002957831136882305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1841457486152649
13 0.0062611217 	 0.1841457517
epoch_time;  34.80445623397827
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029297363944351673
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19741584360599518
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003983313217759132
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21505679190158844
14 0.010120355 	 0.2150567968
epoch_time;  34.86074638366699
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022685471922159195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20997704565525055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036018099635839462
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22539111971855164
15 0.0083509484 	 0.2253911229
epoch_time;  34.773977518081665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012796565890312195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5103728175163269
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016672879457473755
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5442054867744446
16 0.0189773349 	 0.5442054956
epoch_time;  35.02621650695801
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003218968166038394
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2691928744316101
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00467647286131978
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28479620814323425
17 0.0072419703 	 0.2847962163
epoch_time;  35.05748510360718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024664446245878935
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22575977444648743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003589695319533348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23923712968826294
18 0.004933402 	 0.2392371371
epoch_time;  34.11713266372681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010210893116891384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2771907150745392
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012194355018436909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28422704339027405
19 0.0111358442 	 0.2842270543
epoch_time;  35.045348167419434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00826314091682434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22764651477336884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008807183243334293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24020317196846008
20 0.0049128908 	 0.2402031659
epoch_time;  35.45505928993225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0705362930893898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.058107614517212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08369752764701843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.1101582050323486
21 0.0368379626 	 1.110158223
epoch_time;  36.0619056224823
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006265122443437576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.310263067483902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008570616133511066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33084923028945923
22 0.0203639355 	 0.330849224
epoch_time;  34.89151930809021
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037339820992201567
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–„â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–‚â–„â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–„â–‚â–â–ƒâ–‚â–ƒâ–‚â–â–‚â–â–â–â–â–â–‚â–„â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–â–â–‚â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–…â–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–ˆâ–‚â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–ˆâ–â–â–â–â–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.15352
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.1442
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00252
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00154
wandb:                         Train loss 0.00443
wandb: 
wandb: ğŸš€ View run vermilion-dumpling-1547 at: https://wandb.ai/nreints/thesis/runs/1zyjo2u9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_054609-1zyjo2u9/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_060504-drb0sb8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-chrysanthemum-1554
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/drb0sb8z
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23286445438861847
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0052271634340286255
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24073158204555511
23 0.0074507246 	 0.2407315764
epoch_time;  35.119277000427246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005262110382318497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18911044299602509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006635528523474932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2023797333240509
24 0.0055834032 	 0.2023797395
epoch_time;  34.70898246765137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003121374174952507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18197456002235413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0041398354806005955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19196604192256927
25 0.0050317279 	 0.1919660424
epoch_time;  34.81943893432617
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030841603875160217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21517910063266754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004531769081950188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.223097026348114
26 0.0092317991 	 0.2230970262
epoch_time;  35.209391593933105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0056657991372048855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17987899482250214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007139135152101517
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18759533762931824
27 0.0045990164 	 0.1875953329
epoch_time;  34.801047563552856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0070611597038805485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16431623697280884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008262915536761284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17882484197616577
28 0.0043309462 	 0.1788248368
epoch_time;  35.29948139190674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015368256717920303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14484122395515442
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0025147220585495234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15380945801734924
29 0.0044294317 	 0.1538094581
epoch_time;  35.1960027217865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001536320662125945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14419858157634735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0025150803849101067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15352337062358856
It took  1135.3842680454254  seconds.
----- ITERATION 1/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbd3f4f2b0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd37ff10>, <torch.utils.data.dataloader.DataLoader object at 0x14cb8a2400a0>, <torch.utils.data.dataloader.DataLoader object at 0x14cb8a240190>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04320766031742096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3473759591579437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.054440081119537354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49005255103111267
0 1.5749881349 	 0.4900525603
epoch_time;  34.91833257675171
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03242073953151703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3690577745437622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.041224393993616104
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.510796844959259
1 0.0991841272 	 0.510796861
epoch_time;  34.81751871109009
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01420675776898861
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19560907781124115
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01796329766511917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2692822515964508
2 0.0268020139 	 0.2692822459
epoch_time;  34.99386143684387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019495436921715736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35480451583862305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025948025286197662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.47379693388938904
3 0.105940652 	 0.4737969252
epoch_time;  34.98244833946228
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009082521311938763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21262405812740326
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012544807977974415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2882194519042969
4 0.0202695239 	 0.2882194404
epoch_time;  35.03950357437134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.062421850860118866
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7580498456954956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.07651526480913162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9808022975921631
5 0.0472625917 	 0.9808023228
epoch_time;  34.7599081993103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010313736274838448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23713351786136627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01341445092111826
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3186553418636322
6 0.0238311199 	 0.318655354
epoch_time;  35.07993769645691
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0055867633782327175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15959863364696503
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007819578982889652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2181929647922516
7 0.0119824933 	 0.218192962
epoch_time;  34.88122820854187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1042608991265297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9030801653862
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.12632085382938385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.142951250076294
8 0.0399055799 	 1.1429512508
epoch_time;  34.90149164199829
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008442984893918037
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2443760335445404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011438876390457153
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3345666527748108
9 0.0241569574 	 0.3345666522
epoch_time;  34.83116579055786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007130431476980448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1795579046010971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009193169884383678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24375995993614197
10 0.0108334767 	 0.243759962
epoch_time;  34.70872235298157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0046272301115095615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1395154595375061
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005875554867088795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19599755108356476
11 0.0088839116 	 0.1959975551
epoch_time;  34.929510831832886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008109938353300095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20832912623882294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011149932630360126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2942667007446289
12 0.0404331227 	 0.2942667152
epoch_time;  35.10205674171448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019757714122533798
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19223205745220184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022086169570684433
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24841433763504028
13 0.0097886229 	 0.2484143306
epoch_time;  36.49012207984924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007729831617325544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1506061851978302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00842241570353508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19406822323799133
14 0.0076031248 	 0.1940682172
epoch_time;  35.738531827926636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007665632758289576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20645971596240997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011250794865190983
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ƒâ–„â–‚â–ƒâ–‚â–‡â–‚â–‚â–ˆâ–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–ƒâ–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–ƒâ–‚â–ƒâ–‚â–‡â–‚â–â–ˆâ–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–„â–ƒâ–‚â–‚â–‚â–…â–‚â–â–ˆâ–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–„â–ƒâ–‚â–‚â–‚â–…â–‚â–â–ˆâ–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.21448
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.18278
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00643
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00533
wandb:                         Train loss 0.00564
wandb: 
wandb: ğŸš€ View run scintillating-chrysanthemum-1554 at: https://wandb.ai/nreints/thesis/runs/drb0sb8z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_060504-drb0sb8z/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_062350-nepudyc1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run festive-chrysanthemum-1561
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/nepudyc1
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2858189344406128
15 0.0186556571 	 0.285818924
epoch_time;  35.493265867233276
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013552200049161911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17504587769508362
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014075365848839283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21488618850708008
16 0.0077012313 	 0.2148861842
epoch_time;  35.19671177864075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034003499895334244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13031889498233795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00448243785649538
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16425207257270813
17 0.0066010282 	 0.1642520709
epoch_time;  35.016648054122925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004641788545995951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16802987456321716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0061415391974151134
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21640390157699585
18 0.0103017863 	 0.2164039036
epoch_time;  34.29382538795471
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003233478870242834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1232023537158966
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004152357112616301
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15300671756267548
19 0.0052613212 	 0.1530067236
epoch_time;  35.19353461265564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001564130885526538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13342352211475372
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0026202169246971607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16389934718608856
20 0.0070135801 	 0.1638993439
epoch_time;  34.887388944625854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002594762947410345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11724133044481277
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003426484763622284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14279796183109283
21 0.0044634265 	 0.1427979599
epoch_time;  34.868154525756836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003820784157142043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15140606462955475
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0046907151117920876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18125493824481964
22 0.0074327521 	 0.1812549372
epoch_time;  35.110968828201294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016457200981676579
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13811463117599487
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0024604967329651117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16870872676372528
23 0.0046252085 	 0.1687087321
epoch_time;  34.814249992370605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0027200651820749044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13074754178524017
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035780188627541065
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16650690138339996
24 0.0041061061 	 0.1665069027
epoch_time;  34.79303812980652
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030464341398328543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17037737369537354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004373929463326931
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23691070079803467
25 0.0245861397 	 0.2369107019
epoch_time;  34.93363928794861
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008238636888563633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14718705415725708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009415552020072937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19180966913700104
26 0.0053537543 	 0.1918096744
epoch_time;  35.003339529037476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009090832434594631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36488834023475647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012734624557197094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4470391869544983
27 0.0449042887 	 0.4470391864
epoch_time;  35.062392234802246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00491773197427392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23636378347873688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006588493473827839
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28049689531326294
28 0.0084448258 	 0.2804969027
epoch_time;  35.01189303398132
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005335862748324871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18154528737068176
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006424152757972479
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2142651379108429
29 0.0056377096 	 0.2142651376
epoch_time;  35.197200775146484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005333835259079933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18278446793556213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006425931584089994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21447624266147614
It took  1126.526483297348  seconds.
----- ITERATION 2/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbcdae7130>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd37e560>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdac6bc0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdac6e00>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04157179966568947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41143202781677246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.056538648903369904
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4438628554344177
0 1.5905149981 	 0.4438628689
epoch_time;  35.09534215927124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06901028007268906
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8790124654769897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09286563098430634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.9358675479888916
1 0.1240502703 	 0.9358675401
epoch_time;  35.30011987686157
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019810454919934273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33981969952583313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.026501992717385292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3487032949924469
2 0.0447104351 	 0.348703298
epoch_time;  34.87590742111206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010777745395898819
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2409510612487793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015118886716663837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24102294445037842
3 0.0253585394 	 0.2410229455
epoch_time;  34.89427852630615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03434985503554344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7085933685302734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.044708944857120514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6741463541984558
4 0.0920789489 	 0.6741463468
epoch_time;  35.24028515815735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014933436177670956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34394025802612305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01988690346479416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3263947069644928
5 0.0253260119 	 0.3263946948
epoch_time;  34.88308382034302
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00901168305426836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3071347177028656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013032631948590279
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2934345006942749
6 0.0211417439 	 0.2934344888
epoch_time;  36.620362758636475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.016265204176306725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2079351842403412
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01844187080860138
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20176586508750916
7 0.0114401176 	 0.2017658614
epoch_time;  36.202775716781616
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ˆâ–ƒâ–‚â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–â–â–â–â–ƒâ–ƒ
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–ˆâ–ƒâ–‚â–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–â–â–â–â–ƒâ–ƒ
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–…â–ˆâ–ƒâ–‚â–„â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–ˆâ–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.35844
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34517
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.01884
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01459
wandb:                         Train loss 0.07003
wandb: 
wandb: ğŸš€ View run festive-chrysanthemum-1561 at: https://wandb.ai/nreints/thesis/runs/nepudyc1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_062350-nepudyc1/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_064239-5539zfsc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-wonton-1568
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/5539zfsc
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007055460009723902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2004692405462265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00921651441603899
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2005801647901535
8 0.0134662712 	 0.2005801705
epoch_time;  35.434181928634644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0044691492803394794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1900537759065628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00688033364713192
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2025335729122162
9 0.0192761923 	 0.2025335721
epoch_time;  35.08505916595459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003671445185318589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18085640668869019
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005465900991111994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1857224851846695
10 0.0093875758 	 0.1857224778
epoch_time;  35.1248939037323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007619690150022507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15382295846939087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0093120401725173
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15946225821971893
11 0.0073304844 	 0.1594622632
epoch_time;  34.93505144119263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002347685396671295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13775797188282013
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003585144877433777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14555543661117554
12 0.0070627394 	 0.1455554328
epoch_time;  35.05046796798706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010032170452177525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26550284028053284
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013799206353724003
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27277055382728577
13 0.0317527863 	 0.2727705618
epoch_time;  35.30214309692383
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00325830583460629
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1850529909133911
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00522620789706707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18762165307998657
14 0.0089641242 	 0.1876216554
epoch_time;  34.75750756263733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003485085442662239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1629457026720047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005289321765303612
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1764560341835022
15 0.008581747 	 0.1764560365
epoch_time;  34.91722559928894
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003421303816139698
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14157238602638245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004848280921578407
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1471959501504898
16 0.0052854378 	 0.1471959486
epoch_time;  34.730597496032715
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002910366514697671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15208463370800018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0041195605881512165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16762405633926392
17 0.0093810452 	 0.1676240558
epoch_time;  35.121761083602905
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002054151613265276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16910190880298615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003571033477783203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17660550773143768
18 0.0075991968 	 0.1766055127
epoch_time;  34.54082441329956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0031961111817508936
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15225239098072052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004087379667907953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14175400137901306
19 0.0048763183 	 0.1417540006
epoch_time;  34.441402435302734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002558380365371704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1773463636636734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004110598471015692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1852133870124817
20 0.0093722311 	 0.1852133944
epoch_time;  34.662654876708984
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010866247117519379
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.147592693567276
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01205343659967184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14960025250911713
21 0.0047032456 	 0.1496002566
epoch_time;  34.678598165512085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007948470301926136
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13709568977355957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009199796244502068
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13687993586063385
22 0.0042681181 	 0.1368799354
epoch_time;  34.86842441558838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01362923625856638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3875812590122223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018418757244944572
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36040541529655457
23 0.0382691036 	 0.3604054293
epoch_time;  34.82016968727112
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037129262927919626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2032751888036728
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005846822634339333
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2094435691833496
24 0.0099114448 	 0.2094435677
epoch_time;  34.969133615493774
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0025244930293411016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1764843463897705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004074037075042725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17951135337352753
25 0.0064648145 	 0.1795113555
epoch_time;  34.72384238243103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034438008442521095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1579696387052536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004737765993922949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16257382929325104
26 0.0053083375 	 0.162573823
epoch_time;  34.91808843612671
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038354191929101944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1438090205192566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0055833435617387295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15010423958301544
27 0.004992001 	 0.1501042461
epoch_time;  35.17174220085144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005459093023091555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13289117813110352
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006106042303144932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13251309096813202
28 0.0045759409 	 0.1325130981
epoch_time;  34.546141624450684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014597487635910511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34590792655944824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018849216401576996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3580557703971863
29 0.070033002 	 0.3580557832
epoch_time;  34.96255111694336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014593961648643017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34517309069633484
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.018840106204152107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35844355821609497
It took  1128.45854306221  seconds.
----- ITERATION 3/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbcdae7e80>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd304430>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdac7340>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd356b00>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03941395506262779
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3192135691642761
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04891078546643257
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41264426708221436
0 1.6098394401 	 0.4126442624
epoch_time;  36.55303716659546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03674105182290077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34744271636009216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0431482195854187
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4422380030155182
1 0.1138715722 	 0.4422380154
epoch_time;  36.013675689697266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013874673284590244
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17899000644683838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017993446439504623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23187530040740967
2 0.024817684 	 0.2318753015
epoch_time;  35.38883852958679
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.030289195477962494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5088949799537659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03823697939515114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6216374635696411
3 0.1110195532 	 0.6216374423
epoch_time;  34.85539412498474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011584984138607979
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2920382022857666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01553529966622591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36072975397109985
4 0.0241078919 	 0.3607297592
epoch_time;  34.62284469604492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.019483720883727074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4040334224700928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02485051192343235
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48360303044319153
5 0.054950613 	 0.4836030251
epoch_time;  34.59097504615784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009360596537590027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2321319282054901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012239365838468075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3025515377521515
6 0.0182581735 	 0.3025515231
epoch_time;  34.93033027648926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005655454937368631
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16967126727104187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008044946938753128
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2266182154417038
7 0.0143131845 	 0.2266182107
epoch_time;  35.021629095077515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005080704111605883
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14483828842639923
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007335148751735687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1968703418970108
8 0.0171267476 	 0.1968703486
epoch_time;  34.79759120941162
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004046525340527296
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11501776427030563
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005781705491244793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15583935379981995
9 0.0090438827 	 0.1558393611
epoch_time;  34.966103315353394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010056617669761181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16270266473293304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013521491549909115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21132777631282806
10 0.0398204783 	 0.2113277746
epoch_time;  35.088494300842285
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004890161566436291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11745110899209976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006708751432597637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15651410818099976
11 0.0099546236 	 0.1565141015
epoch_time;  35.01087474822998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038588105235248804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1128000020980835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00603073462843895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15180495381355286
12 0.0109231091 	 0.1518049557
epoch_time;  36.81235861778259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003785469336435199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08938653022050858
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005055240821093321
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1215909868478775
13 0.006693879 	 0.121590986
epoch_time;  35.09436845779419
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008572612889111042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13116858899593353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010609631426632404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16231314837932587
14 0.0152545528 	 0.1623131444
epoch_time;  34.85828495025635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0035564154386520386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10218638181686401
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00474396301433444
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13867850601673126
15 0.0062003811 	 0.1386785017
epoch_time;  34.78861474990845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018916979897767305
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10009787231683731
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0031429575756192207
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13842447102069855
16 0.006657186 	 0.1384244729
epoch_time;  34.7535879611969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002401812467724085
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.08672407269477844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035238945856690407
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11898867040872574
17 0.0046800123 	 0.118988671
epoch_time;  34.77004408836365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002995905000716448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12797869741916656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00424489751458168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15516546368598938
18 0.0100850135 	 0.155165462
epoch_time;  34.90203833580017
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030284218955785036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.104860320687294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0041179172694683075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12959200143814087
19 0.0050768831 	 0.1295920081
epoch_time;  34.18231534957886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.043381694704294205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5070461630821228
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05512980371713638
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5832408666610718
20 0.0294669886 	 0.5832408894
epoch_time;  35.21398329734802
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004625657107681036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16973775625228882
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0064587281085550785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20829179883003235
21 0.0131201834 	 0.2082918058
epoch_time;  34.79636478424072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036026265006512403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.12407835572957993
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004895893391221762
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1529710739850998
22 0.0060673164 	 0.1529710775
epoch_time;  34.90432381629944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003302962752059102
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.11208020895719528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004429219756275415
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13980470597743988
23 0.0053480482 	 0.1398047133
epoch_time;  34.836803913116455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00257360702380538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.10260009765625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036590127274394035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12738801538944244
24 0.0063087989 	 0.127388012
epoch_time;  34.81288933753967
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–†â–ƒâ–ˆâ–„â–†â–„â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–‡â–‚â–â–â–â–â–‚â–‚â–‚â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–…â–…â–ƒâ–ˆâ–„â–†â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–ˆâ–‚â–‚â–â–â–â–‚â–‚â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–†â–ƒâ–†â–ƒâ–„â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‡â–‡â–ƒâ–†â–ƒâ–„â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–ˆâ–â–â–â–â–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.12075
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.09725
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00362
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00282
wandb:                         Train loss 0.00417
wandb: 
wandb: ğŸš€ View run thriving-wonton-1568 at: https://wandb.ai/nreints/thesis/runs/5539zfsc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_064239-5539zfsc/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_070129-zeeydmn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-pig-1574
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/zeeydmn4
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0064513832330703735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09304255247116089
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006761840544641018
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11923493444919586
25 0.0040822257 	 0.1192349322
epoch_time;  36.16923785209656
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002967940177768469
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16509929299354553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004791574086993933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2109263390302658
26 0.0147013942 	 0.210926344
epoch_time;  36.20198655128479
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0029075045604258776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17528149485588074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004718002397567034
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20829692482948303
27 0.0074807282 	 0.2082969228
epoch_time;  34.747968435287476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021472391672432423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1218588650226593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0031030059326440096
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15569694340229034
28 0.003996713 	 0.155696938
epoch_time;  34.61939716339111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028212671168148518
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.0971570760011673
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036246899981051683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12059009820222855
29 0.0041656808 	 0.1205900947
epoch_time;  34.85246920585632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002821207046508789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.09724657982587814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0036222287453711033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.120746910572052
It took  1129.9373350143433  seconds.
----- ITERATION 4/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbcd354fa0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa1240>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa2c20>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa2dd0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04497850313782692
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39450281858444214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.056648336350917816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49724769592285156
0 1.5574246103 	 0.4972477017
epoch_time;  34.657182693481445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021641070023179054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29953426122665405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028668610379099846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3772107660770416
1 0.0741511366 	 0.3772107726
epoch_time;  34.68708157539368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.025347409769892693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49152353405952454
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03404974937438965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.598823070526123
2 0.0655154879 	 0.5988230633
epoch_time;  34.5382194519043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010673013515770435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24366773664951324
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01468118466436863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30203092098236084
3 0.0218127075 	 0.3020309264
epoch_time;  34.75711107254028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04787154495716095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9263206720352173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0609118714928627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.052417278289795
4 0.064945657 	 1.0524173057
epoch_time;  34.587130308151245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010432718321681023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37421533465385437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014346185140311718
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44915300607681274
5 0.0265333939 	 0.4491530127
epoch_time;  34.41130304336548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012240623123943806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27162283658981323
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01460726372897625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32371148467063904
6 0.0135827075 	 0.323711499
epoch_time;  34.59824275970459
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.014333936385810375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23710522055625916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016027655452489853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27792876958847046
7 0.0157862872 	 0.2779287701
epoch_time;  34.45172381401062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036163644399493933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1950375884771347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005441753659397364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2213292419910431
8 0.0101153167 	 0.2213292424
epoch_time;  34.540815353393555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004817800130695105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25393640995025635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0067743584513664246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29766708612442017
9 0.0185074615 	 0.2976670971
epoch_time;  34.468823194503784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007201709784567356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1927907019853592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008841871283948421
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2205776423215866
10 0.0077626469 	 0.2205776434
epoch_time;  34.344971895217896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008470568805932999
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30852046608924866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011859854683279991
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.346813827753067
11 0.0403012197 	 0.3468138358
epoch_time;  34.67325568199158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007763911038637161
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20711749792099
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009760958142578602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2391774207353592
12 0.0092132541 	 0.2391774157
epoch_time;  34.60925078392029
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005396145861595869
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18217423558235168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00693365978077054
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21007081866264343
13 0.0077686086 	 0.2100708146
epoch_time;  34.562700271606445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002837900537997484
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17769336700439453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004195322282612324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2035713493824005
14 0.0079774423 	 0.2035713542
epoch_time;  34.37598752975464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.023176223039627075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19539237022399902
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024682456627488136
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21670278906822205
15 0.0059049341 	 0.2167027868
epoch_time;  34.38994073867798
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004040977451950312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2213408648967743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006294308230280876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25958868861198425
16 0.0128031102 	 0.2595886738
epoch_time;  34.597466707229614
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002165640238672495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1665603667497635
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–ƒâ–„â–‚â–ˆâ–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–ƒâ–â–â–â–â–‚â–‚â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ƒâ–‚â–„â–‚â–ˆâ–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–ƒâ–â–â–â–â–‚â–‚â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‡â–„â–…â–‚â–ˆâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–„â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–…â–‚â–ˆâ–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–„â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.34132
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.29758
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00562
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00325
wandb:                         Train loss 0.02352
wandb: 
wandb: ğŸš€ View run brilliant-pig-1574 at: https://wandb.ai/nreints/thesis/runs/zeeydmn4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_070129-zeeydmn4/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_071953-m25d7ftu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-bao-1580
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/m25d7ftu
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0034726851154118776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19761990010738373
17 0.005579773 	 0.1976198963
epoch_time;  34.86576700210571
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002879002597182989
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16247664391994476
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003880284959450364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.191572368144989
18 0.0055365996 	 0.191572368
epoch_time;  35.0542573928833
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004082016181200743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15513984858989716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005089874844998121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18509472906589508
19 0.0052529219 	 0.1850947239
epoch_time;  34.481467962265015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0092253927141428
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4013468027114868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012882609851658344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4717402756214142
20 0.0175634885 	 0.4717402617
epoch_time;  35.64827513694763
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024808214511722326
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18625472486019135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004062737338244915
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22200554609298706
21 0.0068057881 	 0.2220055387
epoch_time;  35.23536491394043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032297063153237104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18098661303520203
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004484637174755335
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.208557590842247
22 0.0055917921 	 0.2085575899
epoch_time;  34.47272825241089
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00247936206869781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1639094054698944
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003457751590758562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18341295421123505
23 0.0048955294 	 0.1834129495
epoch_time;  34.21995544433594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028734677471220493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15168744325637817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0037903529591858387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16774609684944153
24 0.0044442057 	 0.1677461031
epoch_time;  34.54315805435181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004432014189660549
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25247353315353394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0070295678451657295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3039249777793884
25 0.029730347 	 0.3039249754
epoch_time;  34.86347508430481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005898668430745602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20897401869297028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007182675879448652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25317758321762085
26 0.0070365535 	 0.2531775852
epoch_time;  34.52183675765991
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003541758516803384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19111104309558868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004356582183390856
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22321438789367676
27 0.0052768078 	 0.2232143944
epoch_time;  34.58609700202942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013140032533556223
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16252632439136505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0023096788208931684
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19221380352973938
28 0.0046488908 	 0.1922138018
epoch_time;  34.33027386665344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003247987013310194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29834070801734924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005612556356936693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3407805263996124
29 0.0235153374 	 0.340780529
epoch_time;  34.214754819869995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032481844536960125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2975830137729645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005616967566311359
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3413202166557312
It took  1104.0098350048065  seconds.
----- ITERATION 5/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa1840>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdac6950>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd355bd0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd356380>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.042745303362607956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3923858404159546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05532899126410484
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4636276066303253
0 1.5771456653 	 0.4636276107
epoch_time;  34.75994062423706
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0392201766371727
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5423757433891296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05032763630151749
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6336351037025452
1 0.1121924124 	 0.6336350916
epoch_time;  34.648303747177124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022448590025305748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3042343556880951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02624305710196495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32545632123947144
2 0.0310128246 	 0.3254563254
epoch_time;  34.54312252998352
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01853511855006218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44152510166168213
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025933390483260155
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4878692626953125
3 0.0878632088 	 0.4878692627
epoch_time;  34.7286057472229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010790676809847355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.321556955575943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014324326068162918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33641910552978516
4 0.0195131598 	 0.3364190969
epoch_time;  34.5836238861084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013170733116567135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35458266735076904
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01842442713677883
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40340790152549744
5 0.029138604 	 0.4034079007
epoch_time;  34.467337131500244
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00505403894931078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23815666139125824
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007602833677083254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2507462501525879
6 0.0118130475 	 0.2507462516
epoch_time;  34.581149101257324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007796189747750759
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2107749730348587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010134869255125523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2174995094537735
7 0.0108736762 	 0.2174995169
epoch_time;  34.43093276023865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0267063956707716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5072830319404602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.035656239837408066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5805160403251648
8 0.0490112882 	 0.58051602
epoch_time;  34.69150972366333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01719343662261963
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2749769985675812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01984989270567894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2860063910484314
9 0.0168777397 	 0.2860063858
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–…â–ˆâ–ƒâ–†â–„â–…â–‚â–‚â–‡â–ƒâ–‚â–â–‚â–â–‚â–â–‚â–ˆâ–ƒâ–‚â–‚â–â–‚â–â–â–‚â–â–â–ƒâ–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–…â–‡â–ƒâ–…â–ƒâ–„â–‚â–‚â–†â–ƒâ–â–â–‚â–â–‚â–â–‚â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–‡â–„â–„â–ƒâ–ƒâ–‚â–‚â–…â–ƒâ–â–â–â–â–â–â–â–„â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–…â–„â–ƒâ–ƒâ–‚â–‚â–…â–„â–â–â–â–â–â–â–â–„â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.20273
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.19934
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00334
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00221
wandb:                         Train loss 0.0052
wandb: 
wandb: ğŸš€ View run brilliant-bao-1580 at: https://wandb.ai/nreints/thesis/runs/m25d7ftu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_071953-m25d7ftu/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_073802-tybyuuhs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-peony-1587
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/tybyuuhs
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  34.26466083526611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004127970430999994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2015645056962967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006127852946519852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20299988985061646
10 0.008728319 	 0.2029998871
epoch_time;  34.055049896240234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0030723598320037127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18719786405563354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004610894713550806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18474183976650238
11 0.0077390202 	 0.1847418471
epoch_time;  34.746479749679565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033936097752302885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21112628281116486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004983910359442234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21282415091991425
12 0.0086611756 	 0.2128241502
epoch_time;  34.185139894485474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002630776958540082
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18045267462730408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003933582454919815
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16823124885559082
13 0.0057386666 	 0.1682312496
epoch_time;  34.81362175941467
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024448430631309748
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.238899827003479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003950474318116903
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22824783623218536
14 0.0085964723 	 0.2282478356
epoch_time;  35.585561752319336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004156650044023991
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19193853437900543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005242830142378807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1936059594154358
15 0.0046893639 	 0.1936059589
epoch_time;  35.395347356796265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024930217768996954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26292330026626587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003814506810158491
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2581510841846466
16 0.0112415237 	 0.2581510976
epoch_time;  34.66794204711914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.022133270278573036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5969997048377991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.028663087636232376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6051610112190247
17 0.012243808 	 0.6051609927
epoch_time;  34.32336354255676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002575683407485485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27869024872779846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004045023117214441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2840130627155304
18 0.0074952145 	 0.2840130624
epoch_time;  34.633108139038086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002909352770075202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2601114511489868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004075904376804829
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2589303255081177
19 0.0073650455 	 0.2589303331
epoch_time;  34.47273778915405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0016834535636007786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21933743357658386
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0027339113876223564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2154874950647354
20 0.0056125029 	 0.2154875003
epoch_time;  34.56404137611389
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032171786297112703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19545331597328186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00403494480997324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19266009330749512
21 0.004087495 	 0.1926600983
epoch_time;  32.973692178726196
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0015524305636063218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20076215267181396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002727942541241646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20161430537700653
22 0.0060829319 	 0.2016143107
epoch_time;  33.07699108123779
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010174740105867386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1927615851163864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010695990175008774
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20031166076660156
23 0.0040607514 	 0.2003116665
epoch_time;  32.99439740180969
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002695757197216153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19026786088943481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035771087277680635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18950997292995453
24 0.0054251009 	 0.1895099767
epoch_time;  33.187265157699585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0037246611900627613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20398050546646118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00440835440531373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2045905739068985
25 0.0061401884 	 0.2045905813
epoch_time;  32.6886191368103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0038890079595148563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1930437982082367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004532773979008198
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19282971322536469
26 0.0034976691 	 0.1928297083
epoch_time;  33.01423978805542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005896755028516054
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18896310031414032
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006040475331246853
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18508155643939972
27 0.0035754622 	 0.1850815511
epoch_time;  32.979718923568726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003923268057405949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2694936990737915
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005835070740431547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2743043601512909
28 0.0232683611 	 0.2743043467
epoch_time;  33.16437578201294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002207851968705654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19969163835048676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003342171898111701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20258298516273499
29 0.005198108 	 0.2025829903
epoch_time;  32.780686140060425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022071106359362602
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19933639466762543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0033419064711779356
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2027333825826645
It took  1088.8777904510498  seconds.
----- ITERATION 6/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbd3f4d8d0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa3850>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa0d60>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcdaa3580>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04066399112343788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43814408779144287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05247493088245392
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4066126048564911
0 1.5702751674 	 0.4066126152
epoch_time;  32.72626256942749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04116426035761833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.73696368932724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.051419295370578766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6192651391029358
1 0.1193884619 	 0.6192651386
epoch_time;  33.247464418411255
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02051696926355362
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4004216492176056
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02589559741318226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30897000432014465
2 0.0346813505 	 0.3089700105
epoch_time;  33.031758069992065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.026188528165221214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6142611503601074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03310339152812958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5511070489883423
3 0.098682366 	 0.5511070782
epoch_time;  33.101656436920166
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012657755054533482
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3709276020526886
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.016494667157530785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31194499135017395
4 0.0211080079 	 0.3119449904
epoch_time;  32.746264696121216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03464183956384659
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31622424721717834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03668058663606644
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2526257038116455
5 0.0135786734 	 0.2526257103
epoch_time;  32.90106129646301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01838305965065956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34430721402168274
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02224871702492237
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2732026278972626
6 0.0341310812 	 0.2732026253
epoch_time;  33.053285121917725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006850677076727152
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24767522513866425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.009198863990604877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18894925713539124
7 0.0113141394 	 0.1889492623
epoch_time;  32.947797536849976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003260803408920765
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22243621945381165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005062608048319817
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16478046774864197
8 0.0095140405 	 0.1647804698
epoch_time;  33.70826268196106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005843402352184057
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3370673656463623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008831284008920193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24551700055599213
9 0.018600435 	 0.2455170047
epoch_time;  33.7663676738739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005838154815137386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22161135077476501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007018554024398327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16007710993289948
10 0.0062889161 	 0.1600771094
epoch_time;  33.48272204399109
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004585947375744581
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2917499244213104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006894398480653763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2050473988056183
11 0.0163503917 	 0.2050474
epoch_time;  32.882901191711426
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003007931401953101
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24083000421524048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004701269790530205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16051387786865234
12 0.0063525834 	 0.160513875
epoch_time;  32.74129796028137
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022990480065345764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22306221723556519
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0039861975237727165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14892952144145966
13 0.0093625146 	 0.1489295268
epoch_time;  33.06977891921997
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002322040032595396
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2079836130142212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0037376333493739367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1372770369052887
14 0.0054526275 	 0.1372770442
epoch_time;  33.032392740249634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00914850179105997
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38368433713912964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011635098606348038
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2859095335006714
15 0.0311873535 	 0.2859095317
epoch_time;  32.966795921325684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004166627302765846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29997488856315613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005759412422776222
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21791961789131165
16 0.0073366288 	 0.2179196176
epoch_time;  33.107046127319336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3769807517528534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.6732544898986816
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.415221244096756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 1.5758720636367798
17 0.0117494459 	 1.5758720467
epoch_time;  33.08057975769043
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005242532584816217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3031396269798279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006935541518032551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23070774972438812
18 0.0118944816 	 0.2307077506
epoch_time;  33.547921657562256
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002077647252008319
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2669473886489868
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003393246326595545
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18378229439258575
19 0.006273897 	 0.1837822952
epoch_time;  33.14477586746216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002373556373640895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23763468861579895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003444983623921871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17142528295516968
20 0.0048100651 	 0.1714252806
epoch_time;  32.90093779563904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0017230032244697213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22264938056468964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0028292571660131216
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16354453563690186
21 0.0054966398 	 0.1635445425
epoch_time;  33.126261711120605
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004542704205960035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3766487240791321
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007256665732711554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2967323958873749
22 0.0359992086 	 0.2967323926
epoch_time;  33.110358476638794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002877491293475032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31583744287490845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004573414102196693
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23563769459724426
23 0.0070755401 	 0.2356376994
epoch_time;  32.843313455581665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02296966314315796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3039998412132263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024153918027877808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24009577929973602
24 0.005855748 	 0.2400957782
epoch_time;  32.951606035232544
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004528673365712166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34400343894958496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007256702985614538
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.26651784777641296
25 0.0168706575 	 0.2665178466
epoch_time;  32.89621138572693
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0018327630823478103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26553207635879517
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0032993187196552753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1964508444070816
26 0.0057801735 	 0.1964508472
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–ˆâ–â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–ˆâ–â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.16626
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.21679
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.0029
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00191
wandb:                         Train loss 0.0041
wandb: 
wandb: ğŸš€ View run enchanting-peony-1587 at: https://wandb.ai/nreints/thesis/runs/tybyuuhs
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_073802-tybyuuhs/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_075540-wba39ldn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run alight-tiger-1593
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/wba39ldn
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  32.76354098320007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021297456696629524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23800772428512573
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003268096363171935
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18342378735542297
27 0.0047456855 	 0.1834237943
epoch_time;  32.74484705924988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002342619001865387
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22271151840686798
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003281987737864256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16759207844734192
28 0.0044523368 	 0.1675920746
epoch_time;  33.01132106781006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019133631139993668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21627706289291382
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0028988770209252834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16600379347801208
29 0.004095193 	 0.1660037891
epoch_time;  33.16606378555298
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0019139692885801196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2167939990758896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0028992919251322746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16626408696174622
It took  1057.881909608841  seconds.
----- ITERATION 7/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cb8a242ad0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd357850>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd3542b0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbd3f4d870>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04160657897591591
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4109354615211487
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.05233243852853775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5428556799888611
0 1.5789448394 	 0.542855669
epoch_time;  32.82913088798523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02924395352602005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.424550861120224
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03613204136490822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5261455774307251
1 0.090044667 	 0.5261455893
epoch_time;  32.80781364440918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.021860096603631973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26769840717315674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.025129055604338646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3225727379322052
2 0.0252223683 	 0.3225727369
epoch_time;  32.67725706100464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.028825484216213226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44207122921943665
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03480101376771927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5415817499160767
3 0.1027012608 	 0.5415817676
epoch_time;  33.09534525871277
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00945066288113594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27474862337112427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013061784207820892
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33407649397850037
4 0.0186758727 	 0.334076504
epoch_time;  34.086998462677
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018545931205153465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48180899024009705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.024253450334072113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5603351593017578
5 0.0825823857 	 0.560335142
epoch_time;  32.864009857177734
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007832329720258713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30947059392929077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011137482710182667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35961344838142395
6 0.0157255769 	 0.3596134474
epoch_time;  33.015607595443726
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00608839513733983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27192506194114685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008690252900123596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31475937366485596
7 0.0150855337 	 0.3147593841
epoch_time;  32.86786961555481
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003651009639725089
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24503971636295319
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005675206892192364
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.28606292605400085
8 0.0107289725 	 0.2860629263
epoch_time;  32.73939824104309
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004452337045222521
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22285158932209015
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006556946784257889
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2749398350715637
9 0.0207236216 	 0.2749398453
epoch_time;  32.873584032058716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028583307284861803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2033071368932724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0044489153660833836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.243617981672287
10 0.0083118817 	 0.2436179769
epoch_time;  32.72340488433838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005201977677643299
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19179822504520416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006381308659911156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22931908071041107
11 0.0061502998 	 0.2293190855
epoch_time;  32.73142170906067
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003988048993051052
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23036138713359833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005812770687043667
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2722785174846649
12 0.018667351 	 0.2722785235
epoch_time;  32.60663032531738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004448119085282087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18677721917629242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005937067326158285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2188948392868042
13 0.00724349 	 0.2188948432
epoch_time;  32.74636220932007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004509921185672283
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14055028557777405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005730052944272757
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17518556118011475
14 0.0090500776 	 0.1751855579
epoch_time;  32.74620723724365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004780756775289774
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14886528253555298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005989870056509972
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1845611035823822
15 0.0056895968 	 0.1845611042
epoch_time;  32.54917073249817
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002260948531329632
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13527454435825348
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0032012646552175283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17177961766719818
16 0.0070334484 	 0.171779621
epoch_time;  32.83994483947754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003217558143660426
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23355326056480408
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00522623211145401
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27894696593284607
17 0.0172183003 	 0.2789469601
epoch_time;  32.755101442337036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00219257571734488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.196337029337883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003474502358585596
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22996848821640015
18 0.005346443 	 0.229968483
epoch_time;  32.78534460067749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022151314187794924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16289405524730682
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–‡â–„â–ˆâ–„â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–ƒâ–‚â–â–†â–â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–‡â–„â–‡â–„â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–ƒâ–‚â–‚â–†â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–„â–†â–‚â–„â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–„â–‚â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–…â–†â–‚â–„â–‚â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–„â–ƒâ–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.22864
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.19148
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00277
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.0018
wandb:                         Train loss 0.00346
wandb: 
wandb: ğŸš€ View run alight-tiger-1593 at: https://wandb.ai/nreints/thesis/runs/wba39ldn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_075540-wba39ldn/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_081310-c5q7h2nt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-rat-1597
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/c5q7h2nt
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0033990663941949606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1983734369277954
19 0.0077697067 	 0.1983734315
epoch_time;  32.83441138267517
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.018642647191882133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4043636620044708
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.02195134572684765
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45554542541503906
20 0.0078473037 	 0.4555454312
epoch_time;  32.381855487823486
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012560089118778706
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16930177807807922
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012877922505140305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19252178072929382
21 0.003980146 	 0.1925217781
epoch_time;  32.88493847846985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0013973480090498924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16890068352222443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0024273539893329144
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20748218894004822
22 0.0065111583 	 0.207482191
epoch_time;  32.810362815856934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001562625402584672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15526732802391052
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0025660241954028606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1863023340702057
23 0.0039296309 	 0.1863023349
epoch_time;  32.71330976486206
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0041637904942035675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2491707056760788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00568168330937624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.29389867186546326
24 0.0268168915 	 0.2938986603
epoch_time;  32.80859851837158
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0034470180980861187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1894097626209259
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004537316970527172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2266441434621811
25 0.0050420321 	 0.2266441414
epoch_time;  32.65651607513428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0028818980790674686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2152421772480011
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004269262310117483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2573978006839752
26 0.0113514692 	 0.2573977929
epoch_time;  32.86392426490784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004038063809275627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18634198606014252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0048903655260801315
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.216775581240654
27 0.00436711 	 0.2167755772
epoch_time;  32.66506242752075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001594731118530035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20286020636558533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002651127288118005
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.24287541210651398
28 0.0079712958 	 0.2428754132
epoch_time;  32.61753439903259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001795286312699318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19121873378753662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0027707922272384167
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2319682389497757
29 0.0034614869 	 0.2319682372
epoch_time;  32.6531126499176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001796626835130155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19147928059101105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.002769664628431201
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22863905131816864
It took  1050.5602586269379  seconds.
----- ITERATION 8/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cb8a2407c0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd355f00>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd356530>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd3545e0>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04212787002325058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4241878390312195
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.051125530153512955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3926606774330139
0 1.5866083785 	 0.3926606884
epoch_time;  34.17184901237488
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03017757087945938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41682368516921997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03561278060078621
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37507516145706177
1 0.0912232496 	 0.3750751645
epoch_time;  32.770087480545044
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.033739134669303894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6078428030014038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.041385967284440994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5407357811927795
2 0.0777694101 	 0.5407358037
epoch_time;  32.62536931037903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.012079155072569847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3287830650806427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.015445434488356113
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2729659378528595
3 0.0248450049 	 0.2729659297
epoch_time;  32.77190136909485
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02593623660504818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6140663027763367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.032329022884368896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5290550589561462
4 0.0735055489 	 0.5290550393
epoch_time;  32.733954429626465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.010754182934761047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3603246510028839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.014121456071734428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3143710792064667
5 0.0203517998 	 0.3143710917
epoch_time;  32.66756987571716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06845556944608688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8148660659790039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.08704856783151627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.7749431729316711
6 0.0453714375 	 0.7749431875
epoch_time;  32.742090463638306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.00858267117291689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.334514319896698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.012091660872101784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.27947625517845154
7 0.0244090176 	 0.2794762695
epoch_time;  32.57138395309448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008097866550087929
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.261005163192749
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010201876051723957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.2189347892999649
8 0.011473089 	 0.2189347881
epoch_time;  32.57574439048767
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004473097622394562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25399744510650635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007016254588961601
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.21438510715961456
9 0.0125169689 	 0.2143851104
epoch_time;  32.75069761276245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.02249966561794281
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22163605690002441
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.023701315745711327
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18675553798675537
10 0.0069188314 	 0.1867555347
epoch_time;  32.885740756988525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.06418278813362122
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3997038006782532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.06091513857245445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3456878662109375
11 0.0092644081 	 0.3456878662
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–„â–„â–…â–ƒâ–…â–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–â–â–‚â–â–â–â–â–†â–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–„â–„â–†â–ƒâ–†â–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–„â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–…â–„â–„â–‚â–ƒâ–‚â–ˆâ–‚â–‚â–â–ƒâ–†â–â–â–‚â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–…â–„â–„â–‚â–„â–‚â–ˆâ–‚â–‚â–â–ƒâ–ˆâ–â–â–‚â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.13943
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.1587
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.00307
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.00206
wandb:                         Train loss 0.00502
wandb: 
wandb: ğŸš€ View run bright-rat-1597 at: https://wandb.ai/nreints/thesis/runs/c5q7h2nt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_081310-c5q7h2nt/logs
/gpfs/home2/nreints/MScThesis/code/lstm.py:270: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230127_083040-tpq7rfuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cheerful-laughter-1604
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/tpq7rfuw
/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
epoch_time;  33.07517743110657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005691261496394873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18613380193710327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007023288402706385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15968318283557892
12 0.0054224987 	 0.1596831814
epoch_time;  33.06516695022583
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004196786787360907
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2461339682340622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006406364031136036
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.20470276474952698
13 0.0105046829 	 0.2047027634
epoch_time;  33.002886056900024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013126642443239689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18167103826999664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013910485431551933
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15584951639175415
14 0.0047011476 	 0.1558495144
epoch_time;  33.00525379180908
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024606946390122175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20833474397659302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003979409113526344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18037959933280945
15 0.0126753756 	 0.1803795967
epoch_time;  32.59444212913513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002193834399804473
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14965437352657318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0032505937851965427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12177267670631409
16 0.0043544455 	 0.1217726739
epoch_time;  32.651570081710815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033784108236432076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1497691124677658
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00441256957128644
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.12175265699625015
17 0.0051698692 	 0.1217526554
epoch_time;  33.022048234939575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002869703806936741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20925354957580566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004286442883312702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1767403930425644
18 0.0131404597 	 0.1767403871
epoch_time;  32.86981749534607
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002696541603654623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19363343715667725
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004183205310255289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.16546063125133514
19 0.0082177069 	 0.1654606384
epoch_time;  32.66053295135498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0024488181807100773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16666042804718018
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0034321071580052376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14725318551063538
20 0.0046062693 	 0.1472531921
epoch_time;  32.973795652389526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.001661506132222712
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1854988932609558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0027342538814991713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15531927347183228
21 0.0062985734 	 0.1553192715
epoch_time;  32.86930203437805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002317021368071437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15194715559482574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003124770475551486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13010892271995544
22 0.0040172448 	 0.1301089284
epoch_time;  33.0172917842865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03572608157992363
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7300002574920654
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.04548143222928047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6303457021713257
23 0.0774954623 	 0.6303456932
epoch_time;  33.120952129364014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.01206088438630104
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.410027414560318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01505301333963871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34920722246170044
24 0.023516377 	 0.3492072298
epoch_time;  32.74579477310181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.007935264147818089
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3586842715740204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011174649931490421
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3025813698768616
25 0.0119088005 	 0.3025813722
epoch_time;  32.62747669219971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005241421516984701
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2553654611110687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.007103730458766222
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.22476314008235931
26 0.0082576486 	 0.2247631384
epoch_time;  33.05728197097778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0026065020356327295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21257148683071136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004224979784339666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1826004683971405
27 0.0066316346 	 0.1826004651
epoch_time;  34.20257019996643
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004027353134006262
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18740051984786987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0052292244508862495
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15921460092067719
28 0.0056206193 	 0.159214596
epoch_time;  33.19120407104492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020569416228681803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15736110508441925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0030668224208056927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13971145451068878
29 0.0050195857 	 0.1397114549
epoch_time;  32.968021869659424
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0020558370742946863
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1587035357952118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0030673365108668804
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.139430433511734
It took  1050.103298664093  seconds.
----- ITERATION 9/10 ------
[<torch.utils.data.dataloader.DataLoader object at 0x14cbcd3056f0>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd357520>, <torch.utils.data.dataloader.DataLoader object at 0x14cbcd357a30>, <torch.utils.data.dataloader.DataLoader object at 0x14cbd5581480>]
LSTM(
  (lstm): LSTM(24, 96, batch_first=True, dropout=0.2)
  (layers): Sequential(
    (0): Linear(in_features=96, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.04671180993318558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4072904884815216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.055531758815050125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3980569541454315
0 1.5926261399 	 0.3980569523
epoch_time;  32.76820135116577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.03016393817961216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39992037415504456
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.03722546249628067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3722468614578247
1 0.1102344937 	 0.3722468489
epoch_time;  32.883734703063965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0777483731508255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8813093304634094
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.09799228608608246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.8643752932548523
2 0.0601157247 	 0.8643752729
epoch_time;  32.840574741363525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.013247624970972538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3297938406467438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.017503509297966957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.30263814330101013
3 0.0345301834 	 0.3026381432
epoch_time;  32.76273322105408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0076288883574306965
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26070672273635864
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.010862699709832668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23154589533805847
4 0.02099722 	 0.2315459007
epoch_time;  32.73204040527344
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.311273455619812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.9986636638641357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35446977615356445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.041534423828125
5 0.0254128519 	 2.041534516
epoch_time;  32.700074911117554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008570529520511627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26495110988616943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011610446497797966
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.23195406794548035
6 0.0242047436 	 0.2319540617
epoch_time;  32.735663414001465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.011112423613667488
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.207071453332901
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.013255799189209938
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17905433475971222
7 0.0109351004 	 0.1790543294
epoch_time;  32.99036145210266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006167792249470949
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.208360493183136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008473404683172703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18848258256912231
8 0.0171971419 	 0.18848259
epoch_time;  32.84366464614868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032070300076156855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16393014788627625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.00477620679885149
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1337946355342865
9 0.0086441285 	 0.1337946405
epoch_time;  32.91102743148804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003632284002378583
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16400201618671417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004724013153463602
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13075566291809082
10 0.0083764486 	 0.1307556636
epoch_time;  32.66598892211914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003979105968028307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20263566076755524
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005984761752188206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18885798752307892
11 0.0132761586 	 0.1888579861
epoch_time;  32.438013315200806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0032363564241677523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20185106992721558
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004941209219396114
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.18559607863426208
12 0.0103229924 	 0.1855960742
epoch_time;  32.59380912780762
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0036691506393253803
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14486755430698395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004571573808789253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1298530101776123
13 0.0054250446 	 0.1298530095
epoch_time;  32.74286508560181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002449475461617112
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13042962551116943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0034466967917978764
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11384483426809311
14 0.0062665152 	 0.1138448341
epoch_time;  32.59315586090088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.003340034279972315
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1672755479812622
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0044247787445783615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.15598219633102417
15 0.010156155 	 0.155982199
epoch_time;  32.538098096847534
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0021751781459897757
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1604132503271103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0031183825340121984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13085077702999115
16 0.0055534538 	 0.1308507775
epoch_time;  32.5314736366272
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.008456985466182232
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27416759729385376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.011193430051207542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.25184088945388794
17 0.0472920755 	 0.2518408968
epoch_time;  32.39843392372131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.006082737818360329
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2099880427122116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.008020452223718166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.19382324814796448
18 0.008939133 	 0.1938232468
epoch_time;  32.75330209732056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005584884434938431
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20043861865997314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.006553946062922478
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1775871217250824
19 0.0067219081 	 0.1775871231
epoch_time;  32.53935885429382
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002997159492224455
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1630929708480835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004136977717280388
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13636399805545807
20 0.0057309214 	 0.1363639947
epoch_time;  32.63361120223999
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002639918588101864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14100059866905212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035281425807625055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.11815779656171799
21 0.0052723426 	 0.118157793
epoch_time;  33.04453158378601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.004630749579519033
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20158395171165466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0066204373724758625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.17613893747329712
22 0.0096191642 	 0.1761389441
epoch_time;  34.81243634223938
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.009550642222166061
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17150071263313293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.01067859586328268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14759674668312073
23 0.0048844856 	 0.1475967453
epoch_time;  34.35689115524292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.005456209648400545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1587928980588913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.005722525529563427
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.13751542568206787
24 0.0051049656 	 0.1375154224
epoch_time;  32.846648931503296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0022721763234585524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17633521556854248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.0035240051802247763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1551615446805954
25 0.0075966404 	 0.1551615436
epoch_time;  32.71351099014282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0033003222197294235
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1613251268863678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.004091780632734299
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14083854854106903
26 0.0036401214 	 0.1408385539
epoch_time;  32.60540008544922
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002038313075900078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19354593753814697
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003106829011812806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.1677262783050537
27 0.0085913971 	 0.1677262805
epoch_time;  32.814525842666626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.002910984680056572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16707392036914825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.003610030747950077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.14198555052280426
28 0.0040008374 	 0.1419855446
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–‚â–‚â–„â–‚â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‚â–‚â–„â–‚â–â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–‚â–‚â–ƒâ–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–‚â–‚â–ƒâ–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         Train loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 29
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36095
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3915
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.02255
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.01753
wandb:                         Train loss 0.01146
wandb: 
wandb: ğŸš€ View run cheerful-laughter-1604 at: https://wandb.ai/nreints/thesis/runs/tpq7rfuw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230127_083040-tpq7rfuw/logs
epoch_time;  32.656755685806274
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017533114179968834
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.392191082239151
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022541994228959084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3601734936237335
29 0.0114608342 	 0.3601735049
epoch_time;  32.93305730819702
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.017529770731925964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3915008008480072
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.022549092769622803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3609526753425598
It took  1050.3964879512787  seconds.

JOB STATISTICS
==============
Job ID: 2142216
Array Job ID: 2141141_15
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-06:41:24 core-walltime
Job Wall-clock time: 03:02:18
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
