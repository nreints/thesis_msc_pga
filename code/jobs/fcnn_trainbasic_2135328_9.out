wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_164933-3zm9ewem
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-horse-1138
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/3zm9ewem
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–†â–‡â–†â–„â–‚â–â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–…â–†â–‡â–„â–â–‚â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 3.3713
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.25015
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.3052
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.16115
wandb:                         Train loss 2.44204
wandb: 
wandb: ðŸš€ View run dancing-horse-1138 at: https://wandb.ai/nreints/thesis/runs/3zm9ewem
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_164933-3zm9ewem/logs
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.257525771856308
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4039151966571808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.860633850097656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.00355863571167
0 5.2609835181 	 5.0035588445 	 5.0035588445
epoch_time;  29.222341060638428
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20797236263751984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3489622473716736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.131160736083984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.237769603729248
1 2.8370029912 	 4.2377695444 	 4.2377695444
epoch_time;  27.708860874176025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22287292778491974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34655192494392395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.682720899581909
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.786212682723999
2 2.7100678833 	 3.7862126531 	 3.7862126531
epoch_time;  29.223832368850708
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23609524965286255
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3818090856075287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.508512020111084
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6516919136047363
3 2.6480193038 	 3.6516918285 	 3.6516918285
epoch_time;  28.733930826187134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24531689286231995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3643932044506073
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6694741249084473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.763331651687622
4 2.5988878626 	 3.7633317277 	 3.7633317277
epoch_time;  28.077393293380737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19705946743488312
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2950369417667389
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.506476402282715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6020286083221436
5 2.5642729535 	 3.6020286766 	 3.6020286766
epoch_time;  27.989179611206055
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1662936806678772
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25681060552597046
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.519185781478882
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6167664527893066
6 2.5447142646 	 3.6167665224 	 3.6167665224
epoch_time;  28.125688552856445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16801702976226807
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2331714928150177
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.464306354522705
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4833474159240723
7 2.5254645118 	 3.4833472999 	 3.4833472999
epoch_time;  28.08270287513733
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2215493619441986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3294261693954468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.567121744155884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.6712851524353027
8 2.5105699502 	 3.6712851035 	 3.6712851035
epoch_time;  28.139432907104492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19334203004837036
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2986583113670349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3687143325805664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.420349359512329
9 2.5008844932 	 3.420349451 	 3.420349451
epoch_time;  28.26651692390442
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1981426179409027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28971657156944275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.485682725906372
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5581228733062744
10 2.4907839807 	 3.5581229545 	 3.5581229545
epoch_time;  28.202736616134644
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1841251105070114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2698812484741211
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4183876514434814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4911723136901855
11 2.4791159762 	 3.4911723369 	 3.4911723369
epoch_time;  28.544775009155273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18199583888053894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29094529151916504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.35550594329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.4500386714935303
12 2.4781951181 	 3.4500386006 	 3.4500386006
epoch_time;  27.829687118530273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18455827236175537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26892325282096863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4537222385406494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.5048718452453613
13 2.472109289 	 3.5048719251 	 3.5048719251
epoch_time;  27.662506103515625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18202485144138336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25150901079177856
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.3473336696624756
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3743207454681396
14 2.4625573904 	 3.3743206952 	 3.3743206952
epoch_time;  27.807188510894775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17834509909152985
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27741578221321106
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.291783571243286
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.381334066390991
15 2.4584327858 	 3.3813341295 	 3.3813341295
epoch_time;  28.012965440750122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1930759996175766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27228298783302307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.4027013778686523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.454301118850708
16 2.4536484891 	 3.454301164 	 3.454301164
epoch_time;  28.06382393836975
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17546780407428741
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27199631929397583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.359300136566162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.441237688064575
17 2.4537086564 	 3.441237661 	 3.441237661
epoch_time;  27.94471025466919
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17891350388526917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28769150376319885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.281285524368286
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.37079119682312
18 2.4477390297 	 3.3707912136 	 3.3707912136
epoch_time;  28.081233024597168
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16112108528614044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2501985430717468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.304241418838501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3708133697509766
19 2.4420394949 	 3.3708133182 	 3.3708133182
epoch_time;  28.1477530002594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16114842891693115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2501533031463623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.30519962310791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.371297597885132
It took 625.9385898113251 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn57: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135360.0

JOB STATISTICS
==============
Job ID: 2135360
Array Job ID: 2135328_9
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:55:41
CPU Efficiency: 28.91% of 03:12:36 core-walltime
Job Wall-clock time: 00:10:42
Memory Utilized: 4.45 GB
Memory Efficiency: 14.25% of 31.25 GB
