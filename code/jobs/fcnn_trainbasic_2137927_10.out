wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_141403-41j10a9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-rooster-1244
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/41j10a9k
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▅▆█▅▄▄▂▃▃▂▄▃▃▃▅▄▅▃▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅▅▇█▃▄▃▂▃▃▁▃▁▂▄▂▂▃▃▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▅▇█▆▅▄▃▃▃▂▄▃▄▃▅▅▅▃▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▅▇█▃▅▃▂▃▃▁▂▁▂▄▂▂▃▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.36069
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20893
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.08536
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12717
wandb:                         Train loss 2.21205
wandb: 
wandb: 🚀 View run beaming-rooster-1244 at: https://wandb.ai/nreints/thesis/runs/41j10a9k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_141403-41j10a9k/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142722-kmlp3lu4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run incandescent-chrysanthemum-1252
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/kmlp3lu4
Number of train simulations: 8000
Number of test simulations: 2000
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1535419523715973
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24686774611473083
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.507221221923828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.841586112976074
0 4.9448443847 	 3.8415860563 	 3.8441584407
epoch_time;  39.46273851394653
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1510637253522873
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24347156286239624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.379245281219482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.706150054931641
1 2.574172798 	 4.7061500343 	 4.7084532042
epoch_time;  37.35637712478638
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18697597086429596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28508126735687256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7269392013549805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0874834060668945
2 2.4586769264 	 5.087483504 	 5.0900595175
epoch_time;  35.15392255783081
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19879764318466187
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3126969635486603
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.012879848480225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.523864269256592
3 2.3943024818 	 5.5238644162 	 5.5262523754
epoch_time;  35.01857781410217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12150038778781891
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1987176239490509
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.550655364990234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.870968341827393
4 2.3563013406 	 4.8709683805 	 4.8732385584
epoch_time;  35.51191020011902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15750974416732788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22156761586666107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.28491735458374
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.605283260345459
5 2.3307221141 	 4.6052833351 	 4.6068817963
epoch_time;  35.0325140953064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12959827482700348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19364026188850403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2312541007995605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.517006874084473
6 2.3019151733 	 4.517006704 	 4.5190759607
epoch_time;  35.17468285560608
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10898008942604065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1730566918849945
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8363287448883057
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1155242919921875
7 2.2924373268 	 4.1155243745 	 4.1170043945
epoch_time;  35.08369779586792
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11535342782735825
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.178524911403656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.037476062774658
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.353450298309326
8 2.2716268592 	 4.3534503009 	 4.3551239838
epoch_time;  35.438358545303345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12087543308734894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19213524460792542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.927515983581543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.250313758850098
9 2.2669230612 	 4.2503137537 	 4.2517795872
epoch_time;  34.997196674346924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09162735939025879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13973785936832428
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.722039222717285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.005494594573975
10 2.2546737144 	 4.0054948137 	 4.0070771088
epoch_time;  35.15686273574829
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11013555526733398
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17702539265155792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.239928722381592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.600452899932861
11 2.2562213709 	 4.6004529798 	 4.6015806456
epoch_time;  35.435842514038086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0979737639427185
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14330381155014038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.933497428894043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.231861114501953
12 2.2404100835 	 4.2318613413 	 4.2329537057
epoch_time;  34.829957246780396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11202511936426163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16725340485572815
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.099710464477539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.386694431304932
13 2.2338318408 	 4.3866943359 	 4.3876557221
epoch_time;  35.29656505584717
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13069036602973938
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20366829633712769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.978116035461426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.264522552490234
14 2.2274337696 	 4.2645227381 	 4.2654824747
epoch_time;  35.397979497909546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11301110684871674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16884273290634155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.458012580871582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.807051181793213
15 2.2289501854 	 4.8070510452 	 4.808145389
epoch_time;  35.19307494163513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11297988146543503
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17235486209392548
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.331294059753418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.615044116973877
16 2.227535426 	 4.6150440113 	 4.6159308356
epoch_time;  35.17739462852478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11879563331604004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1833893060684204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.452408313751221
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.791311740875244
17 2.2166297618 	 4.7913118929 	 4.7921601167
epoch_time;  35.52880144119263
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11555692553520203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18096771836280823
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.981339454650879
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.262719631195068
18 2.2184665192 	 4.2627197266 	 4.2635309993
epoch_time;  35.20400023460388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12718799710273743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2088802307844162
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.085311412811279
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.361353397369385
19 2.2120543979 	 4.3613535288 	 4.3621984533
epoch_time;  35.09735703468323
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12716849148273468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20893414318561554
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.085356712341309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.360685348510742
It took 799.0865004062653 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆██▇▇██▇▇▅▄▄▂▂▂▂▂▂▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▆▅▅▅▅▅▄█▂▄▃▅▄▃▁▄▄▃▅▅
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▇▆▆▇▇▆▅▅▄▄▃▂▃▃▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆█▄▄▄▅▄▄█▂▄▃▆▃▂▁▄▃▂▅▅
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.12007
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.22267
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.66977
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.14186
wandb:                         Train loss 2.2034
wandb: 
wandb: 🚀 View run incandescent-chrysanthemum-1252 at: https://wandb.ai/nreints/thesis/runs/kmlp3lu4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142722-kmlp3lu4/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144018-w3czo9mf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brilliant-mandu-1258
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/w3czo9mf
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16064660251140594
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27110856771469116
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.3694610595703125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.782345294952393
0 5.2616962699 	 4.7823453336 	 4.7872271564
epoch_time;  34.41493272781372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.182402104139328
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2638544738292694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.61989164352417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.948311805725098
1 2.5836005602 	 4.9483118006 	 4.9519937051
epoch_time;  34.54996871948242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1318361908197403
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21975475549697876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.496387481689453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.921895980834961
2 2.4409692102 	 4.9218957849 	 4.9260359481
epoch_time;  34.99544548988342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12964953482151031
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22402173280715942
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.373098850250244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.812220573425293
3 2.3854585414 	 4.812220558 	 4.8163828257
epoch_time;  37.31480288505554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13357603549957275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22449491918087006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.358221054077148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8112874031066895
4 2.3440323238 	 4.8112872149 	 4.8144970043
epoch_time;  36.4575355052948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15001988410949707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22910191118717194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.416611671447754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.93998908996582
5 2.316598488 	 4.9399889147 	 4.9425177497
epoch_time;  35.906209230422974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13948217034339905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22692695260047913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.449948310852051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.979626655578613
6 2.2995693075 	 4.9796264648 	 4.9821731155
epoch_time;  34.54198741912842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13566260039806366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20399312674999237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.331477165222168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.844643592834473
7 2.2726531755 	 4.8446434227 	 4.8469165699
epoch_time;  34.9119291305542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.17716297507286072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3014829754829407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.241600036621094
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.810678482055664
8 2.2654982188 	 4.810678513 	 4.8126101932
epoch_time;  34.9883189201355
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10581204295158386
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16187679767608643
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.231410980224609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.59690523147583
9 2.2529460517 	 4.5969053526 	 4.5990201383
epoch_time;  35.05250787734985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12824469804763794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20234915614128113
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.024364471435547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.502926349639893
10 2.238641593 	 4.5029263883 	 4.504744906
epoch_time;  34.36638641357422
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1175546795129776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19235852360725403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.021605491638184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.467640399932861
11 2.2394672412 	 4.4676404798 	 4.4692834803
epoch_time;  34.68396353721619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15863174200057983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23369064927101135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9313182830810547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.287266731262207
12 2.2296105387 	 4.2872667467 	 4.2886560389
epoch_time;  34.693684577941895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12351074069738388
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20354408025741577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.826631784439087
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.195952415466309
13 2.2258714019 	 4.1959525443 	 4.1975724504
epoch_time;  34.79335618019104
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10482997447252274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1775701940059662
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8992388248443604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.30021858215332
14 2.2124077985 	 4.3002184069 	 4.3017343882
epoch_time;  34.954564809799194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09758196771144867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.13754689693450928
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9398276805877686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2085137367248535
15 2.2153526931 	 4.2085139094 	 4.210168457
epoch_time;  35.03923177719116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12856662273406982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2017856240272522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.801150321960449
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.249136447906494
16 2.2094068689 	 4.2491362701 	 4.2503566433
epoch_time;  35.0516619682312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12081935256719589
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.201941579580307
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8343513011932373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.230123996734619
17 2.2087724669 	 4.2301239838 	 4.2313357791
epoch_time;  34.87521147727966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11018725484609604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17943912744522095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.821897506713867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.215061664581299
18 2.1954080405 	 4.215061827 	 4.2164890599
epoch_time;  34.829665422439575
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14184285700321198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22263740003108978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.670442819595337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.117370128631592
19 2.2033977366 	 4.1173699456 	 4.1186434359
epoch_time;  35.017613649368286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14185595512390137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22267305850982666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.669766426086426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.12006950378418
It took 775.8032348155975 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▁▂▃▃▃▄▄▅▅▆▅▆▆█▆▆▇▆██
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▁▁▃▂▃▄▄▅▅▅▅▅▆█▆▆▇▆██
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 9.02776
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.36802
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.87406
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26425
wandb:                         Train loss 2.57737
wandb: 
wandb: 🚀 View run brilliant-mandu-1258 at: https://wandb.ai/nreints/thesis/runs/w3czo9mf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144018-w3czo9mf/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_145314-q6m7z9ix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-chrysanthemum-1266
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/q6m7z9ix
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8560447096824646
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1154885292053223
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.753070831298828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.756781578063965
0 6.3314824708 	 8.7567818307 	 8.7980811867
epoch_time;  34.738075971603394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5661201477050781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7309261560440063
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.205663204193115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.877169609069824
1 3.5262795812 	 6.8771695524 	 6.9091301995
epoch_time;  35.24113368988037
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3851313889026642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5448041558265686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.314887523651123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.069497108459473
2 3.1698626994 	 7.0694969383 	 7.0895963102
epoch_time;  35.304261922836304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3389756381511688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4859383702278137
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.670654773712158
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.469456195831299
3 3.0084039253 	 7.4694560283 	 7.4842238967
epoch_time;  34.81787371635437
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3064274489879608
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43349167704582214
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.4929728507995605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.3600664138793945
4 2.9330495991 	 7.3600665118 	 7.3701501795
epoch_time;  35.01786828041077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.297804057598114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41739988327026367
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.746645450592041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.623998165130615
5 2.8438901701 	 7.6239983636 	 7.6321539802
epoch_time;  34.99584722518921
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2817707359790802
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39755386114120483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.977293014526367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.91273832321167
6 2.8054303355 	 7.9127382021 	 7.9200168919
epoch_time;  34.766786336898804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2584831118583679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36016029119491577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.966794967651367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.811644077301025
7 2.758836998 	 7.8116441881 	 7.8183686128
epoch_time;  37.37812662124634
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2640165090560913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.371608167886734
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.154264450073242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.023247718811035
8 2.7290111792 	 8.0232481261 	 8.0294440192
epoch_time;  38.20766019821167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2457546591758728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3437804579734802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.107129096984863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.045465469360352
9 2.7020578559 	 8.0454655828 	 8.0518079603
epoch_time;  35.04797863960266
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25561338663101196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3671335279941559
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.307486057281494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.332293510437012
10 2.6832056375 	 8.3322932063 	 8.3375554265
epoch_time;  34.90237069129944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24559997022151947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3709021508693695
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.140750885009766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.117244720458984
11 2.6613333153 	 8.1172442462 	 8.1233411634
epoch_time;  35.22574305534363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2641929090023041
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3934992253780365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.301384449005127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.337953567504883
12 2.6403041625 	 8.3379533098 	 8.3440805796
epoch_time;  34.858067989349365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2668074667453766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36711838841438293
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.562262535095215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.604214668273926
13 2.626031771 	 8.6042143951 	 8.6086438978
epoch_time;  34.88555145263672
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2365490049123764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34253421425819397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.994797229766846
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.121861457824707
14 2.6174718073 	 9.1218618032 	 9.1278584248
epoch_time;  35.00095224380493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2361254096031189
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3484969139099121
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.42052698135376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.513228416442871
15 2.6027483259 	 8.5132284628 	 8.5214276288
epoch_time;  34.515191078186035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2620197534561157
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3643544018268585
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.442848205566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.589929580688477
16 2.5987300059 	 8.5899295291 	 8.5985100823
epoch_time;  34.96906781196594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23249129951000214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31553658843040466
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.673984050750732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.769479751586914
17 2.594665734 	 8.7694797825 	 8.7797877956
epoch_time;  34.77887201309204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2480449378490448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32399609684944153
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.566007137298584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.638071060180664
18 2.5842490122 	 8.6380707612 	 8.6466203019
epoch_time;  35.0210816860199
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26429155468940735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3679830729961395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.872446537017822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.02806282043457
19 2.5773724121 	 9.0280629751 	 9.0346118824
epoch_time;  34.94468927383423
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26424962282180786
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3680160641670227
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.874063014984131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.027762413024902
It took 775.6946818828583 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▅▆▇██▅▅▃▃▆▄▅▃▃▄▁▃▂▄▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▇▆▅▅▆▅▄▅▃▅▁▄▂▁▁▄▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅▇▇██▅▄▃▂▆▄▅▃▃▄▁▄▂▄▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▂▅▆▄▄▄▃▄▄▃▃▁▄▂▁▁▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.2234
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20723
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.97
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11756
wandb:                         Train loss 2.20829
wandb: 
wandb: 🚀 View run sparkling-chrysanthemum-1266 at: https://wandb.ai/nreints/thesis/runs/q6m7z9ix
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_145314-q6m7z9ix/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150603-sn2wiruv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-pig-1273
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sn2wiruv
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18201418220996857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27730801701545715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.417337417602539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.664316177368164
0 5.4233150495 	 4.6643162083 	 4.6719073321
epoch_time;  34.68740892410278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15651993453502655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24072657525539398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.705636501312256
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.863664627075195
1 2.6199859433 	 4.8636646168 	 4.8667751003
epoch_time;  34.559643030166626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10874379426240921
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17438505589962006
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.818755149841309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.14007043838501
2 2.4665247995 	 5.1400704049 	 5.1442897487
epoch_time;  34.86210536956787
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14943623542785645
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2593752145767212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.904390335083008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.297812461853027
3 2.3949793646 	 5.297812302 	 5.3016070392
epoch_time;  34.71819233894348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.150760680437088
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24290814995765686
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.846835613250732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.257127285003662
4 2.3609871215 	 5.2571272567 	 5.2602327914
epoch_time;  34.81258153915405
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12411791831254959
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21406596899032593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.471242427825928
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.754155158996582
5 2.3334236946 	 4.7541550095 	 4.7573687579
epoch_time;  34.79644060134888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13523665070533752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2232406884431839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.317774772644043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.683820724487305
6 2.3082426635 	 4.6838207348 	 4.6866121859
epoch_time;  34.856350898742676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13151457905769348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24119405448436737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.018300533294678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.419668197631836
7 2.2899702998 	 4.4196681667 	 4.4223088445
epoch_time;  34.883201122283936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12115177512168884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21423716843128204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9970476627349854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.340153217315674
8 2.2797965616 	 4.3401532147 	 4.3430805928
epoch_time;  34.81734108924866
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13520164787769318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20419640839099884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.516519546508789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.884446620941162
9 2.2586893897 	 4.8844465926 	 4.8863531989
epoch_time;  35.220836877822876
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1366504281759262
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22828251123428345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.180358409881592
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.526925563812256
10 2.2592811455 	 4.5269254117 	 4.5288112331
epoch_time;  35.38046312332153
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11458128690719604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18729443848133087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.353227138519287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.689572334289551
11 2.2482675437 	 4.689572226 	 4.6914418813
epoch_time;  36.962159156799316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12210226058959961
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2144826352596283
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.082324028015137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.384696960449219
12 2.2331359491 	 4.3846970017 	 4.3861489786
epoch_time;  36.31528377532959
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0931415855884552
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15129482746124268
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.085291385650635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.384512424468994
13 2.2290698589 	 4.3845122466 	 4.3863215266
epoch_time;  35.037392139434814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13598597049713135
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20165537297725677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.274545669555664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.596907615661621
14 2.2220848254 	 4.5969076621 	 4.59810857
epoch_time;  34.634565114974976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10158029198646545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16151046752929688
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.754497528076172
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.013519287109375
15 2.2223344208 	 4.0135191221 	 4.0152660473
epoch_time;  34.60631227493286
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.0943981260061264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14751437306404114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.218552112579346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.470126152038574
16 2.219143402 	 4.4701260953 	 4.4714906435
epoch_time;  34.955873250961304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09144122898578644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14441357553005219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.970193386077881
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.244134902954102
17 2.2120742333 	 4.2441350164 	 4.2454352302
epoch_time;  34.81921076774597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11878255009651184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20118916034698486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.210725784301758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.532761573791504
18 2.2014864313 	 4.5327616924 	 4.5339395059
epoch_time;  34.805887937545776
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11756883561611176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20729514956474304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9701366424560547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.2219157218933105
19 2.2082931116 	 4.2219159101 	 4.2230128933
epoch_time;  34.77236533164978
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11755868047475815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20723438262939453
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.970004081726074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.223400115966797
It took 769.1417167186737 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▁▂▃▂▂▄▃▃▂▂▂▃▂▂▂▂▂▁▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▇▅▂▁▇▃▂▄▁▂▂▃▄▄▅▃▂▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▁▃▃▃▃▄▄▄▃▂▃▃▂▂▂▂▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▅▄▂▁█▃▂▃▁▂▂▃▅▄▆▄▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.20171
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.18508
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.97637
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.11599
wandb:                         Train loss 2.21582
wandb: 
wandb: 🚀 View run auspicious-pig-1273 at: https://wandb.ai/nreints/thesis/runs/sn2wiruv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150603-sn2wiruv/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151900-6h4qq8f1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-rat-1281
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/6h4qq8f1
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19867804646492004
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.342105895280838
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.412103652954102
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.032281875610352
0 6.6639335699 	 7.032281989 	 7.0715444204
epoch_time;  35.0805778503418
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14845357835292816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24567866325378418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8116378784179688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.029147624969482
1 2.7627824229 	 4.0291477513 	 4.0309778162
epoch_time;  35.32673239707947
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15836553275585175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3085945248603821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.314011096954346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.500671863555908
2 2.5458663703 	 4.5006720466 	 4.5029356261
epoch_time;  34.84186100959778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13983556628227234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2564533054828644
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.492286205291748
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.706945419311523
3 2.4466766791 	 4.7069454709 	 4.7091579128
epoch_time;  34.619978189468384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11393256485462189
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17967535555362701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.418416500091553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.523675441741943
4 2.3938533755 	 4.5236753721 	 4.5261989284
epoch_time;  35.33610510826111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10058870911598206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15920649468898773
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.346306324005127
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.473499298095703
5 2.3587749794 	 4.473499195 	 4.4757984058
epoch_time;  34.950987339019775
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19425049424171448
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31332993507385254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.86511754989624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.105003356933594
6 2.3300258918 	 5.1050035631 	 5.1070780986
epoch_time;  34.96660351753235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12861551344394684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21616852283477783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.759742736816406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.020628929138184
7 2.3143721305 	 5.0206288931 	 5.0224107897
epoch_time;  35.01719689369202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10837294161319733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1790681779384613
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.670138359069824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.869616508483887
8 2.3019837707 	 4.8696163693 	 4.8712280273
epoch_time;  35.14586639404297
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13436193764209747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22398622334003448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.353915691375732
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5549702644348145
9 2.28160026 	 4.5549702412 	 4.556475995
epoch_time;  35.224201679229736
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10276839882135391
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1536320596933365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.260381698608398
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.457153797149658
10 2.273504775 	 4.4571536502 	 4.4585614179
epoch_time;  34.8185396194458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11825468391180038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16909439861774445
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.326460361480713
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.535817623138428
11 2.2626341534 	 4.5358174092 	 4.5372090108
epoch_time;  35.035446882247925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1119520366191864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1832345873117447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.419600486755371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.6538519859313965
12 2.2539943522 	 4.6538518132 	 4.6551734718
epoch_time;  34.96927738189697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12910649180412292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19973942637443542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1271443367004395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.35914421081543
13 2.2446358486 	 4.3591440562 	 4.3602710621
epoch_time;  35.348655700683594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15476873517036438
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23397338390350342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.082043170928955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.35441255569458
14 2.2364142437 	 4.3544126768 	 4.3555396827
epoch_time;  37.61049461364746
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13675624132156372
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23613348603248596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9480297565460205
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.204818248748779
15 2.2379311553 	 4.2048181482 	 4.2058151658
epoch_time;  36.40142107009888
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1664658784866333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25187021493911743
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.094939708709717
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.383054733276367
16 2.2337619211 	 4.3830546611 	 4.3839873443
epoch_time;  35.395315170288086
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14185120165348053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2106119692325592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9530909061431885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.194545269012451
17 2.2279480022 	 4.1945454365 	 4.1954464474
epoch_time;  35.2348473072052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10475803166627884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17496150732040405
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.6859188079833984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9132206439971924
18 2.2207394899 	 3.9132205448 	 3.9142413165
epoch_time;  35.038976192474365
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11603967845439911
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18510748445987701
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.97650146484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.200806617736816
19 2.2158232878 	 4.2008066538 	 4.2015816353
epoch_time;  35.399423360824585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11598601192235947
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18508170545101166
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.976365566253662
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.20170783996582
It took 777.3547077178955 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂▅▆▆██▆▅▅▅▆▆▅▅▅▂▃▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▅█▄▅▅▄▄▃▂▃▁▄▁▄▄▁▂▃▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▅▇▆██▇▆▆▆▇▆▆▅▆▂▄▃▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄█▃▄▃▄▄▃▂▃▁▄▁▄▃▁▂▃▁▂▂
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.24026
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.15943
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.96959
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10376
wandb:                         Train loss 2.20769
wandb: 
wandb: 🚀 View run legendary-rat-1281 at: https://wandb.ai/nreints/thesis/runs/6h4qq8f1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151900-6h4qq8f1/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_153154-junzc6vx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-kumquat-1288
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/junzc6vx
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15764226019382477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25845280289649963
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.047904014587402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.364095211029053
0 5.4961459832 	 4.3640951621 	 4.3722068993
epoch_time;  34.77649927139282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24344222247600555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3389666974544525
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.46394157409668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.862131118774414
1 2.612140219 	 4.8621311497 	 4.8646253431
epoch_time;  34.94995999336243
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1386602371931076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22445784509181976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.604306697845459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.976329803466797
2 2.4662935519 	 4.9763295766 	 4.9802087732
epoch_time;  34.71754503250122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15122772753238678
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24478422105312347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.572622776031494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.999690532684326
3 2.3941915689 	 4.9996905353 	 5.0033282306
epoch_time;  34.98347806930542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14198645949363708
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2629026770591736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.74635648727417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.269277095794678
4 2.3581376021 	 5.2692768819 	 5.2725173538
epoch_time;  34.889649868011475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1496717780828476
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2215997278690338
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.765357494354248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.200948715209961
5 2.3251533276 	 5.2009485193 	 5.2033655115
epoch_time;  35.11733675003052
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15758536756038666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23511897027492523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.624114036560059
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.0236029624938965
6 2.310176449 	 5.0236031197 	 5.0259963577
epoch_time;  34.969244718551636
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13478803634643555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19476757943630219
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4912285804748535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.88789176940918
7 2.2912648898 	 4.8878919447 	 4.8900024414
epoch_time;  34.60110950469971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11690523475408554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17726197838783264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5047197341918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.886308193206787
8 2.2787049538 	 4.8863083298 	 4.8881146405
epoch_time;  34.90550422668457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1313352733850479
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1987522393465042
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.497644424438477
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.879312515258789
9 2.2650608484 	 4.8793127111 	 4.8809309676
epoch_time;  35.09800887107849
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09747795015573502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14595164358615875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.634603977203369
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.942074775695801
10 2.2571352866 	 4.9420746674 	 4.9439166095
epoch_time;  35.14416003227234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15386506915092468
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23873181641101837
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5779619216918945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.987700939178467
11 2.250342987 	 4.9877009211 	 4.9893930796
epoch_time;  34.97116136550903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09292852878570557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1400587260723114
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.514346122741699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.81836462020874
12 2.2346034376 	 4.8183646537 	 4.8198631493
epoch_time;  34.91381335258484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1506907194852829
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21284157037734985
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.444726467132568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.77949333190918
13 2.2310117439 	 4.7794931773 	 4.780739944
epoch_time;  34.43621206283569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13965396583080292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21529851853847504
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.491298198699951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.815781593322754
14 2.2305442898 	 4.815781382 	 4.8170512431
epoch_time;  34.87358546257019
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09878795593976974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1538604348897934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.073019027709961
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.353714942932129
15 2.2212158699 	 4.3537148965 	 4.3551569758
epoch_time;  34.98298144340515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10926377773284912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1694260835647583
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.2950968742370605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.525121212005615
16 2.2212221511 	 4.5251210806 	 4.5263490419
epoch_time;  35.42785573005676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1311740130186081
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18708322942256927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1864213943481445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.47182035446167
17 2.2128215317 	 4.4718205632 	 4.4730161925
epoch_time;  34.813255071640015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09864065051078796
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15335619449615479
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0493035316467285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.325623035430908
18 2.2115909109 	 4.3256232184 	 4.3267000765
epoch_time;  37.28083062171936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10376720130443573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15934550762176514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.967939615249634
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.240795135498047
19 2.2076935868 	 4.2407949087 	 4.2418117214
epoch_time;  37.232378244400024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10375697165727615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1594284027814865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9695873260498047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.240262985229492
It took 773.9133143424988 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▆█▆▆▅▄▄▅▆▃▄▇▇▂▆▄▅▄▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▅▃▅▃▂▃▄█▂▁▃▃▂▄▂▄▃▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▆█▆▅▅▅▄▅▆▄▅▇▇▃▇▄▆▅▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▄▂▃▃▂▂▃▄▂▁▂▂▁▄▁▄▂▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.68925
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20673
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.39124
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.13437
wandb:                         Train loss 2.20666
wandb: 
wandb: 🚀 View run golden-kumquat-1288 at: https://wandb.ai/nreints/thesis/runs/junzc6vx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_153154-junzc6vx/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154443-370wdl3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glittering-lantern-1295
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/370wdl3d
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16343961656093597
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28173375129699707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.776155948638916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.1434407234191895
0 5.0076194355 	 4.1434408652 	 4.1471165013
epoch_time;  34.91518759727478
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21477149426937103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30069050192832947
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.443660736083984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.809021949768066
1 2.5903277336 	 4.8090219859 	 4.8108866924
epoch_time;  35.49173378944397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15351854264736176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.240195170044899
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.745167255401611
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.115119934082031
2 2.4566041279 	 5.1151198928 	 5.1175104914
epoch_time;  34.953067779541016
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12457960098981857
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20672571659088135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4890666007995605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.849691390991211
3 2.3990626455 	 4.849691525 	 4.8518063107
epoch_time;  34.831114768981934
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13326026499271393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23225390911102295
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.388075828552246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.773580551147461
4 2.3499578167 	 4.7735806852 	 4.7761623073
epoch_time;  34.95965909957886
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14265961945056915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20591209828853607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.268073081970215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.639718532562256
5 2.3278978848 	 4.6397187104 	 4.6413072741
epoch_time;  34.96816921234131
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11355417966842651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18248967826366425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.280959129333496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.581892967224121
6 2.299626169 	 4.5818930136 	 4.5836132153
epoch_time;  34.550766706466675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11704496294260025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20472824573516846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.235469341278076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.564245700836182
7 2.2843732479 	 4.5642456055 	 4.5658780485
epoch_time;  34.87657308578491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1362495869398117
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23009620606899261
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.394676685333252
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.735621929168701
8 2.2697221657 	 4.7356220967 	 4.7369424356
epoch_time;  34.93055319786072
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14655587077140808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29250502586364746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.483992099761963
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8654656410217285
9 2.2641188325 	 4.8654656488 	 4.867220492
epoch_time;  34.755754470825195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1238737553358078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1912280172109604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.217174530029297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.403596878051758
10 2.2481736562 	 4.4035967853 	 4.405065918
epoch_time;  35.109867811203
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10370488464832306
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16150011122226715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.275057315826416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.512142181396484
11 2.2365454878 	 4.5121420371 	 4.5133647197
epoch_time;  34.69024467468262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12323795258998871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19318820536136627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.638648509979248
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.909170627593994
12 2.2373674996 	 4.9091704497 	 4.9104584565
epoch_time;  35.27693486213684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12000413984060287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19584858417510986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.617884159088135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.938506603240967
13 2.2280529645 	 4.9385065852 	 4.9398846601
epoch_time;  34.86966848373413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10970939695835114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1720951944589615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.086904525756836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.330756664276123
14 2.2193797737 	 4.33075644 	 4.3319814321
epoch_time;  34.72404599189758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.14952100813388824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.221664160490036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5705156326293945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.894738674163818
15 2.2159393249 	 4.8947384396 	 4.8955952412
epoch_time;  35.051910638809204
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10463277250528336
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18523797392845154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.220364570617676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.49855899810791
16 2.2174988826 	 4.4985589105 	 4.4997311154
epoch_time;  35.257018089294434
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15214727818965912
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21938931941986084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.434628009796143
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.756608486175537
17 2.2112856111 	 4.7566082929 	 4.7572839685
epoch_time;  34.38326334953308
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12517569959163666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19483555853366852
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.282055854797363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5551347732543945
18 2.2071066245 	 4.5551345413 	 4.555994972
epoch_time;  35.161438941955566
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13439324498176575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2066965401172638
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.389553546905518
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.690164566040039
19 2.2066618158 	 4.690164432 	 4.6911522118
epoch_time;  34.79826283454895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13437271118164062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20673273503780365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.3912434577941895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.689253330230713
It took 768.7880713939667 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▁▇█▇▅▃▃▅▃▃▄▂▂▅▃▃▄▄▄▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▃▂▃▁▄▃▂▃▃▁█▃▅▄▂▄▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▁▇█▇▆▄▄▅▄▄▄▂▂▅▃▃▅▄▃▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none █▆▅▃▂▃▁▂▃▂▂▂▁▇▂▄▃▂▃▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.5163
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.20107
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.2419
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12228
wandb:                         Train loss 2.2082
wandb: 
wandb: 🚀 View run glittering-lantern-1295 at: https://wandb.ai/nreints/thesis/runs/370wdl3d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154443-370wdl3d/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155732-8qplwlrw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-dumpling-1302
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/8qplwlrw
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19228164851665497
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28052979707717896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.622321605682373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.883896827697754
0 4.9396690002 	 3.8838969463 	 3.8871766786
epoch_time;  36.395042419433594
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1607106775045395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2509169280529022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.724905490875244
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.151780128479004
1 2.571229391 	 5.1517799171 	 5.1551071579
epoch_time;  36.01147103309631
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15501084923744202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2270910143852234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.938239097595215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.304657936096191
2 2.445092823 	 5.3046578072 	 5.3080635689
epoch_time;  34.530099630355835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11849027872085571
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18779923021793365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.676686763763428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.018341064453125
3 2.3860471567 	 5.0183412294 	 5.0220386402
epoch_time;  34.87873101234436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1096729040145874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16649308800697327
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.493405818939209
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.795875549316406
4 2.3380889789 	 4.7958753431 	 4.7988944389
epoch_time;  34.805503368377686
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1190800815820694
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18565835058689117
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.13564395904541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.334263324737549
5 2.3195599121 	 4.3342634871 	 4.3370892499
epoch_time;  34.886603593826294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09444698691368103
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14110314846038818
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.09806489944458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.367198944091797
6 2.3000369269 	 4.3671987173 	 4.3691544816
epoch_time;  34.82376027107239
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11226022988557816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19387711584568024
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.346654415130615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.61489725112915
7 2.285301379 	 4.614897197 	 4.6170505833
epoch_time;  34.66885495185852
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12402630597352982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18532733619213104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.134610652923584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.388574123382568
8 2.2721024128 	 4.3885738888 	 4.3903442383
epoch_time;  34.88653874397278
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10228223353624344
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16064909100532532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.097548007965088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.318171977996826
9 2.25770043 	 4.3181719806 	 4.3200772672
epoch_time;  34.77320432662964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10726769268512726
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1828203797340393
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.254889965057373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.543276309967041
10 2.2479053825 	 4.5432762352 	 4.5449057089
epoch_time;  34.97249150276184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1077522486448288
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17196646332740784
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8315041065216064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.040282249450684
11 2.2403803304 	 4.0402822134 	 4.0426523569
epoch_time;  34.742844343185425
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09540073573589325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.14562171697616577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.900918483734131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.108993053436279
12 2.2368978967 	 4.1089929529 	 4.1104502085
epoch_time;  34.78070592880249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1734417825937271
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2710312604904175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.45200252532959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.780240535736084
13 2.2314367172 	 4.7802404455 	 4.781352935
epoch_time;  34.812466859817505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1058284118771553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17612063884735107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.987015724182129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.243899822235107
14 2.2266876283 	 4.2438997836 	 4.2454467773
epoch_time;  34.68530488014221
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13478578627109528
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2200363427400589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9934608936309814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.28767204284668
15 2.2187562812 	 4.2876718882 	 4.2889585753
epoch_time;  35.12109923362732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12031406909227371
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20770201086997986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.286994457244873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.54808235168457
16 2.216944814 	 4.5480825063 	 4.5494526631
epoch_time;  34.600499391555786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10351084172725677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17063789069652557
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.259644508361816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.503763198852539
17 2.2108036111 	 4.5037630648 	 4.5050560204
epoch_time;  35.08427429199219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11880515515804291
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2022273689508438
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.079581260681152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.394828796386719
18 2.2061264255 	 4.3948288376 	 4.3961666623
epoch_time;  34.77479124069214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12228883057832718
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20106378197669983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.243509769439697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.516565322875977
19 2.2082041829 	 4.5165652713 	 4.5176833694
epoch_time;  34.58114838600159
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1222766563296318
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20107130706310272
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.241898059844971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.516304969787598
It took 768.910270690918 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▆██▅▆▄█▄▇▇▇▅▂▆▂▃▁▃▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▇█▄▅▁▆▂█▅▆▄▂▂▁▄▃▁▂▄▄
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▅██▄▅▄█▄▆▇▇▅▂▇▂▃▁▄▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none ▆▆█▄▆▁█▂▆▄▅▄▁▂▁▃▃▂▁▃▃
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.2575
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.2046
wandb:    Test loss t(0, 0)_r(-5, 5)_none 4.07308
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.12485
wandb:                         Train loss 2.19165
wandb: 
wandb: 🚀 View run legendary-dumpling-1302 at: https://wandb.ai/nreints/thesis/runs/8qplwlrw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155732-8qplwlrw/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_161016-hck3bhg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-rat-1310
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/hck3bhg1
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1607307493686676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26413804292678833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.265527248382568
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.676937580108643
0 4.9774732751 	 4.6769376188 	 4.6816782359
epoch_time;  34.61611366271973
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15757283568382263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26987728476524353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.581249237060547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.892189025878906
1 2.5819540573 	 4.8921888197 	 4.8946147197
epoch_time;  34.838526010513306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19001317024230957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2815248370170593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.586780548095703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.88031005859375
2 2.4399124796 	 4.8803100586 	 4.8835416227
epoch_time;  34.80132007598877
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13087065517902374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21725180745124817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.101995468139648
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.485058784484863
3 2.3771746202 	 4.4850589237 	 4.4883116026
epoch_time;  37.42223119735718
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16064077615737915
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22802504897117615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.208676338195801
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.5627264976501465
4 2.3366566983 	 4.562726325 	 4.5652624182
epoch_time;  35.98818111419678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09733670204877853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1629035472869873
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.12874698638916
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.38934326171875
5 2.3043716214 	 4.3893432617 	 4.3921271247
epoch_time;  35.48397469520569
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18428561091423035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25630322098731995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.581260681152344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.894314289093018
6 2.2860508262 	 4.8943141628 	 4.8962055928
epoch_time;  34.51870131492615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1068425178527832
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.17297814786434174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.141574859619141
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.414604663848877
7 2.2670964555 	 4.4146048881 	 4.4170037347
epoch_time;  34.510642528533936
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1578134447336197
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28627580404281616
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.4197540283203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.771893501281738
8 2.2592401524 	 4.7718934755 	 4.7739336993
epoch_time;  34.539342403411865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1429254561662674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22900749742984772
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.465154647827148
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.750283241271973
9 2.2449996628 	 4.7502834011 	 4.7520174594
epoch_time;  34.61566424369812
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15168951451778412
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24092842638492584
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.530013561248779
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.801197052001953
10 2.2357069466 	 4.8011969489 	 4.8025997677
epoch_time;  34.52505016326904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13577847182750702
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21392123401165009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.251346588134766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.51234245300293
11 2.2285467004 	 4.5123422984 	 4.514235378
epoch_time;  34.37398147583008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10081710666418076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1832670420408249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9274919033050537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.152840614318848
12 2.2239643941 	 4.1528406092 	 4.1545914933
epoch_time;  34.57965683937073
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1129939928650856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18149814009666443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.474235534667969
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.673777103424072
13 2.2143136491 	 4.6737773174 	 4.6754460515
epoch_time;  34.52757692337036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.09987998008728027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1556396335363388
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9720044136047363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.126682758331299
14 2.2120742333 	 4.1266825908 	 4.1282084697
epoch_time;  34.57788348197937
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11924834549427032
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20508408546447754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.042386054992676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.17567777633667
15 2.2114854527 	 4.1756779851 	 4.1771715319
epoch_time;  34.51039743423462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12663665413856506
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19141268730163574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.821742296218872
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.9576590061187744
16 2.2045465365 	 3.9576590873 	 3.9594023833
epoch_time;  34.68860936164856
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10928979516029358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16378165781497955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.154357433319092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.234687328338623
17 2.1988853501 	 4.234687434 	 4.2359183647
epoch_time;  34.52230095863342
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10331978648900986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1717025339603424
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9026780128479004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.131435871124268
18 2.1934360719 	 4.1314360747 	 4.132865947
epoch_time;  34.7521185874939
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1248246282339096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2046189159154892
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.07438325881958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.256383419036865
19 2.1916550189 	 4.2563836175 	 4.2575772672
epoch_time;  34.70401906967163
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12484657019376755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20459887385368347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.0730767250061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.257503509521484
It took 764.2701964378357 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▂████▅▅▄▃▄▃▂▄▃▂▃▂▃▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▄▄▃▃▅▃▂▄▂▅▅▄▄▁▅▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▂▇▇▇█▅▅▃▂▄▂▂▄▃▂▄▃▃▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▄▄▂▃▅▃▁▃▁▅▇▃▄▁▄▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 4.17875
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.16508
wandb:    Test loss t(0, 0)_r(-5, 5)_none 3.93353
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.10158
wandb:                         Train loss 2.20689
wandb: 
wandb: 🚀 View run bright-rat-1310 at: https://wandb.ai/nreints/thesis/runs/hck3bhg1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_161016-hck3bhg1/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.16212204098701477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.25576671957969666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9686460494995117
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.273568630218506
0 4.9696055666 	 4.2735684781 	 4.2766538878
epoch_time;  34.453744888305664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13218580186367035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22140750288963318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.723340034484863
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.282163143157959
1 2.5886207327 	 5.2821632179 	 5.2841094146
epoch_time;  34.54235553741455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.128191277384758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1911182552576065
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.694753170013428
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.212163925170898
2 2.4694366352 	 5.2121638118 	 5.2145125106
epoch_time;  34.51567482948303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1267547607421875
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2048032134771347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.645816802978516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.2162885665893555
3 2.4007086525 	 5.2162884686 	 5.2189298063
epoch_time;  34.774367332458496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1261453628540039
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20047079026699066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.7889814376831055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.228508472442627
4 2.365152554 	 5.2285086967 	 5.2311721389
epoch_time;  34.47848701477051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11180152744054794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18546418845653534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.40102481842041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.834202289581299
5 2.3349896988 	 4.834202122 	 4.8365491712
epoch_time;  34.88696360588074
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11376682668924332
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18468277156352997
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.365572452545166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.8156633377075195
6 2.3115401323 	 4.8156632707 	 4.8174953151
epoch_time;  34.65289235115051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13253790140151978
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21398553252220154
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.1830525398254395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.541542053222656
7 2.2955194162 	 4.5415421769 	 4.542907055
epoch_time;  37.19857931137085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11729143559932709
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.18874886631965637
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.026210308074951
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.374866485595703
8 2.2810977634 	 4.3748667124 	 4.376570748
epoch_time;  36.8375506401062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10302256047725677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16601896286010742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.273876190185547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.609463214874268
9 2.2704519428 	 4.6094630886 	 4.611237727
epoch_time;  34.617371797561646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.1216214969754219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20551154017448425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.04951286315918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.473202705383301
10 2.2557035019 	 4.4732025971 	 4.4748855178
epoch_time;  34.419952154159546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10032232850790024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.16370005905628204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9639852046966553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.267694473266602
11 2.2464567377 	 4.2676942568 	 4.2692122176
epoch_time;  34.522273540496826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.13607077300548553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.21249769628047943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.238556861877441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.6292901039123535
12 2.2403568585 	 4.6292899467 	 4.6305875211
epoch_time;  34.189645767211914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.15174058079719543
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.22013923525810242
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.180589199066162
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.442283630371094
13 2.241865836 	 4.4422838366 	 4.4432663376
epoch_time;  34.140289306640625
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.11860359460115433
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19587208330631256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.052954196929932
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.336214065551758
14 2.2357092607 	 4.3362143027 	 4.3375673036
epoch_time;  34.15363430976868
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12387043982744217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.19885927438735962
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.310409069061279
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.501062393188477
15 2.2262975322 	 4.5010623416 	 4.502162624
epoch_time;  34.40387511253357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10325171053409576
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15621688961982727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.116781234741211
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.3221845626831055
16 2.2236501685 	 4.3221844647 	 4.3234685125
epoch_time;  34.23734474182129
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.12327583134174347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.20712609589099884
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.148148059844971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.4297661781311035
17 2.2164191761 	 4.4297660209 	 4.4307960304
epoch_time;  34.64359641075134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.107689768075943
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.15982769429683685
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.8882205486297607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.106922149658203
18 2.2143842301 	 4.1069220466 	 4.1080827043
epoch_time;  34.52816152572632
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10158544778823853
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1651148647069931
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9349517822265625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.179651737213135
19 2.2068948817 	 4.1796515387 	 4.1807415936
epoch_time;  34.38136386871338
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.10158214718103409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.1650770902633667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.9335293769836426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 4.17875337600708
It took 765.2227683067322 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138558
Array Job ID: 2137927_10
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-14:48:18 core-walltime
Job Wall-clock time: 02:09:21
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
