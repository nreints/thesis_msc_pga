wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_121553-4rr5qfov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-firecracker-1181
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/4rr5qfov
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–…â–ƒâ–ƒâ–„â–ƒâ–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–„â–‚â–ƒâ–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.14226
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.37222
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.03726
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27452
wandb:                         Train loss 2.91388
wandb: 
wandb: ðŸš€ View run enchanting-firecracker-1181 at: https://wandb.ai/nreints/thesis/runs/4rr5qfov
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_121553-4rr5qfov/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_122902-9ryrt4db
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glistening-envelope-1186
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/9ryrt4db
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4289975166320801
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6411078572273254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.2999989986419678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.553636312484741
0 6.7009417844 	 3.5536363756 	 3.5536363756
epoch_time;  37.28418302536011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3461697995662689
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47053590416908264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8076112270355225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9989211559295654
1 3.3210860803 	 2.9989211624 	 2.9989211624
epoch_time;  35.8067307472229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.335641473531723
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48556163907051086
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6066925525665283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8136484622955322
2 3.2275582367 	 2.8136484507 	 2.8136484507
epoch_time;  35.1618390083313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29034262895584106
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39244532585144043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.464186429977417
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6081888675689697
3 3.1778097495 	 2.6081887735 	 2.6081887735
epoch_time;  35.71727395057678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3005546033382416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41052353382110596
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.562798500061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.710822343826294
4 3.1287290538 	 2.710822358 	 2.710822358
epoch_time;  35.20432806015015
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3137333393096924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4350390136241913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4125583171844482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5598134994506836
5 3.1005102324 	 2.5598134634 	 2.5598134634
epoch_time;  36.17512845993042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3029089570045471
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40430060029029846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.356194257736206
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4722204208374023
6 3.0620424636 	 2.472220426 	 2.472220426
epoch_time;  35.88236355781555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2813197672367096
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37281039357185364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3500828742980957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4508302211761475
7 3.043248429 	 2.4508302431 	 2.4508302431
epoch_time;  35.53582239151001
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3295738697052002
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43267348408699036
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4297289848327637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5436112880706787
8 3.0239852874 	 2.5436112687 	 2.5436112687
epoch_time;  35.99693512916565
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2618345618247986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.373262882232666
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.287902355194092
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.405241012573242
9 3.0038721992 	 2.4052409404 	 2.4052409404
epoch_time;  36.21829342842102
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2978561222553253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40891632437705994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3509652614593506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4604804515838623
10 2.9906889361 	 2.4604805611 	 2.4604805611
epoch_time;  35.83717393875122
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2650524377822876
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.353495717048645
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2086079120635986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.294816732406616
11 2.9800274125 	 2.2948166306 	 2.2948166306
epoch_time;  35.82711935043335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28251543641090393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3614228665828705
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2587907314300537
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3428778648376465
12 2.963692636 	 2.3428778571 	 2.3428778571
epoch_time;  36.89442491531372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2555300295352936
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35136187076568604
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1673731803894043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.270392894744873
13 2.9534671605 	 2.2703928355 	 2.2703928355
epoch_time;  35.8983416557312
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2439585030078888
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3082752525806427
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1342244148254395
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.204024076461792
14 2.9443405658 	 2.2040240314 	 2.2040240314
epoch_time;  36.38861560821533
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25942736864089966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34078553318977356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.143402099609375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2153513431549072
15 2.9349610697 	 2.2153513316 	 2.2153513316
epoch_time;  36.03636646270752
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2589080035686493
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3551400601863861
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.095320701599121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.199047565460205
16 2.9225741976 	 2.1990475216 	 2.1990475216
epoch_time;  36.16142559051514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26795414090156555
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35386231541633606
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1218020915985107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2142083644866943
17 2.9210756337 	 2.2142083245 	 2.2142083245
epoch_time;  36.05018663406372
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25022074580192566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3196328580379486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0275068283081055
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1018614768981934
18 2.9228462731 	 2.1018614073 	 2.1018614073
epoch_time;  36.076032638549805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27457302808761597
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.372225284576416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0364954471588135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.141265630722046
19 2.9138810062 	 2.1412655392 	 2.1412655392
epoch_time;  36.0391526222229
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27452290058135986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37222352623939514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.037263870239258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.142258405685425
It took 788.7662534713745 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.08866
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3392
wandb:    Test loss t(0, 0)_r(-5, 5)_none 1.96284
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25905
wandb:                         Train loss 2.9128
wandb: 
wandb: ðŸš€ View run glistening-envelope-1186 at: https://wandb.ai/nreints/thesis/runs/9ryrt4db
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_122902-9ryrt4db/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_124158-5se1rs96
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-horse-1193
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/5se1rs96
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41139745712280273
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5795938372612
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.0647523403167725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.345590353012085
0 6.6350433205 	 3.3455902924 	 3.3455902924
epoch_time;  35.87549114227295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34262815117836
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4880589544773102
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.612177848815918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8761520385742188
1 3.3317294214 	 2.8761520798 	 2.8761520798
epoch_time;  35.48291873931885
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3044796586036682
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40764403343200684
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.44624400138855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6479811668395996
2 3.2342325158 	 2.647981056 	 2.647981056
epoch_time;  35.46078157424927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2696376144886017
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3688462972640991
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4211795330047607
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6154558658599854
3 3.1710353017 	 2.6154557512 	 2.6154557512
epoch_time;  35.39572525024414
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29799047112464905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40733206272125244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.350470542907715
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5396323204040527
4 3.1303429936 	 2.5396324364 	 2.5396324364
epoch_time;  35.59906840324402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.278362512588501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4006617069244385
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.31112003326416
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4987571239471436
5 3.1037014165 	 2.4987571923 	 2.4987571923
epoch_time;  35.1600866317749
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27362918853759766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3624846935272217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.266988754272461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4218575954437256
6 3.0707637552 	 2.4218575143 	 2.4218575143
epoch_time;  35.66894888877869
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2907029390335083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.383416086435318
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2967567443847656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4490630626678467
7 3.0528791734 	 2.4490630279 	 2.4490630279
epoch_time;  35.36786723136902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2873575985431671
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3917892873287201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.235104560852051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3884689807891846
8 3.0379474203 	 2.3884689743 	 2.3884689743
epoch_time;  35.02939295768738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26602351665496826
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35856351256370544
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2434213161468506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3800554275512695
9 3.0112106334 	 2.3800553605 	 2.3800553605
epoch_time;  35.80344796180725
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2617703676223755
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3456418514251709
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.222562789916992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3502955436706543
10 3.0049823201 	 2.3502956081 	 2.3502956081
epoch_time;  35.076459884643555
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2838931083679199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38786980509757996
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2485604286193848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.397143602371216
11 2.9847323677 	 2.3971435547 	 2.3971435547
epoch_time;  35.15321373939514
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25812140107154846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3273189961910248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2073802947998047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.323239326477051
12 2.9735448756 	 2.3232393832 	 2.3232393832
epoch_time;  35.892173051834106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27520647644996643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3651844263076782
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1396169662475586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2752468585968018
13 2.9650278886 	 2.27524678 	 2.27524678
epoch_time;  35.4384081363678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27991983294487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3596242666244507
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1447041034698486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.260808229446411
14 2.9522793511 	 2.2608081715 	 2.2608081715
epoch_time;  34.75649428367615
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26942571997642517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3568190336227417
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1352593898773193
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2657594680786133
15 2.9444995795 	 2.2657594423 	 2.2657594423
epoch_time;  36.04010987281799
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2782785892486572
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3537699580192566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.06713604927063
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.183612823486328
16 2.9349852028 	 2.1836127204 	 2.1836127204
epoch_time;  36.59546089172363
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2681639492511749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36807695031166077
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0123744010925293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1562986373901367
17 2.9360245774 	 2.1562986632 	 2.1562986632
epoch_time;  36.469120502471924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2633974552154541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3452266752719879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0201992988586426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1416661739349365
18 2.9240149083 	 2.1416662268 	 2.1416662268
epoch_time;  35.55994534492493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2590145766735077
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3391540050506592
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9628599882125854
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0890607833862305
19 2.9128029526 	 2.0890606854 	 2.0890606854
epoch_time;  35.49629878997803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.259054034948349
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3392028212547302
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9628398418426514
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0886645317077637
It took 775.6327366828918 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–„â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–ƒâ–…â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–‚â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.07944
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.34267
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.01448
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25485
wandb:                         Train loss 2.92614
wandb: 
wandb: ðŸš€ View run crimson-horse-1193 at: https://wandb.ai/nreints/thesis/runs/5se1rs96
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_124158-5se1rs96/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_125451-x54txisn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-kumquat-1200
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/x54txisn
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4237081706523895
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6013154983520508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.144460916519165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.314950704574585
0 6.6567855556 	 3.314950644 	 3.314950644
epoch_time;  35.66904091835022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32380732893943787
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47585341334342957
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6594765186309814
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8231048583984375
1 3.3225152204 	 2.8231049409 	 2.8231049409
epoch_time;  35.75369381904602
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27323347330093384
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3941303491592407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5520739555358887
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6784777641296387
2 3.232655602 	 2.6784778492 	 2.6784778492
epoch_time;  35.6044340133667
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32951781153678894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4706537127494812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.619666576385498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.753281831741333
3 3.180811175 	 2.7532818768 	 2.7532818768
epoch_time;  35.26192808151245
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3098280131816864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4526694715023041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4678256511688232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.599214553833008
4 3.1436736999 	 2.599214461 	 2.599214461
epoch_time;  35.34753942489624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30559709668159485
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41656598448753357
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4813432693481445
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5742504596710205
5 3.1092381358 	 2.5742504223 	 2.5742504223
epoch_time;  35.528414726257324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29751187562942505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4234488308429718
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.398965835571289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.503727436065674
6 3.0799720056 	 2.5037274335 	 2.5037274335
epoch_time;  35.67972803115845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2777414619922638
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37459561228752136
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.445003032684326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5072832107543945
7 3.0631330796 	 2.5072831437 	 2.5072831437
epoch_time;  35.498501777648926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2765718698501587
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3713742196559906
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3436830043792725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3903121948242188
8 3.0401908033 	 2.3903122361 	 2.3903122361
epoch_time;  35.33273100852966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2623719871044159
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3682457506656647
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.345550775527954
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.413422107696533
9 3.0152170521 	 2.4134221257 	 2.4134221257
epoch_time;  35.417157888412476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27932634949684143
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3751247823238373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.329676628112793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3761613368988037
10 3.0004267915 	 2.3761613176 	 2.3761613176
epoch_time;  35.29928779602051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26224225759506226
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3638046681880951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.283720016479492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.343020439147949
11 2.9878194159 	 2.3430203824 	 2.3430203824
epoch_time;  35.61836862564087
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27720028162002563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36829516291618347
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2214064598083496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.269099473953247
12 2.9768517661 	 2.26909955 	 2.26909955
epoch_time;  35.55758023262024
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25967907905578613
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34892404079437256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1688010692596436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2111830711364746
13 2.969474653 	 2.2111829603 	 2.2111829603
epoch_time;  35.20550060272217
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24324947595596313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31645241379737854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1282639503479004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.155954122543335
14 2.9593939759 	 2.155954062 	 2.155954062
epoch_time;  35.29000997543335
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26180779933929443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36694106459617615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0982935428619385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.167578935623169
15 2.9458153272 	 2.1675789498 	 2.1675789498
epoch_time;  35.20616030693054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25630709528923035
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3628540337085724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.12935209274292
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.213588237762451
16 2.9415930333 	 2.2135882403 	 2.2135882403
epoch_time;  35.57673740386963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2567138373851776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36172086000442505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.05145263671875
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.124364137649536
17 2.9317232725 	 2.1243640797 	 2.1243640797
epoch_time;  35.34345602989197
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2442285120487213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32542669773101807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.057880163192749
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1045191287994385
18 2.9378031509 	 2.1045192409 	 2.1045192409
epoch_time;  35.56992506980896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2548080384731293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3426501154899597
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0144312381744385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0795793533325195
19 2.9261372953 	 2.0795792863 	 2.0795792863
epoch_time;  36.201478719711304
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2548544704914093
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34267014265060425
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0144805908203125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.079435348510742
It took 773.9213712215424 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–ƒâ–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–„â–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–â–‚â–„â–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.12708
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35076
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.06185
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26836
wandb:                         Train loss 2.93136
wandb: 
wandb: ðŸš€ View run bright-kumquat-1200 at: https://wandb.ai/nreints/thesis/runs/x54txisn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_125451-x54txisn/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_130745-lewyuh1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-wonton-1207
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/lewyuh1s
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4215678572654724
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5907124876976013
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.9590375423431396
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1588666439056396
0 6.7347895597 	 3.1588665936 	 3.1588665936
epoch_time;  35.25328612327576
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3242168426513672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44901230931282043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6122045516967773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7811989784240723
1 3.3307816203 	 2.7811988624 	 2.7811988624
epoch_time;  35.296886920928955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32989969849586487
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4526233971118927
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5861222743988037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7367007732391357
2 3.2384720004 	 2.7367007694 	 2.7367007694
epoch_time;  35.35587525367737
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2864324748516083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3836316466331482
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.533686876296997
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6439712047576904
3 3.1783856371 	 2.6439712112 	 2.6439712112
epoch_time;  35.69444417953491
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3194175362586975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4472092092037201
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.53818678855896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6753876209259033
4 3.1391396597 	 2.6753876557 	 2.6753876557
epoch_time;  36.07314848899841
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2926555275917053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39950618147850037
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.54447078704834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6449339389801025
5 3.1033936373 	 2.6449339171 	 2.6449339171
epoch_time;  35.43297076225281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28574028611183167
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3744608163833618
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.557978630065918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.641300678253174
6 3.0833552625 	 2.6413006757 	 2.6413006757
epoch_time;  35.37210536003113
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2783181965351105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3641842305660248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4504947662353516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.515829086303711
7 3.0554475261 	 2.5158290554 	 2.5158290554
epoch_time;  35.67843222618103
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28960224986076355
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3846181035041809
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.457209587097168
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.542217969894409
8 3.0386631474 	 2.5422178526 	 2.5422178526
epoch_time;  36.00796818733215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2860472500324249
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37812983989715576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3901584148406982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4640707969665527
9 3.0219382749 	 2.464070748 	 2.464070748
epoch_time;  35.39172172546387
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25443097949028015
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3195749819278717
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3459010124206543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.391345500946045
10 3.010012245 	 2.3913455448 	 2.3913455448
epoch_time;  35.15851092338562
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2868073582649231
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3854323923587799
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3283333778381348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.406751871109009
11 2.9959549023 	 2.406751808 	 2.406751808
epoch_time;  35.53228688240051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26620519161224365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3468368947505951
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.282461166381836
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3471548557281494
12 2.9844345062 	 2.347154772 	 2.347154772
epoch_time;  35.70486545562744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29300403594970703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40321579575538635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.283900737762451
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.385657548904419
13 2.9698260701 	 2.3856575631 	 2.3856575631
epoch_time;  35.13797998428345
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2606034278869629
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34429702162742615
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1988513469696045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.26235032081604
14 2.9618152161 	 2.2623503814 	 2.2623503814
epoch_time;  35.030656814575195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2854675352573395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36220166087150574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.224097967147827
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.271817684173584
15 2.9546318287 	 2.2718177589 	 2.2718177589
epoch_time;  35.49363136291504
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3220575749874115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40417301654815674
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2355082035064697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.292808771133423
16 2.9497857116 	 2.2928087389 	 2.2928087389
epoch_time;  34.83074951171875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2813228666782379
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3824898898601532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0756349563598633
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.15116024017334
17 2.9441851885 	 2.1511603278 	 2.1511603278
epoch_time;  35.68304657936096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2610936164855957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34798774123191833
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.063168525695801
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1300694942474365
18 2.9348675128 	 2.1300693821 	 2.1300693821
epoch_time;  35.44585061073303
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2683298587799072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3508068323135376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.062027931213379
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1276211738586426
19 2.9313576401 	 2.1276212125 	 2.1276212125
epoch_time;  35.33226656913757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2683602273464203
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35076314210891724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.061854124069214
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1270787715911865
It took 773.2622199058533 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–â–â–‚â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–‡â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–‚â–ƒâ–â–â–ƒâ–‚â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.04186
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.35089
wandb:    Test loss t(0, 0)_r(-5, 5)_none 1.97612
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.25765
wandb:                         Train loss 2.93635
wandb: 
wandb: ðŸš€ View run sweet-wonton-1207 at: https://wandb.ai/nreints/thesis/runs/lewyuh1s
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_130745-lewyuh1s/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_132037-4kc2uer4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-rat-1214
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/4kc2uer4
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38945361971855164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5396865010261536
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.190077781677246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.358860731124878
0 6.7342421029 	 3.3588606551 	 3.3588606551
epoch_time;  35.46589994430542
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3587404787540436
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5159198641777039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.71321702003479
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.885228157043457
1 3.3589617627 	 2.8852281725 	 2.8852281725
epoch_time;  35.83704090118408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31798937916755676
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42448315024375916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.626720428466797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7394208908081055
2 3.2585292189 	 2.7394209578 	 2.7394209578
epoch_time;  35.657835960388184
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27661752700805664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3787822425365448
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5590665340423584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6603002548217773
3 3.2035022031 	 2.66030026 	 2.66030026
epoch_time;  35.782201528549194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25643396377563477
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3693627119064331
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.492614507675171
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5914463996887207
4 3.1584656134 	 2.5914465002 	 2.5914465002
epoch_time;  35.38041067123413
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27832484245300293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35834214091300964
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.433866500854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4813268184661865
5 3.1265253417 	 2.4813267063 	 2.4813267063
epoch_time;  35.47484230995178
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26959848403930664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3529244065284729
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4645586013793945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.53314208984375
6 3.0934666845 	 2.5331420898 	 2.5331420898
epoch_time;  35.39838767051697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2644748091697693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3769040107727051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.359844207763672
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.449381113052368
7 3.0750822508 	 2.4493810705 	 2.4493810705
epoch_time;  35.31788206100464
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2639428675174713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36840468645095825
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.334501266479492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4082655906677246
8 3.0463619245 	 2.4082656448 	 2.4082656448
epoch_time;  34.901652097702026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26296913623809814
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3608279228210449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2143092155456543
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.277008533477783
9 3.0309425514 	 2.2770085515 	 2.2770085515
epoch_time;  35.56426787376404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29146242141723633
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37199312448501587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3370871543884277
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3715689182281494
10 3.0156398765 	 2.3715689994 	 2.3715689994
epoch_time;  35.29744577407837
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2533964216709137
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3324388563632965
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.219524383544922
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.256288528442383
11 3.0020817245 	 2.2562884357 	 2.2562884357
epoch_time;  35.297011375427246
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27331915497779846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3375648558139801
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2156083583831787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.22904896736145
12 2.9856527299 	 2.2290489403 	 2.2290489403
epoch_time;  35.63440990447998
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.255628377199173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32960245013237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1506435871124268
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1807708740234375
13 2.9808449613 	 2.1807707915 	 2.1807707915
epoch_time;  35.34588289260864
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2894856929779053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3721240758895874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2385289669036865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2727224826812744
14 2.9694495282 	 2.2727225639 	 2.2727225639
epoch_time;  35.21429657936096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23562127351760864
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31016620993614197
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.125513792037964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.162443161010742
15 2.9567981841 	 2.1624432538 	 2.1624432538
epoch_time;  35.95444202423096
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24612770974636078
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32471132278442383
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0347485542297363
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.077571153640747
16 2.9512006363 	 2.0775710647 	 2.0775710647
epoch_time;  35.259469985961914
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2705465257167816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35544395446777344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.129936695098877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.175417423248291
17 2.9473767032 	 2.1754173485 	 2.1754173485
epoch_time;  35.16909837722778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2635839283466339
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3367147743701935
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0409810543060303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0708556175231934
18 2.9404908466 	 2.0708557129 	 2.0708557129
epoch_time;  35.851370096206665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2576850354671478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3507908582687378
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9762319326400757
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.04203200340271
19 2.9363518614 	 2.0420321078 	 2.0420321078
epoch_time;  35.796024799346924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2576543390750885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.350894033908844
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.976117730140686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0418572425842285
It took 772.6524393558502 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–…â–†â–…â–‚â–„â–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–ƒâ–ƒâ–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–‡â–†â–ƒâ–„â–ƒâ–ƒâ–â–ƒâ–â–‚â–‚â–â–â–ƒâ–ƒâ–â–‚â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.17078
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.32617
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.10492
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2556
wandb:                         Train loss 2.92202
wandb: 
wandb: ðŸš€ View run crimson-rat-1214 at: https://wandb.ai/nreints/thesis/runs/4kc2uer4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_132037-4kc2uer4/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_133335-6w4uzaw5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run enchanting-chrysanthemum-1221
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6w4uzaw5
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3943999111652374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5959880352020264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1314046382904053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.37400221824646
0 6.6455302926 	 3.3740023226 	 3.3740023226
epoch_time;  35.725351095199585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3517434895038605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4817443788051605
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.8116164207458496
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.0055904388427734
1 3.330968073 	 3.0055904904 	 3.0055904904
epoch_time;  35.733882188797
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3683929145336151
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5069953203201294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.707831382751465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8996951580047607
2 3.2389824311 	 2.8996951541 	 2.8996951541
epoch_time;  35.69281315803528
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3535930812358856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48775941133499146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.607877731323242
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.770861864089966
3 3.1811414343 	 2.7708618164 	 2.7708618164
epoch_time;  35.99947381019592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2919362485408783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38130587339401245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4978036880493164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6154701709747314
4 3.1380106952 	 2.6154702676 	 2.6154702676
epoch_time;  35.797778367996216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3028477728366852
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43377935886383057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4638893604278564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6119918823242188
5 3.1106728959 	 2.6119919236 	 2.6119919236
epoch_time;  36.41392803192139
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28838038444519043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39918872714042664
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3874504566192627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4965813159942627
6 3.0791610687 	 2.4965813714 	 2.4965813714
epoch_time;  36.152546405792236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29968032240867615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3809927999973297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.472644090652466
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5655620098114014
7 3.0534567139 	 2.5655619853 	 2.5655619853
epoch_time;  36.21008896827698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.257252961397171
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3480364680290222
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3294358253479004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4133641719818115
8 3.0341320826 	 2.4133640599 	 2.4133640599
epoch_time;  35.46402144432068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29376137256622314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39883682131767273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4236509799957275
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.52103328704834
9 3.0183897232 	 2.5210333747 	 2.5210333747
epoch_time;  35.42102909088135
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2516898214817047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3436579406261444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.324455738067627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4057111740112305
10 3.0084997964 	 2.4057110761 	 2.4057110761
epoch_time;  35.32574725151062
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2718181610107422
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3783988058567047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.260021448135376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3524317741394043
11 2.9925491653 	 2.3524318386 	 2.3524318386
epoch_time;  35.44713282585144
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2652484178543091
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3532349169254303
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.24424147605896
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3228142261505127
12 2.9798379845 	 2.3228142816 	 2.3228142816
epoch_time;  35.31650185585022
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25711649656295776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33964622020721436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2729523181915283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3514108657836914
13 2.9719144063 	 2.3514109019 	 2.3514109019
epoch_time;  35.46978545188904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25605884194374084
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.348126620054245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.228058338165283
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3112924098968506
14 2.9589691679 	 2.3112923287 	 2.3112923287
epoch_time;  36.037267446517944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29799404740333557
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39209476113319397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2201826572418213
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.313477039337158
15 2.9492858597 	 2.3134770574 	 2.3134770574
epoch_time;  35.2831916809082
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2836518883705139
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3940833806991577
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1776063442230225
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2875242233276367
16 2.9442172557 	 2.2875242491 	 2.2875242491
epoch_time;  35.5235378742218
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2506358027458191
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3309359848499298
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1066510677337646
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.181901216506958
17 2.937799845 	 2.1819012616 	 2.1819012616
epoch_time;  36.0791175365448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26735228300094604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36618608236312866
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0894997119903564
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1748054027557373
18 2.9317166607 	 2.1748055123 	 2.1748055123
epoch_time;  35.67921733856201
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2555792033672333
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32618436217308044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.104965925216675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1711862087249756
19 2.922018476 	 2.1711862925 	 2.1711862925
epoch_time;  36.35616946220398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2555983364582062
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32616692781448364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.104921817779541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.170779228210449
It took 777.8489942550659 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–„â–„â–„â–„â–„â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–‡â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.243
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.38665
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.15186
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27655
wandb:                         Train loss 2.92052
wandb: 
wandb: ðŸš€ View run enchanting-chrysanthemum-1221 at: https://wandb.ai/nreints/thesis/runs/6w4uzaw5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_133335-6w4uzaw5/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_134631-6ix9qmpq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-rat-1229
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/6ix9qmpq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4559262990951538
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6500970721244812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1322484016418457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.311561346054077
0 6.693778563 	 3.3115613783 	 3.3115613783
epoch_time;  35.82544755935669
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38723984360694885
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5876747369766235
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7241971492767334
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9402265548706055
1 3.3432015514 	 2.9402264569 	 2.9402264569
epoch_time;  35.63870596885681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3260499835014343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44950610399246216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5850491523742676
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7217109203338623
2 3.2353700755 	 2.7217108649 	 2.7217108649
epoch_time;  35.37705159187317
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2837536633014679
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4200122356414795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.526385545730591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6688313484191895
3 3.1734406737 	 2.6688313252 	 2.6688313252
epoch_time;  35.14600229263306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2994186282157898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41511738300323486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.629246950149536
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.738816738128662
4 3.1349957156 	 2.7388167098 	 2.7388167098
epoch_time;  35.28631830215454
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30642709136009216
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.420222669839859
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.629699468612671
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7291312217712402
5 3.1069964058 	 2.7291312553 	 2.7291312553
epoch_time;  35.62282752990723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3218928873538971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4270471930503845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.604963541030884
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6906983852386475
6 3.0736980709 	 2.6906984071 	 2.6906984071
epoch_time;  35.449283599853516
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2799966037273407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38180145621299744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4870874881744385
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5651745796203613
7 3.0468082208 	 2.5651744946 	 2.5651744946
epoch_time;  35.4880485534668
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2696765065193176
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.365604430437088
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3916003704071045
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4590349197387695
8 3.0294380369 	 2.4590348527 	 2.4590348527
epoch_time;  37.112359046936035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3247010111808777
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42143601179122925
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6016805171966553
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.667922019958496
9 3.0232950158 	 2.6679220664 	 2.6679220664
epoch_time;  35.50923943519592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2970922589302063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3925875425338745
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.464108467102051
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.52987003326416
10 3.0042993214 	 2.5298699456 	 2.5298699456
epoch_time;  35.71041965484619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25901177525520325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3265955448150635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.353698968887329
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3894853591918945
11 2.9857740564 	 2.3894852922 	 2.3894852922
epoch_time;  35.69056963920593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27332356572151184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38604551553726196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2799370288848877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3719701766967773
12 2.9774514562 	 2.3719701819 	 2.3719701819
epoch_time;  35.36089491844177
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2672518789768219
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34803861379623413
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.264976739883423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.313953161239624
13 2.9632010489 	 2.3139531316 	 2.3139531316
epoch_time;  35.331796407699585
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28445643186569214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3663614094257355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.284883975982666
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.34114408493042
14 2.9497357926 	 2.3411441287 	 2.3411441287
epoch_time;  35.17860221862793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2898167073726654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3780316710472107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2688775062561035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.329662561416626
15 2.9459310337 	 2.3296625911 	 2.3296625911
epoch_time;  35.36820983886719
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2737806439399719
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3726617693901062
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1778016090393066
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.254504919052124
16 2.9389053376 	 2.2545048894 	 2.2545048894
epoch_time;  35.14759874343872
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2599468231201172
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3404613435268402
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1719582080841064
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2291994094848633
17 2.9335230038 	 2.2291993837 	 2.2291993837
epoch_time;  35.77392053604126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2629747688770294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37052327394485474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.143332004547119
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2359671592712402
18 2.9231983513 	 2.2359671928 	 2.2359671928
epoch_time;  35.65668249130249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2765324115753174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3867208659648895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1524176597595215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.243525266647339
19 2.9205245403 	 2.2435253246 	 2.2435253246
epoch_time;  35.12462091445923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2765533924102783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3866543471813202
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.151864767074585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2430028915405273
It took 775.5920357704163 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–„â–…â–„â–„â–‚â–â–â–â–‚â–…â–‚â–„â–â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–ƒâ–ƒâ–…â–ƒâ–„â–‚â–â–‚â–â–â–…â–‚â–„â–â–‚â–â–‚â–ƒâ–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.16792
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.36015
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.05826
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26995
wandb:                         Train loss 2.92178
wandb: 
wandb: ðŸš€ View run sparkling-rat-1229 at: https://wandb.ai/nreints/thesis/runs/6ix9qmpq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_134631-6ix9qmpq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_135928-y1bya3hw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-festival-1236
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/y1bya3hw
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3916049301624298
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5509880185127258
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.224804639816284
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.474724292755127
0 6.6736238205 	 3.4747241871 	 3.4747241871
epoch_time;  35.45262050628662
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30169904232025146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43292784690856934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7199578285217285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.9156458377838135
1 3.3278201961 	 2.9156457849 	 2.9156457849
epoch_time;  35.761691093444824
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3022310733795166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4444943070411682
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7928802967071533
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.99053955078125
2 3.2394317028 	 2.9905395508 	 2.9905395508
epoch_time;  35.1240074634552
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3268389403820038
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45866191387176514
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6297969818115234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.804816722869873
3 3.1776437934 	 2.8048168285 	 2.8048168285
epoch_time;  35.86255860328674
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2990870475769043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41605138778686523
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.551422357559204
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7148520946502686
4 3.1348013287 	 2.714852163 	 2.714852163
epoch_time;  35.19501614570618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3081408739089966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.42459022998809814
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.44055438041687
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5822854042053223
5 3.1028746112 	 2.5822852882 	 2.5822852882
epoch_time;  35.404995918273926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2725379467010498
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37004968523979187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.475719451904297
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6089069843292236
6 3.0734220284 	 2.6089070088 	 2.6089070088
epoch_time;  35.35528492927551
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2650233209133148
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3505436182022095
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4449448585510254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.569645643234253
7 3.0523660979 	 2.5696457322 	 2.5696457322
epoch_time;  35.3992063999176
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2717895209789276
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34794124960899353
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3649001121520996
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.473466157913208
8 3.0304886515 	 2.473466203 	 2.473466203
epoch_time;  35.48375201225281
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26474860310554504
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3526233434677124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.371037721633911
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4914958477020264
9 3.0148296008 	 2.4914958232 	 2.4914958232
epoch_time;  35.7736701965332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2645682990550995
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3554845154285431
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3763949871063232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5156941413879395
10 3.0038824475 	 2.5156941182 	 2.5156941182
epoch_time;  35.000945806503296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3250873386859894
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45432519912719727
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3815555572509766
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5642879009246826
11 2.9879407424 	 2.5642880002 	 2.5642880002
epoch_time;  35.836748123168945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27879950404167175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3646238446235657
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2717669010162354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3893470764160156
12 2.9752715465 	 2.3893470558 	 2.3893470558
epoch_time;  36.46256637573242
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3068377375602722
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44263243675231934
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.229266405105591
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4036107063293457
13 2.9640519872 	 2.4036106419 	 2.4036106419
epoch_time;  36.09113907814026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25834497809410095
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33859312534332275
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2237658500671387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3292171955108643
14 2.9500624154 	 2.3292171994 	 2.3292171994
epoch_time;  35.810741901397705
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28613168001174927
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3616284430027008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1960153579711914
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.285601854324341
15 2.9451333203 	 2.2856019716 	 2.2856019716
epoch_time;  35.875508069992065
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2613804042339325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3540261387825012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1252825260162354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.238964080810547
16 2.9379489411 	 2.2389641839 	 2.2389641839
epoch_time;  34.96686315536499
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.282130628824234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38236674666404724
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.137458562850952
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.279555320739746
17 2.9349683427 	 2.2795552022 	 2.2795552022
epoch_time;  35.62070155143738
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29253873229026794
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39830219745635986
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0600991249084473
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1884377002716064
18 2.9287684601 	 2.188437632 	 2.188437632
epoch_time;  35.69173884391785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2699081003665924
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36012300848960876
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.058286428451538
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1679301261901855
19 2.9217817737 	 2.1679301494 	 2.1679301494
epoch_time;  35.43343544006348
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2699519097805023
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3601548969745636
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0582633018493652
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1679160594940186
It took 776.8422088623047 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–†â–‡â–ƒâ–ƒâ–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–„â–ƒâ–‚â–â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–†â–…â–„â–„â–…â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–…â–‡â–ƒâ–ƒâ–…â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–‚â–ƒâ–ƒâ–ƒâ–‚â–â–â–
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.04124
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.30267
wandb:    Test loss t(0, 0)_r(-5, 5)_none 1.99554
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.24583
wandb:                         Train loss 2.92542
wandb: 
wandb: ðŸš€ View run legendary-festival-1236 at: https://wandb.ai/nreints/thesis/runs/y1bya3hw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_135928-y1bya3hw/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_141229-24v4y9rg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dancing-rocket-1243
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/24v4y9rg
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3870706856250763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5389038324356079
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.947047472000122
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.1287240982055664
0 6.7114399966 	 3.1287241343 	 3.1287241343
epoch_time;  35.756165742874146
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3325454592704773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4623376727104187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6183924674987793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.7957522869110107
1 3.3411082563 	 2.795752283 	 2.795752283
epoch_time;  35.45294380187988
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3750230073928833
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5213928818702698
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5723986625671387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.739178419113159
2 3.2468425361 	 2.7391784668 	 2.7391784668
epoch_time;  35.46408176422119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2889809012413025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38433730602264404
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.445549488067627
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.571810483932495
3 3.1917348563 	 2.5718105007 	 2.5718105007
epoch_time;  35.88919448852539
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27851516008377075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36912834644317627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4650890827178955
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.574040651321411
4 3.145281689 	 2.5740407583 	 2.5740407583
epoch_time;  35.420018672943115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33046671748161316
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41095924377441406
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.487407922744751
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5770983695983887
5 3.1120676546 	 2.5770984547 	 2.5770984547
epoch_time;  35.15162754058838
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29856356978416443
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41092410683631897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4342989921569824
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5613598823547363
6 3.089183562 	 2.5613599623 	 2.5613599623
epoch_time;  35.900256872177124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27492982149124146
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34677910804748535
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4318573474884033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5123581886291504
7 3.0611118531 	 2.5123581345 	 2.5123581345
epoch_time;  36.19663715362549
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27093037962913513
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.351056843996048
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.307966470718384
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3914761543273926
8 3.0418817705 	 2.391476193 	 2.391476193
epoch_time;  35.77356314659119
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28361499309539795
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37407010793685913
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3210055828094482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.405287027359009
9 3.024044463 	 2.4052871292 	 2.4052871292
epoch_time;  36.22282385826111
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2955789268016815
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3843706250190735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.369676351547241
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4456095695495605
10 3.0053238194 	 2.4456095927 	 2.4456095927
epoch_time;  35.7830753326416
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2785564661026001
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3841215670108795
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.24600887298584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3591599464416504
11 3.0006449809 	 2.3591598923 	 2.3591598923
epoch_time;  35.80835008621216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3247391879558563
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3948958218097687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2978272438049316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.371950149536133
12 2.9818661534 	 2.3719500567 	 2.3719500567
epoch_time;  35.864240646362305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26050639152526855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33113569021224976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1609673500061035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.21742844581604
13 2.9712876079 	 2.2174283414 	 2.2174283414
epoch_time;  36.927366733551025
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2910182774066925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3910970389842987
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1701128482818604
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2663893699645996
14 2.9643571217 	 2.2663894241 	 2.2663894241
epoch_time;  35.015870332717896
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2894740402698517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4046076238155365
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.10939621925354
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2118265628814697
15 2.958952969 	 2.2118264688 	 2.2118264688
epoch_time;  36.859052419662476
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2837804853916168
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35982587933540344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1387460231781006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.195950508117676
16 2.9508336816 	 2.1959503999 	 2.1959503999
epoch_time;  36.295745849609375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2637537717819214
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3308439254760742
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0616893768310547
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1239922046661377
17 2.932725621 	 2.1239920951 	 2.1239920951
epoch_time;  35.84072422981262
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24966871738433838
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31494417786598206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.997032880783081
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0525453090667725
18 2.9366794759 	 2.052545331 	 2.052545331
epoch_time;  35.888500928878784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2458568662405014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3026089668273926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9950875043869019
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.0410335063934326
19 2.9254205764 	 2.0410334407 	 2.0410334407
epoch_time;  36.114659547805786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.245834618806839
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.302674263715744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 1.9955426454544067
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.041236639022827
It took 781.6691131591797 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
pos
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–…â–†â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–‡â–†â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–„â–ƒâ–â–‚â–â–‚â–â–â–â–‚â–‚
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–†â–ˆâ–ƒâ–ƒâ–„â–‚â–ƒâ–„â–ƒâ–ƒâ–„â–â–ƒâ–‚â–ƒâ–â–‚â–â–‚â–‚
wandb:                         Train loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.12632
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.38042
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.09392
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.28873
wandb:                         Train loss 2.93664
wandb: 
wandb: ðŸš€ View run dancing-rocket-1243 at: https://wandb.ai/nreints/thesis/runs/24v4y9rg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_141229-24v4y9rg/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39530161023139954
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5220894813537598
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.1889595985412598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.300569534301758
0 6.7286095125 	 3.3005694415 	 3.3005694415
epoch_time;  35.724783182144165
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3542496860027313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48779457807540894
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.72182559967041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8345537185668945
1 3.3494748249 	 2.8345538165 	 2.8345538165
epoch_time;  36.047454595565796
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3923285901546478
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.555896520614624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.7536301612854004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8962574005126953
2 3.2570481762 	 2.8962573902 	 2.8962573902
epoch_time;  34.88251614570618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29738104343414307
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4170713722705841
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.582963705062866
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6706883907318115
3 3.1990871752 	 2.6706884436 	 2.6706884436
epoch_time;  35.27430582046509
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30760255455970764
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40781036019325256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6229004859924316
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.682255744934082
4 3.1571905282 	 2.6822557604 	 2.6822557604
epoch_time;  35.89374566078186
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3221248388290405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40152856707572937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5698466300964355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5807607173919678
5 3.1240049245 	 2.580760729 	 2.580760729
epoch_time;  35.37680435180664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2809748351573944
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37637534737586975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.586428642272949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6221818923950195
6 3.0906358433 	 2.6221818254 	 2.6221818254
epoch_time;  35.46413731575012
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29898470640182495
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3852328360080719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5589675903320312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5843749046325684
7 3.0695518127 	 2.584374835 	 2.584374835
epoch_time;  35.44292998313904
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32771578431129456
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41760939359664917
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5168769359588623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5571341514587402
8 3.0507571169 	 2.557134185 	 2.557134185
epoch_time;  35.394092321395874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29085850715637207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3882809281349182
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4343554973602295
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4620625972747803
9 3.0274931766 	 2.4620626914 	 2.4620626914
epoch_time;  35.13892722129822
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2970638871192932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4226870536804199
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4127321243286133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.478893995285034
10 3.0159539369 	 2.478894043 	 2.478894043
epoch_time;  35.299553871154785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32161012291908264
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3944879174232483
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4587950706481934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4815447330474854
11 3.0106228446 	 2.4815446183 	 2.4815446183
epoch_time;  35.48853158950806
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2607455253601074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3412921130657196
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.266707420349121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2948927879333496
12 2.9931802614 	 2.2948928421 	 2.2948928421
epoch_time;  35.4218385219574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30105558037757874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38055744767189026
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2372655868530273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.257826805114746
13 2.9849584911 	 2.2578266865 	 2.2578266865
epoch_time;  35.394394874572754
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2748010456562042
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3426849842071533
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1844594478607178
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1975817680358887
14 2.9743534984 	 2.1975816881 	 2.1975816881
epoch_time;  35.38001275062561
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30842429399490356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3726799190044403
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.204068183898926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.203608989715576
15 2.9691794363 	 2.2036089923 	 2.2036089923
epoch_time;  34.85540175437927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26487934589385986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.345172256231308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.102330207824707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1240415573120117
16 2.9624558993 	 2.1240415831 	 2.1240415831
epoch_time;  35.48555779457092
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27556970715522766
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3371107280254364
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.155463933944702
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1462087631225586
17 2.9525018381 	 2.146208727 	 2.146208727
epoch_time;  36.07830595970154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26425787806510925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34013545513153076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.129443407058716
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1566531658172607
18 2.9485836868 	 2.156653162 	 2.156653162
epoch_time;  35.209426403045654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28881555795669556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3803631663322449
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.093590259552002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1263082027435303
19 2.9366414581 	 2.1263082968 	 2.1263082968
epoch_time;  36.05109000205994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.288730651140213
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.380415141582489
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0939154624938965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1263248920440674
It took 774.7456052303314 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2137943
Array Job ID: 2137927_8
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-14:56:06 core-walltime
Job Wall-clock time: 02:09:47
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
