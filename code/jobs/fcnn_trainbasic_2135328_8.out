wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163919-sb8m9zu8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-cake-1137
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/sb8m9zu8
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▅▄▄▄▃▃▃▃▃▂▃▃▂▂▂▁▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▂▃▂▃▁▃▁▁▁▁▂▃▂▁▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▅▄▄▄▃▃▃▃▃▂▃▃▃▂▂▂▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▃▂▄▂▂▁▂▂▁▁▁▂▃▂▁▂▂▁▁
wandb:                         Train loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 2.17164
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.36552
wandb:    Test loss t(0, 0)_r(-5, 5)_none 2.08183
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26158
wandb:                         Train loss 2.92991
wandb: 
wandb: 🚀 View run red-cake-1137 at: https://wandb.ai/nreints/thesis/runs/sb8m9zu8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163919-sb8m9zu8/logs
Number of train simulations: 8000
Number of test simulations: 2000
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42742908000946045
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5880128145217896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.131537437438965
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 3.3116843700408936
0 6.6733196778 	 3.3116844383 	 3.3116844383
epoch_time;  30.9472553730011
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30860406160354614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4539124667644501
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6670920848846436
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.8606576919555664
1 3.3507634643 	 2.860657728 	 2.860657728
epoch_time;  30.67244243621826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.305395245552063
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.45339319109916687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.6615543365478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.875404119491577
2 3.2465565758 	 2.8754041517 	 2.8754041517
epoch_time;  30.603137969970703
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28962722420692444
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40021172165870667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.531259298324585
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.684605360031128
3 3.1957753258 	 2.684605449 	 2.684605449
epoch_time;  30.162320375442505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32123997807502747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41606009006500244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.585958242416382
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.708430051803589
4 3.1464119758 	 2.7084301098 	 2.7084301098
epoch_time;  30.375622749328613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29164040088653564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39264219999313354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.5314981937408447
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.6519744396209717
5 3.1139417686 	 2.6519744048 	 2.6519744048
epoch_time;  30.220158576965332
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28611335158348083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41740646958351135
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4312620162963867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5671329498291016
6 3.088852311 	 2.5671330632 	 2.5671330632
epoch_time;  30.244422674179077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2656806707382202
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3578563928604126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.378018379211426
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4804723262786865
7 3.0571229556 	 2.4804723791 	 2.4804723791
epoch_time;  30.132087469100952
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2924269735813141
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4279085695743561
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3742074966430664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.523123025894165
8 3.0445192165 	 2.5231230865 	 2.5231230865
epoch_time;  30.72410798072815
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28289860486984253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3659380078315735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.4422974586486816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.5194923877716064
9 3.026394957 	 2.5194924844 	 2.5194924844
epoch_time;  30.01837682723999
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2567375898361206
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3624393045902252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3327231407165527
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.445794105529785
10 3.0093937114 	 2.4457940179 	 2.4457940179
epoch_time;  30.377252101898193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26524612307548523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3557544946670532
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3017654418945312
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.3911807537078857
11 2.9988267365 	 2.3911807498 	 2.3911807498
epoch_time;  30.04010319709778
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2624865472316742
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.363718181848526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3366947174072266
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.44357967376709
12 2.9740057179 	 2.4435797614 	 2.4435797614
epoch_time;  30.573120594024658
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28479695320129395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39739635586738586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3665640354156494
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4747371673583984
13 2.9741908482 	 2.4747370539 	 2.4747370539
epoch_time;  30.468081951141357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29436495900154114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40727317333221436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.3106255531311035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.4140071868896484
14 2.9619781969 	 2.4140070735 	 2.4140070735
epoch_time;  30.33503246307373
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29315805435180664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3864530026912689
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.2933902740478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.373863697052002
15 2.95241886 	 2.3738635914 	 2.3738635914
epoch_time;  30.004931211471558
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.266129732131958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3572438061237335
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.21829891204834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.297276258468628
16 2.951536185 	 2.2972763474 	 2.2972763474
epoch_time;  30.234713792800903
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29099974036216736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38178396224975586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.1667211055755615
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2490649223327637
17 2.9410796272 	 2.2490650074 	 2.2490650074
epoch_time;  30.53797435760498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28529244661331177
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39361193776130676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.157634735107422
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.2629432678222656
18 2.9330803439 	 2.2629432472 	 2.2629432472
epoch_time;  30.230430364608765
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2615276575088501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36552178859710693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.0814714431762695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1714770793914795
19 2.9299093258 	 2.1714771168 	 2.1714771168
epoch_time;  30.418520212173462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26158082485198975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3655177056789398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 2.081829309463501
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 2.1716384887695312
It took 669.6631300449371 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn54: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135337.0

JOB STATISTICS
==============
Job ID: 2135337
Array Job ID: 2135328_8
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:26:06 core-walltime
Job Wall-clock time: 00:11:27
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
