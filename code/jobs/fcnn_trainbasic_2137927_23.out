wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_164013-v56f8yjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flashing-paper-1326
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/v56f8yjg
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▂▂▃▃▃▃▂▃▄▃▂▂▃▁▂▁▃▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▁▂▃▄▂▂▁▂▄▂▂▂▃▂▁▁▁▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▁▁▃▂▃▂▁▃▄▂▂▃▂▁▂▁▂▁▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▁▂▄▄▃▃▂▃▅▃▃▃▄▃▃▃▂▄▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.40845
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.3229
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.317
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2505
wandb:                         Train loss 3.5676
wandb: 
wandb: 🚀 View run flashing-paper-1326 at: https://wandb.ai/nreints/thesis/runs/v56f8yjg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_164013-v56f8yjg/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_165142-j8j2ct9y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-fuse-1332
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/j8j2ct9y
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3544537127017975
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4952195882797241
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4383435547351837
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.595493495464325
0 6.605167252 	 0.5954935022 	 0.5954935022
epoch_time;  32.563109159469604
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20409558713436127
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28438183665275574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31134194135665894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41213399171829224
1 4.0312952908 	 0.4121339953 	 0.4121339953
epoch_time;  31.466461658477783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23350727558135986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3045767843723297
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31102827191352844
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40393540263175964
2 3.8741725335 	 0.4039354067 	 0.4039354067
epoch_time;  31.334187507629395
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2655528783798218
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3453311622142792
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3439956605434418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4330899119377136
3 3.7930798282 	 0.4330899213 	 0.4330899213
epoch_time;  31.111721754074097
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26510441303253174
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.362338125705719
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32195600867271423
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42784932255744934
4 3.752149826 	 0.4278493108 	 0.4278493108
epoch_time;  31.004211902618408
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2410401850938797
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3153785467147827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33714625239372253
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4344928562641144
5 3.7172772221 	 0.4344928638 	 0.4344928638
epoch_time;  31.20465588569641
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2381855547428131
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31833913922309875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33019211888313293
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4269016683101654
6 3.6927051376 	 0.4269016575 	 0.4269016575
epoch_time;  31.150365114212036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22074051201343536
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28975144028663635
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3101280629634857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4016439914703369
7 3.6809248979 	 0.4016439902 	 0.4016439902
epoch_time;  30.910825967788696
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2533133924007416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3077281415462494
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3452833294868469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42675501108169556
8 3.6547550726 	 0.4267550082 	 0.4267550082
epoch_time;  31.225919008255005
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29120752215385437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37293973565101624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3602028787136078
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45830002427101135
9 3.645897578 	 0.4583000389 	 0.4583000389
epoch_time;  31.32046604156494
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24074243009090424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30662593245506287
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32564809918403625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41802144050598145
10 3.6297112364 	 0.4180214547 	 0.4180214547
epoch_time;  31.271754026412964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25526097416877747
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30844011902809143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3213139474391937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3939209282398224
11 3.6215093015 	 0.3939209397 	 0.3939209397
epoch_time;  31.425153255462646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24226653575897217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2983124852180481
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3368679881095886
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4091932475566864
12 3.6066466417 	 0.4091932555 	 0.4091932555
epoch_time;  31.089922189712524
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26153621077537537
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33499160408973694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32654106616973877
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4206770956516266
13 3.6031685056 	 0.4206771026 	 0.4206771026
epoch_time;  31.025904178619385
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25212132930755615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30758142471313477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30967092514038086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36936262249946594
14 3.5904655896 	 0.3693626198 	 0.3693626198
epoch_time;  30.989309072494507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24761155247688293
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2896944284439087
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33252832293510437
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39165398478507996
15 3.5929496419 	 0.3916539785 	 0.3916539785
epoch_time;  31.11173987388611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23955948650836945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2766181230545044
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3057894706726074
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3659888505935669
16 3.575237628 	 0.3659888603 	 0.3659888603
epoch_time;  31.14706778526306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23220530152320862
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2918754518032074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3282208740711212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4206426739692688
17 3.5741661862 	 0.4206426672 	 0.4206426672
epoch_time;  31.092222929000854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2696661353111267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3443993628025055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3151855766773224
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3956451416015625
18 3.5695227208 	 0.3956451416 	 0.3956451416
epoch_time;  31.187066793441772
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25049781799316406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3229970633983612
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3169785141944885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4083714187145233
19 3.5675973653 	 0.4083714253 	 0.4083714253
epoch_time;  30.87280821800232
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2504972219467163
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3229036331176758
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3170012831687927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.408451110124588
It took 688.4979541301727 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▅▄▃▂▅▂▅▄▄▃▂▃▃▂▅▂▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇█▅▅▅▃▇▂▇▄▄▄▁▃▄▃▆▃▄▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆█▅▃▄▃▄▂▆▃▃▄▁▃▃▂▄▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none ▄█▄▅▅▄▆▂█▄▃▄▁▃▄▃▅▃▄▂▂
wandb:                         Train loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.32822
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.23998
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.27745
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.19814
wandb:                         Train loss 3.54267
wandb: 
wandb: 🚀 View run filigreed-fuse-1332 at: https://wandb.ai/nreints/thesis/runs/j8j2ct9y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_165142-j8j2ct9y/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_170256-03kxx89k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-cake-1338
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/03kxx89k
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2469475418329239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3774981200695038
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3770236074924469
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5405977368354797
0 6.6166638456 	 0.5405977404 	 0.5405977404
epoch_time;  31.08443593978882
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3222823441028595
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41121745109558105
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4176490902900696
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5375381708145142
1 4.056267389 	 0.537538147 	 0.537538147
epoch_time;  30.820894241333008
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24383065104484558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3244348168373108
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34989866614341736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4550029933452606
2 3.9069862237 	 0.4550029858 	 0.4550029858
epoch_time;  30.710158586502075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2585217356681824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3358169496059418
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32204970717430115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4083045721054077
3 3.8222983534 	 0.4083045753 	 0.4083045753
epoch_time;  30.38180136680603
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26653948426246643
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3214212954044342
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33992213010787964
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3980177938938141
4 3.7633274003 	 0.3980178008 	 0.3980178008
epoch_time;  30.81728959083557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2367946356534958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2785879671573639
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3195023536682129
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3642292022705078
5 3.7187195857 	 0.364229192 	 0.364229192
epoch_time;  30.69813847541809
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28342393040657043
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3745773434638977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34412944316864014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4367915689945221
6 3.6904908466 	 0.4367915798 	 0.4367915798
epoch_time;  30.80903148651123
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21262741088867188
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26012176275253296
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30349835753440857
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3709864914417267
7 3.6639243134 	 0.3709864848 	 0.3709864848
epoch_time;  30.64600157737732
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32775720953941345
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3800307810306549
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3820856511592865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44453132152557373
8 3.6437236189 	 0.4445313325 	 0.4445313325
epoch_time;  30.30068063735962
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23805874586105347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.303436279296875
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32314765453338623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4111839234828949
9 3.6257517614 	 0.4111839088 	 0.4111839088
epoch_time;  31.07295322418213
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23049211502075195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30771663784980774
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32349008321762085
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41223692893981934
10 3.6186156483 	 0.4122369302 	 0.4122369302
epoch_time;  30.98504948616028
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24603892862796783
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3057493269443512
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3295840919017792
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40329796075820923
11 3.6071372371 	 0.4032979604 	 0.4032979604
epoch_time;  30.69733691215515
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.18342989683151245
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2262834757566452
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2870486080646515
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35078164935112
12 3.5958525516 	 0.3507816624 	 0.3507816624
epoch_time;  30.91838002204895
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22247135639190674
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28156110644340515
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31448429822921753
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38540831208229065
13 3.5884698185 	 0.3854083087 	 0.3854083087
epoch_time;  31.192574501037598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23502135276794434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2974569797515869
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31303173303604126
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3917807638645172
14 3.5733572328 	 0.3917807502 	 0.3917807502
epoch_time;  31.136974811553955
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23461699485778809
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29137730598449707
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2939515709877014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3637843430042267
15 3.5701967935 	 0.3637843364 	 0.3637843364
epoch_time;  31.379591941833496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2618347108364105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3500116169452667
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3400723934173584
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4485371708869934
16 3.5622011468 	 0.448537177 	 0.448537177
epoch_time;  31.146769762039185
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2295515388250351
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.281628280878067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29196250438690186
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3651650547981262
17 3.5614193017 	 0.3651650506 	 0.3651650506
epoch_time;  31.030561685562134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25135067105293274
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29918837547302246
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31368887424468994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3726160526275635
18 3.5527935506 	 0.372616041 	 0.372616041
epoch_time;  30.709845066070557
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19812943041324615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23996993899345398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.27751147747039795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3281978666782379
19 3.5426682438 	 0.328197871 	 0.328197871
epoch_time;  30.916407823562622
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19814415276050568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.23998111486434937
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2774515450000763
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.32821840047836304
It took 674.3395302295685 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▅▂▁▃▃▂▃▁▃▃▄▄▃▂▃▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▂▁▃▃▂▄▂▂▃▅▄▂▂▃▅▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▆▂▂▃▄▂▃▁▄▃▄▄▂▃▃▃▂▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▃▂▁▄▄▃▅▃▃▃▅▄▂▂▄▅▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36382
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.26813
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.30529
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20872
wandb:                         Train loss 3.54083
wandb: 
wandb: 🚀 View run vermilion-cake-1338 at: https://wandb.ai/nreints/thesis/runs/03kxx89k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_170256-03kxx89k/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_171411-2cwaohqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-mandu-1345
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2cwaohqw
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33353734016418457
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46932438015937805
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4150722622871399
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5468528866767883
0 6.6115681333 	 0.5468528954 	 0.5468528954
epoch_time;  31.03974151611328
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2882153391838074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3741177022457123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34706494212150574
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42860710620880127
1 4.0506017397 	 0.4286070953 	 0.4286070953
epoch_time;  30.773213386535645
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23807530105113983
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33759450912475586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3747558295726776
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46994537115097046
2 3.8809562378 	 0.4699453818 	 0.4699453818
epoch_time;  30.743267059326172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22755396366119385
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29889267683029175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32083481550216675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3941730856895447
3 3.7957361179 	 0.3941730809 	 0.3941730809
epoch_time;  30.942900896072388
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21659234166145325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26056841015815735
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32235854864120483
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36737802624702454
4 3.7415973971 	 0.3673780287 	 0.3673780287
epoch_time;  31.07155442237854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26498523354530334
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32699909806251526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33790865540504456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4057464897632599
5 3.7074110978 	 0.4057465012 	 0.4057465012
epoch_time;  31.81939148902893
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26375889778137207
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33306220173835754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34546682238578796
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4163902997970581
6 3.673493568 	 0.4163902901 	 0.4163902901
epoch_time;  31.423601150512695
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23959796130657196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29961511492729187
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31783008575439453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3776695430278778
7 3.6605251883 	 0.3776695458 	 0.3776695458
epoch_time;  31.08082413673401
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27656084299087524
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3551834225654602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33635199069976807
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41444236040115356
8 3.6404279684 	 0.4144423614 	 0.4144423614
epoch_time;  31.05549192428589
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2379353642463684
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29360464215278625
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29873475432395935
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35708093643188477
9 3.620779359 	 0.3570809442 	 0.3570809442
epoch_time;  30.93076801300049
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25126954913139343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30035480856895447
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3448497951030731
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40556278824806213
10 3.616038039 	 0.4055627771 	 0.4055627771
epoch_time;  30.77389621734619
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25016680359840393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32457879185676575
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3262215554714203
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4089397192001343
11 3.5986939716 	 0.4089397121 	 0.4089397121
epoch_time;  30.799196481704712
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2861897349357605
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36714163422584534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3451119363307953
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4294194281101227
12 3.5895541533 	 0.4294194402 	 0.4294194402
epoch_time;  30.950883865356445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26798152923583984
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3443167507648468
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35035333037376404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42819902300834656
13 3.5773107571 	 0.4281990257 	 0.4281990257
epoch_time;  31.06025505065918
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22109651565551758
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28939732909202576
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3204442858695984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40001776814460754
14 3.5696556179 	 0.4000177744 	 0.4000177744
epoch_time;  30.889022827148438
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23147103190422058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2790580689907074
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3250490427017212
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37956002354621887
15 3.5620851097 	 0.3795600273 	 0.3795600273
epoch_time;  31.1446533203125
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25679680705070496
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31503766775131226
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3392266631126404
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4015553295612335
16 3.5589454976 	 0.4015553242 	 0.4015553242
epoch_time;  31.11323094367981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2714766263961792
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3681572675704956
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32769709825515747
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42139938473701477
17 3.5503931375 	 0.4213993794 	 0.4213993794
epoch_time;  31.095810651779175
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22161860764026642
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3011434078216553
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30837976932525635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3819883167743683
18 3.5368432501 	 0.3819883192 	 0.3819883192
epoch_time;  30.80652379989624
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.208717942237854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2681216597557068
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3053610622882843
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3638293147087097
19 3.540826197 	 0.3638293292 	 0.3638293292
epoch_time;  30.857714891433716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20871953666210175
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26812881231307983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3052874207496643
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3638209104537964
It took 675.1367404460907 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▆▃▄▃▄▃▃▂▃▃▂▃▄▄▁▃▃▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▇▂▄▃▅▃▃▁▃▃▃▁▂▅▁▃▄▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▃▅▄▆▃▅▂▄▂▃▃▃▄▁▃▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▆▂▃▄▇▃▅▂▄▂▅▁▂▆▁▄▅▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.3371
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.27009
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.2913
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22602
wandb:                         Train loss 3.56132
wandb: 
wandb: 🚀 View run thriving-mandu-1345 at: https://wandb.ai/nreints/thesis/runs/2cwaohqw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_171411-2cwaohqw/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_172529-dryg4c3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-moon-1352
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/dryg4c3l
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32453781366348267
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.46736758947372437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39531180262565613
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5461107492446899
0 6.6783757194 	 0.5461107409 	 0.5461107409
epoch_time;  31.4780056476593
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2574581801891327
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38906246423721313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3768453598022461
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5053016543388367
1 4.0481662841 	 0.5053016456 	 0.5053016456
epoch_time;  31.374992847442627
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2931041717529297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4316648244857788
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.360186904668808
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4928910434246063
2 3.8922505104 	 0.4928910539 	 0.4928910539
epoch_time;  31.00404381752014
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22491610050201416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2909744679927826
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32488420605659485
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40020400285720825
3 3.822019005 	 0.4002040141 	 0.4002040141
epoch_time;  30.894968271255493
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2445797324180603
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34009528160095215
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.344566285610199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4391116499900818
4 3.7641399903 	 0.439111658 	 0.439111658
epoch_time;  31.280959606170654
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2613867521286011
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3220826983451843
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33418992161750793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3907895088195801
5 3.715732045 	 0.3907895062 	 0.3907895062
epoch_time;  30.98794174194336
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3063168227672577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3710029125213623
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3681480288505554
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44085025787353516
6 3.687624963 	 0.4408502527 	 0.4408502527
epoch_time;  30.923486948013306
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24873527884483337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3150128424167633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32522574067115784
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40072157979011536
7 3.6688299366 	 0.4007215758 	 0.4007215758
epoch_time;  30.72087264060974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2705065608024597
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32096031308174133
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3547976613044739
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41039639711380005
8 3.6558873429 	 0.4103963903 	 0.4103963903
epoch_time;  30.951476335525513
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2303188294172287
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2736550271511078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30504921078681946
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35361868143081665
9 3.6368876814 	 0.3536186837 	 0.3536186837
epoch_time;  31.04548740386963
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25364574790000916
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32192668318748474
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3387957513332367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40869367122650146
10 3.6239999656 	 0.4086936744 	 0.4086936744
epoch_time;  30.652087450027466
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23089857399463654
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3077314496040344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3014572858810425
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38337287306785583
11 3.615383471 	 0.3833728687 	 0.3833728687
epoch_time;  30.960652828216553
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2815949618816376
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32899343967437744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3155986964702606
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.35992342233657837
12 3.60854555 	 0.3599234091 	 0.3599234091
epoch_time;  31.227418661117554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20663006603717804
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26973968744277954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3220355808734894
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39358314871788025
13 3.5934524691 	 0.3935831431 	 0.3935831431
epoch_time;  32.17878580093384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22364507615566254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.304781049489975
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3180941045284271
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41930603981018066
14 3.5901264043 	 0.4193060385 	 0.4193060385
epoch_time;  31.402099132537842
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29209795594215393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3861069679260254
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3402757942676544
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4400264024734497
15 3.5776502729 	 0.4400264018 	 0.4400264018
epoch_time;  31.509027242660522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2116215080022812
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26255080103874207
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28875404596328735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34540897607803345
16 3.5780856598 	 0.3454089809 	 0.3454089809
epoch_time;  31.16374444961548
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2651618421077728
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32403290271759033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3166849911212921
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.382242351770401
17 3.5632524226 	 0.3822423574 	 0.3822423574
epoch_time;  31.154834270477295
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28093281388282776
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3457857370376587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32317212224006653
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3955150246620178
18 3.5656267191 	 0.3955150295 	 0.3955150295
epoch_time;  31.078722715377808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22599071264266968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2701154053211212
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2912980020046234
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3371201753616333
19 3.5613181412 	 0.3371201799 	 0.3371201799
epoch_time;  30.847344160079956
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22601565718650818
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2700939476490021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29129794239997864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3371002674102783
It took 677.7052826881409 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇█▅▄▆▄▅▆▂▁▃▄▃▄▂▃▃▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▇▅█▅▃▅▅▄▆▁▁▃▄▃▃▁▃▂▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇█▆▃▇▅▄▇▁▁▃▃▂▃▃▃▃▂▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅█▆▃▇▇▃█▂▂▄▄▃▂▂▃▃▁▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36366
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.30099
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.30045
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.23671
wandb:                         Train loss 3.56147
wandb: 
wandb: 🚀 View run luminous-moon-1352 at: https://wandb.ai/nreints/thesis/runs/dryg4c3l
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_172529-dryg4c3l/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_173642-86j0xf07
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-dumpling-1358
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/86j0xf07
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3124605715274811
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4200945496559143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38669174909591675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5106199383735657
0 6.778204606 	 0.510619911 	 0.510619911
epoch_time;  30.85547947883606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2797931134700775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38943377137184143
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37805038690567017
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49047979712486267
1 4.0616953442 	 0.4904797941 	 0.4904797941
epoch_time;  30.90018916130066
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3173482418060303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4588589370250702
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.38330885767936707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5086979866027832
2 3.9085056147 	 0.5086979634 	 0.5086979634
epoch_time;  30.949039697647095
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2923114001750946
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3633034825325012
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3543582856655121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42133259773254395
3 3.8144465132 	 0.4213326119 	 0.4213326119
epoch_time;  30.81009078025818
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24430353939533234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3264791667461395
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32459309697151184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4114123284816742
4 3.7626017556 	 0.411412337 	 0.411412337
epoch_time;  30.73350167274475
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2976352274417877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38717103004455566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.37179017066955566
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45396894216537476
5 3.7253869224 	 0.4539689348 	 0.4539689348
epoch_time;  30.87551760673523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29921868443489075
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3732065260410309
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34172871708869934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40631622076034546
6 3.6961422806 	 0.4063162314 	 0.4063162314
epoch_time;  30.98684334754944
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24201621115207672
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3421325385570526
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33548280596733093
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4406081736087799
7 3.6721133553 	 0.4406081741 	 0.4406081741
epoch_time;  31.078563928604126
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31967127323150635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4082338809967041
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3780001401901245
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45561766624450684
8 3.6498302752 	 0.4556176675 	 0.4556176675
epoch_time;  30.78949999809265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2286730855703354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2737768888473511
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2974469065666199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34795066714286804
9 3.6397277791 	 0.3479506621 	 0.3479506621
epoch_time;  30.902857542037964
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2355206459760666
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2707575261592865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2915023863315582
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3296171724796295
10 3.626993457 	 0.3296171859 	 0.3296171859
epoch_time;  30.694379091262817
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2619721591472626
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3129994869232178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31972548365592957
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3746635317802429
11 3.613652833 	 0.3746635231 	 0.3746635231
epoch_time;  30.87571668624878
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2628895044326782
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3445335328578949
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32253384590148926
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3993035852909088
12 3.5976612088 	 0.3993035806 	 0.3993035806
epoch_time;  30.877662897109985
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24853885173797607
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.326955109834671
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30664145946502686
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39100173115730286
13 3.596975896 	 0.3910017271 	 0.3910017271
epoch_time;  30.794299840927124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2287425547838211
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30798032879829407
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3151518404483795
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.405972957611084
14 3.5865249582 	 0.4059729499 	 0.4059729499
epoch_time;  30.775145053863525
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23158317804336548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26618146896362305
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3159594237804413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3607322573661804
15 3.5791197449 	 0.3607322487 	 0.3607322487
epoch_time;  30.870929479599
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24410974979400635
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3111113905906677
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3121229112148285
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38396766781806946
16 3.5791812347 	 0.3839676728 	 0.3839676728
epoch_time;  30.58957266807556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24412287771701813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30633318424224854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31499963998794556
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3836134970188141
17 3.5637704569 	 0.3836135039 	 0.3836135039
epoch_time;  30.994983434677124
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21335624158382416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27259179949760437
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30976274609565735
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38128992915153503
18 3.5670571815 	 0.3812899203 	 0.3812899203
epoch_time;  30.82920527458191
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23670916259288788
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3010335862636566
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30046436190605164
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36357009410858154
19 3.5614672372 	 0.3635700948 	 0.3635700948
epoch_time;  31.029006242752075
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2367093861103058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3009949326515198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30045148730278015
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36365899443626404
It took 672.7409362792969 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▃▄▄▄▃▂▃▃▁▃▃▃▂▂▃▂▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▃▂▅▂▄▃▂▄▄▂▄▃▄▂▃▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▆▃▃▄▄▃▃▄▃▁▄▃▄▂▂▃▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▄▁▄▂▄▃▃▄▄▃▅▃▅▂▂▂▂▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.31687
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24335
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28235
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20408
wandb:                         Train loss 3.5502
wandb: 
wandb: 🚀 View run filigreed-dumpling-1358 at: https://wandb.ai/nreints/thesis/runs/86j0xf07
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_173642-86j0xf07/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_174752-f6jiuptr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-springroll-1364
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/f6jiuptr
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32173195481300354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4963168501853943
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4279173016548157
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.6023507714271545
0 6.6601668289 	 0.6023507608 	 0.6023507608
epoch_time;  31.179676294326782
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30683621764183044
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.41186007857322693
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4034311771392822
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5085779428482056
1 4.0482892635 	 0.5085779139 	 0.5085779139
epoch_time;  30.909706592559814
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25265631079673767
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3334406614303589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3777230381965637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46047425270080566
2 3.8857851642 	 0.4604742514 	 0.4604742514
epoch_time;  30.75482702255249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20444746315479279
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2640842795372009
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3343038260936737
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3997175097465515
3 3.8028781816 	 0.3997175062 	 0.3997175062
epoch_time;  31.053357124328613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26246553659439087
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.37632930278778076
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33423399925231934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.44109469652175903
4 3.749185096 	 0.441094682 	 0.441094682
epoch_time;  30.707398653030396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2207249253988266
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2921738028526306
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35119256377220154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43176352977752686
5 3.7161228022 	 0.4317635201 	 0.4317635201
epoch_time;  30.914182662963867
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25054070353507996
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35230839252471924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3438522219657898
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43017274141311646
6 3.6903427424 	 0.4301727295 	 0.4301727295
epoch_time;  31.039032697677612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24208883941173553
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3088396489620209
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31925085186958313
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3862511217594147
7 3.6725447751 	 0.38625113 	 0.38625113
epoch_time;  31.093603134155273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23934152722358704
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29124951362609863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3172498345375061
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36454999446868896
8 3.6511569323 	 0.3645499977 	 0.3645499977
epoch_time;  31.059217929840088
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2524360716342926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.336775541305542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3349396884441376
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.411628395318985
9 3.6368552836 	 0.4116283932 	 0.4116283932
epoch_time;  31.026790618896484
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2560770809650421
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3410693407058716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3270376920700073
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4029427170753479
10 3.6269203966 	 0.4029427193 	 0.4029427193
epoch_time;  30.97205138206482
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23389574885368347
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2890605926513672
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28397098183631897
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3305809497833252
11 3.6134283625 	 0.3305809433 	 0.3305809433
epoch_time;  30.94306254386902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2699657380580902
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33975377678871155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3378968834877014
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3999488353729248
12 3.6030934617 	 0.3999488212 	 0.3999488212
epoch_time;  30.99790930747986
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24116891622543335
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30998554825782776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31818637251853943
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3786388635635376
13 3.5880569117 	 0.37863885 	 0.37863885
epoch_time;  30.565125942230225
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26789793372154236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34880658984184265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3437572419643402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4161827266216278
14 3.5816051196 	 0.4161827294 	 0.4161827294
epoch_time;  30.87166428565979
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21451450884342194
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2779698967933655
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3053763508796692
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.367444783449173
15 3.5738617129 	 0.3674447962 	 0.3674447962
epoch_time;  31.20467710494995
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22617162764072418
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3023279011249542
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30779755115509033
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3767245411872864
16 3.5709991352 	 0.3767245318 	 0.3767245318
epoch_time;  30.946980714797974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22299513220787048
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2866303026676178
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3196868300437927
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3846440613269806
17 3.5580952205 	 0.3846440496 	 0.3846440496
epoch_time;  30.716587781906128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21527165174484253
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26659777760505676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28704118728637695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3378249406814575
18 3.5596466788 	 0.3378249503 	 0.3378249503
epoch_time;  30.838221311569214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2040625959634781
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24342185258865356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2823736071586609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3168225884437561
19 3.550200073 	 0.3168225881 	 0.3168225881
epoch_time;  30.79675793647766
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.204081192612648
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2433549016714096
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28234967589378357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.31687280535697937
It took 670.3252115249634 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▅▅▃▄▃▄▅▃▄▃▄▂▃▁▃▂▂▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▅▃▄▂▂▆▁▃▃▄▂▄▁▃▁▄▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▅▅▄▅▄▅▅▃▄▂▅▂▄▁▄▃▂▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▇▅▅▃▅▂▃▇▁▃▃▄▃▅▂▅▂▅▃▃
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.39268
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.31851
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.30573
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.24135
wandb:                         Train loss 3.5567
wandb: 
wandb: 🚀 View run lunar-springroll-1364 at: https://wandb.ai/nreints/thesis/runs/f6jiuptr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_174752-f6jiuptr/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_175905-aaole7fy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twinkling-kumquat-1370
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/aaole7fy
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3159516751766205
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4487212598323822
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39301779866218567
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.549095869064331
0 6.6054621382 	 0.5490958549 	 0.5490958549
epoch_time;  30.830345392227173
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2965005934238434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.411278635263443
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36977770924568176
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.48641958832740784
1 4.0611029272 	 0.4864195953 	 0.4864195953
epoch_time;  30.863518953323364
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2662888765335083
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3530557453632355
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35236844420433044
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45443013310432434
2 3.9091221649 	 0.4544301213 	 0.4544301213
epoch_time;  31.26555347442627
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2775533199310303
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3745352625846863
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3426559865474701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4462583065032959
3 3.8273305925 	 0.4462582975 	 0.4462582975
epoch_time;  30.856836080551147
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24181613326072693
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3182481825351715
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32932043075561523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4084640145301819
4 3.7663830426 	 0.4084640091 	 0.4084640091
epoch_time;  30.712486267089844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2739909589290619
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3527982234954834
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3443753719329834
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43007001280784607
5 3.7309160381 	 0.4300700007 	 0.4300700007
epoch_time;  31.34947156906128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22398149967193604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29735276103019714
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3274555206298828
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.407293438911438
6 3.6967581695 	 0.4072934537 	 0.4072934537
epoch_time;  31.976852893829346
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24647708237171173
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3168913424015045
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3439202308654785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.42811012268066406
7 3.6736763842 	 0.4281101124 	 0.4281101124
epoch_time;  31.0544490814209
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3073989748954773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40145695209503174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3526672422885895
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45494329929351807
8 3.6527507061 	 0.4549433115 	 0.4549433115
epoch_time;  30.949493885040283
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21089710295200348
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28524717688560486
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3192223906517029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4073602557182312
9 3.6356631765 	 0.4073602625 	 0.4073602625
epoch_time;  31.13349676132202
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2456483542919159
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3382743000984192
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33037716150283813
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4293108284473419
10 3.6162945767 	 0.4293108141 	 0.4293108141
epoch_time;  30.61844778060913
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23900309205055237
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32246625423431396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.304525762796402
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3898141086101532
11 3.6077501507 	 0.3898140985 	 0.3898140985
epoch_time;  31.192456483840942
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25833651423454285
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35520851612091064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33763009309768677
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.43514788150787354
12 3.6028871736 	 0.4351478783 	 0.4351478783
epoch_time;  30.727776527404785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24398259818553925
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2966991066932678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30446064472198486
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37043818831443787
13 3.5922514361 	 0.3704381994 	 0.3704381994
epoch_time;  30.62354588508606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2781320810317993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35268089175224304
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3303166925907135
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41055187582969666
14 3.5754049065 	 0.410551865 	 0.410551865
epoch_time;  30.50578284263611
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22978773713111877
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28916487097740173
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2804275453090668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3338269293308258
15 3.5733241738 	 0.3338269414 	 0.3338269414
epoch_time;  30.902749061584473
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2724306583404541
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3342757523059845
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32435327768325806
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39176318049430847
16 3.5643463445 	 0.3917631819 	 0.3917631819
epoch_time;  30.70104718208313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22415822744369507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28144142031669617
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3079228103160858
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37358102202415466
17 3.5594879956 	 0.3735810151 	 0.3735810151
epoch_time;  30.631879568099976
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2766074538230896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3482673764228821
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3011743426322937
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37900641560554504
18 3.5518021115 	 0.3790064219 	 0.3790064219
epoch_time;  30.242986917495728
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24135766923427582
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3185371160507202
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30574557185173035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39266857504844666
19 3.556697817 	 0.3926685643 	 0.3926685643
epoch_time;  30.842015266418457
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24135488271713257
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31850627064704895
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3057346045970917
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39268290996551514
It took 672.6403238773346 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▅▇▂▃▃▃▂▄▂▁▄▁▃▄▄▂▄▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄█▄▂▂▄▂▅▁▂▄▃▂▂▄▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▆▄█▃▅▃▃▂▄▁▁▂▁▂▄▃▁▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅▅▄█▅▃▃▅▃▆▂▃▃▄▂▂▄▂▁▂▂
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.37856
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.28936
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.29644
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.22331
wandb:                         Train loss 3.54554
wandb: 
wandb: 🚀 View run twinkling-kumquat-1370 at: https://wandb.ai/nreints/thesis/runs/aaole7fy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_175905-aaole7fy/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_181016-zolw1y5t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-peony-1377
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/zolw1y5t
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27587950229644775
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4267607033252716
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.36884748935699463
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.534325122833252
0 6.6938612104 	 0.5343251409 	 0.5343251409
epoch_time;  30.935766458511353
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27207812666893005
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3963806927204132
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.357318639755249
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4959917664527893
1 4.0442511081 	 0.4959917636 	 0.4959917636
epoch_time;  30.646028757095337
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24372228980064392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33771347999572754
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.33360227942466736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4504069685935974
2 3.8807641651 	 0.450406956 	 0.450406956
epoch_time;  30.66033387184143
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3231127858161926
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43168383836746216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39396271109580994
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5093379616737366
3 3.7959510013 	 0.5093379665 	 0.5093379665
epoch_time;  30.68081831932068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2720577120780945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3348970413208008
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30895301699638367
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37453070282936096
4 3.7476637213 	 0.3745306891 	 0.3745306891
epoch_time;  30.89710760116577
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23071341216564178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2815372943878174
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3408820927143097
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40437427163124084
5 3.7067109084 	 0.4043742824 	 0.4043742824
epoch_time;  31.188007593154907
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2258935421705246
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29648056626319885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30744802951812744
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3852598965167999
6 3.6788682983 	 0.385259886 	 0.385259886
epoch_time;  31.05443572998047
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2615315914154053
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3291764259338379
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31817564368247986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38796699047088623
7 3.6592590291 	 0.3879670014 	 0.3879670014
epoch_time;  30.738422632217407
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22713154554367065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2840208411216736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30092695355415344
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37265390157699585
8 3.6417014006 	 0.3726538993 	 0.3726538993
epoch_time;  30.709640979766846
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28814566135406494
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3674301505088806
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3256366550922394
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.411546915769577
9 3.6319410651 	 0.411546903 	 0.411546903
epoch_time;  30.916157722473145
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20856697857379913
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2719969153404236
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2881963849067688
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36408090591430664
10 3.6188394576 	 0.364080893 	 0.364080893
epoch_time;  30.925538063049316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24087439477443695
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2923331558704376
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2828957736492157
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34345510601997375
11 3.5985858687 	 0.3434551136 	 0.3434551136
epoch_time;  31.081417083740234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23932820558547974
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34118273854255676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.296923965215683
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40802279114723206
12 3.5924035074 	 0.4080227826 	 0.4080227826
epoch_time;  30.849164485931396
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24819886684417725
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30515173077583313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28337034583091736
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33603566884994507
13 3.5848439087 	 0.3360356717 	 0.3360356717
epoch_time;  31.95098567008972
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21481220424175262
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28249022364616394
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29980790615081787
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3804646134376526
14 3.5744749572 	 0.380464626 	 0.380464626
epoch_time;  31.308018922805786
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22339333593845367
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29100608825683594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3246001899242401
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40886226296424866
15 3.5697465301 	 0.4088622634 	 0.4088622634
epoch_time;  30.799400329589844
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24921464920043945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32886970043182373
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3224312663078308
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.41912519931793213
16 3.5569497265 	 0.4191252013 	 0.4191252013
epoch_time;  30.905871152877808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21925389766693115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.276691198348999
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2878900468349457
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3604944944381714
17 3.5520004655 	 0.3604945002 	 0.3604945002
epoch_time;  30.929494619369507
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.19709698855876923
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26935333013534546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3163490295410156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4101271629333496
18 3.5488945736 	 0.4101271758 	 0.4101271758
epoch_time;  30.76737141609192
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22329968214035034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2893770933151245
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29639652371406555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3786032497882843
19 3.5455397475 	 0.3786032599 	 0.3786032599
epoch_time;  30.771469831466675
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22331400215625763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.28936320543289185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2964368164539337
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37855634093284607
It took 671.5400331020355 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ██▇▄▄▃▅▂▂▅▂▂▃▂▁▄▃▂▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none ▆█▇▄▃▂▅▃▂▅▂▄▃▃▂▃▅▃▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇█▇▄▄▃▆▂▂▄▂▂▃▂▁▃▃▂▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none ▅█▇▃▂▂▆▃▂▅▂▄▃▃▂▃▆▂▄▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36569
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.24681
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.31332
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.20566
wandb:                         Train loss 3.56079
wandb: 
wandb: 🚀 View run glowing-peony-1377 at: https://wandb.ai/nreints/thesis/runs/zolw1y5t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_181016-zolw1y5t/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_182134-hpcvnzub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-festival-1384
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/hpcvnzub
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29393213987350464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4402579367160797
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.42174822092056274
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5869466662406921
0 6.668296034 	 0.5869466833 	 0.5869466833
epoch_time;  30.484573125839233
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.368847131729126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5152729749679565
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.43595555424690247
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.583605945110321
1 4.0513244092 	 0.5836059158 	 0.5836059158
epoch_time;  30.643789291381836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34576329588890076
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4754573404788971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.40687882900238037
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.5450102686882019
2 3.9027844263 	 0.5450102935 	 0.5450102935
epoch_time;  30.51104974746704
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2635110914707184
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.35286518931388855
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.35000166296958923
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4495421051979065
3 3.8196681803 	 0.4495421126 	 0.4495421126
epoch_time;  30.672558546066284
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.233209028840065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3285031318664551
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34566327929496765
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45195597410202026
4 3.7622301725 	 0.4519559706 	 0.4519559706
epoch_time;  30.86430311203003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23087450861930847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29454323649406433
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3306921720504761
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4045795202255249
5 3.7190138107 	 0.4045795338 	 0.4045795338
epoch_time;  31.08558988571167
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31125175952911377
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.40672045946121216
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.39502057433128357
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4885753393173218
6 3.6938314573 	 0.4885753322 	 0.4885753322
epoch_time;  30.844773054122925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25037044286727905
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3240579664707184
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31371137499809265
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.38426637649536133
7 3.6623007865 	 0.3842663739 	 0.3842663739
epoch_time;  30.75862979888916
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22388999164104462
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29181236028671265
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3105674684047699
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39403972029685974
8 3.6428703665 	 0.3940397108 	 0.3940397108
epoch_time;  31.031726360321045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28944942355155945
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38319504261016846
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3585747480392456
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.46635371446609497
9 3.6293459345 	 0.4663537103 	 0.4663537103
epoch_time;  30.7698016166687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22869594395160675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2909703850746155
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.305072158575058
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37408116459846497
10 3.6232495266 	 0.3740811735 	 0.3740811735
epoch_time;  30.907408952713013
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2830517888069153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3484061658382416
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3182935118675232
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3892580270767212
11 3.6105614871 	 0.3892580187 	 0.3892580187
epoch_time;  30.903289794921875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2589518427848816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32113730907440186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3411460816860199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4076526165008545
12 3.5975174023 	 0.4076526126 	 0.4076526126
epoch_time;  31.00473403930664
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24431684613227844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.31262293457984924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3171001076698303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3953871428966522
13 3.5904202987 	 0.3953871443 	 0.3953871443
epoch_time;  31.091559648513794
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22370970249176025
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.26840877532958984
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2909475564956665
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.345210999250412
14 3.5844716645 	 0.3452110084 	 0.3452110084
epoch_time;  30.48819851875305
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24936693906784058
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.34021925926208496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3333618640899658
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4450382590293884
15 3.5729344083 	 0.4450382542 	 0.4450382542
epoch_time;  30.283328771591187
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3248327374458313
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.38517725467681885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3417086899280548
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.40314093232154846
16 3.5720140461 	 0.4031409186 	 0.4031409186
epoch_time;  30.68644881248474
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23858442902565002
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.30827003717422485
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.30593377351760864
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.386661559343338
17 3.5601610766 	 0.3866615502 	 0.3866615502
epoch_time;  30.830808877944946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.268953800201416
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.32087403535842896
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3277008533477783
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3897060453891754
18 3.5692889937 	 0.3897060497 	 0.3897060497
epoch_time;  30.82076597213745
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20568306744098663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2467951774597168
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31332719326019287
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36567938327789307
19 3.5607931645 	 0.3656793955 	 0.3656793955
epoch_time;  30.907515287399292
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.20565690100193024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.24681371450424194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3133157789707184
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3656873404979706
It took 677.923172712326 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
eucl_motion
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▄▂▂▂▂▁▁▁▂▁▁▂▁▂▂▃▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▄▃▄▁▂▁▂▂▂▃▁▂▂▃▂▂▃▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▄▄▄▃▂▃▂▁▁▁▂▂▁▂▁▂▂▃▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▃▅▂▃▂▂▂▃▃▃▂▃▄▃▃▃▁▁
wandb:                         Train loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 0.36959
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.2989
wandb:    Test loss t(0, 0)_r(-5, 5)_none 0.28647
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2104
wandb:                         Train loss 3.56551
wandb: 
wandb: 🚀 View run beaming-festival-1384 at: https://wandb.ai/nreints/thesis/runs/hpcvnzub
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_182134-hpcvnzub/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3616075813770294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5419851541519165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.4702318012714386
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.615387499332428
0 6.7278213863 	 0.615387499 	 0.615387499
epoch_time;  31.30667471885681
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30101367831230164
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.43311697244644165
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3754029870033264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.49322178959846497
1 4.0808163322 	 0.4932217985 	 0.4932217985
epoch_time;  30.981730937957764
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2973770797252655
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.39431631565093994
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3574564456939697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4334578216075897
2 3.9220816187 	 0.4334578231 	 0.4334578231
epoch_time;  30.68881392478943
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2592868208885193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.36599960923194885
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3671812415122986
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.45617470145225525
3 3.8304860729 	 0.4561746958 	 0.4561746958
epoch_time;  30.486016511917114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30430859327316284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.383136123418808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.327470600605011
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3877786099910736
4 3.7667066901 	 0.3877786172 	 0.3877786172
epoch_time;  30.662630796432495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2229667603969574
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.27709242701530457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.309591680765152
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3586603105068207
5 3.728638935 	 0.3586603113 	 0.3586603113
epoch_time;  30.462512254714966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24376866221427917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.33187663555145264
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.32773280143737793
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.39547374844551086
6 3.6926092666 	 0.3954737483 	 0.3954737483
epoch_time;  30.768999099731445
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.22733071446418762
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2892691195011139
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3132117986679077
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3619304299354553
7 3.6707691769 	 0.3619304348 	 0.3619304348
epoch_time;  30.857450008392334
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23684941232204437
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3038416802883148
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29359525442123413
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3473760187625885
8 3.6560139588 	 0.3473760244 	 0.3473760244
epoch_time;  30.850792169570923
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23931792378425598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29815948009490967
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2899428606033325
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.33580148220062256
9 3.6442347109 	 0.33580147 	 0.33580147
epoch_time;  30.83198642730713
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2451745718717575
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3126601278781891
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29502367973327637
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3525364100933075
10 3.6295350319 	 0.3525364231 	 0.3525364231
epoch_time;  30.614960432052612
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2552628517150879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3350886404514313
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.301990270614624
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3724253475666046
11 3.613569855 	 0.3724253474 	 0.3724253474
epoch_time;  30.459826707839966
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24363744258880615
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2827567756175995
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3198215663433075
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3529427945613861
12 3.6043811093 	 0.3529428018 	 0.3529428018
epoch_time;  30.6347815990448
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2337295264005661
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.2992672622203827
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29193493723869324
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.34804216027259827
13 3.6060439764 	 0.3480421736 	 0.3480421736
epoch_time;  30.90522313117981
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2462754100561142
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3168179988861084
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.31135374307632446
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.37216585874557495
14 3.5878790544 	 0.3721658655 	 0.3721658655
epoch_time;  30.91287589073181
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2662941813468933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3342093527317047
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.29794782400131226
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3492298722267151
15 3.582980043 	 0.3492298848 	 0.3492298848
epoch_time;  30.826961040496826
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25281065702438354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3195044994354248
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3026279807090759
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.3634648621082306
16 3.5751044003 	 0.3634648503 	 0.3634648503
epoch_time;  30.417729139328003
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24303491413593292
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3162968158721924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.3062465488910675
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36810076236724854
17 3.5801753184 	 0.3681007591 	 0.3681007591
epoch_time;  30.673444271087646
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24984483420848846
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.3485575020313263
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.34450796246528625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.4318534731864929
18 3.5726596881 	 0.4318534645 	 0.4318534645
epoch_time;  30.591964721679688
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.21041461825370789
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29887694120407104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.28648555278778076
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.36956730484962463
19 3.5655073761 	 0.3695672937 	 0.3695672937
epoch_time;  30.887229442596436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2104032337665558
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.29890432953834534
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 0.2864682674407959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 0.369592547416687
It took 667.9460589885712 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2139744
Array Job ID: 2137927_23
Cluster: snellius
User/Group: nreints/nreints
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-09:49:30 core-walltime
Job Wall-clock time: 01:52:45
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 31.25 GB (31.25 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
