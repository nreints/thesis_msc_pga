wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230125_163857-hr9rgdb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run auspicious-envelope-1136
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ðŸš€ View run at https://wandb.ai/nreints/thesis/runs/hr9rgdb6
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: / 0.156 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: Test loss t(-10, 10)_r(-5, 5)_none â–ˆâ–†â–â–â–‚â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:  Test loss t(-10, 10)_r(0, 0)_none â–ˆâ–„â–ƒâ–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–
wandb:    Test loss t(0, 0)_r(-5, 5)_none â–ˆâ–…â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:     Test loss t(0, 0)_r(0, 0)_none â–ˆâ–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:                         Train loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 34.6026
wandb:  Test loss t(-10, 10)_r(0, 0)_none 2.4052
wandb:    Test loss t(0, 0)_r(-5, 5)_none 19.99082
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.47789
wandb:                         Train loss 1.5044
wandb: 
wandb: ðŸš€ View run auspicious-envelope-1136 at: https://wandb.ai/nreints/thesis/runs/hr9rgdb6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230125_163857-hr9rgdb6/logs
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.452235698699951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 5.754554748535156
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 45.61268615722656
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 77.98729705810547
0 7.8486837233 	 77.9872941301 	 78.0181112753
epoch_time;  44.62426567077637
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.2828264236450195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.73720383644104
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 34.69231033325195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 66.91616821289062
1 4.0795336435 	 66.9161687078 	 66.9266786318
epoch_time;  43.44747233390808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.9512876868247986
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.2107160091400146
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.054736137390137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.70187759399414
2 2.6885985502 	 32.7018765836 	 32.7029534417
epoch_time;  42.25330877304077
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.0170390605926514
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.7912771701812744
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 15.867460250854492
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 31.022830963134766
3 2.1771687357 	 31.0228304476 	 31.0229043497
epoch_time;  43.26728129386902
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7755571603775024
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.3034446239471436
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.31948471069336
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 40.26212692260742
4 1.9089282739 	 40.2621252111 	 40.2622413429
epoch_time;  42.525906801223755
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5716330409049988
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6901817321777344
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 22.312545776367188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 39.186180114746094
5 1.7570812352 	 39.1861803209 	 39.1866026182
epoch_time;  42.19184684753418
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7043152451515198
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.0391347408294678
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.096837997436523
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.81431579589844
6 1.6846286616 	 32.8143158784 	 32.814983636
epoch_time;  42.36430859565735
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.710930347442627
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.182983636856079
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.26858139038086
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.28498840332031
7 1.6365693234 	 35.2849873311 	 35.2860061233
epoch_time;  42.288124799728394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5936501026153564
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7769861221313477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 18.934892654418945
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.616233825683594
8 1.6028475029 	 32.6162320524 	 32.6175596495
epoch_time;  42.2923367023468
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5555035471916199
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.7544195652008057
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.76397705078125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 35.13920974731445
9 1.585162432 	 35.1392103041 	 35.141015625
epoch_time;  42.09640860557556
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.581320583820343
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.692580461502075
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.987722396850586
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.15635681152344
10 1.5693013909 	 34.1563555743 	 34.1585963894
epoch_time;  42.55338907241821
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49462077021598816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6518149375915527
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.679841995239258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.891056060791016
11 1.5565219432 	 34.8910552154 	 34.8935335726
epoch_time;  42.52189302444458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5639270544052124
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.80145525932312
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.774202346801758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 36.65842819213867
12 1.5489755681 	 36.6584274704 	 36.6612647804
epoch_time;  42.116604804992676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49096935987472534
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.691507577896118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.787717819213867
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.87334060668945
13 1.5385137208 	 34.8733424831 	 34.8765677787
epoch_time;  42.402310371398926
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5445089936256409
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.727426767349243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.753450393676758
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.953792572021484
14 1.5316078635 	 34.9537927576 	 34.9572846284
epoch_time;  42.359108448028564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.532185435295105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6596522331237793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.164981842041016
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 33.07838439941406
15 1.5263071854 	 33.0783836571 	 33.0821552998
epoch_time;  42.362693071365356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.531901478767395
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.644192695617676
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.319717407226562
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.488929748535156
16 1.5181440949 	 34.4889305321 	 34.4929766681
epoch_time;  42.53465127944946
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5156723260879517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.6122610569000244
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.42181396484375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.939720153808594
17 1.5099094316 	 34.9397197002 	 34.9439954603
epoch_time;  43.99321937561035
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6333403587341309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.853667736053467
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 20.46661376953125
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.563331604003906
18 1.5110481484 	 34.5633313978 	 34.5678103885
epoch_time;  43.11214876174927
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.477649062871933
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.4043526649475098
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.9787540435791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.60383224487305
19 1.5044042837 	 34.603832348 	 34.608984375
epoch_time;  41.75803995132446
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.477885365486145
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.405202865600586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 19.99082374572754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.602603912353516
It took 987.7598493099213 seconds to train & eval the model.
Traceback (most recent call last):
  File "/gpfs/home2/nreints/MScThesis/code/fcnn.py", line 439, in <module>
    torch.save(
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 422, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 309, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/nreints/.conda/envs/thesis/lib/python3.10/site-packages/torch/serialization.py", line 287, in __init__
    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory models/fcnn does not exist.
srun: error: gcn15: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2135333.0

JOB STATISTICS
==============
Job ID: 2135333
Array Job ID: 2135328_5
Cluster: snellius
User/Group: nreints/nreints
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:09:06
CPU Efficiency: 22.58% of 05:06:00 core-walltime
Job Wall-clock time: 00:17:00
Memory Utilized: 4.05 GB
Memory Efficiency: 12.96% of 31.25 GB
