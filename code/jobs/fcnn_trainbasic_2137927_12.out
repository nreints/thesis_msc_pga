wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_142005-iuqjjby8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-rat-1246
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/iuqjjby8
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: | 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: / 0.155 MB of 0.155 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▄▂█▃▄▂▃▄▁▂▂▆▄▂▅▅▂▄▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▇▅▆▃▅▂▃▂▂▂▃▂▂▂▃▃▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▄▂█▃▄▂▃▄▁▂▂▇▄▃▆▆▂▅▄▄
wandb:     Test loss t(0, 0)_r(0, 0)_none ▇█▅▆▄▆▂▃▃▃▃▃▂▂▃▃▃▁▁▂▂
wandb:                         Train loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 8.40005
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.54344
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.9171
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.26231
wandb:                         Train loss 3.02676
wandb: 
wandb: 🚀 View run dazzling-rat-1246 at: https://wandb.ai/nreints/thesis/runs/iuqjjby8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_142005-iuqjjby8/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_143352-wzubfqob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-dragon-1254
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/wzubfqob
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4470730721950531
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9500558376312256
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.058647155761719
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.760477066040039
0 8.7144582822 	 12.760476932 	 12.7661911423
epoch_time;  39.389914989471436
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4655880033969879
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8614087104797363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.205883026123047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.898749351501465
1 4.6064496102 	 8.8987489443 	 8.899469489
epoch_time;  38.515467166900635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.367136150598526
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7352176904678345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.890068054199219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.073659420013428
2 3.9033113866 	 7.0736592061 	 7.0740141997
epoch_time;  38.16909432411194
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3916797935962677
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8263517618179321
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.474376678466797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.991792678833008
3 3.5938809136 	 12.991792916 	 12.9920357369
epoch_time;  37.986648082733154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32530108094215393
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6556416153907776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.52839469909668
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.326544761657715
4 3.4203989162 	 8.3265446843 	 8.3269814981
epoch_time;  37.97161340713501
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3896828293800354
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7552045583724976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.298145771026611
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.521546363830566
5 3.3185147522 	 9.5215463999 	 9.5221125422
epoch_time;  38.597184896469116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2818557620048523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5397972464561462
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.050484657287598
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.397467613220215
6 3.2497715624 	 7.3974675359 	 7.3984929265
epoch_time;  38.58395743370056
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31479543447494507
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6140236258506775
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.388066291809082
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.028643608093262
7 3.2044519878 	 8.0286436339 	 8.0297805374
epoch_time;  38.62532424926758
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2972991168498993
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5582861304283142
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.3723297119140625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.121973037719727
8 3.1708574444 	 9.1219733161 	 9.1235747466
epoch_time;  38.282511472702026
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30642858147621155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5612175464630127
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.451131820678711
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.3625054359436035
9 3.1357950819 	 6.3625052787 	 6.3644234322
epoch_time;  38.464022636413574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31412455439567566
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5943623185157776
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0024237632751465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.154018402099609
10 3.116788478 	 7.1540184227 	 7.1560309333
epoch_time;  37.98691987991333
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31831303238868713
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6138116717338562
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.112959861755371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.145493507385254
11 3.1064734126 	 7.145493296 	 7.1477242135
epoch_time;  37.77624225616455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2796085774898529
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5680851936340332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.717606067657471
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.231122016906738
12 3.0896344866 	 11.2311219911 	 11.2331925676
epoch_time;  38.412782430648804
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2840938866138458
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5416604280471802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.347903251647949
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.087018013000488
13 3.0731704495 	 9.0870183171 	 9.0896616343
epoch_time;  38.248708724975586
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29683342576026917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5569779276847839
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.494880676269531
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.655452251434326
14 3.0682585451 	 7.655452254 	 7.6584459459
epoch_time;  38.20777201652527
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2997954189777374
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6151293516159058
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.234903335571289
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.526814460754395
15 3.0581557184 	 10.5268145587 	 10.5293938714
epoch_time;  38.263964891433716
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3196147680282593
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.656262993812561
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.2203216552734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.969720840454102
16 3.0506235586 	 9.9697212838 	 9.9730006862
epoch_time;  38.24124836921692
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24095316231250763
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5016953349113464
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.230372905731201
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.517224311828613
17 3.0464931687 	 7.517224451 	 7.5206635346
epoch_time;  37.06491422653198
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24983268976211548
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5297149419784546
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.6690449714660645
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.52972412109375
18 3.0372683887 	 9.5297237912 	 9.5332301784
epoch_time;  37.885149002075195
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26246652007102966
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5387313365936279
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.919306755065918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.401033401489258
19 3.0267553 	 8.4010333087 	 8.4048075908
epoch_time;  38.47009468078613
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2623083293437958
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5434412956237793
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.917099952697754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.400053024291992
It took 826.9986507892609 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▃▁▂▂▁▂▃▄▄▃▁▂▃▂▃▇▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▂▁▂▁▂▂▂▂▁▂▁▁▁▁▂▂▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▇▅▃▁▂▂▁▃▄▄▅▄▂▃▃▃▄█▃▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 8.08185
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.52518
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.71374
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2435
wandb:                         Train loss 2.99466
wandb: 
wandb: 🚀 View run dazzling-dragon-1254 at: https://wandb.ai/nreints/thesis/runs/wzubfqob
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_143352-wzubfqob/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_144734-l4q8r3f7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-peony-1262
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/l4q8r3f7
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8702796697616577
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.778235912322998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.461548805236816
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.003270149230957
0 7.8165523707 	 14.0032701647 	 14.0083034206
epoch_time;  38.044328451156616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4540257751941681
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9560428857803345
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.402079105377197
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.51852798461914
1 4.6631510637 	 11.5185282939 	 11.5189941406
epoch_time;  38.05698752403259
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.41460028290748596
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7534673810005188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.76253080368042
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.829380989074707
2 3.9411417781 	 8.8293813345 	 8.8296782622
epoch_time;  38.20384764671326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31050223112106323
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5776334404945374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.642866134643555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.3255391120910645
3 3.5758455828 	 7.3255390889 	 7.3260121938
epoch_time;  38.17804312705994
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.393711656332016
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7498781085014343
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.209287166595459
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.9076008796691895
4 3.4009493219 	 7.9076006915 	 7.9082433752
epoch_time;  37.91836619377136
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2856886684894562
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5716456770896912
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.2088303565979
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.960643768310547
5 3.2895269788 	 7.9606438714 	 7.9616428685
epoch_time;  38.256995677948
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30554717779159546
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6237176656723022
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.767247676849365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.300369739532471
6 3.2282498307 	 7.3003695101 	 7.301488598
epoch_time;  38.46080660820007
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3034244477748871
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6474514603614807
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.681771755218506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.548346519470215
7 3.1848975965 	 8.5483464421 	 8.5496331292
epoch_time;  39.44461679458618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3221966028213501
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6640617847442627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.331557273864746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.434202194213867
8 3.1464893338 	 9.434202122 	 9.4357138144
epoch_time;  38.51290774345398
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3153766393661499
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6348131895065308
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.746676921844482
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.121514320373535
9 3.1248403251 	 10.1215147276 	 10.1231775232
epoch_time;  38.30395555496216
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28613364696502686
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5406625866889954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.052436351776123
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.279999732971191
10 3.0941982799 	 10.2799996041 	 10.2818412162
epoch_time;  38.05403971672058
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.309581458568573
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6619295477867126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.27848482131958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.348738670349121
11 3.0796966243 	 9.3487383868 	 9.3508478938
epoch_time;  38.04821467399597
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25316885113716125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5381729006767273
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.141669750213623
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.612853527069092
12 3.0615852578 	 7.612853674 	 7.6154837943
epoch_time;  38.276854515075684
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28294533491134644
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5352428555488586
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.642861843109131
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.225969314575195
13 3.0541430185 	 8.2259693043 	 8.2285327808
epoch_time;  38.23227334022522
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2501111626625061
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.533622682094574
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.0677385330200195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.904053688049316
14 3.0232064177 	 8.9040540541 	 8.9067013302
epoch_time;  37.544532775878906
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2570209205150604
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5669657588005066
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.964296817779541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.632732391357422
15 3.0260537883 	 8.6327326594 	 8.6357540646
epoch_time;  38.264232873916626
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2877638041973114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6222249865531921
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.568589210510254
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.673125267028809
16 3.0162782456 	 9.6731253959 	 9.676299224
epoch_time;  38.03460764884949
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3078603446483612
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.683083176612854
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.938826560974121
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.060392379760742
17 3.003815999 	 13.0603924726 	 13.0634356524
epoch_time;  37.93782377243042
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.23905052244663239
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5335943102836609
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.016666889190674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.73129653930664
18 2.9971681671 	 8.7312961888 	 8.7347359322
epoch_time;  37.74043154716492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2434384673833847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5251479148864746
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.711779594421387
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.083569526672363
19 2.9946636182 	 8.0835693359 	 8.0872888514
epoch_time;  38.6194281578064
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.24349582195281982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5251821279525757
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.71373987197876
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.0818452835083
It took 821.7974853515625 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▂▂▂▁▃▃▂▃▅▅▃▇▃▃▅▆▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▂▃▂▁▂▂▂▁▂▂▂▂▂▁▂▂▂▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▃▂▂▂▁▃▄▂▃▆▅▃▇▄▃▅▇▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▂▄▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▁
wandb:                         Train loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 10.0142
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.58222
wandb:    Test loss t(0, 0)_r(-5, 5)_none 6.57505
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2772
wandb:                         Train loss 2.98829
wandb: 
wandb: 🚀 View run lunar-peony-1262 at: https://wandb.ai/nreints/thesis/runs/l4q8r3f7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_144734-l4q8r3f7/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_150114-6ah1owyw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run filigreed-rooster-1270
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/6ah1owyw
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7272638082504272
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5240830183029175
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.834199905395508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.63179874420166
0 7.5413158006 	 14.6317989865 	 14.6382271432
epoch_time;  38.06554388999939
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.500434160232544
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9504005908966064
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.504210472106934
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.742511749267578
1 4.8441351372 	 11.742512141 	 11.743606155
epoch_time;  37.956965923309326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34882524609565735
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6998869776725769
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.845794677734375
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.605403900146484
2 4.1004669251 	 8.6054040857 	 8.6057465424
epoch_time;  38.031867265701294
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4440164864063263
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8674071431159973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.939371585845947
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.438239574432373
3 3.6736198534 	 7.4382396801 	 7.4385326489
epoch_time;  38.017587184906006
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30798158049583435
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.606633186340332
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.768083572387695
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.302676200866699
4 3.4375495885 	 7.3026763091 	 7.3032041807
epoch_time;  38.292699098587036
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2959297001361847
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5788236260414124
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.713057518005371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.319516658782959
5 3.3110080479 	 7.3195167335 	 7.3201455606
epoch_time;  38.20789837837219
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32789480686187744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7034860849380493
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.203450679779053
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.23521614074707
6 3.2375248604 	 6.2352162954 	 6.2363182274
epoch_time;  38.355135917663574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3406287431716919
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6489708423614502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.459161758422852
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.20770263671875
7 3.1678649448 	 8.2077029666 	 8.2088669236
epoch_time;  38.075814962387085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2970617413520813
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.612875759601593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.905404567718506
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.709933280944824
8 3.1390074237 	 8.7099332242 	 8.711237727
epoch_time;  37.815568685531616
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2922457754611969
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5354415774345398
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.812678813934326
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.233183860778809
9 3.1047216169 	 7.2331839897 	 7.2349840319
epoch_time;  38.85624670982361
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30977490544319153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6755848526954651
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.406246662139893
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.232290267944336
10 3.0915892646 	 8.2322899071 	 8.2342819626
epoch_time;  38.57897329330444
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2689596116542816
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6057132482528687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.741274833679199
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.426542282104492
11 3.0717439542 	 11.4265427048 	 11.4285209037
epoch_time;  38.72657084465027
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3047039210796356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6735230684280396
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.013717174530029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.848614692687988
12 3.0470227737 	 10.8486149968 	 10.8508379962
epoch_time;  38.04453158378601
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33925870060920715
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6877408623695374
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.614782810211182
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.388711929321289
13 3.0424431121 	 8.3887114654 	 8.3910684122
epoch_time;  37.913334131240845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3067123591899872
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7100939154624939
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.49660587310791
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.939969062805176
14 3.0351403817 	 12.9399691195 	 12.9425543708
epoch_time;  37.815996170043945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25794076919555664
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5246906280517578
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.995148658752441
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.006707191467285
15 3.0200195312 	 9.0067072688 	 9.0095736117
epoch_time;  38.43961524963379
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29389524459838867
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6064419746398926
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.617284297943115
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.47139835357666
16 3.004761486 	 8.471397936 	 8.4744345175
epoch_time;  38.147688150405884
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3126157224178314
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6079438328742981
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.827648162841797
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.433818817138672
17 2.9988822756 	 10.4338187553 	 10.4369549726
epoch_time;  38.28661251068115
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3214903473854065
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6508966088294983
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.198043823242188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.395727157592773
18 3.0000505803 	 12.3957268792 	 12.3989851668
epoch_time;  38.17194604873657
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27720800042152405
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5827610492706299
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.576760292053223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.011055946350098
19 2.9882881924 	 10.0110562711 	 10.0145811339
epoch_time;  37.75419235229492
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2771979868412018
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5822198987007141
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.575052738189697
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.014201164245605
It took 820.2441074848175 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: / 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▂▁▁▁▃▂▂▃▂▃▃▂▂▃▃▅▃▄▄
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▅▃▁▃▅▂▃▁▁▁▂▁▂▂▂▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▂▁▁▁▃▂▃▄▂▅▄▃▃▄▄▇▄▅▅
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▅▂▄█▂▅▁▁▁▂▁▂▃▂▃▁▁▁▁
wandb:                         Train loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 10.96837
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.7268
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.15666
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.29155
wandb:                         Train loss 2.95125
wandb: 
wandb: 🚀 View run filigreed-rooster-1270 at: https://wandb.ai/nreints/thesis/runs/6ah1owyw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_150114-6ah1owyw/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_151456-bkv6hpce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-snake-1277
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/bkv6hpce
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5582712888717651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3202542066574097
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.08157730102539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.39252281188965
0 7.496491119 	 16.3925226985 	 16.3976852829
epoch_time;  37.9676628112793
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.45222970843315125
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0190294981002808
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.579447746276855
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.568930625915527
1 4.745199835 	 13.568930796 	 13.5695945946
epoch_time;  37.984713554382324
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4303201138973236
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9036998152732849
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.418808937072754
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.531519889831543
2 4.0016879919 	 8.5315198744 	 8.5317389411
epoch_time;  38.123117208480835
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3411332666873932
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7145058512687683
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.994592189788818
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.180243492126465
3 3.6164476399 	 8.1802437447 	 8.1805835621
epoch_time;  38.38134837150574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4190370440483093
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8786277174949646
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.739393711090088
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.558095455169678
4 3.3891849505 	 7.5580955712 	 7.5585666966
epoch_time;  38.073665142059326
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5651893019676208
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0109988451004028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.835015296936035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.724909782409668
5 3.2917736676 	 7.724909602 	 7.7255912162
epoch_time;  38.34893465042114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32343775033950806
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7377095222473145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.126588821411133
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.76203441619873
6 3.2147435813 	 9.7620348131 	 9.7628384977
epoch_time;  38.31400656700134
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42862918972969055
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9023807048797607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.430075645446777
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.635672569274902
7 3.1491512435 	 8.6356722445 	 8.6366936761
epoch_time;  37.885730266571045
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30929651856422424
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7130471467971802
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.878146648406982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.349605560302734
8 3.117121382 	 9.349605416 	 9.350958087
epoch_time;  37.76109194755554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30624276399612427
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6908640265464783
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.581944942474365
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.358260154724121
9 3.0770036391 	 10.358260531 	 10.3597042599
epoch_time;  38.07427668571472
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29548341035842896
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6911174654960632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.571440696716309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.405956268310547
10 3.0582241505 	 8.4059563714 	 8.4080196896
epoch_time;  37.82534670829773
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33799582719802856
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7752667665481567
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.9255290031433105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.701896667480469
11 3.0402926249 	 10.7018970386 	 10.7038778769
epoch_time;  39.02526569366455
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2958703339099884
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.708823025226593
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.526645660400391
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.002573013305664
12 3.0185877465 	 10.0025733742 	 10.0050200591
epoch_time;  38.42987632751465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33315348625183105
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7872028350830078
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.944399356842041
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.960405349731445
13 2.9985596199 	 8.9604056693 	 8.9629176784
epoch_time;  38.79194235801697
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3517324924468994
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8243533372879028
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.161477088928223
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.289426803588867
14 2.9897662597 	 9.2894267314 	 9.292191459
epoch_time;  38.30021333694458
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34534499049186707
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7742661833763123
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.526612758636475
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.650739669799805
15 2.9747611158 	 9.6507396801 	 9.6538026552
epoch_time;  38.84046292304993
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3669441342353821
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8241757750511169
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.5458807945251465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.064238548278809
16 2.9607533614 	 10.0642386772 	 10.0673452016
epoch_time;  38.056944608688354
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29187899827957153
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6970552206039429
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.357596397399902
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.818647384643555
17 2.9538172552 	 12.818647065 	 12.8217852618
epoch_time;  38.01825451850891
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30665576457977295
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7138628363609314
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.8331522941589355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.268162727355957
18 2.9554282197 	 10.2681627428 	 10.2718782992
epoch_time;  38.17617106437683
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29148194193840027
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7270877361297607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.151619911193848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.965951919555664
19 2.9512548531 	 10.9659516206 	 10.9695589633
epoch_time;  38.17259955406189
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29155123233795166
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.726800799369812
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.156657695770264
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.968369483947754
It took 821.7668259143829 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▃▂▂▁▃▁▁▃▃▂▁▃▃▂▃▅▂▂▂
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▃▄▂▂▂▃▂▂▂▁▃▂▂▁▂▁▃▃
wandb:    Test loss t(0, 0)_r(-5, 5)_none ██▄▂▂▁▃▁▁▄▄▃▁▃▃▃▄▆▃▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▃▄▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▂
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 7.44904
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.62232
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.14569
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.33414
wandb:                         Train loss 3.00791
wandb: 
wandb: 🚀 View run floating-snake-1277 at: https://wandb.ai/nreints/thesis/runs/bkv6hpce
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_151456-bkv6hpce/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_152828-lgd7un04
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run thriving-rocket-1285
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/lgd7un04
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6160717010498047
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1709537506103516
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.850214958190918
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.77396011352539
0 7.9037004909 	 14.7739600929 	 14.7821724557
epoch_time;  37.8685097694397
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.40884703397750854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7346602082252502
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.873602867126465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.894408226013184
1 4.7346708779 	 13.8944085198 	 13.8955751161
epoch_time;  37.64184308052063
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42126259207725525
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7392150163650513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.212254047393799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.561001777648926
2 3.9920301392 	 9.5610015044 	 9.5613162479
epoch_time;  37.84045624732971
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.38111913204193115
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6639871597290039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.142249584197998
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.005881309509277
3 3.6281455627 	 8.0058811497 	 8.0063575538
epoch_time;  37.66330075263977
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39745521545410156
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7083057165145874
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.256656646728516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.040380477905273
4 3.4392144391 	 8.0403808594 	 8.0412492082
epoch_time;  37.74471569061279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30666714906692505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5438637733459473
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.5692620277404785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.728498458862305
5 3.3261239395 	 6.7284984692 	 6.7298597181
epoch_time;  37.995222091674805
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30009597539901733
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5839879512786865
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.585920333862305
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.56218433380127
6 3.2456990257 	 8.5621839369 	 8.563421136
epoch_time;  37.72753119468689
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2919561266899109
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5099384188652039
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.67125129699707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.914254665374756
7 3.1990716375 	 6.9142545133 	 6.9159793338
epoch_time;  37.56000471115112
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33036166429519653
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6447876691818237
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.635299205780029
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.801212787628174
8 3.1744912883 	 6.8012127851 	 6.803198902
epoch_time;  37.98493933677673
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31598374247550964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5734227895736694
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.2482428550720215
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.380437850952148
9 3.1372523221 	 9.3804377375 	 9.3823915224
epoch_time;  37.67833971977234
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31050318479537964
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5397753119468689
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.26810884475708
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.246613502502441
10 3.1159967153 	 9.2466137035 	 9.2488703547
epoch_time;  37.81296372413635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29159456491470337
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5439584851264954
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.805928707122803
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.235538482666016
11 3.094689867 	 8.2355382971 	 8.2381717166
epoch_time;  37.859068632125854
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2621702551841736
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48860907554626465
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.671760082244873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.739856719970703
12 3.0790880083 	 6.7398569468 	 6.7428981472
epoch_time;  37.872761964797974
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3139064610004425
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.633204460144043
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.875985622406006
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.561244010925293
13 3.0713822888 	 8.5612443254 	 8.564115947
epoch_time;  38.883835554122925
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.306591659784317
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5520737767219543
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.068958282470703
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.549576759338379
14 3.0595154346 	 8.549576383 	 8.5529798353
epoch_time;  38.39828658103943
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2738037109375
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5288933515548706
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.804502964019775
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.411763191223145
15 3.053629943 	 8.4117636191 	 8.4153181746
epoch_time;  38.38638472557068
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.262407124042511
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.44639405608177185
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.276100158691406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.269448280334473
16 3.0338983555 	 9.2694481102 	 9.2728429846
epoch_time;  37.742177963256836
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.29924631118774414
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5484211444854736
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.577559471130371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.042129516601562
17 3.0357275093 	 11.0421294341 	 11.0456509977
epoch_time;  37.766191244125366
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2721763849258423
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47503748536109924
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.608441352844238
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.924012660980225
18 3.015639546 	 7.9240128801 	 7.9279633393
epoch_time;  37.6825315952301
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3342517912387848
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6224819421768188
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.145659446716309
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.452485084533691
19 3.0079083711 	 7.4524849557 	 7.4564261772
epoch_time;  37.49144625663757
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3341384828090668
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6223194003105164
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.145691394805908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.449041366577148
It took 812.0354812145233 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.024 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: | 0.024 MB of 0.156 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▇▄▃▄▁▃▁▂▂▅▂▃▂▃▂▃▃▄▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▄▆▃▃▂▃▂▁▂▂▂▁▁▁▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▄▃▅▁▃▁▂▂▆▂▃▃▃▂▃▃▄▂▂
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▄▇▃▃▂▃▂▂▂▂▂▂▁▂▁▁▁▁▁
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 8.17798
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.62151
wandb:    Test loss t(0, 0)_r(-5, 5)_none 5.47007
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.27854
wandb:                         Train loss 2.98941
wandb: 
wandb: 🚀 View run thriving-rocket-1285 at: https://wandb.ai/nreints/thesis/runs/lgd7un04
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_152828-lgd7un04/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_154202-20kvujyq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floating-rabbit-1294
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/20kvujyq
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6585507392883301
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3620719909667969
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.855667114257812
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.778582572937012
0 9.021550493 	 14.7785829286 	 14.7869180215
epoch_time;  37.767311573028564
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5155895948410034
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1054482460021973
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.57947826385498
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.28369426727295
1 4.7578359719 	 14.2836940456 	 14.284547878
epoch_time;  37.77917146682739
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.43581530451774597
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.9206100702285767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.800144672393799
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.695842742919922
2 4.0230262462 	 10.695843011 	 10.6960871516
epoch_time;  38.07126021385193
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5809884667396545
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.131701946258545
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.172456741333008
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.864978790283203
3 3.6578847669 	 9.8649783573 	 9.8652647276
epoch_time;  37.574148654937744
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.39396461844444275
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7986027002334595
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.962478160858154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.295784950256348
4 3.4736579373 	 11.2957849451 	 11.296167652
epoch_time;  38.139495849609375
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.35349661111831665
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7431619763374329
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.016242504119873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.880885124206543
5 3.341177019 	 7.8808851087 	 7.8814954603
epoch_time;  37.949355602264404
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3246992528438568
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6712290048599243
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.079921722412109
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.778759002685547
6 3.2535287164 	 9.7787591058 	 9.7796188767
epoch_time;  37.67030572891235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36130860447883606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7478033304214478
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.0553998947143555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.8914642333984375
7 3.2072788618 	 7.8914643159 	 7.892791913
epoch_time;  37.871598958969116
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33846747875213623
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7291703224182129
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.8165459632873535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.168521881103516
8 3.162983124 	 9.1685223554 	 9.1698737067
epoch_time;  37.96616506576538
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2906951606273651
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.59710294008255
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4333391189575195
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.52048397064209
9 3.128944929 	 8.5204840583 	 8.522175227
epoch_time;  38.305790424346924
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2930571436882019
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6176506876945496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.487759113311768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.818033218383789
10 3.1118841772 	 11.8180334143 	 11.8198228991
epoch_time;  37.762152671813965
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34415581822395325
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6882809400558472
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.5059614181518555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.448001861572266
11 3.0774691097 	 8.4480020059 	 8.4501167916
epoch_time;  37.760223388671875
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.32981252670288086
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6576212048530579
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.294686317443848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.773950576782227
12 3.0662029372 	 9.7739508552 	 9.7762523754
epoch_time;  37.84286022186279
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30798155069351196
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6154969334602356
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.133695602416992
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.302227973937988
13 3.0505045463 	 9.3022282781 	 9.3049923459
epoch_time;  37.969669818878174
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.261065810918808
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.559059202671051
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.1574506759643555
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.711392402648926
14 3.0379213037 	 9.7113927893 	 9.7140328072
epoch_time;  37.84879207611084
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28963807225227356
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5713952779769897
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.611458778381348
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.610601425170898
15 3.0290069477 	 8.6106009818 	 8.613618428
epoch_time;  37.83358645439148
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2711760997772217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5846138000488281
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.3316874504089355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.740252494812012
16 3.0157162428 	 9.7402521907 	 9.7432617188
epoch_time;  39.24912929534912
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27170249819755554
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5638431310653687
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.358085632324219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.39836597442627
17 3.0049320704 	 9.3983662373 	 9.4017993824
epoch_time;  38.43999481201172
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2870746850967407
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6045697331428528
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.859177112579346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.501785278320312
18 3.0026377766 	 10.5017855258 	 10.5050464527
epoch_time;  37.97318077087402
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2785314917564392
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6207608580589294
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.472597599029541
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.17783260345459
19 2.9894112062 	 8.1778326911 	 8.1815548459
epoch_time;  37.56951689720154
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.27854493260383606
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6215053200721741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.470067977905273
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.177979469299316
It took 814.5294835567474 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none ▇▅▃▃▂▄▂▁▂▄▃█▂▃▄▅▄▃▅▃▃
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▅▄▂▂▂▂▁▂▂▃▃▂▃▁▁▃▃▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none ▆▅▃▃▂▄▂▁▂▃▃█▃▃▄▅▄▃▅▃▃
wandb:     Test loss t(0, 0)_r(0, 0)_none █▅▇▄▃▂▂▂▁▂▁▃▄▂▃▁▁▂▃▁▁
wandb:                         Train loss █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 9.55321
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.49888
wandb:    Test loss t(0, 0)_r(-5, 5)_none 6.6364
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2583
wandb:                         Train loss 3.0347
wandb: 
wandb: 🚀 View run floating-rabbit-1294 at: https://wandb.ai/nreints/thesis/runs/20kvujyq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_154202-20kvujyq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_155521-8xxrdg2r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fortuitous-envelope-1301
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/8xxrdg2r
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5274812579154968
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1476253271102905
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.076026916503906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 15.335346221923828
0 7.6928621678 	 15.3353462838 	 15.3409865921
epoch_time;  37.59897017478943
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42613744735717773
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8083018660545349
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.520973205566406
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.80294418334961
1 4.7229342761 	 12.802944204 	 12.8035380595
epoch_time;  37.571420431137085
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5018882751464844
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8906530141830444
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.467103481292725
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.114229202270508
2 3.9988148353 	 10.1142287796 	 10.1143976985
epoch_time;  37.226322412490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.377251535654068
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8101239800453186
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.313246726989746
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.531249046325684
3 3.6372268666 	 9.5312486803 	 9.5315099768
epoch_time;  37.026888608932495
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3199596405029297
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5979583263397217
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.680816650390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.754129409790039
4 3.4508452522 	 8.7541292758 	 8.7546327333
epoch_time;  37.08804941177368
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.295917272567749
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6245203018188477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.169832229614258
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.04428482055664
5 3.324713643 	 11.04428447 	 11.04490142
epoch_time;  37.066473960876465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2872595191001892
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6138278245925903
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.441482067108154
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.21786880493164
6 3.2732212942 	 8.2178684544 	 8.2188344595
epoch_time;  37.011651277542114
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3080267310142517
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5590375661849976
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.677628040313721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.805471420288086
7 3.2205358465 	 6.8054713894 	 6.8068280194
epoch_time;  37.24610185623169
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2682057023048401
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5356215238571167
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.246701240539551
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.076817512512207
8 3.181979149 	 8.0768171981 	 8.0784305057
epoch_time;  37.50397276878357
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2837902903556824
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5608952641487122
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.025552272796631
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.497982025146484
9 3.1470976192 	 10.4979822107 	 10.499485985
epoch_time;  37.11931872367859
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2757398784160614
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5712968707084656
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.2519636154174805
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.097171783447266
10 3.12642914 	 9.0971719278 	 9.0991784998
epoch_time;  36.88175916671753
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3289529085159302
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7083858847618103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 11.296675682067871
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.733911514282227
11 3.1137642418 	 16.7339117927 	 16.7356656461
epoch_time;  36.59011244773865
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.36560121178627014
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6704202890396118
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.26041841506958
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.801224708557129
12 3.0939169479 	 8.8012246622 	 8.8036482527
epoch_time;  36.99576449394226
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.28804436326026917
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6289835572242737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.525818347930908
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.276876449584961
13 3.0917866267 	 9.2768765836 	 9.2793470228
epoch_time;  36.7749924659729
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3226451277732849
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6719883680343628
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.75724458694458
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.860615730285645
14 3.0676562103 	 10.8606161582 	 10.8630945154
epoch_time;  36.771342515945435
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2626311779022217
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5442304611206055
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.237369537353516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.874387741088867
15 3.0612738421 	 11.8743876689 	 11.877240815
epoch_time;  36.89154577255249
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.26732638478279114
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5445585250854492
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.130035400390625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.36479663848877
16 3.0545241886 	 10.3647962416 	 10.3676427893
epoch_time;  36.603273153305054
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2940622866153717
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6596969366073608
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.371750831604004
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.368376731872559
17 3.0406199089 	 9.3683771907 	 9.3716024863
epoch_time;  36.77359223365784
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3264888525009155
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6602869033813477
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.326308250427246
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.090583801269531
18 3.0448319546 	 12.0905840899 	 12.094136666
epoch_time;  37.821921586990356
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25839757919311523
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49925804138183594
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.633846282958984
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.553840637207031
19 3.0346963995 	 9.553840266 	 9.5575261296
epoch_time;  37.13940668106079
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.25830069184303284
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.49888136982917786
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.636397361755371
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.553206443786621
It took 799.0006949901581 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▅▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▅▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Train loss █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 12.06689
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.20503
wandb:    Test loss t(0, 0)_r(-5, 5)_none 8.01482
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.5016
wandb:                         Train loss 2.91457
wandb: 
wandb: 🚀 View run fortuitous-envelope-1301 at: https://wandb.ai/nreints/thesis/runs/8xxrdg2r
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_155521-8xxrdg2r/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_160827-blj4cbqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-laughter-1308
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/blj4cbqz
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.5517795085906982
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 4.343108177185059
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 35.665199279785156
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 48.961368560791016
0 12.2259462805 	 48.9613703547 	 48.9927364865
epoch_time;  36.50416898727417
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7669155597686768
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.8968716859817505
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 21.864356994628906
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 32.82212448120117
1 6.6546129189 	 32.8221230997 	 32.8317382812
epoch_time;  36.118215799331665
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6305872201919556
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5310823917388916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.578433990478516
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.80951690673828
2 4.5930759272 	 16.8095162057 	 16.8111908784
epoch_time;  36.26283073425293
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5162246227264404
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.260012149810791
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.281792640686035
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.710225105285645
3 3.9633511367 	 14.7102248733 	 14.7108451225
epoch_time;  36.83687615394592
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4358804523944855
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0389398336410522
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 9.026741027832031
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.184286117553711
4 3.6553749286 	 14.1842865815 	 14.1849714949
epoch_time;  36.571014165878296
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.44887855648994446
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0547716617584229
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.79243278503418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.146034240722656
5 3.4485856704 	 14.1460343644 	 14.1467799831
epoch_time;  36.74668574333191
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4790596663951874
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1933138370513916
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.573461532592773
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.810502052307129
6 3.300501571 	 13.8105020059 	 13.8113901499
epoch_time;  36.58804750442505
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5187322497367859
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2915723323822021
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.345693111419678
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.735602378845215
7 3.2182253539 	 11.7356023015 	 11.7369813661
epoch_time;  36.52047920227051
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4652455449104309
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1465460062026978
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.53706169128418
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.766143798828125
8 3.1560995816 	 11.7661436339 	 11.7676573057
epoch_time;  36.98355412483215
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49563783407211304
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1985315084457397
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.3325419425964355
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.506355285644531
9 3.1085138134 	 11.5063555743 	 11.5081978463
epoch_time;  36.82624650001526
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4506334960460663
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0320490598678589
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.217427253723145
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.837474822998047
10 3.0678882844 	 12.8374749261 	 12.8393554688
epoch_time;  36.69545602798462
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46365389227867126
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1224111318588257
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.103759288787842
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.228011131286621
11 3.0457738051 	 11.2280115076 	 11.2301309122
epoch_time;  36.40905499458313
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.46092891693115234
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0925525426864624
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.297760963439941
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.501444816589355
12 3.0058547467 	 11.5014450486 	 11.503799356
epoch_time;  36.40360116958618
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.49507880210876465
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1985704898834229
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.461822509765625
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.93621826171875
13 2.9904066123 	 12.9362185916 	 12.9388460726
epoch_time;  36.43740725517273
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5246783494949341
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2292473316192627
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.495856761932373
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.602598190307617
14 2.9648563124 	 11.6025984481 	 11.6052312078
epoch_time;  36.407963037490845
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4763047397136688
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0998140573501587
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.96041202545166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.654520988464355
15 2.9609543601 	 13.6545212204 	 13.6573770059
epoch_time;  36.47859048843384
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4483550190925598
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0598289966583252
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.717049598693848
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.726665496826172
16 2.9462140186 	 11.726665435 	 11.7298471812
epoch_time;  36.39625096321106
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5201559662818909
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2430394887924194
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.7520670890808105
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.807575225830078
17 2.9330664591 	 11.8075749578 	 11.8107857369
epoch_time;  36.828327655792236
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5288339853286743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.246801733970642
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.792191028594971
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.387510299682617
18 2.9202190753 	 10.3875105574 	 10.3910367399
epoch_time;  36.87093806266785
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5015281438827515
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2059228420257568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.011009216308594
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.065285682678223
19 2.9145653272 	 12.0652858425 	 12.0687843117
epoch_time;  36.90573716163635
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.501600980758667
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2050338983535767
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.014824867248535
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.066893577575684
It took 786.222181558609 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▆▂▂▂▂▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▆▃▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁
wandb:                         Train loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 13.10161
wandb:  Test loss t(-10, 10)_r(0, 0)_none 1.05249
wandb:    Test loss t(0, 0)_r(-5, 5)_none 8.32551
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.48034
wandb:                         Train loss 2.85806
wandb: 
wandb: 🚀 View run vivid-laughter-1308 at: https://wandb.ai/nreints/thesis/runs/blj4cbqz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_160827-blj4cbqz/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:364: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims_train, int(0.8 * n_sims_train)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230126_162140-2mxb0j4g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run luminous-rabbit-1316
wandb: ⭐️ View project at https://wandb.ai/nreints/thesis
wandb: 🚀 View run at https://wandb.ai/nreints/thesis/runs/2mxb0j4g
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 2.6718735694885254
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 3.8259036540985107
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 33.55147171020508
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 45.94770431518555
0 12.0936428889 	 45.9477037584 	 45.9886138091
epoch_time;  38.24353814125061
	 Logging test loss: t(0, 0)_r(0, 0)_none => 1.4961191415786743
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 2.933915853500366
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 25.1207218170166
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 34.65229034423828
1 7.2649228271 	 34.6522909628 	 34.6774783573
epoch_time;  37.2580931186676
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7167503833770752
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.595535397529602
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 14.083871841430664
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 21.70505714416504
2 5.3461193377 	 21.7050570101 	 21.7154732369
epoch_time;  37.518019676208496
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6431791186332703
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.4413503408432007
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 10.358818054199219
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 16.311607360839844
3 4.2185106529 	 16.3116065773 	 16.3141113281
epoch_time;  36.80182361602783
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8098916411399841
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5607728958129883
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.525440216064453
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.470905303955078
4 3.6936162433 	 13.4709050359 	 13.471948902
epoch_time;  36.778154373168945
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.733771562576294
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5212764739990234
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.717921257019043
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.439634323120117
5 3.4316743452 	 14.439633921 	 14.4403175148
epoch_time;  36.69779849052429
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8363683223724365
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7600831985473633
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.95152473449707
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.579795837402344
6 3.2793471247 	 14.5797957137 	 14.580501742
epoch_time;  36.66099309921265
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8574133515357971
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.7385156154632568
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.941210746765137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.718127250671387
7 3.174285992 	 12.7181271115 	 12.7191538218
epoch_time;  36.57031774520874
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6902524828910828
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.443065881729126
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.27819538116455
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.266825675964355
8 3.1054029626 	 13.2668259079 	 13.2679581926
epoch_time;  36.82634949684143
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.8036220073699951
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5989699363708496
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.628321170806885
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.143219947814941
9 3.0587597061 	 12.143219489 	 12.1447001689
epoch_time;  37.043864250183105
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6789659261703491
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3432202339172363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.83568811416626
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.909927368164062
10 3.0004271221 	 10.9099272857 	 10.9116177946
epoch_time;  36.67332649230957
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5760548114776611
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2825103998184204
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.053759574890137
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.05780029296875
11 2.9807540491 	 13.0578006229 	 13.0595663535
epoch_time;  36.68980360031128
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6969444751739502
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3793028593063354
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.464401245117188
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.292543411254883
12 2.9552533376 	 13.2925438133 	 13.2946223079
epoch_time;  36.77377772331238
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6554067134857178
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2969212532043457
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.32966947555542
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.352295875549316
13 2.9275350293 	 11.3522962416 	 11.3547587627
epoch_time;  36.51346135139465
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7271663546562195
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.5232809782028198
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.17902660369873
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.931368827819824
14 2.9205397475 	 12.9313687711 	 12.933733636
epoch_time;  36.79110884666443
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5897606015205383
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.2451459169387817
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.924745559692383
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.10504150390625
15 2.9097734269 	 14.105041174 	 14.1073163007
epoch_time;  36.524325370788574
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.6420848369598389
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.316300392150879
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.51187515258789
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.191577911376953
16 2.8922204268 	 13.1915778083 	 13.1945180532
epoch_time;  37.10573768615723
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4901323914527893
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0501184463500977
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.497043132781982
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 11.59519100189209
17 2.8809102858 	 11.5951910895 	 11.5985061233
epoch_time;  37.14784479141235
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.5328353643417358
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.1056854724884033
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.965188980102539
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.156545639038086
18 2.8772870208 	 12.1565456081 	 12.1597603463
epoch_time;  36.879239559173584
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.48053932189941406
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0525885820388794
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.330389976501465
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.1084566116333
19 2.8580579299 	 13.1084565034 	 13.1116620038
epoch_time;  37.03292155265808
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4803384840488434
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.0524914264678955
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.325511932373047
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 13.101606369018555
It took 792.6868686676025 seconds to train & eval the model.
Number of train simulations: 8000
Number of test simulations: 2000
dual_quat
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                              Epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: Test loss t(-10, 10)_r(-5, 5)_none █▆▄▃▁▂▃▄▄▅▂▂▂▄▄▃▂▄▃▅▅
wandb:  Test loss t(-10, 10)_r(0, 0)_none █▄▄▂▃▂▂▂▂▂▁▃▂▂▃▁▁▁▁▂▂
wandb:    Test loss t(0, 0)_r(-5, 5)_none █▇▅▄▁▃▄▅▅▆▃▃▃▆▅▅▃▅▄▇▇
wandb:     Test loss t(0, 0)_r(0, 0)_none █▄▄▂▃▂▂▂▂▂▁▃▂▂▂▁▁▁▁▂▂
wandb:                         Train loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                              Epoch 19
wandb: Test loss t(-10, 10)_r(-5, 5)_none 10.8245
wandb:  Test loss t(-10, 10)_r(0, 0)_none 0.51335
wandb:    Test loss t(0, 0)_r(-5, 5)_none 7.56208
wandb:     Test loss t(0, 0)_r(0, 0)_none 0.2857
wandb:                         Train loss 3.05275
wandb: 
wandb: 🚀 View run luminous-rabbit-1316 at: https://wandb.ai/nreints/thesis/runs/2mxb0j4g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230126_162140-2mxb0j4g/logs
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.7139282822608948
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 1.3796623945236206
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 8.40219783782959
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 14.226408958435059
0 8.3208473944 	 14.2264094172 	 14.2321077914
epoch_time;  36.44188380241394
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.42564770579338074
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.8294190168380737
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.810104846954346
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 12.05296516418457
1 4.6676401437 	 12.0529653188 	 12.053482633
epoch_time;  36.42430520057678
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4606102705001831
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7904108166694641
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.3907999992370605
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.779525756835938
2 3.9826870081 	 9.7795258393 	 9.7797158731
epoch_time;  37.03479266166687
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3073921501636505
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.573421061038971
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.486656665802002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.58521842956543
3 3.6414832113 	 8.5852182749 	 8.5855541332
epoch_time;  37.47852826118469
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.37282606959342957
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7219991087913513
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 3.7828927040100098
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 5.9332122802734375
4 3.4529600357 	 5.9332123628 	 5.9338820999
epoch_time;  37.149415016174316
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3105420768260956
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.532817006111145
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.903404712677002
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.577939510345459
5 3.3516626687 	 7.5779395851 	 7.5787188556
epoch_time;  37.15174388885498
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.34829196333885193
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6257228851318359
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.81862735748291
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.42373275756836
6 3.2834223061 	 8.4237324483 	 8.4247604783
epoch_time;  36.56301760673523
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.30366384983062744
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5156181454658508
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.219264507293701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.028640747070312
7 3.2223302884 	 9.0286409945 	 9.0299685916
epoch_time;  35.98555660247803
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3088795840740204
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5906131267547607
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.209020137786865
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.942541122436523
8 3.2041871853 	 8.942541174 	 8.9441492029
epoch_time;  35.2444007396698
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3292088806629181
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.564612865447998
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.184047698974609
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.555800437927246
9 3.174539885 	 10.5557999894 	 10.5575644003
epoch_time;  35.20618677139282
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2673344016075134
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48808664083480835
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.358728885650635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.440892219543457
10 3.1562447106 	 7.440892235 	 7.4431158942
epoch_time;  35.02926063537598
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.4067124128341675
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.7032629251480103
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 4.841578960418701
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 6.766429901123047
11 3.1378301932 	 6.7664300042 	 6.7690660631
epoch_time;  35.33015179634094
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.31266993284225464
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6123431921005249
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.4228997230529785
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.641632556915283
12 3.1231311754 	 7.641632575 	 7.6441333668
epoch_time;  35.446006059646606
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.3071022629737854
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5469107627868652
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.863912105560303
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.66484260559082
13 3.1008437976 	 9.6648424303 	 9.6673676362
epoch_time;  35.00720191001892
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.33496102690696716
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.6461020112037659
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.459558963775635
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 9.209179878234863
14 3.0973825216 	 9.2091803473 	 9.2118573163
epoch_time;  34.8975396156311
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2629498541355133
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4556276500225067
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.47732400894165
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.863605499267578
15 3.086726618 	 8.863605891 	 8.8668410842
epoch_time;  35.010772466659546
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2619841992855072
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.47830435633659363
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.380264759063721
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 7.347121715545654
16 3.0824626698 	 7.34712178 	 7.3509066195
epoch_time;  35.106319189071655
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2527039051055908
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.4353841543197632
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 6.363735675811768
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.982970237731934
17 3.0761530314 	 8.9829702016 	 8.9865531303
epoch_time;  34.90220522880554
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2506187856197357
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.48963433504104614
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 5.892581462860107
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 8.22057819366455
18 3.069392799 	 8.2205784153 	 8.2240848026
epoch_time;  36.288904905319214
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2856246829032898
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5131427049636841
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.563399791717529
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.819854736328125
19 3.0527469374 	 10.8198545714 	 10.8233959301
epoch_time;  35.02903175354004
	 Logging test loss: t(0, 0)_r(0, 0)_none => 0.2857031524181366
	 Logging test loss: t(-10, 10)_r(0, 0)_none => 0.5133470296859741
	 Logging test loss: t(0, 0)_r(-5, 5)_none => 7.562077045440674
	 Logging test loss: t(-10, 10)_r(-5, 5)_none => 10.824498176574707
It took 768.8076009750366 seconds to train & eval the model.

JOB STATISTICS
==============
Job ID: 2138595
Array Job ID: 2137927_12
Cluster: snellius
User/Group: nreints/nreints
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 10:34:24
CPU Efficiency: 26.17% of 1-16:23:42 core-walltime
Job Wall-clock time: 02:14:39
Memory Utilized: 5.36 GB
Memory Efficiency: 17.16% of 31.25 GB
