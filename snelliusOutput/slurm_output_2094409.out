qvel_range_t=(0, 0), qvel_range_r=(0, 0)
sim: 0/4999
sim: 10/4999
sim: 20/4999
sim: 30/4999
sim: 40/4999
sim: 50/4999
sim: 60/4999
sim: 70/4999
sim: 80/4999
sim: 90/4999
sim: 100/4999
sim: 110/4999
sim: 120/4999
sim: 130/4999
sim: 140/4999
sim: 150/4999
sim: 160/4999
sim: 170/4999
sim: 180/4999
sim: 190/4999
sim: 200/4999
sim: 210/4999
sim: 220/4999
sim: 230/4999
sim: 240/4999
sim: 250/4999
sim: 260/4999
sim: 270/4999
sim: 280/4999
sim: 290/4999
sim: 300/4999
sim: 310/4999
sim: 320/4999
sim: 330/4999
sim: 340/4999
sim: 350/4999
sim: 360/4999
sim: 370/4999
sim: 380/4999
sim: 390/4999
sim: 400/4999
sim: 410/4999
sim: 420/4999
sim: 430/4999
sim: 440/4999
sim: 450/4999
sim: 460/4999
sim: 470/4999
sim: 480/4999
sim: 490/4999
sim: 500/4999
sim: 510/4999
sim: 520/4999
sim: 530/4999
sim: 540/4999
sim: 550/4999
sim: 560/4999
sim: 570/4999
sim: 580/4999
sim: 590/4999
sim: 600/4999
sim: 610/4999
sim: 620/4999
sim: 630/4999
sim: 640/4999
sim: 650/4999
sim: 660/4999
sim: 670/4999
sim: 680/4999
sim: 690/4999
sim: 700/4999
sim: 710/4999
sim: 720/4999
sim: 730/4999
sim: 740/4999
sim: 750/4999
sim: 760/4999
sim: 770/4999
sim: 780/4999
sim: 790/4999
sim: 800/4999
sim: 810/4999
sim: 820/4999
sim: 830/4999
sim: 840/4999
sim: 850/4999
sim: 860/4999
sim: 870/4999
sim: 880/4999
sim: 890/4999
sim: 900/4999
sim: 910/4999
sim: 920/4999
sim: 930/4999
sim: 940/4999
sim: 950/4999
sim: 960/4999
sim: 970/4999
sim: 980/4999
sim: 990/4999
sim: 1000/4999
sim: 1010/4999
sim: 1020/4999
sim: 1030/4999
sim: 1040/4999
sim: 1050/4999
sim: 1060/4999
sim: 1070/4999
sim: 1080/4999
sim: 1090/4999
sim: 1100/4999
sim: 1110/4999
sim: 1120/4999
sim: 1130/4999
sim: 1140/4999
sim: 1150/4999
sim: 1160/4999
sim: 1170/4999
sim: 1180/4999
sim: 1190/4999
sim: 1200/4999
sim: 1210/4999
sim: 1220/4999
sim: 1230/4999
sim: 1240/4999
sim: 1250/4999
sim: 1260/4999
sim: 1270/4999
sim: 1280/4999
sim: 1290/4999
sim: 1300/4999
sim: 1310/4999
sim: 1320/4999
sim: 1330/4999
sim: 1340/4999
sim: 1350/4999
sim: 1360/4999
sim: 1370/4999
sim: 1380/4999
sim: 1390/4999
sim: 1400/4999
sim: 1410/4999
sim: 1420/4999
sim: 1430/4999
sim: 1440/4999
sim: 1450/4999
sim: 1460/4999
sim: 1470/4999
sim: 1480/4999
sim: 1490/4999
sim: 1500/4999
sim: 1510/4999
sim: 1520/4999
sim: 1530/4999
sim: 1540/4999
sim: 1550/4999
sim: 1560/4999
sim: 1570/4999
sim: 1580/4999
sim: 1590/4999
sim: 1600/4999
sim: 1610/4999
sim: 1620/4999
sim: 1630/4999
sim: 1640/4999
sim: 1650/4999
sim: 1660/4999
sim: 1670/4999
sim: 1680/4999
sim: 1690/4999
sim: 1700/4999
sim: 1710/4999
sim: 1720/4999
sim: 1730/4999
sim: 1740/4999
sim: 1750/4999
sim: 1760/4999
sim: 1770/4999
sim: 1780/4999
sim: 1790/4999
sim: 1800/4999
sim: 1810/4999
sim: 1820/4999
sim: 1830/4999
sim: 1840/4999
sim: 1850/4999
sim: 1860/4999
sim: 1870/4999
sim: 1880/4999
sim: 1890/4999
sim: 1900/4999
sim: 1910/4999
sim: 1920/4999
sim: 1930/4999
sim: 1940/4999
sim: 1950/4999
sim: 1960/4999
sim: 1970/4999
sim: 1980/4999
sim: 1990/4999
sim: 2000/4999
sim: 2010/4999
sim: 2020/4999
sim: 2030/4999
sim: 2040/4999
sim: 2050/4999
sim: 2060/4999
sim: 2070/4999
sim: 2080/4999
sim: 2090/4999
sim: 2100/4999
sim: 2110/4999
sim: 2120/4999
sim: 2130/4999
sim: 2140/4999
sim: 2150/4999
sim: 2160/4999
sim: 2170/4999
sim: 2180/4999
sim: 2190/4999
sim: 2200/4999
sim: 2210/4999
sim: 2220/4999
sim: 2230/4999
sim: 2240/4999
sim: 2250/4999
sim: 2260/4999
sim: 2270/4999
sim: 2280/4999
sim: 2290/4999
sim: 2300/4999
sim: 2310/4999
sim: 2320/4999
sim: 2330/4999
sim: 2340/4999
sim: 2350/4999
sim: 2360/4999
sim: 2370/4999
sim: 2380/4999
sim: 2390/4999
sim: 2400/4999
sim: 2410/4999
sim: 2420/4999
sim: 2430/4999
sim: 2440/4999
sim: 2450/4999
sim: 2460/4999
sim: 2470/4999
sim: 2480/4999
sim: 2490/4999
sim: 2500/4999
sim: 2510/4999
sim: 2520/4999
sim: 2530/4999
sim: 2540/4999
sim: 2550/4999
sim: 2560/4999
sim: 2570/4999
sim: 2580/4999
sim: 2590/4999
sim: 2600/4999
sim: 2610/4999
sim: 2620/4999
sim: 2630/4999
sim: 2640/4999
sim: 2650/4999
sim: 2660/4999
sim: 2670/4999
sim: 2680/4999
sim: 2690/4999
sim: 2700/4999
sim: 2710/4999
sim: 2720/4999
sim: 2730/4999
sim: 2740/4999
sim: 2750/4999
sim: 2760/4999
sim: 2770/4999
sim: 2780/4999
sim: 2790/4999
sim: 2800/4999
sim: 2810/4999
sim: 2820/4999
sim: 2830/4999
sim: 2840/4999
sim: 2850/4999
sim: 2860/4999
sim: 2870/4999
sim: 2880/4999
sim: 2890/4999
sim: 2900/4999
sim: 2910/4999
sim: 2920/4999
sim: 2930/4999
sim: 2940/4999
sim: 2950/4999
sim: 2960/4999
sim: 2970/4999
sim: 2980/4999
sim: 2990/4999
sim: 3000/4999
sim: 3010/4999
sim: 3020/4999
sim: 3030/4999
sim: 3040/4999
sim: 3050/4999
sim: 3060/4999
sim: 3070/4999
sim: 3080/4999
sim: 3090/4999
sim: 3100/4999
sim: 3110/4999
sim: 3120/4999
sim: 3130/4999
sim: 3140/4999
sim: 3150/4999
sim: 3160/4999
sim: 3170/4999
sim: 3180/4999
sim: 3190/4999
sim: 3200/4999
sim: 3210/4999
sim: 3220/4999
sim: 3230/4999
sim: 3240/4999
sim: 3250/4999
sim: 3260/4999
sim: 3270/4999
sim: 3280/4999
sim: 3290/4999
sim: 3300/4999
sim: 3310/4999
sim: 3320/4999
sim: 3330/4999
sim: 3340/4999
sim: 3350/4999
sim: 3360/4999
sim: 3370/4999
sim: 3380/4999
sim: 3390/4999
sim: 3400/4999
sim: 3410/4999
sim: 3420/4999
sim: 3430/4999
sim: 3440/4999
sim: 3450/4999
sim: 3460/4999
sim: 3470/4999
sim: 3480/4999
sim: 3490/4999
sim: 3500/4999
sim: 3510/4999
sim: 3520/4999
sim: 3530/4999
sim: 3540/4999
sim: 3550/4999
sim: 3560/4999
sim: 3570/4999
sim: 3580/4999
sim: 3590/4999
sim: 3600/4999
sim: 3610/4999
sim: 3620/4999
sim: 3630/4999
sim: 3640/4999
sim: 3650/4999
sim: 3660/4999
sim: 3670/4999
sim: 3680/4999
sim: 3690/4999
sim: 3700/4999
sim: 3710/4999
sim: 3720/4999
sim: 3730/4999
sim: 3740/4999
sim: 3750/4999
sim: 3760/4999
sim: 3770/4999
sim: 3780/4999
sim: 3790/4999
sim: 3800/4999
sim: 3810/4999
sim: 3820/4999
sim: 3830/4999
sim: 3840/4999
sim: 3850/4999
sim: 3860/4999
sim: 3870/4999
sim: 3880/4999
sim: 3890/4999
sim: 3900/4999
sim: 3910/4999
sim: 3920/4999
sim: 3930/4999
sim: 3940/4999
sim: 3950/4999
sim: 3960/4999
sim: 3970/4999
sim: 3980/4999
sim: 3990/4999
sim: 4000/4999
sim: 4010/4999
sim: 4020/4999
sim: 4030/4999
sim: 4040/4999
sim: 4050/4999
sim: 4060/4999
sim: 4070/4999
sim: 4080/4999
sim: 4090/4999
sim: 4100/4999
sim: 4110/4999
sim: 4120/4999
sim: 4130/4999
sim: 4140/4999
sim: 4150/4999
sim: 4160/4999
sim: 4170/4999
sim: 4180/4999
sim: 4190/4999
sim: 4200/4999
sim: 4210/4999
sim: 4220/4999
sim: 4230/4999
sim: 4240/4999
sim: 4250/4999
sim: 4260/4999
sim: 4270/4999
sim: 4280/4999
sim: 4290/4999
sim: 4300/4999
sim: 4310/4999
sim: 4320/4999
sim: 4330/4999
sim: 4340/4999
sim: 4350/4999
sim: 4360/4999
sim: 4370/4999
sim: 4380/4999
sim: 4390/4999
sim: 4400/4999
sim: 4410/4999
sim: 4420/4999
sim: 4430/4999
sim: 4440/4999
sim: 4450/4999
sim: 4460/4999
sim: 4470/4999
sim: 4480/4999
sim: 4490/4999
sim: 4500/4999
sim: 4510/4999
sim: 4520/4999
sim: 4530/4999
sim: 4540/4999
sim: 4550/4999
sim: 4560/4999
sim: 4570/4999
sim: 4580/4999
sim: 4590/4999
sim: 4600/4999
sim: 4610/4999
sim: 4620/4999
sim: 4630/4999
sim: 4640/4999
sim: 4650/4999
sim: 4660/4999
sim: 4670/4999
sim: 4680/4999
sim: 4690/4999
sim: 4700/4999
sim: 4710/4999
sim: 4720/4999
sim: 4730/4999
sim: 4740/4999
sim: 4750/4999
sim: 4760/4999
sim: 4770/4999
sim: 4780/4999
sim: 4790/4999
sim: 4800/4999
sim: 4810/4999
sim: 4820/4999
sim: 4830/4999
sim: 4840/4999
sim: 4850/4999
sim: 4860/4999
sim: 4870/4999
sim: 4880/4999
sim: 4890/4999
sim: 4900/4999
sim: 4910/4999
sim: 4920/4999
sim: 4930/4999
sim: 4940/4999
sim: 4950/4999
sim: 4960/4999
sim: 4970/4999
sim: 4980/4999
sim: 4990/4999
sim: 4999/4999
qvel_range_t=(-10, 10), qvel_range_r=(-5, 5)
sim: 0/4999
sim: 10/4999
sim: 20/4999
sim: 30/4999
sim: 40/4999
sim: 50/4999
sim: 60/4999
sim: 70/4999
sim: 80/4999
sim: 90/4999
sim: 100/4999
sim: 110/4999
sim: 120/4999
sim: 130/4999
sim: 140/4999
sim: 150/4999
sim: 160/4999
sim: 170/4999
sim: 180/4999
sim: 190/4999
sim: 200/4999
sim: 210/4999
sim: 220/4999
sim: 230/4999
sim: 240/4999
sim: 250/4999
sim: 260/4999
sim: 270/4999
sim: 280/4999
sim: 290/4999
sim: 300/4999
sim: 310/4999
sim: 320/4999
sim: 330/4999
sim: 340/4999
sim: 350/4999
sim: 360/4999
sim: 370/4999
sim: 380/4999
sim: 390/4999
sim: 400/4999
sim: 410/4999
sim: 420/4999
sim: 430/4999
sim: 440/4999
sim: 450/4999
sim: 460/4999
sim: 470/4999
sim: 480/4999
sim: 490/4999
sim: 500/4999
sim: 510/4999
sim: 520/4999
sim: 530/4999
sim: 540/4999
sim: 550/4999
sim: 560/4999
sim: 570/4999
sim: 580/4999
sim: 590/4999
sim: 600/4999
sim: 610/4999
sim: 620/4999
sim: 630/4999
sim: 640/4999
sim: 650/4999
sim: 660/4999
sim: 670/4999
sim: 680/4999
sim: 690/4999
sim: 700/4999
sim: 710/4999
sim: 720/4999
sim: 730/4999
sim: 740/4999
sim: 750/4999
sim: 760/4999
sim: 770/4999
sim: 780/4999
sim: 790/4999
sim: 800/4999
sim: 810/4999
sim: 820/4999
sim: 830/4999
sim: 840/4999
sim: 850/4999
sim: 860/4999
sim: 870/4999
sim: 880/4999
sim: 890/4999
sim: 900/4999
sim: 910/4999
sim: 920/4999
sim: 930/4999
sim: 940/4999
sim: 950/4999
sim: 960/4999
sim: 970/4999
sim: 980/4999
sim: 990/4999
sim: 1000/4999
sim: 1010/4999
sim: 1020/4999
sim: 1030/4999
sim: 1040/4999
sim: 1050/4999
sim: 1060/4999
sim: 1070/4999
sim: 1080/4999
sim: 1090/4999
sim: 1100/4999
sim: 1110/4999
sim: 1120/4999
sim: 1130/4999
sim: 1140/4999
sim: 1150/4999
sim: 1160/4999
sim: 1170/4999
sim: 1180/4999
sim: 1190/4999
sim: 1200/4999
sim: 1210/4999
sim: 1220/4999
sim: 1230/4999
sim: 1240/4999
sim: 1250/4999
sim: 1260/4999
sim: 1270/4999
sim: 1280/4999
sim: 1290/4999
sim: 1300/4999
sim: 1310/4999
sim: 1320/4999
sim: 1330/4999
sim: 1340/4999
sim: 1350/4999
sim: 1360/4999
sim: 1370/4999
sim: 1380/4999
sim: 1390/4999
sim: 1400/4999
sim: 1410/4999
sim: 1420/4999
sim: 1430/4999
sim: 1440/4999
sim: 1450/4999
sim: 1460/4999
sim: 1470/4999
sim: 1480/4999
sim: 1490/4999
sim: 1500/4999
sim: 1510/4999
sim: 1520/4999
sim: 1530/4999
sim: 1540/4999
sim: 1550/4999
sim: 1560/4999
sim: 1570/4999
sim: 1580/4999
sim: 1590/4999
sim: 1600/4999
sim: 1610/4999
sim: 1620/4999
sim: 1630/4999
sim: 1640/4999
sim: 1650/4999
sim: 1660/4999
sim: 1670/4999
sim: 1680/4999
sim: 1690/4999
sim: 1700/4999
sim: 1710/4999
sim: 1720/4999
sim: 1730/4999
sim: 1740/4999
sim: 1750/4999
sim: 1760/4999
sim: 1770/4999
sim: 1780/4999
sim: 1790/4999
sim: 1800/4999
sim: 1810/4999
sim: 1820/4999
sim: 1830/4999
sim: 1840/4999
sim: 1850/4999
sim: 1860/4999
sim: 1870/4999
sim: 1880/4999
sim: 1890/4999
sim: 1900/4999
sim: 1910/4999
sim: 1920/4999
sim: 1930/4999
sim: 1940/4999
sim: 1950/4999
sim: 1960/4999
sim: 1970/4999
sim: 1980/4999
sim: 1990/4999
sim: 2000/4999
sim: 2010/4999
sim: 2020/4999
sim: 2030/4999
sim: 2040/4999
sim: 2050/4999
sim: 2060/4999
sim: 2070/4999
sim: 2080/4999
sim: 2090/4999
sim: 2100/4999
sim: 2110/4999
sim: 2120/4999
sim: 2130/4999
sim: 2140/4999
sim: 2150/4999
sim: 2160/4999
sim: 2170/4999
sim: 2180/4999
sim: 2190/4999
sim: 2200/4999
sim: 2210/4999
sim: 2220/4999
sim: 2230/4999
sim: 2240/4999
sim: 2250/4999
sim: 2260/4999
sim: 2270/4999
sim: 2280/4999
sim: 2290/4999
sim: 2300/4999
sim: 2310/4999
sim: 2320/4999
sim: 2330/4999
sim: 2340/4999
sim: 2350/4999
sim: 2360/4999
sim: 2370/4999
sim: 2380/4999
sim: 2390/4999
sim: 2400/4999
sim: 2410/4999
sim: 2420/4999
sim: 2430/4999
sim: 2440/4999
sim: 2450/4999
sim: 2460/4999
sim: 2470/4999
sim: 2480/4999
sim: 2490/4999
sim: 2500/4999
sim: 2510/4999
sim: 2520/4999
sim: 2530/4999
sim: 2540/4999
sim: 2550/4999
sim: 2560/4999
sim: 2570/4999
sim: 2580/4999
sim: 2590/4999
sim: 2600/4999
sim: 2610/4999
sim: 2620/4999
sim: 2630/4999
sim: 2640/4999
sim: 2650/4999
sim: 2660/4999
sim: 2670/4999
sim: 2680/4999
sim: 2690/4999
sim: 2700/4999
sim: 2710/4999
sim: 2720/4999
sim: 2730/4999
sim: 2740/4999
sim: 2750/4999
sim: 2760/4999
sim: 2770/4999
sim: 2780/4999
sim: 2790/4999
sim: 2800/4999
sim: 2810/4999
sim: 2820/4999
sim: 2830/4999
sim: 2840/4999
sim: 2850/4999
sim: 2860/4999
sim: 2870/4999
sim: 2880/4999
sim: 2890/4999
sim: 2900/4999
sim: 2910/4999
sim: 2920/4999
sim: 2930/4999
sim: 2940/4999
sim: 2950/4999
sim: 2960/4999
sim: 2970/4999
sim: 2980/4999
sim: 2990/4999
sim: 3000/4999
sim: 3010/4999
sim: 3020/4999
sim: 3030/4999
sim: 3040/4999
sim: 3050/4999
sim: 3060/4999
sim: 3070/4999
sim: 3080/4999
sim: 3090/4999
sim: 3100/4999
sim: 3110/4999
sim: 3120/4999
sim: 3130/4999
sim: 3140/4999
sim: 3150/4999
sim: 3160/4999
sim: 3170/4999
sim: 3180/4999
sim: 3190/4999
sim: 3200/4999
sim: 3210/4999
sim: 3220/4999
sim: 3230/4999
sim: 3240/4999
sim: 3250/4999
sim: 3260/4999
sim: 3270/4999
sim: 3280/4999
sim: 3290/4999
sim: 3300/4999
sim: 3310/4999
sim: 3320/4999
sim: 3330/4999
sim: 3340/4999
sim: 3350/4999
sim: 3360/4999
sim: 3370/4999
sim: 3380/4999
sim: 3390/4999
sim: 3400/4999
sim: 3410/4999
sim: 3420/4999
sim: 3430/4999
sim: 3440/4999
sim: 3450/4999
sim: 3460/4999
sim: 3470/4999
sim: 3480/4999
sim: 3490/4999
sim: 3500/4999
sim: 3510/4999
sim: 3520/4999
sim: 3530/4999
sim: 3540/4999
sim: 3550/4999
sim: 3560/4999
sim: 3570/4999
sim: 3580/4999
sim: 3590/4999
sim: 3600/4999
sim: 3610/4999
sim: 3620/4999
sim: 3630/4999
sim: 3640/4999
sim: 3650/4999
sim: 3660/4999
sim: 3670/4999
sim: 3680/4999
sim: 3690/4999
sim: 3700/4999
sim: 3710/4999
sim: 3720/4999
sim: 3730/4999
sim: 3740/4999
sim: 3750/4999
sim: 3760/4999
sim: 3770/4999
sim: 3780/4999
sim: 3790/4999
sim: 3800/4999
sim: 3810/4999
sim: 3820/4999
sim: 3830/4999
sim: 3840/4999
sim: 3850/4999
sim: 3860/4999
sim: 3870/4999
sim: 3880/4999
sim: 3890/4999
sim: 3900/4999
sim: 3910/4999
sim: 3920/4999
sim: 3930/4999
sim: 3940/4999
sim: 3950/4999
sim: 3960/4999
sim: 3970/4999
sim: 3980/4999
sim: 3990/4999
sim: 4000/4999
sim: 4010/4999
sim: 4020/4999
sim: 4030/4999
sim: 4040/4999
sim: 4050/4999
sim: 4060/4999
sim: 4070/4999
sim: 4080/4999
sim: 4090/4999
sim: 4100/4999
sim: 4110/4999
sim: 4120/4999
sim: 4130/4999
sim: 4140/4999
sim: 4150/4999
sim: 4160/4999
sim: 4170/4999
sim: 4180/4999
sim: 4190/4999
sim: 4200/4999
sim: 4210/4999
sim: 4220/4999
sim: 4230/4999
sim: 4240/4999
sim: 4250/4999
sim: 4260/4999
sim: 4270/4999
sim: 4280/4999
sim: 4290/4999
sim: 4300/4999
sim: 4310/4999
sim: 4320/4999
sim: 4330/4999
sim: 4340/4999
sim: 4350/4999
sim: 4360/4999
sim: 4370/4999
sim: 4380/4999
sim: 4390/4999
sim: 4400/4999
sim: 4410/4999
sim: 4420/4999
sim: 4430/4999
sim: 4440/4999
sim: 4450/4999
sim: 4460/4999
sim: 4470/4999
sim: 4480/4999
sim: 4490/4999
sim: 4500/4999
sim: 4510/4999
sim: 4520/4999
sim: 4530/4999
sim: 4540/4999
sim: 4550/4999
sim: 4560/4999
sim: 4570/4999
sim: 4580/4999
sim: 4590/4999
sim: 4600/4999
sim: 4610/4999
sim: 4620/4999
sim: 4630/4999
sim: 4640/4999
sim: 4650/4999
sim: 4660/4999
sim: 4670/4999
sim: 4680/4999
sim: 4690/4999
sim: 4700/4999
sim: 4710/4999
sim: 4720/4999
sim: 4730/4999
sim: 4740/4999
sim: 4750/4999
sim: 4760/4999
sim: 4770/4999
sim: 4780/4999
sim: 4790/4999
sim: 4800/4999
sim: 4810/4999
sim: 4820/4999
sim: 4830/4999
sim: 4840/4999
sim: 4850/4999
sim: 4860/4999
sim: 4870/4999
sim: 4880/4999
sim: 4890/4999
sim: 4900/4999
sim: 4910/4999
sim: 4920/4999
sim: 4930/4999
sim: 4940/4999
sim: 4950/4999
sim: 4960/4999
sim: 4970/4999
sim: 4980/4999
sim: 4990/4999
sim: 4999/4999
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/nreints/.netrc
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: Currently logged in as: nreints. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_155357-qkszl76k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-darkness-976
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/qkszl76k
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–†â–…â–‡â–„â–ƒâ–„â–‡â–„â–„â–„â–ƒâ–ƒâ–â–…â–„â–…â–ƒâ–â–ƒâ–…â–â–„â–‚â–‡â–â–‚â–‚â–‡â–ƒâ–ƒ
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.38694
wandb:               Epoch 29
wandb:          Train loss 1.0296
wandb: 
wandb: ğŸš€ View run dashing-darkness-976 at: https://wandb.ai/nreints/thesis/runs/qkszl76k
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_155357-qkszl76k/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_155943-uepgp2l1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-gorge-977
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/uepgp2l1
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Converted test loss â–‡â–…â–„â–…â–ˆâ–†â–…â–ƒâ–„â–…â–ˆâ–„â–…â–‚â–‡â–…â–ƒâ–„â–…â–‡â–ˆâ–â–‚â–‚â–â–‡â–‚â–â–…â–„â–„
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.36822
wandb:               Epoch 29
wandb:          Train loss 0.91287
wandb: 
wandb: ğŸš€ View run revived-gorge-977 at: https://wandb.ai/nreints/thesis/runs/uepgp2l1
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_155943-uepgp2l1/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_160518-yb5fum2z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-grass-978
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/yb5fum2z
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–…â–„â–‚â–„â–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–„â–â–„â–â–‚â–„â–„â–‚â–‚â–„â–„â–†â–ƒâ–†â–ƒâ–‚â–‚
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.30336
wandb:               Epoch 29
wandb:          Train loss 0.88025
wandb: 
wandb: ğŸš€ View run quiet-grass-978 at: https://wandb.ai/nreints/thesis/runs/yb5fum2z
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_160518-yb5fum2z/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_161135-6bl3xq72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sun-979
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/6bl3xq72
pos
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
0 1.2874726952 	 0.4711530118
epoch_time;  11.78144121170044
1 1.0944849822 	 0.4466073938
epoch_time;  10.66520619392395
2 1.0742105734 	 0.4262917596
epoch_time;  10.581198930740356
3 1.0614059097 	 0.4607615703
epoch_time;  10.611806869506836
4 1.0520831406 	 0.4080441862
epoch_time;  10.581008434295654
5 1.0501670166 	 0.3976697767
epoch_time;  10.631257772445679
6 1.0457005481 	 0.4101589306
epoch_time;  10.593902349472046
7 1.0440591237 	 0.4520468222
epoch_time;  10.628987789154053
8 1.0414955645 	 0.4035364924
epoch_time;  10.61999797821045
9 1.0407381174 	 0.4076372714
epoch_time;  10.649832010269165
10 1.042276798 	 0.4115621206
epoch_time;  10.57323956489563
11 1.0362104486 	 0.396207696
epoch_time;  10.610991954803467
12 1.0383299129 	 0.3847436441
epoch_time;  10.652461528778076
13 1.0358865245 	 0.3586237315
epoch_time;  10.587549924850464
14 1.0334021706 	 0.4212492247
epoch_time;  10.466322660446167
15 1.0334139812 	 0.4129298855
epoch_time;  10.411562204360962
16 1.0357535521 	 0.4144828178
epoch_time;  10.487056732177734
17 1.0327187256 	 0.395076195
epoch_time;  10.40763258934021
18 1.0322306921 	 0.3611879503
epoch_time;  10.499940156936646
19 1.0296772091 	 0.3842890147
epoch_time;  10.618616580963135
20 1.0347806236 	 0.4277046822
epoch_time;  10.573137760162354
21 1.0308797421 	 0.357307599
epoch_time;  10.60817813873291
22 1.0317760256 	 0.4129502168
epoch_time;  10.646705150604248
23 1.0327489541 	 0.3701726553
epoch_time;  10.660605430603027
24 1.0340607557 	 0.4599153261
epoch_time;  10.598982095718384
25 1.0308656189 	 0.3621813387
epoch_time;  10.58343243598938
26 1.0301732539 	 0.380535435
epoch_time;  10.50633955001831
27 1.0301476506 	 0.3733845479
epoch_time;  10.44710087776184
28 1.0303162199 	 0.4576430037
epoch_time;  10.423890829086304
29 1.0295958564 	 0.3869452399
epoch_time;  10.428444147109985
It took 345.6883044242859 seconds.
eucl_motion
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=12, bias=True)
  )
)
0 1.1218543356 	 0.4537643433
epoch_time;  10.362384796142578
1 0.9948520686 	 0.4088255599
epoch_time;  10.423313617706299
2 0.9736470182 	 0.3832444062
epoch_time;  10.432714462280273
3 0.9573463368 	 0.3911442112
epoch_time;  11.010382175445557
4 0.9516696543 	 0.4678122856
epoch_time;  10.621342897415161
5 0.9461181806 	 0.4278215151
epoch_time;  10.643569946289062
6 0.940119804 	 0.3936535397
epoch_time;  10.504559993743896
7 0.9349711293 	 0.3412926545
epoch_time;  10.422824144363403
8 0.9342126085 	 0.3784554146
epoch_time;  10.49819016456604
9 0.9337593461 	 0.3891310821
epoch_time;  10.52146577835083
10 0.9280503703 	 0.461756732
epoch_time;  10.53671932220459
11 0.9261204535 	 0.3671026281
epoch_time;  10.492273569107056
12 0.9287540503 	 0.3992020891
epoch_time;  10.537821054458618
13 0.9251729632 	 0.336637631
epoch_time;  10.418421030044556
14 0.9216705859 	 0.447718976
epoch_time;  10.545663118362427
15 0.9225097161 	 0.3980411014
epoch_time;  10.505959033966064
16 0.9206708978 	 0.3556836102
epoch_time;  10.504539966583252
17 0.9162744669 	 0.3791752996
epoch_time;  10.531363725662231
18 0.919167236 	 0.3894003378
epoch_time;  10.485854148864746
19 0.9207418439 	 0.4395769686
epoch_time;  10.507325649261475
20 0.9150139547 	 0.4642025509
epoch_time;  10.506301879882812
21 0.9218879669 	 0.3170622336
epoch_time;  10.516834735870361
22 0.9201774133 	 0.3317036397
epoch_time;  10.499391794204712
23 0.9188992264 	 0.3362065083
epoch_time;  10.53485369682312
24 0.9152815513 	 0.3085559742
epoch_time;  10.543412685394287
25 0.9136254255 	 0.4497122687
epoch_time;  10.458683490753174
26 0.9149158359 	 0.3392946398
epoch_time;  10.512978076934814
27 0.9159577283 	 0.3061622413
epoch_time;  10.488038539886475
28 0.9176889298 	 0.388953709
epoch_time;  10.57211947441101
29 0.9128685565 	 0.3682422122
epoch_time;  10.500922918319702
It took 335.88548707962036 seconds.
quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
0 1.1239911442 	 0.5370699083
epoch_time;  12.02639651298523
1 0.9490481984 	 0.4212135934
epoch_time;  12.008249998092651
2 0.9212700169 	 0.3844335195
epoch_time;  12.091739654541016
3 0.9147259579 	 0.29363958
epoch_time;  11.984825611114502
4 0.9132098248 	 0.3754778475
epoch_time;  11.937230348587036
5 0.9001950482 	 0.3744848715
epoch_time;  11.975370645523071
6 0.8979506228 	 0.2899338387
epoch_time;  11.901758670806885
7 0.8934681316 	 0.3671039066
epoch_time;  12.034671783447266
8 0.893455908 	 0.348949288
epoch_time;  11.901874780654907
9 0.8912847413 	 0.3641674145
epoch_time;  12.008617877960205
10 0.8894469141 	 0.3587890213
epoch_time;  11.971541404724121
11 0.8821041949 	 0.3436470444
epoch_time;  12.032973766326904
12 0.8903554212 	 0.4547679179
epoch_time;  12.016391038894653
13 0.889193771 	 0.3783982148
epoch_time;  11.932743787765503
14 0.8862211359 	 0.3703177375
epoch_time;  11.945884227752686
15 0.8810634587 	 0.2870689392
epoch_time;  11.969475269317627
16 0.8815181252 	 0.3886020145
epoch_time;  12.009438276290894
17 0.8800089298 	 0.2737025081
epoch_time;  11.979917526245117
18 0.8850400767 	 0.3081607097
epoch_time;  11.921606063842773
19 0.8865943671 	 0.3793088346
epoch_time;  11.9819815158844
20 0.8811736359 	 0.3754102552
epoch_time;  11.968656063079834
21 0.8839503638 	 0.3177792523
epoch_time;  11.993236780166626
22 0.8773955679 	 0.3294623916
epoch_time;  11.968046188354492
23 0.8834202086 	 0.4017447394
epoch_time;  11.898226499557495
24 0.879682115 	 0.3975333136
epoch_time;  11.963278532028198
25 0.8794653122 	 0.461845233
epoch_time;  12.077433586120605
26 0.8818732689 	 0.3565865594
epoch_time;  11.943929195404053
27 0.8817372406 	 0.4454983273
epoch_time;  11.985668420791626
28 0.8807974313 	 0.3537392281
epoch_time;  11.894405364990234
29 0.8802501797 	 0.3033664806
epoch_time;  11.97032356262207
It took 377.0096695423126 seconds.
log_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=70, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=7, bias=True)
  )
)
0 1.587716522 	 0.9658646042
epoch_time;  12.216652154922485
1 1.2869604624 	 0.8364131618
epoch_time;  11.77474045753479
2 1.2481951268 	 0.8319236962
epoch_time;  11.784280776977539
3 1.2289842495 	 0.7842114423
epoch_time;  11.73867654800415
4 1.2179254885 	 0.8076919968
epoch_time;  11.842227458953857
5 1.2130259926 	 0.7926642341
epoch_time;  11.781395435333252
6 1.2073841637 	 0.7938530999
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–„â–„â–‚â–ƒâ–ƒâ–ƒâ–…â–„â–‚â–„â–„â–„â–…â–„â–ƒâ–…â–„â–â–…â–‚â–…â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒ
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.80016
wandb:               Epoch 29
wandb:          Train loss 1.18682
wandb: 
wandb: ğŸš€ View run divine-sun-979 at: https://wandb.ai/nreints/thesis/runs/6bl3xq72
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_161135-6bl3xq72/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_161751-ulkxrtzq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-pine-980
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/ulkxrtzq
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Converted test loss â–‡â–‡â–ƒâ–‡â–ˆâ–‡â–†â–‚â–†â–ƒâ–„â–ƒâ–‡â–…â–„â–„â–â–‡â–…â–‚â–ƒâ–„â–…â–…â–ƒâ–ƒâ–ƒâ–â–‡â–†â–†
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.51744
wandb:               Epoch 29
wandb:          Train loss 0.99671
wandb: 
wandb: ğŸš€ View run stilted-pine-980 at: https://wandb.ai/nreints/thesis/runs/ulkxrtzq
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_161751-ulkxrtzq/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_162424-x91vzt4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-pond-981
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/x91vzt4w
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–ƒâ–ƒâ–…â–„â–â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒâ–„â–†â–„â–â–…â–„â–…â–…â–‚â–„â–‚â–„â–â–‡â–ƒâ–„â–†â–†
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.28724
wandb:               Epoch 29
wandb:          Train loss 0.45177
wandb: 
wandb: ğŸš€ View run clean-pond-981 at: https://wandb.ai/nreints/thesis/runs/x91vzt4w
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_162424-x91vzt4w/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_162938-2blcrwsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-bird-982
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/2blcrwsm
epoch_time;  11.784321069717407
7 1.2035791709 	 0.8567857897
epoch_time;  11.70811676979065
8 1.201322522 	 0.8295529752
epoch_time;  11.789560079574585
9 1.1996709387 	 0.7728725949
epoch_time;  11.750421285629272
10 1.1956815191 	 0.8404448638
epoch_time;  11.83271050453186
11 1.1949243197 	 0.8257821573
epoch_time;  11.839515686035156
12 1.1948583291 	 0.8248703415
epoch_time;  11.759110927581787
13 1.1921045616 	 0.8554840088
epoch_time;  11.709338188171387
14 1.1934812801 	 0.8207666346
epoch_time;  11.78823971748352
15 1.1894752595 	 0.7987327782
epoch_time;  11.851836442947388
16 1.190391778 	 0.8559141417
epoch_time;  11.954592227935791
17 1.1902131325 	 0.8389982481
epoch_time;  11.752487182617188
18 1.1874506928 	 0.7385492273
epoch_time;  11.82327151298523
19 1.1883052676 	 0.860707381
epoch_time;  11.77914810180664
20 1.1894200884 	 0.7675918167
epoch_time;  11.930842161178589
21 1.1893740023 	 0.8802543846
epoch_time;  11.875430583953857
22 1.1858986324 	 0.8082603661
epoch_time;  11.787692070007324
23 1.1866231255 	 0.748984342
epoch_time;  11.76288104057312
24 1.1898342024 	 0.7836885195
epoch_time;  11.863559007644653
25 1.1827478473 	 0.7814302187
epoch_time;  11.808716773986816
26 1.1867559327 	 0.7884019182
epoch_time;  12.09501028060913
27 1.1844615394 	 0.7622702109
epoch_time;  12.436405897140503
28 1.1842295398 	 0.7923032194
epoch_time;  11.792572736740112
29 1.1868158942 	 0.800203478
epoch_time;  12.458979606628418
It took 375.148973941803 seconds.
dual_quat
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=80, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=8, bias=True)
  )
)
0 1.4246375222 	 0.5644062764
epoch_time;  12.584784984588623
1 1.1046737237 	 0.5621164683
epoch_time;  12.524914026260376
2 1.066546573 	 0.4462655145
epoch_time;  12.54533052444458
3 1.0532485074 	 0.56728528
epoch_time;  12.371092081069946
4 1.0422312074 	 0.5854612196
epoch_time;  12.537312030792236
5 1.045291307 	 0.5709556373
epoch_time;  12.63072657585144
6 1.0344219285 	 0.5169431532
epoch_time;  12.458425045013428
7 1.0338768243 	 0.4317953574
epoch_time;  12.38926887512207
8 1.027435344 	 0.5176509548
epoch_time;  12.505910158157349
9 1.0330025927 	 0.4421271247
epoch_time;  12.542245626449585
10 1.0266486595 	 0.4804695336
epoch_time;  12.471384525299072
11 1.0158841723 	 0.455795783
epoch_time;  12.377744674682617
12 1.0225900281 	 0.5531631057
epoch_time;  12.408195734024048
13 1.0189867239 	 0.4935552649
epoch_time;  12.393914222717285
14 1.0132484281 	 0.4735915313
epoch_time;  12.568213701248169
15 1.0053413608 	 0.465610566
epoch_time;  12.625752925872803
16 1.0098413615 	 0.3916097693
epoch_time;  12.526956796646118
17 1.00937092 	 0.5670351183
epoch_time;  12.418879508972168
18 1.0079562919 	 0.4923630173
epoch_time;  12.334053754806519
19 1.0060361209 	 0.429473712
epoch_time;  12.589377641677856
20 1.0130839883 	 0.4352685052
epoch_time;  12.502470016479492
21 1.005331202 	 0.4787411664
epoch_time;  12.460385799407959
22 1.0095459315 	 0.4912644257
epoch_time;  12.504543542861938
23 1.0012928055 	 0.5146302507
epoch_time;  12.477772235870361
24 0.9995822519 	 0.4385016364
epoch_time;  12.479680061340332
25 1.0031062682 	 0.4603742032
epoch_time;  12.434102296829224
26 1.0012087274 	 0.4403055139
epoch_time;  12.403090000152588
27 1.0033439667 	 0.4035449054
epoch_time;  12.511704921722412
28 0.9990886022 	 0.5701666755
epoch_time;  12.514276027679443
29 0.9967129389 	 0.5174678081
epoch_time;  12.553976774215698
It took 393.0763032436371 seconds.
pos_diff_start
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=24, bias=True)
  )
)
0 0.64865517 	 0.3391304635
epoch_time;  9.807068347930908
1 0.515188834 	 0.2256294457
epoch_time;  9.861478567123413
2 0.4974031565 	 0.2101537756
epoch_time;  9.882353782653809
3 0.4916988884 	 0.274242463
epoch_time;  9.830355405807495
4 0.4804819234 	 0.2489409988
epoch_time;  9.912394285202026
5 0.4805552646 	 0.1686112172
epoch_time;  9.828250408172607
6 0.4761162166 	 0.2112635226
epoch_time;  9.870639324188232
7 0.4723138667 	 0.2091789246
epoch_time;  9.833951950073242
8 0.4768546263 	 0.2259146201
epoch_time;  9.857925415039062
9 0.4722402364 	 0.1856130239
epoch_time;  9.83488655090332
10 0.4692160228 	 0.2486216777
epoch_time;  9.889853477478027
11 0.4702034874 	 0.1701555407
epoch_time;  9.82429027557373
12 0.4696666836 	 0.2195882849
epoch_time;  9.893038272857666
13 0.4694994357 	 0.2330843745
epoch_time;  9.823026180267334
14 0.4665904374 	 0.2782184498
epoch_time;  9.883432388305664
15 0.4695266497 	 0.2363311768
epoch_time;  9.842934846878052
16 0.4638736709 	 0.1660222853
epoch_time;  9.882957935333252
17 0.4638237443 	 0.276241653
epoch_time;  9.854593992233276
18 0.4625765706 	 0.2476263098
epoch_time;  9.829332828521729
19 0.4590787358 	 0.2580722293
epoch_time;  9.791788101196289
20 0.4629054089 	 0.2709376361
epoch_time;  9.902664422988892
21 0.4582860634 	 0.1828784427
epoch_time;  9.877179622650146
22 0.4601523021 	 0.2360477963
epoch_time;  9.895810604095459
23 0.4522280971 	 0.1782260585
epoch_time;  9.86037826538086
24 0.4556515581 	 0.2500213829
epoch_time;  9.892909049987793
25 0.4523797351 	 0.1640835118
epoch_time;  9.849318027496338
26 0.4507270782 	 0.3122363322
epoch_time;  9.923577308654785
27 0.4542273907 	 0.2165856645
epoch_time;  9.85046648979187
28 0.4549712515 	 0.240329062
epoch_time;  9.892561912536621
29 0.4517706637 	 0.2872360436
epoch_time;  9.848789930343628
It took 313.9662458896637 seconds.
log_dualQ
fcnn(
  (linears): Sequential(
    (0): Linear(in_features=60, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=256, bias=True)
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4, inplace=False)
    (8): Linear(in_features=256, out_features=6, bias=True)
  )
)
0 1.4934594099 	 0.4950353674
epoch_time;  13.746838331222534
1 1.1293787563 	 0.5193846831
epoch_time;  13.793182134628296
2 1.1005752006 	 0.6209150572
epoch_time;  13.820691347122192
3 1.0793480983 	 0.551404221
epoch_time;  13.82703971862793
4 1.0700825651 	 0.4865453359
epoch_time;  13.873670816421509
5 1.0645490964 	 0.6200741742
epoch_time;  13.9395170211792
6 1.0589084238 	 0.4778356191
epoch_time;  13.881119966506958
7 1.0576482419 	 0.4419851355
epoch_time;  13.779380321502686
8 1.0469167087 	 0.4339566205
epoch_time;  13.862873554229736
9 1.0435454042 	 0.4989141103
epoch_time;  13.966121196746826
10 1.0513902801 	 0.4806655884
epoch_time;  13.829708814620972
11 1.0401930957 	 0.546274876
epoch_time;  13.85008978843689
12 1.0417640696 	 0.4880405735
epoch_time;  13.793684244155884
13 1.0386540848 	 0.4486675366
epoch_time;  13.84614372253418
14 1.0394315191 	 0.5268141875
epoch_time;  13.933160305023193
15 1.035375035 	 0.6182889784
epoch_time;  13.946045160293579
16 1.0329010051 	 0.6257543203
epoch_time;  13.747999668121338
17 1.0348545431 	 0.749673297
epoch_time;  13.902676105499268
18 1.0326225064 	 0.5657532563
epoch_time;  13.942365169525146
19 1.038065207 	 0.4594144667
epoch_time;  14.681598424911499
20 1.0364088335 	 0.7377064473
epoch_time;  13.95051646232605
21 1.0396050439 	 0.5218085212
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Converted test loss â–‚â–ƒâ–…â–„â–‚â–…â–‚â–â–â–‚â–‚â–ƒâ–‚â–â–ƒâ–…â–…â–ˆâ–„â–‚â–ˆâ–ƒâ–„â–„â–‚â–ƒâ–‡â–ˆâ–…â–â–
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.44508
wandb:               Epoch 29
wandb:          Train loss 1.01958
wandb: 
wandb: ğŸš€ View run frosty-bird-982 at: https://wandb.ai/nreints/thesis/runs/2blcrwsm
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_162938-2blcrwsm/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_163653-v7dpz7ra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-blaze-983
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/v7dpz7ra
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–‡â–‡â–†â–ƒâ–…â–ƒâ–…â–â–‚â–†â–†â–…â–„â–‚â–…â–„â–…â–â–†â–ƒâ–…â–„â–…â–‚â–…â–„â–…â–‚â–‚â–‚
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.55824
wandb:               Epoch 29
wandb:          Train loss 1.44374
wandb: 
wandb: ğŸš€ View run jolly-blaze-983 at: https://wandb.ai/nreints/thesis/runs/v7dpz7ra
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_163653-v7dpz7ra/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_164220-kbrbvvtj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-planet-984
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/kbrbvvtj
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: - 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: Converted test loss â–ˆâ–‡â–„â–†â–…â–„â–ƒâ–‚â–…â–ƒâ–ƒâ–‚â–†â–â–„â–„â–ƒâ–„â–„â–ƒâ–†â–â–â–â–‚â–‚â–ƒâ–ƒâ–…â–‚â–‚
wandb:               Epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          Train loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: Converted test loss 0.517
wandb:               Epoch 29
wandb:          Train loss 1.39449
wandb: 
wandb: ğŸš€ View run clear-planet-984 at: https://wandb.ai/nreints/thesis/runs/kbrbvvtj
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230119_164220-kbrbvvtj/logs
/gpfs/home2/nreints/MScThesis/code/fcnn.py:341: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  train_sims = set(random.sample(sims, int(0.8 * n_sims)))
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.8
wandb: Run data is saved locally in /gpfs/home2/nreints/MScThesis/code/wandb/run-20230119_164736-u6cpx67w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-glitter-985
wandb: â­ï¸ View project at https://wandb.ai/nreints/thesis
wandb: ğŸš€ View run at https://wandb.ai/nreints/thesis/runs/u6cpx67w
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 2094409 ON gcn37 CANCELLED AT 2023-01-19T16:48:58 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 2094409.2 ON gcn37 CANCELLED AT 2023-01-19T16:48:58 DUE TO TIME LIMIT ***

JOB STATISTICS
==============
Job ID: 2094409
Cluster: snellius
User/Group: nreints/nreints
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:08:45
CPU Efficiency: 6.33% of 18:05:24 core-walltime
Job Wall-clock time: 01:00:18
Memory Utilized: 5.31 GB
Memory Efficiency: 16.98% of 31.25 GB
